{"../../block/block_raking_layout.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_F0775968F31E37F9\n#define _JITIFY_INCLUDE_GUARD_F0775968F31E37F9\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.\n * \n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n * \n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * cub::BlockRakingLayout provides a conflict-free shared memory layout abstraction for warp-raking across thread block data.\n */\n\n\n#include \"../config.cuh\"\n#include \"../util_type.cuh\"\n\nCUB_NAMESPACE_BEGIN\n\n/**\n * \\brief BlockRakingLayout provides a conflict-free shared memory layout abstraction for 1D raking across thread block data.    ![](raking.png)\n * \\ingroup BlockModule\n *\n * \\par Overview\n * This type facilitates a shared memory usage pattern where a block of CUDA\n * threads places elements into shared memory and then reduces the active\n * parallelism to one \"raking\" warp of threads for serially aggregating consecutive\n * sequences of shared items.  Padding is inserted to eliminate bank conflicts\n * (for most data types).\n *\n * \\tparam T                        The data type to be exchanged.\n * \\tparam BLOCK_THREADS            The thread block size in threads.\n * \\tparam LEGACY_PTX_ARCH          <b>[optional]</b> Unused.\n */\ntemplate <\n    typename    T,\n    int         BLOCK_THREADS,\n    int         LEGACY_PTX_ARCH = 0>\nstruct BlockRakingLayout\n{\n    //---------------------------------------------------------------------\n    // Constants and type definitions\n    //---------------------------------------------------------------------\n\n    enum\n    {\n        /// The total number of elements that need to be cooperatively reduced\n        SHARED_ELEMENTS = BLOCK_THREADS,\n\n        /// Maximum number of warp-synchronous raking threads\n        MAX_RAKING_THREADS = CUB_MIN(BLOCK_THREADS, CUB_WARP_THREADS(0)),\n\n        /// Number of raking elements per warp-synchronous raking thread (rounded up)\n        SEGMENT_LENGTH = (SHARED_ELEMENTS + MAX_RAKING_THREADS - 1) / MAX_RAKING_THREADS,\n\n        /// Never use a raking thread that will have no valid data (e.g., when BLOCK_THREADS is 62 and SEGMENT_LENGTH is 2, we should only use 31 raking threads)\n        RAKING_THREADS = (SHARED_ELEMENTS + SEGMENT_LENGTH - 1) / SEGMENT_LENGTH,\n\n        /// Whether we will have bank conflicts (technically we should find out if the GCD is > 1)\n        HAS_CONFLICTS = (CUB_SMEM_BANKS(0) % SEGMENT_LENGTH == 0),\n\n        /// Degree of bank conflicts (e.g., 4-way)\n        CONFLICT_DEGREE = (HAS_CONFLICTS) ?\n            (MAX_RAKING_THREADS * SEGMENT_LENGTH) / CUB_SMEM_BANKS(0) :\n            1,\n\n        /// Pad each segment length with one element if segment length is not relatively prime to warp size and can't be optimized as a vector load\n        USE_SEGMENT_PADDING = ((SEGMENT_LENGTH & 1) == 0) && (SEGMENT_LENGTH > 2),\n\n        /// Total number of elements in the raking grid\n        GRID_ELEMENTS = RAKING_THREADS * (SEGMENT_LENGTH + USE_SEGMENT_PADDING),\n\n        /// Whether or not we need bounds checking during raking (the number of reduction elements is not a multiple of the number of raking threads)\n        UNGUARDED = (SHARED_ELEMENTS % RAKING_THREADS == 0),\n    };\n\n\n    /**\n     * \\brief Shared memory storage type\n     */\n    struct __align__(16) _TempStorage\n    {\n        T buff[BlockRakingLayout::GRID_ELEMENTS];\n    };\n\n    /// Alias wrapper allowing storage to be unioned\n    struct TempStorage : Uninitialized<_TempStorage> {};\n\n\n    /**\n     * \\brief Returns the location for the calling thread to place data into the grid\n     */\n    static __device__ __forceinline__ T* PlacementPtr(\n        TempStorage &temp_storage,\n        unsigned int linear_tid)\n    {\n        // Offset for partial\n        unsigned int offset = linear_tid;\n\n        // Add in one padding element for every segment\n        if (USE_SEGMENT_PADDING > 0)\n        {\n            offset += offset / SEGMENT_LENGTH;\n        }\n\n        // Incorporating a block of padding partials every shared memory segment\n        return temp_storage.Alias().buff + offset;\n    }\n\n\n    /**\n     * \\brief Returns the location for the calling thread to begin sequential raking\n     */\n    static __device__ __forceinline__ T* RakingPtr(\n        TempStorage &temp_storage,\n        unsigned int linear_tid)\n    {\n        return temp_storage.Alias().buff + (linear_tid * (SEGMENT_LENGTH + USE_SEGMENT_PADDING));\n    }\n};\n\nCUB_NAMESPACE_END\n\n\n#endif // _JITIFY_INCLUDE_GUARD_F0775968F31E37F9\n", "../../config.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_1D666048A344D3BF\n#define _JITIFY_INCLUDE_GUARD_1D666048A344D3BF\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * Static configuration header for the CUB project.\n */\n\n#include \"util_arch.cuh\"\n#include \"util_compiler.cuh\"\n#include \"util_cpp_dialect.cuh\"\n#include \"util_deprecated.cuh\"\n#include \"util_macro.cuh\"\n#include \"util_namespace.cuh\"\n\n#endif // _JITIFY_INCLUDE_GUARD_1D666048A344D3BF\n", "../../thread/thread_load.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_F6B33027D22A94A3\n#define _JITIFY_INCLUDE_GUARD_F6B33027D22A94A3\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * Thread utilities for reading memory using PTX cache modifiers.\n */\n\n#include <iterator>\n\n#include \"../config.cuh\"\n#include \"../util_ptx.cuh\"\n#include \"../util_type.cuh\"\n\nCUB_NAMESPACE_BEGIN\n\n/**\n * \\addtogroup UtilIo\n * @{\n */\n\n//-----------------------------------------------------------------------------\n// Tags and constants\n//-----------------------------------------------------------------------------\n\n/**\n * \\brief Enumeration of cache modifiers for memory load operations.\n */\nenum CacheLoadModifier\n{\n    LOAD_DEFAULT,       ///< Default (no modifier)\n    LOAD_CA,            ///< Cache at all levels\n    LOAD_CG,            ///< Cache at global level\n    LOAD_CS,            ///< Cache streaming (likely to be accessed once)\n    LOAD_CV,            ///< Cache as volatile (including cached system lines)\n    LOAD_LDG,           ///< Cache as texture\n    LOAD_VOLATILE,      ///< Volatile (any memory space)\n};\n\n\n/**\n * \\name Thread I/O (cache modified)\n * @{\n */\n\n/**\n * \\brief Thread utility for reading memory using cub::CacheLoadModifier cache modifiers.  Can be used to load any data type.\n *\n * \\par Example\n * \\code\n * #include <cub/cub.cuh>   // or equivalently <cub/thread/thread_load.cuh>\n *\n * // 32-bit load using cache-global modifier:\n * int *d_in;\n * int val = cub::ThreadLoad<cub::LOAD_CA>(d_in + threadIdx.x);\n *\n * // 16-bit load using default modifier\n * short *d_in;\n * short val = cub::ThreadLoad<cub::LOAD_DEFAULT>(d_in + threadIdx.x);\n *\n * // 256-bit load using cache-volatile modifier\n * double4 *d_in;\n * double4 val = cub::ThreadLoad<cub::LOAD_CV>(d_in + threadIdx.x);\n *\n * // 96-bit load using cache-streaming modifier\n * struct TestFoo { bool a; short b; };\n * TestFoo *d_struct;\n * TestFoo val = cub::ThreadLoad<cub::LOAD_CS>(d_in + threadIdx.x);\n * \\endcode\n *\n * \\tparam MODIFIER             <b>[inferred]</b> CacheLoadModifier enumeration\n * \\tparam InputIteratorT       <b>[inferred]</b> Input iterator type \\iterator\n */\ntemplate <CacheLoadModifier MODIFIER,\n          typename InputIteratorT>\n__device__ __forceinline__ cub::detail::value_t<InputIteratorT>\nThreadLoad(InputIteratorT itr);\n\n//@}  end member group\n\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS    // Do not document\n\n\n/// Helper structure for templated load iteration (inductive case)\ntemplate <int COUNT, int MAX>\nstruct IterateThreadLoad\n{\n    template <CacheLoadModifier MODIFIER, typename T>\n    static __device__ __forceinline__ void Load(T const *ptr, T *vals)\n    {\n        vals[COUNT] = ThreadLoad<MODIFIER>(ptr + COUNT);\n        IterateThreadLoad<COUNT + 1, MAX>::template Load<MODIFIER>(ptr, vals);\n    }\n\n    template <typename InputIteratorT, typename T>\n    static __device__ __forceinline__ void Dereference(InputIteratorT itr, T *vals)\n    {\n        vals[COUNT] = itr[COUNT];\n        IterateThreadLoad<COUNT + 1, MAX>::Dereference(itr, vals);\n    }\n};\n\n\n/// Helper structure for templated load iteration (termination case)\ntemplate <int MAX>\nstruct IterateThreadLoad<MAX, MAX>\n{\n    template <CacheLoadModifier MODIFIER, typename T>\n    static __device__ __forceinline__ void Load(T const * /*ptr*/, T * /*vals*/) {}\n\n    template <typename InputIteratorT, typename T>\n    static __device__ __forceinline__ void Dereference(InputIteratorT /*itr*/, T * /*vals*/) {}\n};\n\n\n/**\n * Define a uint4 (16B) ThreadLoad specialization for the given Cache load modifier\n */\n#define _CUB_LOAD_16(cub_modifier, ptx_modifier)                                             \\\n    template<>                                                                              \\\n    __device__ __forceinline__ uint4 ThreadLoad<cub_modifier, uint4 const *>(uint4 const *ptr)                   \\\n    {                                                                                       \\\n        uint4 retval;                                                                       \\\n        asm volatile (\"ld.\"#ptx_modifier\".v4.u32 {%0, %1, %2, %3}, [%4];\" :                 \\\n            \"=r\"(retval.x),                                                                 \\\n            \"=r\"(retval.y),                                                                 \\\n            \"=r\"(retval.z),                                                                 \\\n            \"=r\"(retval.w) :                                                                \\\n            _CUB_ASM_PTR_(ptr));                                                            \\\n        return retval;                                                                      \\\n    }                                                                                       \\\n    template<>                                                                              \\\n    __device__ __forceinline__ ulonglong2 ThreadLoad<cub_modifier, ulonglong2 const *>(ulonglong2 const *ptr)    \\\n    {                                                                                       \\\n        ulonglong2 retval;                                                                  \\\n        asm volatile (\"ld.\"#ptx_modifier\".v2.u64 {%0, %1}, [%2];\" :                         \\\n            \"=l\"(retval.x),                                                                 \\\n            \"=l\"(retval.y) :                                                                \\\n            _CUB_ASM_PTR_(ptr));                                                            \\\n        return retval;                                                                      \\\n    }\n\n/**\n * Define a uint2 (8B) ThreadLoad specialization for the given Cache load modifier\n */\n#define _CUB_LOAD_8(cub_modifier, ptx_modifier)                                              \\\n    template<>                                                                              \\\n    __device__ __forceinline__ ushort4 ThreadLoad<cub_modifier, ushort4 const *>(ushort4 const *ptr)             \\\n    {                                                                                       \\\n        ushort4 retval;                                                                     \\\n        asm volatile (\"ld.\"#ptx_modifier\".v4.u16 {%0, %1, %2, %3}, [%4];\" :                 \\\n            \"=h\"(retval.x),                                                                 \\\n            \"=h\"(retval.y),                                                                 \\\n            \"=h\"(retval.z),                                                                 \\\n            \"=h\"(retval.w) :                                                                \\\n            _CUB_ASM_PTR_(ptr));                                                            \\\n        return retval;                                                                      \\\n    }                                                                                       \\\n    template<>                                                                              \\\n    __device__ __forceinline__ uint2 ThreadLoad<cub_modifier, uint2 const *>(uint2 const *ptr)                   \\\n    {                                                                                       \\\n        uint2 retval;                                                                       \\\n        asm volatile (\"ld.\"#ptx_modifier\".v2.u32 {%0, %1}, [%2];\" :                         \\\n            \"=r\"(retval.x),                                                                 \\\n            \"=r\"(retval.y) :                                                                \\\n            _CUB_ASM_PTR_(ptr));                                                            \\\n        return retval;                                                                      \\\n    }                                                                                       \\\n    template<>                                                                              \\\n    __device__ __forceinline__ unsigned long long ThreadLoad<cub_modifier, unsigned long long const *>(unsigned long long const *ptr)    \\\n    {                                                                                       \\\n        unsigned long long retval;                                                          \\\n        asm volatile (\"ld.\"#ptx_modifier\".u64 %0, [%1];\" :                                  \\\n            \"=l\"(retval) :                                                                  \\\n            _CUB_ASM_PTR_(ptr));                                                            \\\n        return retval;                                                                      \\\n    }\n\n/**\n * Define a uint (4B) ThreadLoad specialization for the given Cache load modifier\n */\n#define _CUB_LOAD_4(cub_modifier, ptx_modifier)                                              \\\n    template<>                                                                              \\\n    __device__ __forceinline__ unsigned int ThreadLoad<cub_modifier, unsigned int const *>(unsigned int const *ptr)                      \\\n    {                                                                                       \\\n        unsigned int retval;                                                                \\\n        asm volatile (\"ld.\"#ptx_modifier\".u32 %0, [%1];\" :                                  \\\n            \"=r\"(retval) :                                                                  \\\n            _CUB_ASM_PTR_(ptr));                                                            \\\n        return retval;                                                                      \\\n    }\n\n\n/**\n * Define a unsigned short (2B) ThreadLoad specialization for the given Cache load modifier\n */\n#define _CUB_LOAD_2(cub_modifier, ptx_modifier)                                              \\\n    template<>                                                                              \\\n    __device__ __forceinline__ unsigned short ThreadLoad<cub_modifier, unsigned short const *>(unsigned short const *ptr)                \\\n    {                                                                                       \\\n        unsigned short retval;                                                              \\\n        asm volatile (\"ld.\"#ptx_modifier\".u16 %0, [%1];\" :                                  \\\n            \"=h\"(retval) :                                                                  \\\n            _CUB_ASM_PTR_(ptr));                                                            \\\n        return retval;                                                                      \\\n    }\n\n\n/**\n * Define an unsigned char (1B) ThreadLoad specialization for the given Cache load modifier\n */\n#define _CUB_LOAD_1(cub_modifier, ptx_modifier)                                              \\\n    template<>                                                                              \\\n    __device__ __forceinline__ unsigned char ThreadLoad<cub_modifier, unsigned char const *>(unsigned char const *ptr)                   \\\n    {                                                                                       \\\n        unsigned short retval;                                                              \\\n        asm volatile (                                                                      \\\n        \"{\"                                                                                 \\\n        \"   .reg .u8 datum;\"                                                                \\\n        \"    ld.\"#ptx_modifier\".u8 datum, [%1];\"                                            \\\n        \"    cvt.u16.u8 %0, datum;\"                                                         \\\n        \"}\" :                                                                               \\\n            \"=h\"(retval) :                                                                  \\\n            _CUB_ASM_PTR_(ptr));                                                            \\\n        return (unsigned char) retval;                                                      \\\n    }\n\n\n/**\n * Define powers-of-two ThreadLoad specializations for the given Cache load modifier\n */\n#define _CUB_LOAD_ALL(cub_modifier, ptx_modifier)                                            \\\n    _CUB_LOAD_16(cub_modifier, ptx_modifier)                                                 \\\n    _CUB_LOAD_8(cub_modifier, ptx_modifier)                                                  \\\n    _CUB_LOAD_4(cub_modifier, ptx_modifier)                                                  \\\n    _CUB_LOAD_2(cub_modifier, ptx_modifier)                                                  \\\n    _CUB_LOAD_1(cub_modifier, ptx_modifier)                                                  \\\n\n\n/**\n * Define powers-of-two ThreadLoad specializations for the various Cache load modifiers\n */\n_CUB_LOAD_ALL(LOAD_CA, ca)\n_CUB_LOAD_ALL(LOAD_CG, cg)\n_CUB_LOAD_ALL(LOAD_CS, cs)\n_CUB_LOAD_ALL(LOAD_CV, cv)\n_CUB_LOAD_ALL(LOAD_LDG, global.nc)\n\n\n// Macro cleanup\n#undef _CUB_LOAD_ALL\n#undef _CUB_LOAD_1\n#undef _CUB_LOAD_2\n#undef _CUB_LOAD_4\n#undef _CUB_LOAD_8\n#undef _CUB_LOAD_16\n\n\n\n/**\n * ThreadLoad definition for LOAD_DEFAULT modifier on iterator types\n */\ntemplate <typename InputIteratorT>\n__device__ __forceinline__ cub::detail::value_t<InputIteratorT>\nThreadLoad(InputIteratorT          itr,\n           Int2Type<LOAD_DEFAULT>  /*modifier*/,\n           Int2Type<false>         /*is_pointer*/)\n{\n    return *itr;\n}\n\n\n/**\n * ThreadLoad definition for LOAD_DEFAULT modifier on pointer types\n */\ntemplate <typename T>\n__device__ __forceinline__ T ThreadLoad(\n    T                       *ptr,\n    Int2Type<LOAD_DEFAULT>  /*modifier*/,\n    Int2Type<true>          /*is_pointer*/)\n{\n    return *ptr;\n}\n\n\n/**\n * ThreadLoad definition for LOAD_VOLATILE modifier on primitive pointer types\n */\ntemplate <typename T>\n__device__ __forceinline__ T ThreadLoadVolatilePointer(\n    T                       *ptr,\n    Int2Type<true>          /*is_primitive*/)\n{\n    T retval = *reinterpret_cast<volatile T*>(ptr);\n    return retval;\n}\n\n\n/**\n * ThreadLoad definition for LOAD_VOLATILE modifier on non-primitive pointer types\n */\ntemplate <typename T>\n__device__ __forceinline__ T ThreadLoadVolatilePointer(\n    T                       *ptr,\n    Int2Type<false>         /*is_primitive*/)\n{\n    typedef typename UnitWord<T>::VolatileWord VolatileWord;   // Word type for memcopying\n\n    const int VOLATILE_MULTIPLE = sizeof(T) / sizeof(VolatileWord);\n\n    T retval;\n    VolatileWord *words = reinterpret_cast<VolatileWord*>(&retval);\n    IterateThreadLoad<0, VOLATILE_MULTIPLE>::Dereference(\n        reinterpret_cast<volatile VolatileWord*>(ptr),\n        words);\n    return retval;\n}\n\n\n/**\n * ThreadLoad definition for LOAD_VOLATILE modifier on pointer types\n */\ntemplate <typename T>\n__device__ __forceinline__ T ThreadLoad(\n    T                       *ptr,\n    Int2Type<LOAD_VOLATILE> /*modifier*/,\n    Int2Type<true>          /*is_pointer*/)\n{\n    // Apply tags for partial-specialization\n    return ThreadLoadVolatilePointer(ptr, Int2Type<Traits<T>::PRIMITIVE>());\n}\n\n\n/**\n * ThreadLoad definition for generic modifiers on pointer types\n */\ntemplate <typename T, int MODIFIER>\n__device__ __forceinline__ T ThreadLoad(\n    T const                 *ptr,\n    Int2Type<MODIFIER>      /*modifier*/,\n    Int2Type<true>          /*is_pointer*/)\n{\n    typedef typename UnitWord<T>::DeviceWord DeviceWord;\n\n    const int DEVICE_MULTIPLE = sizeof(T) / sizeof(DeviceWord);\n\n    DeviceWord words[DEVICE_MULTIPLE];\n\n    IterateThreadLoad<0, DEVICE_MULTIPLE>::template Load<CacheLoadModifier(MODIFIER)>(\n        reinterpret_cast<DeviceWord*>(const_cast<T*>(ptr)),\n        words);\n\n    return *reinterpret_cast<T*>(words);\n}\n\n\n/**\n * ThreadLoad definition for generic modifiers\n */\ntemplate <\n    CacheLoadModifier MODIFIER,\n    typename InputIteratorT>\n__device__ __forceinline__ cub::detail::value_t<InputIteratorT>\nThreadLoad(InputIteratorT itr)\n{\n    // Apply tags for partial-specialization\n    return ThreadLoad(\n        itr,\n        Int2Type<MODIFIER>(),\n        Int2Type<std::is_pointer<InputIteratorT>::value>());\n}\n\n\n\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\n\n/** @} */       // end group UtilIo\n\n\nCUB_NAMESPACE_END\n\n#endif // _JITIFY_INCLUDE_GUARD_F6B33027D22A94A3\n", "../../thread/thread_operators.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_94E67823631CB18F\n#define _JITIFY_INCLUDE_GUARD_94E67823631CB18F\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2022, NVIDIA CORPORATION.  All rights reserved.\n * \n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n * \n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" \n * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE \n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE \n * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * @file\n * Simple binary operator functor types\n */\n\n/******************************************************************************\n * Simple functor operators\n ******************************************************************************/\n\n#include <cub/config.cuh>\n#include <cub/util_cpp_dialect.cuh>\n#include <cub/util_type.cuh>\n\n#include <cuda/std/functional>\n#include <cuda/std/type_traits>\n#include <cuda/std/utility>\n\nCUB_NAMESPACE_BEGIN\n\n\n/**\n * @addtogroup UtilModule\n * @{\n */\n\n/// @brief Inequality functor (wraps equality functor)\ntemplate <typename EqualityOp>\nstruct InequalityWrapper\n{\n  /// Wrapped equality operator\n  EqualityOp op;\n\n  /// Constructor\n  __host__ __device__ __forceinline__ InequalityWrapper(EqualityOp op)\n      : op(op)\n  {}\n\n  /// Boolean inequality operator, returns `t != u`\n  template <typename T, typename U>\n  __host__ __device__ __forceinline__ bool operator()(T &&t, U &&u)\n  {\n    return !op(::cuda::std::forward<T>(t), ::cuda::std::forward<U>(u));\n  }\n};\n\n#if CUB_CPP_DIALECT > 2011\nusing Equality = ::cuda::std::equal_to<>;\nusing Inequality = ::cuda::std::not_equal_to<>;\nusing Sum = ::cuda::std::plus<>;\nusing Difference = ::cuda::std::minus<>;\nusing Division = ::cuda::std::divides<>;\n#else\n/// @brief Default equality functor\nstruct Equality\n{\n  /// Boolean equality operator, returns `t == u`\n  template <typename T, typename U>\n  __host__ __device__ __forceinline__ bool operator()(T &&t, U &&u) const\n  {\n    return ::cuda::std::forward<T>(t) == ::cuda::std::forward<U>(u);\n  }\n};\n\n/// @brief Default inequality functor\nstruct Inequality\n{\n  /// Boolean inequality operator, returns `t != u`\n  template <typename T, typename U>\n  __host__ __device__ __forceinline__ bool operator()(T &&t, U &&u) const\n  {\n    return ::cuda::std::forward<T>(t) != ::cuda::std::forward<U>(u);\n  }\n};\n\n/// @brief Default sum functor\nstruct Sum\n{\n  /// Binary sum operator, returns `t + u`\n  template <typename T, typename U>\n  __host__ __device__ __forceinline__ auto operator()(T &&t, U &&u) const\n    -> decltype(::cuda::std::forward<T>(t) + ::cuda::std::forward<U>(u))\n  {\n    return ::cuda::std::forward<T>(t) + ::cuda::std::forward<U>(u);\n  }\n};\n\n/// @brief Default difference functor\nstruct Difference\n{\n  /// Binary difference operator, returns `t - u`\n  template <typename T, typename U>\n  __host__ __device__ __forceinline__ auto operator()(T &&t, U &&u) const\n    -> decltype(::cuda::std::forward<T>(t) - ::cuda::std::forward<U>(u))\n  {\n    return ::cuda::std::forward<T>(t) - ::cuda::std::forward<U>(u);\n  }\n};\n\n/// @brief Default division functor\nstruct Division\n{\n  /// Binary division operator, returns `t / u`\n  template <typename T, typename U>\n  __host__ __device__ __forceinline__ auto operator()(T &&t, U &&u) const\n    -> decltype(::cuda::std::forward<T>(t) / ::cuda::std::forward<U>(u))\n  {\n    return ::cuda::std::forward<T>(t) / ::cuda::std::forward<U>(u);\n  }\n};\n#endif\n\n/// @brief Default max functor\nstruct Max\n{\n  /// Boolean max operator, returns `(t > u) ? t : u`\n  template <typename T, typename U>\n  __host__ __device__ __forceinline__\n    typename ::cuda::std::common_type<T, U>::type\n    operator()(T &&t, U &&u) const\n  {\n    return CUB_MAX(t, u);\n  }\n};\n\n/// @brief Arg max functor (keeps the value and offset of the first occurrence\n///        of the larger item)\nstruct ArgMax\n{\n  /// Boolean max operator, preferring the item having the smaller offset in\n  /// case of ties\n  template <typename T, typename OffsetT>\n  __host__ __device__ __forceinline__ KeyValuePair<OffsetT, T>\n  operator()(const KeyValuePair<OffsetT, T> &a,\n             const KeyValuePair<OffsetT, T> &b) const\n  {\n    // Mooch BUG (device reduce argmax gk110 3.2 million random fp32)\n    // return ((b.value > a.value) || \n    //         ((a.value == b.value) && (b.key < a.key))) \n    //      ? b : a;\n\n    if ((b.value > a.value) || ((a.value == b.value) && (b.key < a.key)))\n    {\n      return b;\n    }\n\n    return a;\n  }\n};\n\n/// @brief Default min functor\nstruct Min\n{\n  /// Boolean min operator, returns `(t < u) ? t : u`\n  template <typename T, typename U>\n  __host__ __device__ __forceinline__\n    typename ::cuda::std::common_type<T, U>::type\n    operator()(T &&t, U &&u) const\n  {\n    return CUB_MIN(t, u);\n  }\n};\n\n/// @brief Arg min functor (keeps the value and offset of the first occurrence\n///        of the smallest item)\nstruct ArgMin\n{\n  /// Boolean min operator, preferring the item having the smaller offset in\n  /// case of ties\n  template <typename T, typename OffsetT>\n  __host__ __device__ __forceinline__ KeyValuePair<OffsetT, T>\n  operator()(const KeyValuePair<OffsetT, T> &a,\n             const KeyValuePair<OffsetT, T> &b) const\n  {\n    // Mooch BUG (device reduce argmax gk110 3.2 million random fp32)\n    // return ((b.value < a.value) ||\n    //         ((a.value == b.value) && (b.key < a.key)))\n    //      ? b : a;\n\n    if ((b.value < a.value) || ((a.value == b.value) && (b.key < a.key)))\n    {\n      return b;\n    }\n\n    return a;\n  }\n};\n\nnamespace detail\n{\ntemplate <class OpT>\nstruct basic_binary_op_t\n{\n  static constexpr bool value = false;\n};\n\ntemplate <>\nstruct basic_binary_op_t<Sum>\n{\n  static constexpr bool value = true;\n};\n\ntemplate <>\nstruct basic_binary_op_t<Min>\n{\n  static constexpr bool value = true;\n};\n\ntemplate <>\nstruct basic_binary_op_t<Max>\n{\n  static constexpr bool value = true;\n};\n} // namespace detail\n\n/// @brief Default cast functor\ntemplate <typename B>\nstruct CastOp\n{\n  /// Cast operator, returns `(B) a`\n  template <typename A>\n  __host__ __device__ __forceinline__ B operator()(A &&a) const\n  {\n    return (B)a;\n  }\n};\n\n/// @brief Binary operator wrapper for switching non-commutative scan arguments\ntemplate <typename ScanOp>\nclass SwizzleScanOp\n{\nprivate:\n  /// Wrapped scan operator\n  ScanOp scan_op;\n\npublic:\n  /// Constructor\n  __host__ __device__ __forceinline__ SwizzleScanOp(ScanOp scan_op)\n      : scan_op(scan_op)\n  {}\n\n  /// Switch the scan arguments\n  template <typename T>\n  __host__ __device__ __forceinline__ T operator()(const T &a, const T &b)\n  {\n    T _a(a);\n    T _b(b);\n\n    return scan_op(_b, _a);\n  }\n};\n\n/**\n * @brief Reduce-by-segment functor.\n *\n * Given two cub::KeyValuePair inputs `a` and `b` and a binary associative \n * combining operator `f(const T &x, const T &y)`, an instance of this functor \n * returns a cub::KeyValuePair whose `key` field is `a.key + b.key`, and whose \n * `value` field is either `b.value` if `b.key` is non-zero, or \n * `f(a.value, b.value)` otherwise.\n *\n * ReduceBySegmentOp is an associative, non-commutative binary combining \n * operator for input sequences of cub::KeyValuePair pairings. Such sequences \n * are typically used to represent a segmented set of values to be reduced\n * and a corresponding set of {0,1}-valued integer \"head flags\" demarcating the\n * first value of each segment.\n *\n * @tparam ReductionOpT Binary reduction operator to apply to values\n */\ntemplate <typename ReductionOpT>\nstruct ReduceBySegmentOp\n{\n  /// Wrapped reduction operator\n  ReductionOpT op;\n\n  /// Constructor\n  __host__ __device__ __forceinline__ ReduceBySegmentOp() {}\n\n  /// Constructor\n  __host__ __device__ __forceinline__ ReduceBySegmentOp(ReductionOpT op)\n      : op(op)\n  {}\n\n  /**\n   * @brief Scan operator\n   *\n   * @tparam KeyValuePairT\n   *   KeyValuePair pairing of T (value) and OffsetT (head flag)\n   *\n   * @param[in] first\n   *   First partial reduction\n   *\n   * @param[in] second\n   *   Second partial reduction\n   */\n  template <typename KeyValuePairT>\n  __host__ __device__ __forceinline__ KeyValuePairT\n  operator()(const KeyValuePairT &first, const KeyValuePairT &second)\n  {\n    KeyValuePairT retval;\n    retval.key = first.key + second.key;\n#ifdef _NVHPC_CUDA // WAR bug on nvc++\n    if (second.key)\n    {\n      retval.value = second.value;\n    }\n    else\n    {\n      // If second.value isn't copied into a temporary here, nvc++ will\n      // crash while compiling the TestScanByKeyWithLargeTypes test in\n      // thrust/testing/scan_by_key.cu:\n      auto v2      = second.value;\n      retval.value = op(first.value, v2);\n    }\n#else // not nvc++:\n    // if (second.key) {\n    //   The second partial reduction spans a segment reset, so it's value\n    //   aggregate becomes the running aggregate\n    // else {\n    //   The second partial reduction does not span a reset, so accumulate both\n    //   into the running aggregate\n    // } \n    retval.value = (second.key) ? second.value : op(first.value, second.value);\n#endif\n    return retval;\n  }\n};\n\n/**\n * @tparam ReductionOpT Binary reduction operator to apply to values\n */\ntemplate <typename ReductionOpT>\nstruct ReduceByKeyOp\n{\n  /// Wrapped reduction operator\n  ReductionOpT op;\n\n  /// Constructor\n  __host__ __device__ __forceinline__ ReduceByKeyOp() {}\n\n  /// Constructor\n  __host__ __device__ __forceinline__ ReduceByKeyOp(ReductionOpT op)\n      : op(op)\n  {}\n\n  /**\n   * @brief Scan operator\n   *\n   * @param[in] first First partial reduction\n   * @param[in] second Second partial reduction\n   */\n  template <typename KeyValuePairT>\n  __host__ __device__ __forceinline__ KeyValuePairT\n  operator()(const KeyValuePairT &first, const KeyValuePairT &second)\n  {\n    KeyValuePairT retval = second;\n\n    if (first.key == second.key)\n    {\n      retval.value = op(first.value, retval.value);\n    }\n\n    return retval;\n  }\n};\n\ntemplate <typename BinaryOpT>\nstruct BinaryFlip\n{\n  BinaryOpT binary_op;\n\n  __device__ __host__ explicit BinaryFlip(BinaryOpT binary_op)\n      : binary_op(binary_op)\n  {}\n\n  template <typename T, typename U>\n  __device__ auto\n  operator()(T &&t, U &&u) -> decltype(binary_op(::cuda::std::forward<U>(u),\n                                                 ::cuda::std::forward<T>(t)))\n  {\n    return binary_op(::cuda::std::forward<U>(u), ::cuda::std::forward<T>(t));\n  }\n};\n\ntemplate <typename BinaryOpT>\n__device__ __host__ BinaryFlip<BinaryOpT> MakeBinaryFlip(BinaryOpT binary_op)\n{\n  return BinaryFlip<BinaryOpT>(binary_op);\n}\n\n/** @} */       // end group UtilModule\n\n\nCUB_NAMESPACE_END\n\n#endif // _JITIFY_INCLUDE_GUARD_94E67823631CB18F\n", "../../thread/thread_reduce.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_46E54281E645C665\n#define _JITIFY_INCLUDE_GUARD_46E54281E645C665\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.\n * \n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n * \n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * Thread utilities for sequential reduction over statically-sized array types\n */\n\n#include \"../thread/thread_operators.cuh\"\n#include \"../detail/type_traits.cuh\"\n#include \"../config.cuh\"\n\nCUB_NAMESPACE_BEGIN\n\n/// Internal namespace (to prevent ADL mishaps between static functions when mixing different CUB installations)\nnamespace internal {\n\n/**\n * Sequential reduction over statically-sized array types\n */\ntemplate <\n    int         LENGTH,\n    typename    T,\n    typename    ReductionOp,\n    typename    PrefixT,\n    typename    AccumT = detail::accumulator_t<ReductionOp, PrefixT, T>> \n__device__ __forceinline__ AccumT ThreadReduce(\n    T*                  input,                  ///< [in] Input array\n    ReductionOp         reduction_op,           ///< [in] Binary reduction operator\n    PrefixT             prefix,                 ///< [in] Prefix to seed reduction with\n    Int2Type<LENGTH>    /*length*/)\n{\n    AccumT retval = prefix;\n\n    _Pragma(\"unroll\")\n    for (int i = 0; i < LENGTH; ++i)\n        retval = reduction_op(retval, input[i]);\n\n    return retval;\n}\n\n\n/**\n * \\brief Perform a sequential reduction over \\p LENGTH elements of the \\p input array, seeded with the specified \\p prefix.  The aggregate is returned.\n *\n * \\tparam LENGTH       LengthT of input array\n * \\tparam T            <b>[inferred]</b> The data type to be reduced.\n * \\tparam ReductionOp  <b>[inferred]</b> Binary reduction operator type having member <tt>T operator()(const T &a, const T &b)</tt>\n */\ntemplate <\n    int         LENGTH,\n    typename    T,\n    typename    ReductionOp,\n    typename    PrefixT,\n    typename    AccumT = detail::accumulator_t<ReductionOp, PrefixT, T>> \n__device__ __forceinline__ AccumT ThreadReduce(\n    T*          input,                  ///< [in] Input array\n    ReductionOp reduction_op,           ///< [in] Binary reduction operator\n    PrefixT     prefix)                 ///< [in] Prefix to seed reduction with\n{\n    return ThreadReduce(input, reduction_op, prefix, Int2Type<LENGTH>());\n}\n\n\n/**\n * \\brief Perform a sequential reduction over \\p LENGTH elements of the \\p input array.  The aggregate is returned.\n *\n * \\tparam LENGTH       LengthT of input array\n * \\tparam T            <b>[inferred]</b> The data type to be reduced.\n * \\tparam ReductionOp  <b>[inferred]</b> Binary reduction operator type having member <tt>T operator()(const T &a, const T &b)</tt>\n */\ntemplate <\n    int         LENGTH,\n    typename    T,\n    typename    ReductionOp>\n__device__ __forceinline__ T ThreadReduce(\n    T*          input,                  ///< [in] Input array\n    ReductionOp reduction_op)           ///< [in] Binary reduction operator\n{\n    T prefix = input[0];\n    return ThreadReduce<LENGTH - 1>(input + 1, reduction_op, prefix);\n}\n\n\n/**\n * \\brief Perform a sequential reduction over the statically-sized \\p input array, seeded with the specified \\p prefix.  The aggregate is returned.\n *\n * \\tparam LENGTH       <b>[inferred]</b> LengthT of \\p input array\n * \\tparam T            <b>[inferred]</b> The data type to be reduced.\n * \\tparam ReductionOp  <b>[inferred]</b> Binary reduction operator type having member <tt>T operator()(const T &a, const T &b)</tt>\n */\ntemplate <\n    int         LENGTH,\n    typename    T,\n    typename    ReductionOp,\n    typename    PrefixT,\n    typename    AccumT = detail::accumulator_t<ReductionOp, PrefixT, T>> \n__device__ __forceinline__ AccumT ThreadReduce(\n    T           (&input)[LENGTH],       ///< [in] Input array\n    ReductionOp reduction_op,           ///< [in] Binary reduction operator\n    PrefixT     prefix)                 ///< [in] Prefix to seed reduction with\n{\n    return ThreadReduce(input, reduction_op, prefix, Int2Type<LENGTH>());\n}\n\n\n/**\n * \\brief Serial reduction with the specified operator\n *\n * \\tparam LENGTH       <b>[inferred]</b> LengthT of \\p input array\n * \\tparam T            <b>[inferred]</b> The data type to be reduced.\n * \\tparam ReductionOp  <b>[inferred]</b> Binary reduction operator type having member <tt>T operator()(const T &a, const T &b)</tt>\n */\ntemplate <\n    int         LENGTH,\n    typename    T,\n    typename    ReductionOp>\n__device__ __forceinline__ T ThreadReduce(\n    T           (&input)[LENGTH],       ///< [in] Input array\n    ReductionOp reduction_op)           ///< [in] Binary reduction operator\n{\n    return ThreadReduce<LENGTH>((T*) input, reduction_op);\n}\n\n\n}               // internal namespace\nCUB_NAMESPACE_END\n\n#endif // _JITIFY_INCLUDE_GUARD_46E54281E645C665\n", "../../thread/thread_store.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_C77D6E8CBB92E407\n#define _JITIFY_INCLUDE_GUARD_C77D6E8CBB92E407\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * Thread utilities for writing memory using PTX cache modifiers.\n */\n\n#include <cub/config.cuh>\n#include <cub/util_ptx.cuh>\n#include <cub/util_type.cuh>\n\nCUB_NAMESPACE_BEGIN\n\n/**\n * \\addtogroup UtilIo\n * @{\n */\n\n\n//-----------------------------------------------------------------------------\n// Tags and constants\n//-----------------------------------------------------------------------------\n\n/**\n * \\brief Enumeration of cache modifiers for memory store operations.\n */\nenum CacheStoreModifier\n{\n    STORE_DEFAULT,              ///< Default (no modifier)\n    STORE_WB,                   ///< Cache write-back all coherent levels\n    STORE_CG,                   ///< Cache at global level\n    STORE_CS,                   ///< Cache streaming (likely to be accessed once)\n    STORE_WT,                   ///< Cache write-through (to system memory)\n    STORE_VOLATILE,             ///< Volatile shared (any memory space)\n};\n\n\n/**\n * \\name Thread I/O (cache modified)\n * @{\n */\n\n/**\n * \\brief Thread utility for writing memory using cub::CacheStoreModifier cache modifiers.  Can be used to store any data type.\n *\n * \\par Example\n * \\code\n * #include <cub/cub.cuh>   // or equivalently <cub/thread/thread_store.cuh>\n *\n * // 32-bit store using cache-global modifier:\n * int *d_out;\n * int val;\n * cub::ThreadStore<cub::STORE_CG>(d_out + threadIdx.x, val);\n *\n * // 16-bit store using default modifier\n * short *d_out;\n * short val;\n * cub::ThreadStore<cub::STORE_DEFAULT>(d_out + threadIdx.x, val);\n *\n * // 256-bit store using write-through modifier\n * double4 *d_out;\n * double4 val;\n * cub::ThreadStore<cub::STORE_WT>(d_out + threadIdx.x, val);\n *\n * // 96-bit store using cache-streaming cache modifier\n * struct TestFoo { bool a; short b; };\n * TestFoo *d_struct;\n * TestFoo val;\n * cub::ThreadStore<cub::STORE_CS>(d_out + threadIdx.x, val);\n * \\endcode\n *\n * \\tparam MODIFIER             <b>[inferred]</b> CacheStoreModifier enumeration\n * \\tparam InputIteratorT       <b>[inferred]</b> Output iterator type \\iterator\n * \\tparam T                    <b>[inferred]</b> Data type of output value\n */\ntemplate <\n    CacheStoreModifier  MODIFIER,\n    typename            OutputIteratorT,\n    typename            T>\n__device__ __forceinline__ void ThreadStore(OutputIteratorT itr, T val);\n\n\n//@}  end member group\n\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS    // Do not document\n\n\n/// Helper structure for templated store iteration (inductive case)\ntemplate <int COUNT, int MAX>\nstruct IterateThreadStore\n{\n    template <CacheStoreModifier MODIFIER, typename T>\n    static __device__ __forceinline__ void Store(T *ptr, T *vals)\n    {\n        ThreadStore<MODIFIER>(ptr + COUNT, vals[COUNT]);\n        IterateThreadStore<COUNT + 1, MAX>::template Store<MODIFIER>(ptr, vals);\n    }\n\n    template <typename OutputIteratorT, typename T>\n    static __device__ __forceinline__ void Dereference(OutputIteratorT ptr, T *vals)\n    {\n        ptr[COUNT] = vals[COUNT];\n        IterateThreadStore<COUNT + 1, MAX>::Dereference(ptr, vals);\n    }\n\n};\n\n/// Helper structure for templated store iteration (termination case)\ntemplate <int MAX>\nstruct IterateThreadStore<MAX, MAX>\n{\n    template <CacheStoreModifier MODIFIER, typename T>\n    static __device__ __forceinline__ void Store(T * /*ptr*/, T * /*vals*/) {}\n\n    template <typename OutputIteratorT, typename T>\n    static __device__ __forceinline__ void Dereference(OutputIteratorT /*ptr*/, T * /*vals*/) {}\n};\n\n\n/**\n * Define a uint4 (16B) ThreadStore specialization for the given Cache load modifier\n */\n#define _CUB_STORE_16(cub_modifier, ptx_modifier)                                            \\\n    template<>                                                                              \\\n    __device__ __forceinline__ void ThreadStore<cub_modifier, uint4*, uint4>(uint4* ptr, uint4 val)                         \\\n    {                                                                                       \\\n        asm volatile (\"st.\"#ptx_modifier\".v4.u32 [%0], {%1, %2, %3, %4};\" : :               \\\n            _CUB_ASM_PTR_(ptr),                                                             \\\n            \"r\"(val.x),                                                                     \\\n            \"r\"(val.y),                                                                     \\\n            \"r\"(val.z),                                                                     \\\n            \"r\"(val.w));                                                                    \\\n    }                                                                                       \\\n    template<>                                                                              \\\n    __device__ __forceinline__ void ThreadStore<cub_modifier, ulonglong2*, ulonglong2>(ulonglong2* ptr, ulonglong2 val)     \\\n    {                                                                                       \\\n        asm volatile (\"st.\"#ptx_modifier\".v2.u64 [%0], {%1, %2};\" : :                       \\\n            _CUB_ASM_PTR_(ptr),                                                             \\\n            \"l\"(val.x),                                                                     \\\n            \"l\"(val.y));                                                                    \\\n    }\n\n\n/**\n * Define a uint2 (8B) ThreadStore specialization for the given Cache load modifier\n */\n#define _CUB_STORE_8(cub_modifier, ptx_modifier)                                             \\\n    template<>                                                                              \\\n    __device__ __forceinline__ void ThreadStore<cub_modifier, ushort4*, ushort4>(ushort4* ptr, ushort4 val)                 \\\n    {                                                                                       \\\n        asm volatile (\"st.\"#ptx_modifier\".v4.u16 [%0], {%1, %2, %3, %4};\" : :               \\\n            _CUB_ASM_PTR_(ptr),                                                             \\\n            \"h\"(val.x),                                                                     \\\n            \"h\"(val.y),                                                                     \\\n            \"h\"(val.z),                                                                     \\\n            \"h\"(val.w));                                                                    \\\n    }                                                                                       \\\n    template<>                                                                              \\\n    __device__ __forceinline__ void ThreadStore<cub_modifier, uint2*, uint2>(uint2* ptr, uint2 val)                         \\\n    {                                                                                       \\\n        asm volatile (\"st.\"#ptx_modifier\".v2.u32 [%0], {%1, %2};\" : :                       \\\n            _CUB_ASM_PTR_(ptr),                                                             \\\n            \"r\"(val.x),                                                                     \\\n            \"r\"(val.y));                                                                    \\\n    }                                                                                       \\\n    template<>                                                                              \\\n    __device__ __forceinline__ void ThreadStore<cub_modifier, unsigned long long*, unsigned long long>(unsigned long long* ptr, unsigned long long val)     \\\n    {                                                                                       \\\n        asm volatile (\"st.\"#ptx_modifier\".u64 [%0], %1;\" : :                                \\\n            _CUB_ASM_PTR_(ptr),                                                             \\\n            \"l\"(val));                                                                      \\\n    }\n\n/**\n * Define a unsigned int (4B) ThreadStore specialization for the given Cache load modifier\n */\n#define _CUB_STORE_4(cub_modifier, ptx_modifier)                                             \\\n    template<>                                                                              \\\n    __device__ __forceinline__ void ThreadStore<cub_modifier, unsigned int*, unsigned int>(unsigned int* ptr, unsigned int val)                             \\\n    {                                                                                       \\\n        asm volatile (\"st.\"#ptx_modifier\".u32 [%0], %1;\" : :                                \\\n            _CUB_ASM_PTR_(ptr),                                                             \\\n            \"r\"(val));                                                                      \\\n    }\n\n\n/**\n * Define a unsigned short (2B) ThreadStore specialization for the given Cache load modifier\n */\n#define _CUB_STORE_2(cub_modifier, ptx_modifier)                                             \\\n    template<>                                                                              \\\n    __device__ __forceinline__ void ThreadStore<cub_modifier, unsigned short*, unsigned short>(unsigned short* ptr, unsigned short val)                     \\\n    {                                                                                       \\\n        asm volatile (\"st.\"#ptx_modifier\".u16 [%0], %1;\" : :                                \\\n            _CUB_ASM_PTR_(ptr),                                                             \\\n            \"h\"(val));                                                                      \\\n    }\n\n\n/**\n * Define a unsigned char (1B) ThreadStore specialization for the given Cache load modifier\n */\n#define _CUB_STORE_1(cub_modifier, ptx_modifier)                                             \\\n    template<>                                                                              \\\n    __device__ __forceinline__ void ThreadStore<cub_modifier, unsigned char*, unsigned char>(unsigned char* ptr, unsigned char val)                         \\\n    {                                                                                       \\\n        asm volatile (                                                                      \\\n        \"{\"                                                                                 \\\n        \"   .reg .u8 datum;\"                                                                \\\n        \"   cvt.u8.u16 datum, %1;\"                                                          \\\n        \"   st.\"#ptx_modifier\".u8 [%0], datum;\"                                             \\\n        \"}\" : :                                                                             \\\n            _CUB_ASM_PTR_(ptr),                                                             \\\n            \"h\"((unsigned short) val));                                                               \\\n    }\n\n/**\n * Define powers-of-two ThreadStore specializations for the given Cache load modifier\n */\n#define _CUB_STORE_ALL(cub_modifier, ptx_modifier)                                           \\\n    _CUB_STORE_16(cub_modifier, ptx_modifier)                                                \\\n    _CUB_STORE_8(cub_modifier, ptx_modifier)                                                 \\\n    _CUB_STORE_4(cub_modifier, ptx_modifier)                                                 \\\n    _CUB_STORE_2(cub_modifier, ptx_modifier)                                                 \\\n    _CUB_STORE_1(cub_modifier, ptx_modifier)                                                 \\\n\n\n/**\n * Define ThreadStore specializations for the various Cache load modifiers\n */\n_CUB_STORE_ALL(STORE_WB, wb)\n_CUB_STORE_ALL(STORE_CG, cg)\n_CUB_STORE_ALL(STORE_CS, cs)\n_CUB_STORE_ALL(STORE_WT, wt)\n\n// Macro cleanup\n#undef _CUB_STORE_ALL\n#undef _CUB_STORE_1\n#undef _CUB_STORE_2\n#undef _CUB_STORE_4\n#undef _CUB_STORE_8\n#undef _CUB_STORE_16\n\n\n/**\n * ThreadStore definition for STORE_DEFAULT modifier on iterator types\n */\ntemplate <typename OutputIteratorT, typename T>\n__device__ __forceinline__ void ThreadStore(\n    OutputIteratorT             itr,\n    T                           val,\n    Int2Type<STORE_DEFAULT>     /*modifier*/,\n    Int2Type<false>             /*is_pointer*/)\n{\n    *itr = val;\n}\n\n\n/**\n * ThreadStore definition for STORE_DEFAULT modifier on pointer types\n */\ntemplate <typename T>\n__device__ __forceinline__ void ThreadStore(\n    T                           *ptr,\n    T                           val,\n    Int2Type<STORE_DEFAULT>     /*modifier*/,\n    Int2Type<true>              /*is_pointer*/)\n{\n    *ptr = val;\n}\n\n\n/**\n * ThreadStore definition for STORE_VOLATILE modifier on primitive pointer types\n */\ntemplate <typename T>\n__device__ __forceinline__ void ThreadStoreVolatilePtr(\n    T                           *ptr,\n    T                           val,\n    Int2Type<true>              /*is_primitive*/)\n{\n    *reinterpret_cast<volatile T*>(ptr) = val;\n}\n\n\n/**\n * ThreadStore definition for STORE_VOLATILE modifier on non-primitive pointer types\n */\ntemplate <typename T>\n__device__ __forceinline__ void ThreadStoreVolatilePtr(\n    T                           *ptr,\n    T                           val,\n    Int2Type<false>             /*is_primitive*/)\n{\n    // Create a temporary using shuffle-words, then store using volatile-words\n    typedef typename UnitWord<T>::VolatileWord  VolatileWord;\n    typedef typename UnitWord<T>::ShuffleWord   ShuffleWord;\n\n    const int VOLATILE_MULTIPLE = sizeof(T) / sizeof(VolatileWord);\n    const int SHUFFLE_MULTIPLE  = sizeof(T) / sizeof(ShuffleWord);\n\n    VolatileWord words[VOLATILE_MULTIPLE];\n\n    _Pragma(\"unroll\")\n    for (int i = 0; i < SHUFFLE_MULTIPLE; ++i)\n        reinterpret_cast<ShuffleWord*>(words)[i] = reinterpret_cast<ShuffleWord*>(&val)[i];\n\n    IterateThreadStore<0, VOLATILE_MULTIPLE>::template Dereference(\n        reinterpret_cast<volatile VolatileWord*>(ptr),\n        words);\n}\n\n\n/**\n * ThreadStore definition for STORE_VOLATILE modifier on pointer types\n */\ntemplate <typename T>\n__device__ __forceinline__ void ThreadStore(\n    T                           *ptr,\n    T                           val,\n    Int2Type<STORE_VOLATILE>    /*modifier*/,\n    Int2Type<true>              /*is_pointer*/)\n{\n    ThreadStoreVolatilePtr(ptr, val, Int2Type<Traits<T>::PRIMITIVE>());\n}\n\n\n/**\n * ThreadStore definition for generic modifiers on pointer types\n */\ntemplate <typename T, int MODIFIER>\n__device__ __forceinline__ void ThreadStore(\n    T                           *ptr,\n    T                           val,\n    Int2Type<MODIFIER>          /*modifier*/,\n    Int2Type<true>              /*is_pointer*/)\n{\n    // Create a temporary using shuffle-words, then store using device-words\n    typedef typename UnitWord<T>::DeviceWord    DeviceWord;\n    typedef typename UnitWord<T>::ShuffleWord   ShuffleWord;\n\n    const int DEVICE_MULTIPLE   = sizeof(T) / sizeof(DeviceWord);\n    const int SHUFFLE_MULTIPLE  = sizeof(T) / sizeof(ShuffleWord);\n\n    DeviceWord words[DEVICE_MULTIPLE];\n\n    _Pragma(\"unroll\")\n    for (int i = 0; i < SHUFFLE_MULTIPLE; ++i)\n        reinterpret_cast<ShuffleWord*>(words)[i] = reinterpret_cast<ShuffleWord*>(&val)[i];\n\n    IterateThreadStore<0, DEVICE_MULTIPLE>::template Store<CacheStoreModifier(MODIFIER)>(\n        reinterpret_cast<DeviceWord*>(ptr),\n        words);\n}\n\n\n/**\n * ThreadStore definition for generic modifiers\n */\ntemplate <CacheStoreModifier MODIFIER, typename OutputIteratorT, typename T>\n__device__ __forceinline__ void ThreadStore(OutputIteratorT itr, T val)\n{\n    ThreadStore(\n        itr,\n        val,\n        Int2Type<MODIFIER>(),\n        Int2Type<std::is_pointer<OutputIteratorT>::value>());\n}\n\n\n\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\n\n/** @} */       // end group UtilIo\n\n\nCUB_NAMESPACE_END\n\n#endif // _JITIFY_INCLUDE_GUARD_C77D6E8CBB92E407\n", "../../util_ptx.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_8EAE810BB36A0FF0\n#define _JITIFY_INCLUDE_GUARD_8EAE810BB36A0FF0\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.\n * \n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n * \n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * PTX intrinsics\n */\n\n\n#include \"util_type.cuh\"\n#include \"util_arch.cuh\"\n#include \"util_namespace.cuh\"\n#include \"util_debug.cuh\"\n\n\nCUB_NAMESPACE_BEGIN\n\n\n/**\n * \\addtogroup UtilPtx\n * @{\n */\n\n\n/******************************************************************************\n * PTX helper macros\n ******************************************************************************/\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS    // Do not document\n\n/**\n * Register modifier for pointer-types (for inlining PTX assembly)\n */\n#if defined(_WIN64) || defined(__LP64__)\n    #define __CUB_LP64__ 1\n    // 64-bit register modifier for inlined asm\n    #define _CUB_ASM_PTR_ \"l\"\n    #define _CUB_ASM_PTR_SIZE_ \"u64\"\n#else\n    #define __CUB_LP64__ 0\n    // 32-bit register modifier for inlined asm\n    #define _CUB_ASM_PTR_ \"r\"\n    #define _CUB_ASM_PTR_SIZE_ \"u32\"\n#endif\n\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\n\n/******************************************************************************\n * Inlined PTX intrinsics\n ******************************************************************************/\n\nnamespace detail\n{\n/**\n * @brief Shifts @p val left by the amount specified by unsigned 32-bit value in @p num_bits. If @p\n * num_bits is larger than 32 bits, @p num_bits is clamped to 32.\n */\n__device__ __forceinline__ uint32_t LogicShiftLeft(uint32_t val, uint32_t num_bits)\n{\n  uint32_t ret{};\n  asm(\"shl.b32 %0, %1, %2;\" : \"=r\"(ret) : \"r\"(val), \"r\"(num_bits));\n  return ret;\n}\n\n/**\n * @brief Shifts @p val right by the amount specified by unsigned 32-bit value in @p num_bits. If @p\n * num_bits is larger than 32 bits, @p num_bits is clamped to 32.\n */\n__device__ __forceinline__ uint32_t LogicShiftRight(uint32_t val, uint32_t num_bits)\n{\n  uint32_t ret{};\n  asm(\"shr.b32 %0, %1, %2;\" : \"=r\"(ret) : \"r\"(val), \"r\"(num_bits));\n  return ret;\n}\n} // namespace detail\n\n/**\n * \\brief Shift-right then add.  Returns (\\p x >> \\p shift) + \\p addend.\n */\n__device__ __forceinline__ unsigned int SHR_ADD(\n    unsigned int x,\n    unsigned int shift,\n    unsigned int addend)\n{\n    unsigned int ret;\n    asm (\"vshr.u32.u32.u32.clamp.add %0, %1, %2, %3;\" :\n        \"=r\"(ret) : \"r\"(x), \"r\"(shift), \"r\"(addend));\n    return ret;\n}\n\n\n/**\n * \\brief Shift-left then add.  Returns (\\p x << \\p shift) + \\p addend.\n */\n__device__ __forceinline__ unsigned int SHL_ADD(\n    unsigned int x,\n    unsigned int shift,\n    unsigned int addend)\n{\n    unsigned int ret;\n    asm (\"vshl.u32.u32.u32.clamp.add %0, %1, %2, %3;\" :\n        \"=r\"(ret) : \"r\"(x), \"r\"(shift), \"r\"(addend));\n    return ret;\n}\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS    // Do not document\n\n/**\n * Bitfield-extract.\n */\ntemplate <typename UnsignedBits, int BYTE_LEN>\n__device__ __forceinline__ unsigned int BFE(\n    UnsignedBits            source,\n    unsigned int            bit_start,\n    unsigned int            num_bits,\n    Int2Type<BYTE_LEN>      /*byte_len*/)\n{\n    unsigned int bits;\n    asm (\"bfe.u32 %0, %1, %2, %3;\" : \"=r\"(bits) : \"r\"((unsigned int) source), \"r\"(bit_start), \"r\"(num_bits));\n    return bits;\n}\n\n\n/**\n * Bitfield-extract for 64-bit types.\n */\ntemplate <typename UnsignedBits>\n__device__ __forceinline__ unsigned int BFE(\n    UnsignedBits            source,\n    unsigned int            bit_start,\n    unsigned int            num_bits,\n    Int2Type<8>             /*byte_len*/)\n{\n    const unsigned long long MASK = (1ull << num_bits) - 1;\n    return (source >> bit_start) & MASK;\n}\n\n#if CUB_IS_INT128_ENABLED \n/**\n * Bitfield-extract for 128-bit types.\n */\ntemplate <typename UnsignedBits>\n__device__ __forceinline__ unsigned int BFE(\n    UnsignedBits            source,\n    unsigned int            bit_start,\n    unsigned int            num_bits,\n    Int2Type<16>            /*byte_len*/)\n{\n    const __uint128_t MASK = (__uint128_t{1} << num_bits) - 1;\n    return (source >> bit_start) & MASK;\n}\n#endif\n\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\n/**\n * \\brief Bitfield-extract.  Extracts \\p num_bits from \\p source starting at bit-offset \\p bit_start.  The input \\p source may be an 8b, 16b, 32b, or 64b unsigned integer type.\n */\ntemplate <typename UnsignedBits>\n__device__ __forceinline__ unsigned int BFE(\n    UnsignedBits source,\n    unsigned int bit_start,\n    unsigned int num_bits)\n{\n    return BFE(source, bit_start, num_bits, Int2Type<sizeof(UnsignedBits)>());\n}\n\n\n/**\n * \\brief Bitfield insert.  Inserts the \\p num_bits least significant bits of \\p y into \\p x at bit-offset \\p bit_start.\n */\n__device__ __forceinline__ void BFI(\n    unsigned int &ret,\n    unsigned int x,\n    unsigned int y,\n    unsigned int bit_start,\n    unsigned int num_bits)\n{\n    asm (\"bfi.b32 %0, %1, %2, %3, %4;\" :\n        \"=r\"(ret) : \"r\"(y), \"r\"(x), \"r\"(bit_start), \"r\"(num_bits));\n}\n\n\n/**\n * \\brief Three-operand add.  Returns \\p x + \\p y + \\p z.\n */\n__device__ __forceinline__ unsigned int IADD3(unsigned int x, unsigned int y, unsigned int z)\n{\n    asm (\"vadd.u32.u32.u32.add %0, %1, %2, %3;\" : \"=r\"(x) : \"r\"(x), \"r\"(y), \"r\"(z));\n    return x;\n}\n\n\n/**\n * \\brief Byte-permute. Pick four arbitrary bytes from two 32-bit registers, and reassemble them into a 32-bit destination register.  For SM2.0 or later.\n *\n * \\par\n * The bytes in the two source registers \\p a and \\p b are numbered from 0 to 7:\n * {\\p b, \\p a} = {{b7, b6, b5, b4}, {b3, b2, b1, b0}}. For each of the four bytes\n * {b3, b2, b1, b0} selected in the return value, a 4-bit selector is defined within\n * the four lower \"nibbles\" of \\p index: {\\p index } = {n7, n6, n5, n4, n3, n2, n1, n0}\n *\n * \\par Snippet\n * The code snippet below illustrates byte-permute.\n * \\par\n * \\code\n * #include <cub/cub.cuh>\n *\n * __global__ void ExampleKernel(...)\n * {\n *     int a        = 0x03020100;\n *     int b        = 0x07060504;\n *     int index    = 0x00007531;\n *\n *     int selected = PRMT(a, b, index);    // 0x07050301\n *\n * \\endcode\n *\n */\n__device__ __forceinline__ int PRMT(unsigned int a, unsigned int b, unsigned int index)\n{\n    int ret;\n    asm (\"prmt.b32 %0, %1, %2, %3;\" : \"=r\"(ret) : \"r\"(a), \"r\"(b), \"r\"(index));\n    return ret;\n}\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS    // Do not document\n\n/**\n * Sync-threads barrier.\n */\n__device__ __forceinline__ void BAR(int count)\n{\n    asm volatile(\"bar.sync 1, %0;\" : : \"r\"(count));\n}\n\n/**\n * CTA barrier\n */\n__device__  __forceinline__ void CTA_SYNC()\n{\n    __syncthreads();\n}\n\n\n/**\n * CTA barrier with predicate\n */\n__device__  __forceinline__ int CTA_SYNC_AND(int p)\n{\n    return __syncthreads_and(p);\n}\n\n\n/**\n * CTA barrier with predicate\n */\n__device__  __forceinline__ int CTA_SYNC_OR(int p)\n{\n    return __syncthreads_or(p);\n}\n\n\n/**\n * Warp barrier\n */\n__device__  __forceinline__ void WARP_SYNC(unsigned int member_mask)\n{\n    __syncwarp(member_mask);\n}\n\n\n/**\n * Warp any\n */\n__device__  __forceinline__ int WARP_ANY(int predicate, unsigned int member_mask)\n{\n    return __any_sync(member_mask, predicate);\n}\n\n\n/**\n * Warp any\n */\n__device__  __forceinline__ int WARP_ALL(int predicate, unsigned int member_mask)\n{\n    return __all_sync(member_mask, predicate);\n}\n\n\n/**\n * Warp ballot\n */\n__device__  __forceinline__ int WARP_BALLOT(int predicate, unsigned int member_mask)\n{\n    return __ballot_sync(member_mask, predicate);\n}\n\n\n/**\n * Warp synchronous shfl_up\n */\n__device__ __forceinline__ \nunsigned int SHFL_UP_SYNC(unsigned int word, int src_offset, int flags, unsigned int member_mask)\n{\n    asm volatile(\"shfl.sync.up.b32 %0, %1, %2, %3, %4;\"\n        : \"=r\"(word) : \"r\"(word), \"r\"(src_offset), \"r\"(flags), \"r\"(member_mask));\n    return word;\n}\n\n/**\n * Warp synchronous shfl_down\n */\n__device__ __forceinline__ \nunsigned int SHFL_DOWN_SYNC(unsigned int word, int src_offset, int flags, unsigned int member_mask)\n{\n    asm volatile(\"shfl.sync.down.b32 %0, %1, %2, %3, %4;\"\n        : \"=r\"(word) : \"r\"(word), \"r\"(src_offset), \"r\"(flags), \"r\"(member_mask));\n    return word;\n}\n\n/**\n * Warp synchronous shfl_idx\n */\n__device__ __forceinline__ \nunsigned int SHFL_IDX_SYNC(unsigned int word, int src_lane, int flags, unsigned int member_mask)\n{\n    asm volatile(\"shfl.sync.idx.b32 %0, %1, %2, %3, %4;\"\n        : \"=r\"(word) : \"r\"(word), \"r\"(src_lane), \"r\"(flags), \"r\"(member_mask));\n    return word;\n}\n\n/**\n * Warp synchronous shfl_idx\n */\n__device__ __forceinline__ \nunsigned int SHFL_IDX_SYNC(unsigned int word, int src_lane, unsigned int member_mask)\n{\n    return __shfl_sync(member_mask, word, src_lane);\n}\n\n/**\n * Floating point multiply. (Mantissa LSB rounds towards zero.)\n */\n__device__ __forceinline__ float FMUL_RZ(float a, float b)\n{\n    float d;\n    asm (\"mul.rz.f32 %0, %1, %2;\" : \"=f\"(d) : \"f\"(a), \"f\"(b));\n    return d;\n}\n\n\n/**\n * Floating point multiply-add. (Mantissa LSB rounds towards zero.)\n */\n__device__ __forceinline__ float FFMA_RZ(float a, float b, float c)\n{\n    float d;\n    asm (\"fma.rz.f32 %0, %1, %2, %3;\" : \"=f\"(d) : \"f\"(a), \"f\"(b), \"f\"(c));\n    return d;\n}\n\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\n/**\n * \\brief Terminates the calling thread\n */\n__device__ __forceinline__ void ThreadExit() {\n    asm volatile(\"exit;\");\n}    \n\n\n/**\n * \\brief  Abort execution and generate an interrupt to the host CPU\n */\n__device__ __forceinline__ void ThreadTrap() {\n    asm volatile(\"trap;\");\n}\n\n\n/**\n * \\brief Returns the row-major linear thread identifier for a multidimensional thread block\n */\n__device__ __forceinline__ int RowMajorTid(int block_dim_x, int block_dim_y, int block_dim_z)\n{\n    return ((block_dim_z == 1) ? 0 : (threadIdx.z * block_dim_x * block_dim_y)) +\n            ((block_dim_y == 1) ? 0 : (threadIdx.y * block_dim_x)) +\n            threadIdx.x;\n}\n\n\n/**\n * \\brief Returns the warp lane ID of the calling thread\n */\n__device__ __forceinline__ unsigned int LaneId()\n{\n    unsigned int ret;\n    asm (\"mov.u32 %0, %%laneid;\" : \"=r\"(ret) );\n    return ret;\n}\n\n\n/**\n * \\brief Returns the warp ID of the calling thread.  Warp ID is guaranteed to be unique among warps, but may not correspond to a zero-based ranking within the thread block.\n */\n__device__ __forceinline__ unsigned int WarpId()\n{\n    unsigned int ret;\n    asm (\"mov.u32 %0, %%warpid;\" : \"=r\"(ret) );\n    return ret;\n}\n\n/**\n * @brief Returns the warp mask for a warp of @p LOGICAL_WARP_THREADS threads\n *\n * @par\n * If the number of threads assigned to the virtual warp is not a power of two,\n * it's assumed that only one virtual warp exists.\n *\n * @tparam LOGICAL_WARP_THREADS <b>[optional]</b> The number of threads per\n *                              \"logical\" warp (may be less than the number of\n *                              hardware warp threads).\n * @param warp_id Id of virtual warp within architectural warp\n */\ntemplate <int LOGICAL_WARP_THREADS, int LEGACY_PTX_ARCH = 0>\n__host__ __device__ __forceinline__\nunsigned int WarpMask(unsigned int warp_id)\n{\n  constexpr bool is_pow_of_two = PowerOfTwo<LOGICAL_WARP_THREADS>::VALUE;\n  constexpr bool is_arch_warp  = LOGICAL_WARP_THREADS == CUB_WARP_THREADS(0);\n\n  unsigned int member_mask = 0xFFFFFFFFu >>\n                             (CUB_WARP_THREADS(0) - LOGICAL_WARP_THREADS);\n\n  if (is_pow_of_two && !is_arch_warp)\n  {\n    member_mask <<= warp_id * LOGICAL_WARP_THREADS;\n  }\n\n  return member_mask;\n}\n\n/**\n * \\brief Returns the warp lane mask of all lanes less than the calling thread\n */\n__device__ __forceinline__ unsigned int LaneMaskLt()\n{\n    unsigned int ret;\n    asm (\"mov.u32 %0, %%lanemask_lt;\" : \"=r\"(ret) );\n    return ret;\n}\n\n/**\n * \\brief Returns the warp lane mask of all lanes less than or equal to the calling thread\n */\n__device__ __forceinline__ unsigned int LaneMaskLe()\n{\n    unsigned int ret;\n    asm (\"mov.u32 %0, %%lanemask_le;\" : \"=r\"(ret) );\n    return ret;\n}\n\n/**\n * \\brief Returns the warp lane mask of all lanes greater than the calling thread\n */\n__device__ __forceinline__ unsigned int LaneMaskGt()\n{\n    unsigned int ret;\n    asm (\"mov.u32 %0, %%lanemask_gt;\" : \"=r\"(ret) );\n    return ret;\n}\n\n/**\n * \\brief Returns the warp lane mask of all lanes greater than or equal to the calling thread\n */\n__device__ __forceinline__ unsigned int LaneMaskGe()\n{\n    unsigned int ret;\n    asm (\"mov.u32 %0, %%lanemask_ge;\" : \"=r\"(ret) );\n    return ret;\n}\n\n/** @} */       // end group UtilPtx\n\n\n\n\n/**\n * \\brief Shuffle-up for any data type.  Each <em>warp-lane<sub>i</sub></em> obtains the value \\p input contributed by <em>warp-lane</em><sub><em>i</em>-<tt>src_offset</tt></sub>.  For thread lanes \\e i < src_offset, the thread's own \\p input is returned to the thread. ![](shfl_up_logo.png)\n * \\ingroup WarpModule\n *\n * \\tparam LOGICAL_WARP_THREADS     The number of threads per \"logical\" warp.  Must be a power-of-two <= 32.\n * \\tparam T                        <b>[inferred]</b> The input/output element type\n *\n * \\par\n * - Available only for SM3.0 or newer\n *\n * \\par Snippet\n * The code snippet below illustrates each thread obtaining a \\p double value from the\n * predecessor of its predecessor.\n * \\par\n * \\code\n * #include <cub/cub.cuh>   // or equivalently <cub/util_ptx.cuh>\n *\n * __global__ void ExampleKernel(...)\n * {\n *     // Obtain one input item per thread\n *     double thread_data = ...\n *\n *     // Obtain item from two ranks below\n *     double peer_data = ShuffleUp<32>(thread_data, 2, 0, 0xffffffff);\n *\n * \\endcode\n * \\par\n * Suppose the set of input \\p thread_data across the first warp of threads is <tt>{1.0, 2.0, 3.0, 4.0, 5.0, ..., 32.0}</tt>.\n * The corresponding output \\p peer_data will be <tt>{1.0, 2.0, 1.0, 2.0, 3.0, ..., 30.0}</tt>.\n *\n */\ntemplate <\n    int LOGICAL_WARP_THREADS,   ///< Number of threads per logical warp\n    typename T>\n__device__ __forceinline__ T ShuffleUp(\n    T               input,              ///< [in] The value to broadcast\n    int             src_offset,         ///< [in] The relative down-offset of the peer to read from\n    int             first_thread,       ///< [in] Index of first lane in logical warp (typically 0)\n    unsigned int    member_mask)        ///< [in] 32-bit mask of participating warp lanes\n{\n    /// The 5-bit SHFL mask for logically splitting warps into sub-segments starts 8-bits up\n    enum {\n        SHFL_C = (32 - LOGICAL_WARP_THREADS) << 8\n    };\n\n    typedef typename UnitWord<T>::ShuffleWord ShuffleWord;\n\n    const int       WORDS           = (sizeof(T) + sizeof(ShuffleWord) - 1) / sizeof(ShuffleWord);\n \n    T               output;\n    ShuffleWord     *output_alias   = reinterpret_cast<ShuffleWord *>(&output);\n    ShuffleWord     *input_alias    = reinterpret_cast<ShuffleWord *>(&input);\n\n    unsigned int shuffle_word;\n    shuffle_word = SHFL_UP_SYNC((unsigned int)input_alias[0], src_offset, first_thread | SHFL_C, member_mask);\n    output_alias[0] = shuffle_word;\n\n    _Pragma(\"unroll\")\n    for (int WORD = 1; WORD < WORDS; ++WORD)\n    {\n        shuffle_word       = SHFL_UP_SYNC((unsigned int)input_alias[WORD], src_offset, first_thread | SHFL_C, member_mask);\n        output_alias[WORD] = shuffle_word;\n    }\n\n    return output;\n}\n\n\n/**\n * \\brief Shuffle-down for any data type.  Each <em>warp-lane<sub>i</sub></em> obtains the value \\p input contributed by <em>warp-lane</em><sub><em>i</em>+<tt>src_offset</tt></sub>.  For thread lanes \\e i >= WARP_THREADS, the thread's own \\p input is returned to the thread.  ![](shfl_down_logo.png)\n * \\ingroup WarpModule\n *\n * \\tparam LOGICAL_WARP_THREADS     The number of threads per \"logical\" warp.  Must be a power-of-two <= 32.\n * \\tparam T                        <b>[inferred]</b> The input/output element type\n *\n * \\par\n * - Available only for SM3.0 or newer\n *\n * \\par Snippet\n * The code snippet below illustrates each thread obtaining a \\p double value from the\n * successor of its successor.\n * \\par\n * \\code\n * #include <cub/cub.cuh>   // or equivalently <cub/util_ptx.cuh>\n *\n * __global__ void ExampleKernel(...)\n * {\n *     // Obtain one input item per thread\n *     double thread_data = ...\n *\n *     // Obtain item from two ranks below\n *     double peer_data = ShuffleDown<32>(thread_data, 2, 31, 0xffffffff);\n *\n * \\endcode\n * \\par\n * Suppose the set of input \\p thread_data across the first warp of threads is <tt>{1.0, 2.0, 3.0, 4.0, 5.0, ..., 32.0}</tt>.\n * The corresponding output \\p peer_data will be <tt>{3.0, 4.0, 5.0, 6.0, 7.0, ..., 32.0}</tt>.\n *\n */\ntemplate <\n    int LOGICAL_WARP_THREADS,   ///< Number of threads per logical warp\n    typename T>\n__device__ __forceinline__ T ShuffleDown(\n    T               input,              ///< [in] The value to broadcast\n    int             src_offset,         ///< [in] The relative up-offset of the peer to read from\n    int             last_thread,        ///< [in] Index of last thread in logical warp (typically 31 for a 32-thread warp)\n    unsigned int    member_mask)        ///< [in] 32-bit mask of participating warp lanes\n{\n    /// The 5-bit SHFL mask for logically splitting warps into sub-segments starts 8-bits up\n    enum {\n        SHFL_C = (32 - LOGICAL_WARP_THREADS) << 8\n    };\n\n    typedef typename UnitWord<T>::ShuffleWord ShuffleWord;\n\n    const int       WORDS           = (sizeof(T) + sizeof(ShuffleWord) - 1) / sizeof(ShuffleWord);\n\n    T               output;\n    ShuffleWord     *output_alias   = reinterpret_cast<ShuffleWord *>(&output);\n    ShuffleWord     *input_alias    = reinterpret_cast<ShuffleWord *>(&input);\n\n    unsigned int shuffle_word;\n    shuffle_word    = SHFL_DOWN_SYNC((unsigned int)input_alias[0], src_offset, last_thread | SHFL_C, member_mask);\n    output_alias[0] = shuffle_word;\n\n    _Pragma(\"unroll\")\n    for (int WORD = 1; WORD < WORDS; ++WORD)\n    {\n        shuffle_word       = SHFL_DOWN_SYNC((unsigned int)input_alias[WORD], src_offset, last_thread | SHFL_C, member_mask);\n        output_alias[WORD] = shuffle_word;\n    }\n\n    return output;\n}\n\n\n/**\n * \\brief Shuffle-broadcast for any data type.  Each <em>warp-lane<sub>i</sub></em> obtains the value \\p input\n * contributed by <em>warp-lane</em><sub><tt>src_lane</tt></sub>.  For \\p src_lane < 0 or \\p src_lane >= WARP_THREADS,\n * then the thread's own \\p input is returned to the thread. ![](shfl_broadcast_logo.png)\n *\n * \\tparam LOGICAL_WARP_THREADS     The number of threads per \"logical\" warp.  Must be a power-of-two <= 32.\n * \\tparam T                        <b>[inferred]</b> The input/output element type\n *\n * \\ingroup WarpModule\n *\n * \\par\n * - Available only for SM3.0 or newer\n *\n * \\par Snippet\n * The code snippet below illustrates each thread obtaining a \\p double value from <em>warp-lane</em><sub>0</sub>.\n *\n * \\par\n * \\code\n * #include <cub/cub.cuh>   // or equivalently <cub/util_ptx.cuh>\n *\n * __global__ void ExampleKernel(...)\n * {\n *     // Obtain one input item per thread\n *     double thread_data = ...\n *\n *     // Obtain item from thread 0\n *     double peer_data = ShuffleIndex<32>(thread_data, 0, 0xffffffff);\n *\n * \\endcode\n * \\par\n * Suppose the set of input \\p thread_data across the first warp of threads is <tt>{1.0, 2.0, 3.0, 4.0, 5.0, ..., 32.0}</tt>.\n * The corresponding output \\p peer_data will be <tt>{1.0, 1.0, 1.0, 1.0, 1.0, ..., 1.0}</tt>.\n *\n */\ntemplate <\n    int LOGICAL_WARP_THREADS,   ///< Number of threads per logical warp\n    typename T>\n__device__ __forceinline__ T ShuffleIndex(\n    T               input,                  ///< [in] The value to broadcast\n    int             src_lane,               ///< [in] Which warp lane is to do the broadcasting\n    unsigned int    member_mask)            ///< [in] 32-bit mask of participating warp lanes\n{\n    /// The 5-bit SHFL mask for logically splitting warps into sub-segments starts 8-bits up\n    enum {\n        SHFL_C = ((32 - LOGICAL_WARP_THREADS) << 8) | (LOGICAL_WARP_THREADS - 1)\n    };\n\n    typedef typename UnitWord<T>::ShuffleWord ShuffleWord;\n\n    const int       WORDS           = (sizeof(T) + sizeof(ShuffleWord) - 1) / sizeof(ShuffleWord);\n\n    T               output;\n    ShuffleWord     *output_alias   = reinterpret_cast<ShuffleWord *>(&output);\n    ShuffleWord     *input_alias    = reinterpret_cast<ShuffleWord *>(&input);\n\n    unsigned int shuffle_word;\n    shuffle_word = SHFL_IDX_SYNC((unsigned int)input_alias[0],\n                                 src_lane,\n                                 SHFL_C,\n                                 member_mask);\n\n    output_alias[0] = shuffle_word;\n\n    _Pragma(\"unroll\")\n    for (int WORD = 1; WORD < WORDS; ++WORD)\n    {\n        shuffle_word = SHFL_IDX_SYNC((unsigned int)input_alias[WORD],\n                                     src_lane,\n                                     SHFL_C,\n                                     member_mask);\n\n        output_alias[WORD] = shuffle_word;\n    }\n\n    return output;\n}\n\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS    // Do not document\nnamespace detail \n{\n\n/** \n * Implementation detail for `MatchAny`. It provides specializations for full and partial warps. \n * For partial warps, inactive threads must be masked out. This is done in the partial warp \n * specialization below. \n * Usage:\n * ```\n * // returns a mask of threads with the same 4 least-significant bits of `label` \n * // in a warp with 16 active threads\n * warp_matcher_t<4, 16>::match_any(label); \n *\n * // returns a mask of threads with the same 4 least-significant bits of `label` \n * // in a warp with 32 active threads (no extra work is done)\n * warp_matcher_t<4, 32>::match_any(label); \n * ```\n */\ntemplate <int LABEL_BITS, int WARP_ACTIVE_THREADS>\nstruct warp_matcher_t \n{\n\n  static __device__ unsigned int match_any(unsigned int label)\n  {\n    return warp_matcher_t<LABEL_BITS, 32>::match_any(label) & ~(~0 << WARP_ACTIVE_THREADS);\n  }\n\n};\n\ntemplate <int LABEL_BITS>\nstruct warp_matcher_t<LABEL_BITS, CUB_PTX_WARP_THREADS> \n{\n\n  // match.any.sync.b32 is slower when matching a few bits\n  // using a ballot loop instead\n  static __device__ unsigned int match_any(unsigned int label)\n  {\n      unsigned int retval;\n\n      // Extract masks of common threads for each bit\n      _Pragma(\"unroll\")\n      for (int BIT = 0; BIT < LABEL_BITS; ++BIT)\n      {\n          unsigned int mask;\n          unsigned int current_bit = 1 << BIT;\n          asm (\"{\\n\"\n              \"    .reg .pred p;\\n\"\n              \"    and.b32 %0, %1, %2;\"\n              \"    setp.eq.u32 p, %0, %2;\\n\"\n              \"    vote.ballot.sync.b32 %0, p, 0xffffffff;\\n\"\n              \"    @!p not.b32 %0, %0;\\n\"\n              \"}\\n\" : \"=r\"(mask) : \"r\"(label), \"r\"(current_bit));\n\n          // Remove peers who differ\n          retval = (BIT == 0) ? mask : retval & mask;\n      }\n\n      return retval;\n  }\n\n};\n\n} // namespace detail\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\n/**\n * Compute a 32b mask of threads having the same least-significant\n * LABEL_BITS of \\p label as the calling thread.\n */\ntemplate <int LABEL_BITS, int WARP_ACTIVE_THREADS = CUB_PTX_WARP_THREADS>\ninline __device__ unsigned int MatchAny(unsigned int label)\n{\n  return detail::warp_matcher_t<LABEL_BITS, WARP_ACTIVE_THREADS>::match_any(label);\n}\n\nCUB_NAMESPACE_END\n\n#endif // _JITIFY_INCLUDE_GUARD_8EAE810BB36A0FF0\n", "../../util_type.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_1375AABB974C3530\n#define _JITIFY_INCLUDE_GUARD_1375AABB974C3530\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.\n * \n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n * \n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * Common type manipulation (metaprogramming) utilities\n */\n\n#include <cfloat>\n#include <iostream>\n#include <iterator>\n#include <limits>\n\n#include <cuda.h>\n\n#if !_NVHPC_CUDA\n    #include <cuda_fp16.h>\n#endif\n#if !_NVHPC_CUDA && !defined(CUB_DISABLE_BF16_SUPPORT)\n    #include <cuda_bf16.h>\n#endif\n\n#include <cub/detail/uninitialized_copy.cuh>\n#include <cub/util_arch.cuh>\n#include <cub/util_compiler.cuh>\n#include <cub/util_deprecated.cuh>\n#include <cub/util_macro.cuh>\n#include <cub/util_namespace.cuh>\n\n#include <cuda/std/type_traits>\n\nCUB_NAMESPACE_BEGIN\n\n#ifndef CUB_IS_INT128_ENABLED\n#if defined(__CUDACC_RTC__)\n#if defined(__CUDACC_RTC_INT128__)\n#define CUB_IS_INT128_ENABLED 1\n#endif // !defined(__CUDACC_RTC_INT128__)\n#else  // !defined(__CUDACC_RTC__)\n#if CUDA_VERSION >= 11050\n#if (CUB_HOST_COMPILER == CUB_HOST_COMPILER_GCC) || \\\n    (CUB_HOST_COMPILER == CUB_HOST_COMPILER_CLANG) || \\\n    defined(__ICC) || defined(_NVHPC_CUDA)\n#define CUB_IS_INT128_ENABLED 1\n#endif // GCC || CLANG || ICC || NVHPC\n#endif // CTK >= 11.5\n#endif // !defined(__CUDACC_RTC__)\n#endif // !defined(CUB_IS_INT128_ENABLED)\n\n/**\n * \\addtogroup UtilModule\n * @{\n */\n\n\n\n/******************************************************************************\n * Conditional types\n ******************************************************************************/\n\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS // Do not document\nnamespace detail\n{\n\n\ntemplate <bool Test, class T1, class T2>\nusing conditional_t = typename std::conditional<Test, T1, T2>::type;\n\n\ntemplate <typename Iterator>\nusing value_t = typename std::iterator_traits<Iterator>::value_type;\n\ntemplate <typename It,\n          typename FallbackT,\n          bool = ::cuda::std::is_same<\n            typename ::cuda::std::remove_cv<typename ::cuda::std::remove_pointer<It>::type>::type,\n            void>::value>\nstruct non_void_value_impl\n{\n  using type = FallbackT;\n};\n\ntemplate <typename It, typename FallbackT>\nstruct non_void_value_impl<It, FallbackT, false>\n{\n  using type = typename ::cuda::std::conditional<\n    ::cuda::std::is_same<typename std::iterator_traits<It>::value_type, void>::value,\n    FallbackT,\n    typename std::iterator_traits<It>::value_type>::type;\n};\n\n/**\n * The output value type\n * type = (if IteratorT's value type is void) ?\n * ... then the FallbackT,\n * ... else the IteratorT's value type\n */\ntemplate <typename It, typename FallbackT>\nusing non_void_value_t = typename non_void_value_impl<It, FallbackT>::type;\n} // namespace detail\n\n\n/**\n * \\brief Type selection (<tt>IF ? ThenType : ElseType</tt>)\n *\n * \\deprecated [Since 1.16.0] The cub::If APIs are deprecated.\n *             Use cub::detail::conditional_t instead.\n */\ntemplate <bool IF, typename ThenType, typename ElseType>\nstruct CUB_DEPRECATED If\n{\n  using Type = cub::detail::conditional_t<IF, ThenType, ElseType>;\n};\n\n\n/******************************************************************************\n * Type equality\n ******************************************************************************/\n\n/**\n * \\brief Type equality test\n *\n * \\deprecated [Since 1.16.0] The cub::Equals APIs are deprecated.\n *             Use std::is_same instead.\n */\ntemplate <typename A, typename B>\nstruct CUB_DEPRECATED Equals\n{\n  static constexpr int VALUE = std::is_same<A, B>::value ? 1 : 0;\n  static constexpr int NEGATE = VALUE ? 0 : 1;\n};\n\n\n/******************************************************************************\n * Static math\n ******************************************************************************/\n\n/**\n * \\brief Statically determine log2(N), rounded up.\n *\n * For example:\n *     Log2<8>::VALUE   // 3\n *     Log2<3>::VALUE   // 2\n */\ntemplate <int N, int CURRENT_VAL = N, int COUNT = 0>\nstruct Log2\n{\n    /// Static logarithm value\n    enum { VALUE = Log2<N, (CURRENT_VAL >> 1), COUNT + 1>::VALUE };         // Inductive case\n};\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS    // Do not document\n\ntemplate <int N, int COUNT>\nstruct Log2<N, 0, COUNT>\n{\n    enum {VALUE = (1 << (COUNT - 1) < N) ?                                  // Base case\n        COUNT :\n        COUNT - 1 };\n};\n\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\n/**\n * \\brief Statically determine if N is a power-of-two\n */\ntemplate <int N>\nstruct PowerOfTwo\n{\n    enum { VALUE = ((N & (N - 1)) == 0) };\n};\n\n\n\n/******************************************************************************\n * Pointer vs. iterator detection\n ******************************************************************************/\n\n/**\n * \\brief Pointer vs. iterator\n *\n * \\deprecated [Since 1.16.0] The cub::IsPointer APIs are deprecated.\n *             Use std::is_pointer instead.\n */\ntemplate <typename Tp>\nstruct CUB_DEPRECATED IsPointer\n{\n  static constexpr int VALUE = std::is_pointer<Tp>::value;\n};\n\n\n/******************************************************************************\n * Qualifier detection\n ******************************************************************************/\n\n/**\n * \\brief Volatile modifier test\n *\n * \\deprecated [Since 1.16.0] The cub::IsVolatile APIs are deprecated.\n *             Use std::is_volatile instead.\n */\ntemplate <typename Tp>\nstruct CUB_DEPRECATED IsVolatile\n{\n  static constexpr int VALUE = std::is_volatile<Tp>::value;\n};\n\n/******************************************************************************\n * Qualifier removal\n ******************************************************************************/\n\n/**\n * \\brief Removes \\p const and \\p volatile qualifiers from type \\p Tp.\n *\n * \\deprecated [Since 1.16.0] The cub::RemoveQualifiers APIs are deprecated.\n *             Use std::remove_cv instead.\n *\n * For example:\n *     <tt>typename RemoveQualifiers<volatile int>::Type         // int;</tt>\n */\ntemplate <typename Tp, typename Up = Tp>\nstruct CUB_DEPRECATED RemoveQualifiers\n{\n  using Type = typename std::remove_cv<Tp>::type;\n};\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\n\n/******************************************************************************\n * Marker types\n ******************************************************************************/\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS    // Do not document\n\n/**\n * \\brief A simple \"NULL\" marker type\n */\nstruct NullType\n{\n    using value_type = NullType;\n\n    template <typename T>\n    __host__ __device__ __forceinline__ NullType& operator =(const T&) { return *this; }\n\n    __host__ __device__ __forceinline__ bool operator ==(const NullType&) { return true; }\n\n    __host__ __device__ __forceinline__ bool operator !=(const NullType&) { return false; }\n};\n\n\n/**\n * \\brief Allows for the treatment of an integral constant as a type at compile-time (e.g., to achieve static call dispatch based on constant integral values)\n */\ntemplate <int A>\nstruct Int2Type\n{\n    enum {VALUE = A};\n};\n\n/**\n * \\brief Allows algorithms that take a value as input to take a future value that is not computed yet at launch time.\n *\n * Note that it is user's responsibility to ensure that the result will be ready before use via external synchronization\n * or stream-ordering dependencies.\n *\n * \\code\n * int *d_intermediate_result;\n * allocator.DeviceAllocate((void **)&d_intermediate_result, sizeof(int));\n * compute_intermediate_result<<<blocks, threads>>>(\n *     d_intermediate_result,  // output\n *     arg1,                   // input\n *     arg2);                  // input\n * cub::FutureValue<int> init_value(d_intermediate_result);\n * cub::DeviceScan::ExclusiveScan(\n *     d_temp_storage,\n *     temp_storage_bytes,\n *     d_in,\n *     d_out,\n *     cub::Sum(),\n *     init_value,\n *     num_items);\n * allocator.DeviceFree(d_intermediate_result);\n * \\endcode\n */\ntemplate <typename T, typename IterT = T*>\nstruct FutureValue\n{\n    using value_type = T;\n    using iterator_type = IterT;\n    explicit __host__ __device__ __forceinline__ FutureValue(IterT iter):m_iter(iter) {}\n    __host__ __device__ __forceinline__ operator T() {\n        return *m_iter;\n    }\n\nprivate:\n    IterT m_iter;\n};\n\nnamespace detail {\n\n/**\n * \\brief Allows algorithms to instantiate a single kernel to support both immediate value and future value.\n */\ntemplate <typename T, typename IterT = T*>\nstruct InputValue\n{\n    using value_type = T;\n    using iterator_type = IterT;\n    __host__ __device__ __forceinline__ operator T() {\n        if (m_is_future) {\n            return m_future_value;\n        }\n        return m_immediate_value;\n    }\n    explicit __host__ __device__ __forceinline__ InputValue(T immediate_value): m_is_future(false), m_immediate_value(immediate_value) {}\n    explicit __host__ __device__ __forceinline__ InputValue(FutureValue<T, IterT> future_value): m_is_future(true), m_future_value(future_value) {}\n    __host__ __device__ __forceinline__ InputValue(const InputValue &other): m_is_future(other.m_is_future) {\n        if (m_is_future) {\n            m_future_value = other.m_future_value;\n        } else {\n          detail::uninitialized_copy(&m_immediate_value,\n                                     other.m_immediate_value);\n        }\n    }\n\nprivate:\n    bool m_is_future;\n    union\n    {\n        FutureValue<T, IterT> m_future_value;\n        T m_immediate_value;\n    };\n};\n\n} // namespace detail\n\n\n/******************************************************************************\n * Size and alignment\n ******************************************************************************/\n\n/// Structure alignment\ntemplate <typename T>\nstruct AlignBytes\n{\n    struct Pad\n    {\n        T       val;\n        char    byte;\n    };\n\n    enum\n    {\n        /// The \"true CUDA\" alignment of T in bytes\n        ALIGN_BYTES = sizeof(Pad) - sizeof(T)\n    };\n\n    /// The \"truly aligned\" type\n    typedef T Type;\n};\n\n// Specializations where host C++ compilers (e.g., 32-bit Windows) may disagree\n// with device C++ compilers (EDG) on types passed as template parameters through\n// kernel functions\n\n#define __CUB_ALIGN_BYTES(t, b)         \\\n    template <> struct AlignBytes<t>    \\\n    { enum { ALIGN_BYTES = b }; typedef __align__(b) t Type; };\n\n__CUB_ALIGN_BYTES(short4, 8)\n__CUB_ALIGN_BYTES(ushort4, 8)\n__CUB_ALIGN_BYTES(int2, 8)\n__CUB_ALIGN_BYTES(uint2, 8)\n__CUB_ALIGN_BYTES(long long, 8)\n__CUB_ALIGN_BYTES(unsigned long long, 8)\n__CUB_ALIGN_BYTES(float2, 8)\n__CUB_ALIGN_BYTES(double, 8)\n#ifdef _WIN32\n    __CUB_ALIGN_BYTES(long2, 8)\n    __CUB_ALIGN_BYTES(ulong2, 8)\n#else\n    __CUB_ALIGN_BYTES(long2, 16)\n    __CUB_ALIGN_BYTES(ulong2, 16)\n#endif\n__CUB_ALIGN_BYTES(int4, 16)\n__CUB_ALIGN_BYTES(uint4, 16)\n__CUB_ALIGN_BYTES(float4, 16)\n__CUB_ALIGN_BYTES(long4, 16)\n__CUB_ALIGN_BYTES(ulong4, 16)\n__CUB_ALIGN_BYTES(longlong2, 16)\n__CUB_ALIGN_BYTES(ulonglong2, 16)\n__CUB_ALIGN_BYTES(double2, 16)\n__CUB_ALIGN_BYTES(longlong4, 16)\n__CUB_ALIGN_BYTES(ulonglong4, 16)\n__CUB_ALIGN_BYTES(double4, 16)\n\n// clang-format off\ntemplate <typename T> struct AlignBytes<volatile T> : AlignBytes<T> {};\ntemplate <typename T> struct AlignBytes<const T> : AlignBytes<T> {};\ntemplate <typename T> struct AlignBytes<const volatile T> : AlignBytes<T> {};\n// clang-format on\n\n/// Unit-words of data movement\ntemplate <typename T>\nstruct UnitWord\n{\n    enum {\n        ALIGN_BYTES = AlignBytes<T>::ALIGN_BYTES\n    };\n\n    template <typename Unit>\n    struct IsMultiple\n    {\n        enum {\n            UNIT_ALIGN_BYTES    = AlignBytes<Unit>::ALIGN_BYTES,\n            IS_MULTIPLE         = (sizeof(T) % sizeof(Unit) == 0) && (int(ALIGN_BYTES) % int(UNIT_ALIGN_BYTES) == 0)\n        };\n    };\n\n    /// Biggest shuffle word that T is a whole multiple of and is not larger than\n    /// the alignment of T\n    using ShuffleWord = cub::detail::conditional_t<\n      IsMultiple<int>::IS_MULTIPLE,\n      unsigned int,\n      cub::detail::conditional_t<IsMultiple<short>::IS_MULTIPLE,\n                                 unsigned short,\n                                 unsigned char>>;\n\n    /// Biggest volatile word that T is a whole multiple of and is not larger than\n    /// the alignment of T\n    using VolatileWord =\n      cub::detail::conditional_t<IsMultiple<long long>::IS_MULTIPLE,\n                                 unsigned long long,\n                                 ShuffleWord>;\n\n    /// Biggest memory-access word that T is a whole multiple of and is not larger\n    /// than the alignment of T\n    using DeviceWord =\n      cub::detail::conditional_t<IsMultiple<longlong2>::IS_MULTIPLE,\n                                 ulonglong2,\n                                 VolatileWord>;\n\n    /// Biggest texture reference word that T is a whole multiple of and is not\n    /// larger than the alignment of T\n    using TextureWord = cub::detail::conditional_t<\n      IsMultiple<int4>::IS_MULTIPLE,\n      uint4,\n      cub::detail::conditional_t<IsMultiple<int2>::IS_MULTIPLE, uint2, ShuffleWord>>;\n};\n\n// float2 specialization workaround (for SM10-SM13)\ntemplate <>\nstruct UnitWord <float2>\n{\n    typedef int         ShuffleWord;\n    typedef unsigned long long   VolatileWord;\n    typedef unsigned long long   DeviceWord;\n    typedef float2      TextureWord;\n};\n\n// float4 specialization workaround (for SM10-SM13)\ntemplate <>\nstruct UnitWord <float4>\n{\n    typedef int         ShuffleWord;\n    typedef unsigned long long  VolatileWord;\n    typedef ulonglong2          DeviceWord;\n    typedef float4              TextureWord;\n};\n\n\n// char2 specialization workaround (for SM10-SM13)\ntemplate <>\nstruct UnitWord <char2>\n{\n    typedef unsigned short      ShuffleWord;\n    typedef unsigned short      VolatileWord;\n    typedef unsigned short      DeviceWord;\n    typedef unsigned short      TextureWord;\n};\n\n// clang-format off\ntemplate <typename T> struct UnitWord<volatile T> : UnitWord<T> {};\ntemplate <typename T> struct UnitWord<const T> : UnitWord<T> {};\ntemplate <typename T> struct UnitWord<const volatile T> : UnitWord<T> {};\n// clang-format on\n\n\n/******************************************************************************\n * Vector type inference utilities.\n ******************************************************************************/\n\n/**\n * \\brief Exposes a member typedef \\p Type that names the corresponding CUDA vector type if one exists.  Otherwise \\p Type refers to the CubVector structure itself, which will wrap the corresponding \\p x, \\p y, etc. vector fields.\n */\ntemplate <typename T, int vec_elements> struct CubVector;\n\n\nenum\n{\n    /// The maximum number of elements in CUDA vector types\n    MAX_VEC_ELEMENTS = 4,\n};\n\n\n/**\n * Generic vector-1 type\n */\ntemplate <typename T>\nstruct CubVector<T, 1>\n{\n    T x;\n\n    typedef T BaseType;\n    typedef CubVector<T, 1> Type;\n};\n\n/**\n * Generic vector-2 type\n */\ntemplate <typename T>\nstruct CubVector<T, 2>\n{\n    T x;\n    T y;\n\n    typedef T BaseType;\n    typedef CubVector<T, 2> Type;\n};\n\n/**\n * Generic vector-3 type\n */\ntemplate <typename T>\nstruct CubVector<T, 3>\n{\n    T x;\n    T y;\n    T z;\n\n    typedef T BaseType;\n    typedef CubVector<T, 3> Type;\n};\n\n/**\n * Generic vector-4 type\n */\ntemplate <typename T>\nstruct CubVector<T, 4>\n{\n    T x;\n    T y;\n    T z;\n    T w;\n\n    typedef T BaseType;\n    typedef CubVector<T, 4> Type;\n};\n\n\n/**\n * Macro for expanding partially-specialized built-in vector types\n */\n#define CUB_DEFINE_VECTOR_TYPE(base_type,short_type)                                                    \\\n                                                                                                        \\\n    template<> struct CubVector<base_type, 1> : short_type##1                                           \\\n    {                                                                                                   \\\n      typedef base_type       BaseType;                                                                 \\\n      typedef short_type##1   Type;                                                                     \\\n      __host__ __device__ __forceinline__ CubVector operator+(const CubVector &other) const {           \\\n          CubVector retval;                                                                             \\\n          retval.x = x + other.x;                                                                       \\\n          return retval;                                                                                \\\n      }                                                                                                 \\\n      __host__ __device__ __forceinline__ CubVector operator-(const CubVector &other) const {           \\\n          CubVector retval;                                                                             \\\n          retval.x = x - other.x;                                                                       \\\n          return retval;                                                                                \\\n      }                                                                                                 \\\n    };                                                                                                  \\\n                                                                                                        \\\n    template<> struct CubVector<base_type, 2> : short_type##2                                           \\\n    {                                                                                                   \\\n        typedef base_type       BaseType;                                                               \\\n        typedef short_type##2   Type;                                                                   \\\n        __host__ __device__ __forceinline__ CubVector operator+(const CubVector &other) const {         \\\n            CubVector retval;                                                                           \\\n            retval.x = x + other.x;                                                                     \\\n            retval.y = y + other.y;                                                                     \\\n            return retval;                                                                              \\\n        }                                                                                               \\\n        __host__ __device__ __forceinline__ CubVector operator-(const CubVector &other) const {         \\\n            CubVector retval;                                                                           \\\n            retval.x = x - other.x;                                                                     \\\n            retval.y = y - other.y;                                                                     \\\n            return retval;                                                                              \\\n        }                                                                                               \\\n    };                                                                                                  \\\n                                                                                                        \\\n    template<> struct CubVector<base_type, 3> : short_type##3                                           \\\n    {                                                                                                   \\\n        typedef base_type       BaseType;                                                               \\\n        typedef short_type##3   Type;                                                                   \\\n        __host__ __device__ __forceinline__ CubVector operator+(const CubVector &other) const {         \\\n            CubVector retval;                                                                           \\\n            retval.x = x + other.x;                                                                     \\\n            retval.y = y + other.y;                                                                     \\\n            retval.z = z + other.z;                                                                     \\\n            return retval;                                                                              \\\n        }                                                                                               \\\n        __host__ __device__ __forceinline__ CubVector operator-(const CubVector &other) const {         \\\n            CubVector retval;                                                                           \\\n            retval.x = x - other.x;                                                                     \\\n            retval.y = y - other.y;                                                                     \\\n            retval.z = z - other.z;                                                                     \\\n            return retval;                                                                              \\\n        }                                                                                               \\\n    };                                                                                                  \\\n                                                                                                        \\\n    template<> struct CubVector<base_type, 4> : short_type##4                                           \\\n    {                                                                                                   \\\n        typedef base_type       BaseType;                                                               \\\n        typedef short_type##4   Type;                                                                   \\\n        __host__ __device__ __forceinline__ CubVector operator+(const CubVector &other) const {         \\\n            CubVector retval;                                                                           \\\n            retval.x = x + other.x;                                                                     \\\n            retval.y = y + other.y;                                                                     \\\n            retval.z = z + other.z;                                                                     \\\n            retval.w = w + other.w;                                                                     \\\n            return retval;                                                                              \\\n        }                                                                                               \\\n        __host__ __device__ __forceinline__ CubVector operator-(const CubVector &other) const {         \\\n            CubVector retval;                                                                           \\\n            retval.x = x - other.x;                                                                     \\\n            retval.y = y - other.y;                                                                     \\\n            retval.z = z - other.z;                                                                     \\\n            retval.w = w - other.w;                                                                     \\\n            return retval;                                                                              \\\n        }                                                                                               \\\n    };\n\n\n\n// Expand CUDA vector types for built-in primitives\n// clang-format off\nCUB_DEFINE_VECTOR_TYPE(char,               char)\nCUB_DEFINE_VECTOR_TYPE(signed char,        char)\nCUB_DEFINE_VECTOR_TYPE(short,              short)\nCUB_DEFINE_VECTOR_TYPE(int,                int)\nCUB_DEFINE_VECTOR_TYPE(long,               long)\nCUB_DEFINE_VECTOR_TYPE(long long,          longlong)\nCUB_DEFINE_VECTOR_TYPE(unsigned char,      uchar)\nCUB_DEFINE_VECTOR_TYPE(unsigned short,     ushort)\nCUB_DEFINE_VECTOR_TYPE(unsigned int,       uint)\nCUB_DEFINE_VECTOR_TYPE(unsigned long,      ulong)\nCUB_DEFINE_VECTOR_TYPE(unsigned long long, ulonglong)\nCUB_DEFINE_VECTOR_TYPE(float,              float)\nCUB_DEFINE_VECTOR_TYPE(double,             double)\nCUB_DEFINE_VECTOR_TYPE(bool,               uchar)\n// clang-format on\n\n// Undefine macros\n#undef CUB_DEFINE_VECTOR_TYPE\n\n\n/******************************************************************************\n * Wrapper types\n ******************************************************************************/\n\n/**\n * \\brief A storage-backing wrapper that allows types with non-trivial constructors to be aliased in unions\n */\ntemplate <typename T>\nstruct Uninitialized\n{\n    /// Biggest memory-access word that T is a whole multiple of and is not larger than the alignment of T\n    typedef typename UnitWord<T>::DeviceWord DeviceWord;\n\n    static constexpr std::size_t DATA_SIZE = sizeof(T);\n    static constexpr std::size_t WORD_SIZE = sizeof(DeviceWord);\n    static constexpr std::size_t WORDS = DATA_SIZE / WORD_SIZE;\n\n    /// Backing storage\n    DeviceWord storage[WORDS];\n\n    /// Alias\n    __host__ __device__ __forceinline__ T& Alias()\n    {\n        return reinterpret_cast<T&>(*this);\n    }\n};\n\n\n/**\n * \\brief A key identifier paired with a corresponding value\n */\ntemplate <\n    typename    _Key,\n    typename    _Value\n#if defined(_WIN32) && !defined(_WIN64)\n    , bool KeyIsLT = (AlignBytes<_Key>::ALIGN_BYTES < AlignBytes<_Value>::ALIGN_BYTES)\n    , bool ValIsLT = (AlignBytes<_Value>::ALIGN_BYTES < AlignBytes<_Key>::ALIGN_BYTES)\n#endif // #if defined(_WIN32) && !defined(_WIN64)\n    >\nstruct KeyValuePair\n{\n    typedef _Key    Key;                ///< Key data type\n    typedef _Value  Value;              ///< Value data type\n\n    Key     key;                        ///< Item key\n    Value   value;                      ///< Item value\n\n    /// Constructor\n    __host__ __device__ __forceinline__\n    KeyValuePair() {}\n\n    /// Constructor\n    __host__ __device__ __forceinline__\n    KeyValuePair(Key const& key, Value const& value) : key(key), value(value) {}\n\n    /// Inequality operator\n    __host__ __device__ __forceinline__ bool operator !=(const KeyValuePair &b)\n    {\n        return (value != b.value) || (key != b.key);\n    }\n};\n\n#if defined(_WIN32) && !defined(_WIN64)\n\n/**\n * Win32 won't do 16B alignment.  This can present two problems for\n * should-be-16B-aligned (but actually 8B aligned) built-in and intrinsics members:\n * 1) If a smaller-aligned item were to be listed first, the host compiler places the\n *    should-be-16B item at too early an offset (and disagrees with device compiler)\n * 2) Or, if a smaller-aligned item lists second, the host compiler gets the size\n *    of the struct wrong (and disagrees with device compiler)\n *\n * So we put the larger-should-be-aligned item first, and explicitly pad the\n * end of the struct\n */\n\n/// Smaller key specialization\ntemplate <typename K, typename V>\nstruct KeyValuePair<K, V, true, false>\n{\n    typedef K Key;\n    typedef V Value;\n\n    typedef char Pad[AlignBytes<V>::ALIGN_BYTES - AlignBytes<K>::ALIGN_BYTES];\n\n    Value   value;  // Value has larger would-be alignment and goes first\n    Key     key;\n    Pad     pad;\n\n    /// Constructor\n    __host__ __device__ __forceinline__\n    KeyValuePair() {}\n\n    /// Constructor\n    __host__ __device__ __forceinline__\n    KeyValuePair(Key const& key, Value const& value) : key(key), value(value) {}\n\n    /// Inequality operator\n    __host__ __device__ __forceinline__ bool operator !=(const KeyValuePair &b)\n    {\n        return (value != b.value) || (key != b.key);\n    }\n};\n\n\n/// Smaller value specialization\ntemplate <typename K, typename V>\nstruct KeyValuePair<K, V, false, true>\n{\n    typedef K Key;\n    typedef V Value;\n\n    typedef char Pad[AlignBytes<K>::ALIGN_BYTES - AlignBytes<V>::ALIGN_BYTES];\n\n    Key     key;    // Key has larger would-be alignment and goes first\n    Value   value;\n    Pad     pad;\n\n    /// Constructor\n    __host__ __device__ __forceinline__\n    KeyValuePair() {}\n\n    /// Constructor\n    __host__ __device__ __forceinline__\n    KeyValuePair(Key const& key, Value const& value) : key(key), value(value) {}\n\n    /// Inequality operator\n    __host__ __device__ __forceinline__ bool operator !=(const KeyValuePair &b)\n    {\n        return (value != b.value) || (key != b.key);\n    }\n};\n\n#endif // #if defined(_WIN32) && !defined(_WIN64)\n\n\n/**\n * \\brief A wrapper for passing simple static arrays as kernel parameters\n */\ntemplate <typename T, int COUNT>\nstruct ArrayWrapper\n{\n\n    /// Statically-sized array of type \\p T\n    T array[COUNT];\n\n    /// Constructor\n    __host__ __device__ __forceinline__ ArrayWrapper() {}\n};\n\n\n/**\n * \\brief Double-buffer storage wrapper for multi-pass stream transformations that require more than one storage array for streaming intermediate results back and forth.\n *\n * Many multi-pass computations require a pair of \"ping-pong\" storage\n * buffers (e.g., one for reading from and the other for writing to, and then\n * vice-versa for the subsequent pass).  This structure wraps a set of device\n * buffers and a \"selector\" member to track which is \"current\".\n */\ntemplate <typename T>\nstruct DoubleBuffer\n{\n    /// Pair of device buffer pointers\n    T *d_buffers[2];\n\n    ///  Selector into \\p d_buffers (i.e., the active/valid buffer)\n    int selector;\n\n    /// \\brief Constructor\n    __host__ __device__ __forceinline__ DoubleBuffer()\n    {\n        selector = 0;\n        d_buffers[0] = NULL;\n        d_buffers[1] = NULL;\n    }\n\n    /// \\brief Constructor\n    __host__ __device__ __forceinline__ DoubleBuffer(\n        T *d_current,         ///< The currently valid buffer\n        T *d_alternate)       ///< Alternate storage buffer of the same size as \\p d_current\n    {\n        selector = 0;\n        d_buffers[0] = d_current;\n        d_buffers[1] = d_alternate;\n    }\n\n    /// \\brief Return pointer to the currently valid buffer\n    __host__ __device__ __forceinline__ T* Current() { return d_buffers[selector]; }\n\n    /// \\brief Return pointer to the currently invalid buffer\n    __host__ __device__ __forceinline__ T* Alternate() { return d_buffers[selector ^ 1]; }\n\n};\n\n\n\n/******************************************************************************\n * Typedef-detection\n ******************************************************************************/\n\n\n/**\n * \\brief Defines a structure \\p detector_name that is templated on type \\p T.  The \\p detector_name struct exposes a constant member \\p VALUE indicating whether or not parameter \\p T exposes a nested type \\p nested_type_name\n */\n#define CUB_DEFINE_DETECT_NESTED_TYPE(detector_name, nested_type_name)  \\\n    template <typename T>                                               \\\n    struct detector_name                                                \\\n    {                                                                   \\\n        template <typename C>                                           \\\n        static char& test(typename C::nested_type_name*);               \\\n        template <typename>                                             \\\n        static int& test(...);                                          \\\n        enum                                                            \\\n        {                                                               \\\n            VALUE = sizeof(test<T>(0)) < sizeof(int)                    \\\n        };                                                              \\\n    };\n\n\n\n/******************************************************************************\n * Simple enable-if (similar to Boost)\n ******************************************************************************/\n\n/**\n * \\brief Simple enable-if (similar to Boost)\n *\n * \\deprecated [Since 1.16.0] The cub::If APIs are deprecated.\n *             Use std::enable_if instead.\n */\ntemplate <bool Condition, class T = void>\nstruct CUB_DEPRECATED EnableIf\n{\n  using Type = typename std::enable_if<Condition, T>::type;\n};\n\n/******************************************************************************\n * Typedef-detection\n ******************************************************************************/\n\n/**\n * \\brief Determine whether or not BinaryOp's functor is of the form <tt>bool operator()(const T& a, const T&b)</tt> or <tt>bool operator()(const T& a, const T&b, unsigned int idx)</tt>\n */\ntemplate <typename T, typename BinaryOp>\nstruct BinaryOpHasIdxParam\n{\nprivate:\n/*\n    template <typename BinaryOpT, bool (BinaryOpT::*)(const T &a, const T &b, unsigned int idx) const>  struct SFINAE1 {};\n    template <typename BinaryOpT, bool (BinaryOpT::*)(const T &a, const T &b, unsigned int idx)>        struct SFINAE2 {};\n    template <typename BinaryOpT, bool (BinaryOpT::*)(T a, T b, unsigned int idx) const>                struct SFINAE3 {};\n    template <typename BinaryOpT, bool (BinaryOpT::*)(T a, T b, unsigned int idx)>                      struct SFINAE4 {};\n*/\n    template <typename BinaryOpT, bool (BinaryOpT::*)(const T &a, const T &b, int idx) const>           struct SFINAE5 {};\n    template <typename BinaryOpT, bool (BinaryOpT::*)(const T &a, const T &b, int idx)>                 struct SFINAE6 {};\n    template <typename BinaryOpT, bool (BinaryOpT::*)(T a, T b, int idx) const>                         struct SFINAE7 {};\n    template <typename BinaryOpT, bool (BinaryOpT::*)(T a, T b, int idx)>                               struct SFINAE8 {};\n/*\n    template <typename BinaryOpT> static char Test(SFINAE1<BinaryOpT, &BinaryOpT::operator()> *);\n    template <typename BinaryOpT> static char Test(SFINAE2<BinaryOpT, &BinaryOpT::operator()> *);\n    template <typename BinaryOpT> static char Test(SFINAE3<BinaryOpT, &BinaryOpT::operator()> *);\n    template <typename BinaryOpT> static char Test(SFINAE4<BinaryOpT, &BinaryOpT::operator()> *);\n*/\n    template <typename BinaryOpT> __host__ __device__ static char Test(SFINAE5<BinaryOpT, &BinaryOpT::operator()> *);\n    template <typename BinaryOpT> __host__ __device__ static char Test(SFINAE6<BinaryOpT, &BinaryOpT::operator()> *);\n    template <typename BinaryOpT> __host__ __device__ static char Test(SFINAE7<BinaryOpT, &BinaryOpT::operator()> *);\n    template <typename BinaryOpT> __host__ __device__ static char Test(SFINAE8<BinaryOpT, &BinaryOpT::operator()> *);\n\n    template <typename BinaryOpT> static int Test(...);\n\npublic:\n\n    /// Whether the functor BinaryOp has a third <tt>unsigned int</tt> index param\n    static const bool HAS_PARAM = sizeof(Test<BinaryOp>(NULL)) == sizeof(char);\n};\n\n\n\n\n/******************************************************************************\n * Simple type traits utilities.\n *\n * For example:\n *     Traits<int>::CATEGORY             // SIGNED_INTEGER\n *     Traits<NullType>::NULL_TYPE       // true\n *     Traits<uint4>::CATEGORY           // NOT_A_NUMBER\n *     Traits<uint4>::PRIMITIVE;         // false\n *\n ******************************************************************************/\n\n/**\n * \\brief Basic type traits categories\n */\nenum Category\n{\n    NOT_A_NUMBER,\n    SIGNED_INTEGER,\n    UNSIGNED_INTEGER,\n    FLOATING_POINT\n};\n\n\n/**\n * \\brief Basic type traits\n */\ntemplate <Category _CATEGORY, bool _PRIMITIVE, bool _NULL_TYPE, typename _UnsignedBits, typename T>\nstruct BaseTraits\n{\n    /// Category\n    static const Category CATEGORY      = _CATEGORY;\n    enum\n    {\n        PRIMITIVE       = _PRIMITIVE,\n        NULL_TYPE       = _NULL_TYPE,\n    };\n};\n\n\n/**\n * Basic type traits (unsigned primitive specialization)\n */\ntemplate <typename _UnsignedBits, typename T>\nstruct BaseTraits<UNSIGNED_INTEGER, true, false, _UnsignedBits, T>\n{\n    typedef _UnsignedBits       UnsignedBits;\n\n    static const Category       CATEGORY    = UNSIGNED_INTEGER;\n    static const UnsignedBits   LOWEST_KEY  = UnsignedBits(0);\n    static const UnsignedBits   MAX_KEY     = UnsignedBits(-1);\n\n    enum\n    {\n        PRIMITIVE       = true,\n        NULL_TYPE       = false,\n    };\n\n\n    static __host__ __device__ __forceinline__ UnsignedBits TwiddleIn(UnsignedBits key)\n    {\n        return key;\n    }\n\n    static __host__ __device__ __forceinline__ UnsignedBits TwiddleOut(UnsignedBits key)\n    {\n        return key;\n    }\n\n    static __host__ __device__ __forceinline__ T Max()\n    {\n        UnsignedBits retval_bits = MAX_KEY;\n        T retval;\n        memcpy(&retval, &retval_bits, sizeof(T));\n        return retval;\n    }\n\n    static __host__ __device__ __forceinline__ T Lowest()\n    {\n        UnsignedBits retval_bits = LOWEST_KEY;\n        T retval;\n        memcpy(&retval, &retval_bits, sizeof(T));\n        return retval;\n    }\n};\n\n\n/**\n * Basic type traits (signed primitive specialization)\n */\ntemplate <typename _UnsignedBits, typename T>\nstruct BaseTraits<SIGNED_INTEGER, true, false, _UnsignedBits, T>\n{\n    typedef _UnsignedBits       UnsignedBits;\n\n    static const Category       CATEGORY    = SIGNED_INTEGER;\n    static const UnsignedBits   HIGH_BIT    = UnsignedBits(1) << ((sizeof(UnsignedBits) * 8) - 1);\n    static const UnsignedBits   LOWEST_KEY  = HIGH_BIT;\n    static const UnsignedBits   MAX_KEY     = UnsignedBits(-1) ^ HIGH_BIT;\n\n    enum\n    {\n        PRIMITIVE       = true,\n        NULL_TYPE       = false,\n    };\n\n    static __host__ __device__ __forceinline__ UnsignedBits TwiddleIn(UnsignedBits key)\n    {\n        return key ^ HIGH_BIT;\n    };\n\n    static __host__ __device__ __forceinline__ UnsignedBits TwiddleOut(UnsignedBits key)\n    {\n        return key ^ HIGH_BIT;\n    };\n\n    static __host__ __device__ __forceinline__ T Max()\n    {\n        UnsignedBits retval = MAX_KEY;\n        return reinterpret_cast<T&>(retval);\n    }\n\n    static __host__ __device__ __forceinline__ T Lowest()\n    {\n        UnsignedBits retval = LOWEST_KEY;\n        return reinterpret_cast<T&>(retval);\n    }\n};\n\ntemplate <typename _T>\nstruct FpLimits;\n\ntemplate <>\nstruct FpLimits<float>\n{\n    static __host__ __device__ __forceinline__ float Max() {\n        return FLT_MAX;\n    }\n\n    static __host__ __device__ __forceinline__ float Lowest() {\n        return FLT_MAX * float(-1);\n    }\n};\n\ntemplate <>\nstruct FpLimits<double>\n{\n    static __host__ __device__ __forceinline__ double Max() {\n        return DBL_MAX;\n    }\n\n    static __host__ __device__ __forceinline__ double Lowest() {\n        return DBL_MAX  * double(-1);\n    }\n};\n\n#if !_NVHPC_CUDA\ntemplate <>\nstruct FpLimits<__half>\n{\n    static __host__ __device__ __forceinline__ __half Max() {\n        unsigned short max_word = 0x7BFF;\n        return reinterpret_cast<__half&>(max_word);\n    }\n\n    static __host__ __device__ __forceinline__ __half Lowest() {\n        unsigned short lowest_word = 0xFBFF;\n        return reinterpret_cast<__half&>(lowest_word);\n    }\n};\n#endif\n\n#if !_NVHPC_CUDA && !defined(CUB_DISABLE_BF16_SUPPORT)\ntemplate <>\nstruct FpLimits<__nv_bfloat16>\n{\n    static __host__ __device__ __forceinline__ __nv_bfloat16 Max() {\n        unsigned short max_word = 0x7F7F;\n        return reinterpret_cast<__nv_bfloat16&>(max_word);\n    }\n\n    static __host__ __device__ __forceinline__ __nv_bfloat16 Lowest() {\n        unsigned short lowest_word = 0xFF7F;\n        return reinterpret_cast<__nv_bfloat16&>(lowest_word);\n    }\n};\n#endif\n\n/**\n * Basic type traits (fp primitive specialization)\n */\ntemplate <typename _UnsignedBits, typename T>\nstruct BaseTraits<FLOATING_POINT, true, false, _UnsignedBits, T>\n{\n    typedef _UnsignedBits       UnsignedBits;\n\n    static const Category       CATEGORY    = FLOATING_POINT;\n    static const UnsignedBits   HIGH_BIT    = UnsignedBits(1) << ((sizeof(UnsignedBits) * 8) - 1);\n    static const UnsignedBits   LOWEST_KEY  = UnsignedBits(-1);\n    static const UnsignedBits   MAX_KEY     = UnsignedBits(-1) ^ HIGH_BIT;\n\n    enum\n    {\n        PRIMITIVE       = true,\n        NULL_TYPE       = false,\n    };\n\n    static __host__ __device__ __forceinline__ UnsignedBits TwiddleIn(UnsignedBits key)\n    {\n        UnsignedBits mask = (key & HIGH_BIT) ? UnsignedBits(-1) : HIGH_BIT;\n        return key ^ mask;\n    };\n\n    static __host__ __device__ __forceinline__ UnsignedBits TwiddleOut(UnsignedBits key)\n    {\n        UnsignedBits mask = (key & HIGH_BIT) ? HIGH_BIT : UnsignedBits(-1);\n        return key ^ mask;\n    };\n\n    static __host__ __device__ __forceinline__ T Max() {\n        return FpLimits<T>::Max();\n    }\n\n    static __host__ __device__ __forceinline__ T Lowest() {\n        return FpLimits<T>::Lowest();\n    }\n};\n\n\n/**\n * \\brief Numeric type traits\n */\n// clang-format off\ntemplate <typename T> struct NumericTraits :            BaseTraits<NOT_A_NUMBER, false, false, T, T> {};\n\ntemplate <> struct NumericTraits<NullType> :            BaseTraits<NOT_A_NUMBER, false, true, NullType, NullType> {};\n\ntemplate <> struct NumericTraits<char> :                BaseTraits<(std::numeric_limits<char>::is_signed) ? SIGNED_INTEGER : UNSIGNED_INTEGER, true, false, unsigned char, char> {};\ntemplate <> struct NumericTraits<signed char> :         BaseTraits<SIGNED_INTEGER, true, false, unsigned char, signed char> {};\ntemplate <> struct NumericTraits<short> :               BaseTraits<SIGNED_INTEGER, true, false, unsigned short, short> {};\ntemplate <> struct NumericTraits<int> :                 BaseTraits<SIGNED_INTEGER, true, false, unsigned int, int> {};\ntemplate <> struct NumericTraits<long> :                BaseTraits<SIGNED_INTEGER, true, false, unsigned long, long> {};\ntemplate <> struct NumericTraits<long long> :           BaseTraits<SIGNED_INTEGER, true, false, unsigned long long, long long> {};\n\ntemplate <> struct NumericTraits<unsigned char> :       BaseTraits<UNSIGNED_INTEGER, true, false, unsigned char, unsigned char> {};\ntemplate <> struct NumericTraits<unsigned short> :      BaseTraits<UNSIGNED_INTEGER, true, false, unsigned short, unsigned short> {};\ntemplate <> struct NumericTraits<unsigned int> :        BaseTraits<UNSIGNED_INTEGER, true, false, unsigned int, unsigned int> {};\ntemplate <> struct NumericTraits<unsigned long> :       BaseTraits<UNSIGNED_INTEGER, true, false, unsigned long, unsigned long> {};\ntemplate <> struct NumericTraits<unsigned long long> :  BaseTraits<UNSIGNED_INTEGER, true, false, unsigned long long, unsigned long long> {};\n\n\n#if CUB_IS_INT128_ENABLED \ntemplate <>\nstruct NumericTraits<__uint128_t>\n{\n  using T = __uint128_t;\n  using UnsignedBits = __uint128_t;\n\n  static constexpr Category       CATEGORY    = UNSIGNED_INTEGER;\n  static constexpr UnsignedBits   LOWEST_KEY  = UnsignedBits(0);\n  static constexpr UnsignedBits   MAX_KEY     = UnsignedBits(-1);\n\n  static constexpr bool PRIMITIVE = false;\n  static constexpr bool NULL_TYPE = false;\n\n  static __host__ __device__ __forceinline__ UnsignedBits TwiddleIn(UnsignedBits key)\n  {\n    return key;\n  }\n\n  static __host__ __device__ __forceinline__ UnsignedBits TwiddleOut(UnsignedBits key)\n  {\n    return key;\n  }\n\n  static __host__ __device__ __forceinline__ T Max()\n  {\n    return MAX_KEY;\n  }\n\n  static __host__ __device__ __forceinline__ T Lowest()\n  {\n    return LOWEST_KEY;\n  }\n};\n\ntemplate <>\nstruct NumericTraits<__int128_t>\n{\n  using T = __int128_t;\n  using UnsignedBits = __uint128_t;\n\n  static constexpr Category       CATEGORY    = SIGNED_INTEGER;\n  static constexpr UnsignedBits   HIGH_BIT    = UnsignedBits(1) << ((sizeof(UnsignedBits) * 8) - 1);\n  static constexpr UnsignedBits   LOWEST_KEY  = HIGH_BIT;\n  static constexpr UnsignedBits   MAX_KEY     = UnsignedBits(-1) ^ HIGH_BIT;\n\n  static constexpr bool PRIMITIVE = false;\n  static constexpr bool NULL_TYPE = false;\n\n  static __host__ __device__ __forceinline__ UnsignedBits TwiddleIn(UnsignedBits key)\n  {\n    return key ^ HIGH_BIT;\n  };\n\n  static __host__ __device__ __forceinline__ UnsignedBits TwiddleOut(UnsignedBits key)\n  {\n    return key ^ HIGH_BIT;\n  };\n\n  static __host__ __device__ __forceinline__ T Max()\n  {\n    UnsignedBits retval = MAX_KEY;\n    return reinterpret_cast<T&>(retval);\n  }\n\n  static __host__ __device__ __forceinline__ T Lowest()\n  {\n    UnsignedBits retval = LOWEST_KEY;\n    return reinterpret_cast<T&>(retval);\n  }\n};\n#endif\n\ntemplate <> struct NumericTraits<float> :               BaseTraits<FLOATING_POINT, true, false, unsigned int, float> {};\ntemplate <> struct NumericTraits<double> :              BaseTraits<FLOATING_POINT, true, false, unsigned long long, double> {};\n#if !_NVHPC_CUDA\n    template <> struct NumericTraits<__half> :          BaseTraits<FLOATING_POINT, true, false, unsigned short, __half> {};\n#endif\n#if !_NVHPC_CUDA && !defined(CUB_DISABLE_BF16_SUPPORT)\n    template <> struct NumericTraits<__nv_bfloat16> :   BaseTraits<FLOATING_POINT, true, false, unsigned short, __nv_bfloat16> {};\n#endif\n\ntemplate <> struct NumericTraits<bool> :                BaseTraits<UNSIGNED_INTEGER, true, false, typename UnitWord<bool>::VolatileWord, bool> {};\n// clang-format on\n\n/**\n * \\brief Type traits\n */\ntemplate <typename T>\nstruct Traits : NumericTraits<typename std::remove_cv<T>::type> {};\n\n\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\n\n/** @} */       // end group UtilModule\n\nCUB_NAMESPACE_END\n\n#endif // _JITIFY_INCLUDE_GUARD_1375AABB974C3530\n", "../../warp/warp_reduce.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_EAEF1640776910F4\n#define _JITIFY_INCLUDE_GUARD_EAEF1640776910F4\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n//! @file\n//! @rst\n//! The ``cub::WarpReduce`` class provides :ref:`collective <collective-primitives>` methods for\n//! computing a parallel reduction of items partitioned across a CUDA thread warp.\n//! @endrst\n\n#include <cub/config.cuh>\n#include <cub/thread/thread_operators.cuh>\n#include <cub/util_type.cuh>\n#include <cub/warp/specializations/warp_reduce_shfl.cuh>\n#include <cub/warp/specializations/warp_reduce_smem.cuh>\n\nCUB_NAMESPACE_BEGIN\n\n//! @rst\n//! The ``WarpReduce`` class provides :ref:`collective <collective-primitives>` methods for\n//! computing a parallel reduction of items partitioned across a CUDA thread warp.\n//!\n//! .. image:: ../img/warp_reduce_logo.png\n//!     :align: center\n//!\n//! Overview\n//! ++++++++++++++++++++++++++\n//!\n//! - A `reduction <http://en.wikipedia.org/wiki/Reduce_(higher-order_function)>`__ (or *fold*)\n//!   uses a binary combining operator to compute a single aggregate from a list of input elements.\n//! - Supports \"logical\" warps smaller than the physical warp size (e.g., logical warps of 8\n//!   threads)\n//! - The number of entrant threads must be an multiple of ``LOGICAL_WARP_THREADS``\n//!\n//! Performance Considerations\n//! ++++++++++++++++++++++++++\n//!\n//! - Uses special instructions when applicable (e.g., warp ``SHFL`` instructions)\n//! - Uses synchronization-free communication between warp lanes when applicable\n//! - Incurs zero bank conflicts for most types\n//! - Computation is slightly more efficient (i.e., having lower instruction overhead) for:\n//!\n//!   - Summation (**vs.** generic reduction)\n//!   - The architecture's warp size is a whole multiple of ``LOGICAL_WARP_THREADS``\n//!\n//! Simple Examples\n//! ++++++++++++++++++++++++++\n//!\n//! @warpcollective{WarpReduce}\n//!\n//! The code snippet below illustrates four concurrent warp sum reductions within a block of\n//! 128 threads (one per each of the 32-thread warps).\n//!\n//! .. code-block:: c++\n//!\n//!    #include <cub/cub.cuh>\n//!\n//!    __global__ void ExampleKernel(...)\n//!    {\n//!        // Specialize WarpReduce for type int\n//!        typedef cub::WarpReduce<int> WarpReduce;\n//!\n//!        // Allocate WarpReduce shared memory for 4 warps\n//!        __shared__ typename WarpReduce::TempStorage temp_storage[4];\n//!\n//!        // Obtain one input item per thread\n//!        int thread_data = ...\n//!\n//!        // Return the warp-wide sums to each lane0 (threads 0, 32, 64, and 96)\n//!        int warp_id = threadIdx.x / 32;\n//!        int aggregate = WarpReduce(temp_storage[warp_id]).Sum(thread_data);\n//!\n//! Suppose the set of input ``thread_data`` across the block of threads is\n//! ``{0, 1, 2, 3, ..., 127}``. The corresponding output ``aggregate`` in threads 0, 32, 64, and 96\n//! will be ``496``, ``1520``, ``2544``, and ``3568``, respectively\n//! (and is undefined in other threads).\n//!\n//! The code snippet below illustrates a single warp sum reduction within a block of\n//! 128 threads.\n//!\n//! .. code-block:: c++\n//!\n//!    #include <cub/cub.cuh>\n//!\n//!    __global__ void ExampleKernel(...)\n//!    {\n//!        // Specialize WarpReduce for type int\n//!        typedef cub::WarpReduce<int> WarpReduce;\n//!\n//!        // Allocate WarpReduce shared memory for one warp\n//!        __shared__ typename WarpReduce::TempStorage temp_storage;\n//!        ...\n//!\n//!        // Only the first warp performs a reduction\n//!        if (threadIdx.x < 32)\n//!        {\n//!            // Obtain one input item per thread\n//!            int thread_data = ...\n//!\n//!            // Return the warp-wide sum to lane0\n//!            int aggregate = WarpReduce(temp_storage).Sum(thread_data);\n//!\n//! Suppose the set of input ``thread_data`` across the warp of threads is\n//! ``{0, 1, 2, 3, ..., 31}``. The corresponding output ``aggregate`` in thread0 will be ``496``\n//! (and is undefined in other threads).\n//! @endrst\n//!\n//! @tparam T\n//!   The reduction input/output element type\n//!\n//! @tparam LOGICAL_WARP_THREADS\n//!   <b>[optional]</b> The number of threads per \"logical\" warp (may be less than the number of\n//!   hardware warp threads).  Default is the warp size of the targeted CUDA compute-capability\n//!   (e.g., 32 threads for SM20).\n//!\n//! @tparam LEGACY_PTX_ARCH\n//!   <b>[optional]</b> Unused.\ntemplate <typename T, int LOGICAL_WARP_THREADS = CUB_PTX_WARP_THREADS, int LEGACY_PTX_ARCH = 0>\nclass WarpReduce\n{\nprivate:\n  /******************************************************************************\n   * Constants and type definitions\n   ******************************************************************************/\n\n  enum\n  {\n    /// Whether the logical warp size and the PTX warp size coincide\n    IS_ARCH_WARP = (LOGICAL_WARP_THREADS == CUB_WARP_THREADS(0)),\n\n    /// Whether the logical warp size is a power-of-two\n    IS_POW_OF_TWO = PowerOfTwo<LOGICAL_WARP_THREADS>::VALUE,\n  };\n\npublic:\n#ifndef DOXYGEN_SHOULD_SKIP_THIS // Do not document\n\n  /// Internal specialization.\n  /// Use SHFL-based reduction if LOGICAL_WARP_THREADS is a power-of-two\n  using InternalWarpReduce = cub::detail::conditional_t<IS_POW_OF_TWO,\n                                                        WarpReduceShfl<T, LOGICAL_WARP_THREADS>,\n                                                        WarpReduceSmem<T, LOGICAL_WARP_THREADS>>;\n\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\nprivate:\n  /// Shared memory storage layout type for WarpReduce\n  using _TempStorage = typename InternalWarpReduce::TempStorage;\n\n  /******************************************************************************\n   * Thread fields\n   ******************************************************************************/\n\n  /// Shared storage reference\n  _TempStorage &temp_storage;\n\n  /******************************************************************************\n   * Utility methods\n   ******************************************************************************/\n\npublic:\n  /// \\smemstorage{WarpReduce}\n  struct TempStorage : Uninitialized<_TempStorage>\n  {};\n\n  //! @name Collective constructors\n  //! @{\n\n  //! @rst\n  //! Collective constructor using the specified memory allocation as temporary storage.\n  //! Logical warp and lane identifiers are constructed from ``threadIdx.x``.\n  //! @endrst\n  //!\n  //! @param[in] temp_storage Reference to memory allocation having layout type TempStorage\n  __device__ __forceinline__ WarpReduce(TempStorage &temp_storage)\n      : temp_storage(temp_storage.Alias())\n  {}\n\n  //! @}  end member group\n  //! @name Summation reductions\n  //! @{\n\n  //! @rst\n  //! Computes a warp-wide sum in the calling warp.\n  //! The output is valid in warp *lane*\\ :sub:`0`.\n  //!\n  //! @smemwarpreuse\n  //!\n  //! Snippet\n  //! +++++++\n  //!\n  //! The code snippet below illustrates four concurrent warp sum reductions within a block of\n  //! 128 threads (one per each of the 32-thread warps).\n  //!\n  //! .. code-block:: c++\n  //!\n  //!    #include <cub/cub.cuh>\n  //!\n  //!    __global__ void ExampleKernel(...)\n  //!    {\n  //!        // Specialize WarpReduce for type int\n  //!        typedef cub::WarpReduce<int> WarpReduce;\n  //!\n  //!        // Allocate WarpReduce shared memory for 4 warps\n  //!        __shared__ typename WarpReduce::TempStorage temp_storage[4];\n  //!\n  //!        // Obtain one input item per thread\n  //!        int thread_data = ...\n  //!\n  //!        // Return the warp-wide sums to each lane0\n  //!        int warp_id = threadIdx.x / 32;\n  //!        int aggregate = WarpReduce(temp_storage[warp_id]).Sum(thread_data);\n  //!\n  //! Suppose the set of input ``thread_data`` across the block of threads is\n  //! ``{0, 1, 2, 3, ..., 127}``.\n  //! The corresponding output ``aggregate`` in threads 0, 32, 64, and 96 will ``496``, ``1520``,\n  //! ``2544``, and ``3568``, respectively (and is undefined in other threads).\n  //! @endrst\n  //!\n  //! @param[in] input Calling thread's input\n  __device__ __forceinline__ T Sum(T input)\n  {\n    return InternalWarpReduce(temp_storage)\n      .template Reduce<true>(input, LOGICAL_WARP_THREADS, cub::Sum());\n  }\n\n  //! @rst\n  //! Computes a partially-full warp-wide sum in the calling warp.\n  //! The output is valid in warp *lane*\\ :sub:`0`.\n  //!\n  //! All threads across the calling warp must agree on the same value for ``valid_items``.\n  //! Otherwise the result is undefined.\n  //!\n  //! @smemwarpreuse\n  //!\n  //! Snippet\n  //! +++++++\n  //!\n  //! The code snippet below illustrates a sum reduction within a single, partially-full\n  //! block of 32 threads (one warp).\n  //!\n  //! .. code-block:: c++\n  //!\n  //!    #include <cub/cub.cuh>\n  //!\n  //!    __global__ void ExampleKernel(int *d_data, int valid_items)\n  //!    {\n  //!        // Specialize WarpReduce for type int\n  //!        typedef cub::WarpReduce<int> WarpReduce;\n  //!\n  //!        // Allocate WarpReduce shared memory for one warp\n  //!        __shared__ typename WarpReduce::TempStorage temp_storage;\n  //!\n  //!        // Obtain one input item per thread if in range\n  //!        int thread_data;\n  //!        if (threadIdx.x < valid_items)\n  //!            thread_data = d_data[threadIdx.x];\n  //!\n  //!        // Return the warp-wide sums to each lane0\n  //!        int aggregate = WarpReduce(temp_storage).Sum(\n  //!            thread_data, valid_items);\n  //!\n  //! Suppose the input ``d_data`` is ``{0, 1, 2, 3, 4, ...`` and ``valid_items`` is ``4``.\n  //! The corresponding output ``aggregate`` in *lane*\\ :sub:`0` is ``6``\n  //! (and is undefined in other threads).\n  //! @endrst\n  //!\n  //! @param[in] input\n  //!   Calling thread's input\n  //!\n  //! @param[in] valid_items\n  //!   Total number of valid items in the calling thread's logical warp\n  //!   (may be less than ``LOGICAL_WARP_THREADS``)\n  __device__ __forceinline__ T Sum(T input, int valid_items)\n  {\n    // Determine if we don't need bounds checking\n    return InternalWarpReduce(temp_storage).template Reduce<false>(input, valid_items, cub::Sum());\n  }\n\n  //! @rst\n  //! Computes a segmented sum in the calling warp where segments are defined by head-flags.\n  //! The sum of each segment is returned to the first lane in that segment\n  //! (which always includes *lane*\\ :sub:`0`).\n  //!\n  //! @smemwarpreuse\n  //!\n  //! Snippet\n  //! +++++++\n  //!\n  //! The code snippet below illustrates a head-segmented warp sum\n  //! reduction within a block of 32 threads (one warp).\n  //!\n  //! .. code-block:: c++\n  //!\n  //!    #include <cub/cub.cuh>\n  //!\n  //!    __global__ void ExampleKernel(...)\n  //!    {\n  //!        // Specialize WarpReduce for type int\n  //!        typedef cub::WarpReduce<int> WarpReduce;\n  //!\n  //!        // Allocate WarpReduce shared memory for one warp\n  //!        __shared__ typename WarpReduce::TempStorage temp_storage;\n  //!\n  //!        // Obtain one input item and flag per thread\n  //!        int thread_data = ...\n  //!        int head_flag = ...\n  //!\n  //!        // Return the warp-wide sums to each lane0\n  //!        int aggregate = WarpReduce(temp_storage).HeadSegmentedSum(\n  //!            thread_data, head_flag);\n  //!\n  //! Suppose the set of input ``thread_data`` and ``head_flag`` across the block of threads\n  //! is ``{0, 1, 2, 3, ..., 31`` and is ``{1, 0, 0, 0, 1, 0, 0, 0, ..., 1, 0, 0, 0``,\n  //! respectively. The corresponding output ``aggregate`` in threads 0, 4, 8, etc. will be\n  //! ``6``, ``22``, ``38``, etc. (and is undefined in other threads).\n  //! @endrst\n  //!\n  //! @tparam ReductionOp\n  //!   **[inferred]** Binary reduction operator type having member\n  //!   `T operator()(const T &a, const T &b)`\n  //!\n  //! @param[in] input\n  //!   Calling thread's input\n  //!\n  //! @param[in] head_flag\n  //!   Head flag denoting whether or not `input` is the start of a new segment\n  template <typename FlagT>\n  __device__ __forceinline__ T HeadSegmentedSum(T input, FlagT head_flag)\n  {\n    return HeadSegmentedReduce(input, head_flag, cub::Sum());\n  }\n\n  //! @rst\n  //! Computes a segmented sum in the calling warp where segments are defined by tail-flags.\n  //! The sum of each segment is returned to the first lane in that segment\n  //! (which always includes *lane*\\ :sub:`0`).\n  //!\n  //! @smemwarpreuse\n  //!\n  //! Snippet\n  //! +++++++\n  //!\n  //! The code snippet below illustrates a tail-segmented warp sum reduction within a block of 32\n  //! threads (one warp).\n  //!\n  //! .. code-block:: c++\n  //!\n  //!    #include <cub/cub.cuh>\n  //!\n  //!    __global__ void ExampleKernel(...)\n  //!    {\n  //!        // Specialize WarpReduce for type int\n  //!        typedef cub::WarpReduce<int> WarpReduce;\n  //!\n  //!        // Allocate WarpReduce shared memory for one warp\n  //!        __shared__ typename WarpReduce::TempStorage temp_storage;\n  //!\n  //!        // Obtain one input item and flag per thread\n  //!        int thread_data = ...\n  //!        int tail_flag = ...\n  //!\n  //!        // Return the warp-wide sums to each lane0\n  //!        int aggregate = WarpReduce(temp_storage).TailSegmentedSum(\n  //!            thread_data, tail_flag);\n  //!\n  //! Suppose the set of input ``thread_data`` and ``tail_flag`` across the block of threads\n  //! is ``{0, 1, 2, 3, ..., 31}`` and is ``{0, 0, 0, 1, 0, 0, 0, 1, ..., 0, 0, 0, 1}``,\n  //! respectively. The corresponding output ``aggregate`` in threads 0, 4, 8, etc. will be\n  //! ``6``, ``22``, ``38``, etc. (and is undefined in other threads).\n  //! @endrst\n  //!\n  //! @tparam ReductionOp\n  //!   **[inferred]** Binary reduction operator type having member\n  //!   `T operator()(const T &a, const T &b)`\n  //!\n  //! @param[in] input\n  //!   Calling thread's input\n  //!\n  //! @param[in] tail_flag\n  //!   Head flag denoting whether or not `input` is the start of a new segment\n  template <typename FlagT>\n  __device__ __forceinline__ T TailSegmentedSum(T input, FlagT tail_flag)\n  {\n    return TailSegmentedReduce(input, tail_flag, cub::Sum());\n  }\n\n  //! @}  end member group\n  //! @name Generic reductions\n  //! @{\n\n  //! @rst\n  //! Computes a warp-wide reduction in the calling warp using the specified binary reduction\n  //! functor. The output is valid in warp *lane*\\ :sub:`0`.\n  //!\n  //! Supports non-commutative reduction operators\n  //!\n  //! @smemwarpreuse\n  //!\n  //! Snippet\n  //! +++++++\n  //!\n  //! The code snippet below illustrates four concurrent warp max reductions within a block of\n  //! 128 threads (one per each of the 32-thread warps).\n  //!\n  //! .. code-block:: c++\n  //!\n  //!    #include <cub/cub.cuh>\n  //!\n  //!    __global__ void ExampleKernel(...)\n  //!    {\n  //!        // Specialize WarpReduce for type int\n  //!        typedef cub::WarpReduce<int> WarpReduce;\n  //!\n  //!        // Allocate WarpReduce shared memory for 4 warps\n  //!        __shared__ typename WarpReduce::TempStorage temp_storage[4];\n  //!\n  //!        // Obtain one input item per thread\n  //!        int thread_data = ...\n  //!\n  //!        // Return the warp-wide reductions to each lane0\n  //!        int warp_id = threadIdx.x / 32;\n  //!        int aggregate = WarpReduce(temp_storage[warp_id]).Reduce(\n  //!            thread_data, cub::Max());\n  //!\n  //! Suppose the set of input ``thread_data`` across the block of threads is\n  //! ``{0, 1, 2, 3, ..., 127}``. The corresponding output ``aggregate`` in threads 0, 32, 64, and\n  //! 96 will be ``31``, ``63``, ``95``, and ``127``, respectively\n  //! (and is undefined in other threads).\n  //! @endrst\n  //!\n  //! @tparam ReductionOp\n  //!   **[inferred]** Binary reduction operator type having member\n  //!   `T operator()(const T &a, const T &b)`\n  //!\n  //! @param[in] input\n  //!   Calling thread's input\n  //!\n  //! @param[in] reduction_op\n  //!   Binary reduction operator\n  template <typename ReductionOp>\n  __device__ __forceinline__ T Reduce(T input, ReductionOp reduction_op)\n  {\n    return InternalWarpReduce(temp_storage)\n      .template Reduce<true>(input, LOGICAL_WARP_THREADS, reduction_op);\n  }\n\n  //! @rst\n  //! Computes a partially-full warp-wide reduction in the calling warp using the specified binary\n  //! reduction functor. The output is valid in warp *lane*\\ :sub:`0`.\n  //!\n  //! All threads across the calling warp must agree on the same value for ``valid_items``.\n  //! Otherwise the result is undefined.\n  //!\n  //! Supports non-commutative reduction operators\n  //!\n  //! @smemwarpreuse\n  //!\n  //! Snippet\n  //! +++++++\n  //!\n  //! The code snippet below illustrates a max reduction within a single, partially-full\n  //! block of 32 threads (one warp).\n  //!\n  //! .. code-block:: c++\n  //!\n  //!    #include <cub/cub.cuh>\n  //!\n  //!    __global__ void ExampleKernel(int *d_data, int valid_items)\n  //!    {\n  //!        // Specialize WarpReduce for type int\n  //!        typedef cub::WarpReduce<int> WarpReduce;\n  //!\n  //!        // Allocate WarpReduce shared memory for one warp\n  //!        __shared__ typename WarpReduce::TempStorage temp_storage;\n  //!\n  //!        // Obtain one input item per thread if in range\n  //!        int thread_data;\n  //!        if (threadIdx.x < valid_items)\n  //!            thread_data = d_data[threadIdx.x];\n  //!\n  //!        // Return the warp-wide reductions to each lane0\n  //!        int aggregate = WarpReduce(temp_storage).Reduce(\n  //!            thread_data, cub::Max(), valid_items);\n  //!\n  //! Suppose the input ``d_data`` is ``{0, 1, 2, 3, 4, ... }`` and ``valid_items``\n  //! is ``4``. The corresponding output ``aggregate`` in thread0 is ``3`` (and is\n  //! undefined in other threads).\n  //! @endrst\n  //!\n  //! @tparam ReductionOp\n  //!   **[inferred]** Binary reduction operator type having member\n  //!   `T operator()(const T &a, const T &b)`\n  //!\n  //! @param[in] input\n  //!   Calling thread's input\n  //!\n  //! @param[in] reduction_op\n  //!   Binary reduction operator\n  //!\n  //! @param[in] valid_items\n  //!   Total number of valid items in the calling thread's logical warp\n  //!   (may be less than ``LOGICAL_WARP_THREADS``)\n  template <typename ReductionOp>\n  __device__ __forceinline__ T Reduce(T input, ReductionOp reduction_op, int valid_items)\n  {\n    return InternalWarpReduce(temp_storage).template Reduce<false>(input, valid_items, reduction_op);\n  }\n\n  //! @rst\n  //! Computes a segmented reduction in the calling warp where segments are defined by head-flags.\n  //! The reduction of each segment is returned to the first lane in that segment\n  //! (which always includes *lane*\\ :sub:`0`).\n  //!\n  //! Supports non-commutative reduction operators\n  //!\n  //! @smemwarpreuse\n  //!\n  //! Snippet\n  //! +++++++\n  //!\n  //! The code snippet below illustrates a head-segmented warp max\n  //! reduction within a block of 32 threads (one warp).\n  //!\n  //! .. code-block:: c++\n  //!\n  //!    #include <cub/cub.cuh>\n  //!\n  //!    __global__ void ExampleKernel(...)\n  //!    {\n  //!        // Specialize WarpReduce for type int\n  //!        typedef cub::WarpReduce<int> WarpReduce;\n  //!\n  //!        // Allocate WarpReduce shared memory for one warp\n  //!        __shared__ typename WarpReduce::TempStorage temp_storage;\n  //!\n  //!        // Obtain one input item and flag per thread\n  //!        int thread_data = ...\n  //!        int head_flag = ...\n  //!\n  //!        // Return the warp-wide reductions to each lane0\n  //!        int aggregate = WarpReduce(temp_storage).HeadSegmentedReduce(\n  //!            thread_data, head_flag, cub::Max());\n  //!\n  //! Suppose the set of input ``thread_data`` and ``head_flag`` across the block of threads\n  //! is ``{0, 1, 2, 3, ..., 31}`` and is ``{1, 0, 0, 0, 1, 0, 0, 0, ..., 1, 0, 0, 0}``,\n  //! respectively. The corresponding output ``aggregate`` in threads 0, 4, 8, etc. will be\n  //! ``3``, ``7``, ``11``, etc. (and is undefined in other threads).\n  //! @endrst\n  //!\n  //! @tparam ReductionOp\n  //!   **[inferred]** Binary reduction operator type having member\n  //!   `T operator()(const T &a, const T &b)`\n  //!\n  //! @param[in] input\n  //!   Calling thread's input\n  //!\n  //! @param[in] head_flag\n  //!   Head flag denoting whether or not `input` is the start of a new segment\n  //!\n  //! @param[in] reduction_op\n  //!   Reduction operator\n  template <typename ReductionOp, typename FlagT>\n  __device__ __forceinline__ T HeadSegmentedReduce(T input,\n                                                   FlagT head_flag,\n                                                   ReductionOp reduction_op)\n  {\n    return InternalWarpReduce(temp_storage)\n      .template SegmentedReduce<true>(input, head_flag, reduction_op);\n  }\n\n  //! @rst\n  //! Computes a segmented reduction in the calling warp where segments are defined by tail-flags.\n  //! The reduction of each segment is returned to the first lane in that segment\n  //! (which always includes *lane*\\ :sub:`0`).\n  //!\n  //! Supports non-commutative reduction operators\n  //!\n  //! @smemwarpreuse\n  //!\n  //! Snippet\n  //! +++++++\n  //!\n  //! The code snippet below illustrates a tail-segmented warp max\n  //! reduction within a block of 32 threads (one warp).\n  //!\n  //! .. code-block:: c++\n  //!\n  //!    #include <cub/cub.cuh>\n  //!\n  //!    __global__ void ExampleKernel(...)\n  //!    {\n  //!        // Specialize WarpReduce for type int\n  //!        typedef cub::WarpReduce<int> WarpReduce;\n  //!\n  //!        // Allocate WarpReduce shared memory for one warp\n  //!        __shared__ typename WarpReduce::TempStorage temp_storage;\n  //!\n  //!        // Obtain one input item and flag per thread\n  //!        int thread_data = ...\n  //!        int tail_flag = ...\n  //!\n  //!        // Return the warp-wide reductions to each lane0\n  //!        int aggregate = WarpReduce(temp_storage).TailSegmentedReduce(\n  //!            thread_data, tail_flag, cub::Max());\n  //!\n  //! Suppose the set of input ``thread_data`` and ``tail_flag`` across the block of threads\n  //! is ``{0, 1, 2, 3, ..., 31}`` and is ``{0, 0, 0, 1, 0, 0, 0, 1, ..., 0, 0, 0, 1}``,\n  //! respectively. The corresponding output ``aggregate`` in threads 0, 4, 8, etc. will be\n  //! ``3``, ``7``, ``11``, etc. (and is undefined in other threads).\n  //! @endrst\n  //!\n  //! @tparam ReductionOp\n  //!   **[inferred]** Binary reduction operator type having member\n  //!   `T operator()(const T &a, const T &b)`\n  //!\n  //! @param[in] input\n  //!   Calling thread's input\n  //!\n  //! @param[in] tail_flag\n  //!   Tail flag denoting whether or not \\p input is the end of the current segment\n  //!\n  //! @param[in] reduction_op\n  //!   Reduction operator\n  template <typename ReductionOp, typename FlagT>\n  __device__ __forceinline__ T TailSegmentedReduce(T input,\n                                                   FlagT tail_flag,\n                                                   ReductionOp reduction_op)\n  {\n    return InternalWarpReduce(temp_storage)\n      .template SegmentedReduce<false>(input, tail_flag, reduction_op);\n  }\n\n  //! @}  end member group\n};\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS // Do not document\ntemplate <typename T, int LEGACY_PTX_ARCH>\nclass WarpReduce<T, 1, LEGACY_PTX_ARCH>\n{\nprivate:\n  using _TempStorage = cub::NullType;\n\npublic:\n  struct InternalWarpReduce\n  {\n    struct TempStorage : Uninitialized<_TempStorage>\n    {};\n\n    __device__ __forceinline__ InternalWarpReduce(TempStorage & /*temp_storage */) {}\n\n    template <bool ALL_LANES_VALID, typename ReductionOp>\n    __device__ __forceinline__ T Reduce(T input,\n                                        int /* valid_items */,\n                                        ReductionOp /* reduction_op */)\n    {\n      return input;\n    }\n\n    template <bool HEAD_SEGMENTED, typename FlagT, typename ReductionOp>\n    __device__ __forceinline__ T SegmentedReduce(T input,\n                                                 FlagT /* flag */,\n                                                 ReductionOp /* reduction_op */)\n    {\n      return input;\n    }\n  };\n\n  using TempStorage = typename InternalWarpReduce::TempStorage;\n\n  __device__ __forceinline__ WarpReduce(TempStorage & /*temp_storage */) {}\n\n  __device__ __forceinline__ T Sum(T input) { return input; }\n\n  __device__ __forceinline__ T Sum(T input, int /* valid_items */) { return input; }\n\n  template <typename FlagT>\n  __device__ __forceinline__ T HeadSegmentedSum(T input, FlagT /* head_flag */)\n  {\n    return input;\n  }\n\n  template <typename FlagT>\n  __device__ __forceinline__ T TailSegmentedSum(T input, FlagT /* tail_flag */)\n  {\n    return input;\n  }\n\n  template <typename ReductionOp>\n  __device__ __forceinline__ T Reduce(T input, ReductionOp /* reduction_op */)\n  {\n    return input;\n  }\n\n  template <typename ReductionOp>\n  __device__ __forceinline__ T Reduce(T input,\n                                      ReductionOp /* reduction_op */,\n                                      int /* valid_items */)\n  {\n    return input;\n  }\n\n  template <typename ReductionOp, typename FlagT>\n  __device__ __forceinline__ T HeadSegmentedReduce(T input,\n                                                   FlagT /* head_flag */,\n                                                   ReductionOp /* reduction_op */)\n  {\n    return input;\n  }\n\n  template <typename ReductionOp, typename FlagT>\n  __device__ __forceinline__ T TailSegmentedReduce(T input,\n                                                   FlagT /* tail_flag */,\n                                                   ReductionOp /* reduction_op */)\n  {\n    return input;\n  }\n};\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\nCUB_NAMESPACE_END\n\n#endif // _JITIFY_INCLUDE_GUARD_EAEF1640776910F4\n", "../__assert": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___ASSERT\n#define _LIBCUDACXX___ASSERT\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"__verbose_abort\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n// This is for backwards compatibility with code that might have been enabling\n// assertions through the Debug mode previously.\n// TODO: In LLVM 16, make it an error to define _LIBCUDACXX_DEBUG\n#if defined(_LIBCUDACXX_DEBUG)\n# ifndef _LIBCUDACXX_ENABLE_ASSERTIONS\n#   define _LIBCUDACXX_ENABLE_ASSERTIONS 1\n# endif\n#endif\n\n// Automatically enable assertions when the debug mode is enabled.\n#if defined(_LIBCUDACXX_ENABLE_DEBUG_MODE)\n# ifndef _LIBCUDACXX_ENABLE_ASSERTIONS\n#   define _LIBCUDACXX_ENABLE_ASSERTIONS 1\n# endif\n#endif\n\n#ifndef _LIBCUDACXX_ENABLE_ASSERTIONS\n# define _LIBCUDACXX_ENABLE_ASSERTIONS _LIBCUDACXX_ENABLE_ASSERTIONS_DEFAULT\n#endif\n\n#if _LIBCUDACXX_ENABLE_ASSERTIONS != 0 && _LIBCUDACXX_ENABLE_ASSERTIONS != 1\n# error \"_LIBCUDACXX_ENABLE_ASSERTIONS must be set to 0 or 1\"\n#endif\n\n#if _LIBCUDACXX_ENABLE_ASSERTIONS\n# define _LIBCUDACXX_ASSERT(expression, message)                                \\\n    (_LIBCUDACXX_DIAGNOSTIC_PUSH                                                \\\n    _LIBCUDACXX_CLANG_DIAGNOSTIC_IGNORED(\"-Wassume\")                            \\\n    __builtin_expect(static_cast<bool>(expression), 1) ?                        \\\n      (void)0 :                                                                 \\\n      ::_CUDA_VSTD::__libcpp_verbose_abort(\"%s:%d: assertion %s failed: %s\", __FILE__, __LINE__, #expression, message)\n    _LIBCUDACXX_DIAGNOSTIC_POP)\n#elif 0 // !defined(_LIBCUDACXX_ASSERTIONS_DISABLE_ASSUME) && __has_builtin(__builtin_assume)\n# define _LIBCUDACXX_ASSERT(expression, message)                                \\\n    (_LIBCUDACXX_DIAGNOSTIC_PUSH                                                \\\n    _LIBCUDACXX_CLANG_DIAGNOSTIC_IGNORED(\"-Wassume\")                            \\\n    __builtin_assume(static_cast<bool>(expression))                             \\\n    _LIBCUDACXX_DIAGNOSTIC_POP)\n#else\n# define _LIBCUDACXX_ASSERT(expression, message) ((void)0)\n#endif\n\n#endif // _LIBCUDACXX___ASSERT\n", "../__concepts/__concept_macros.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Copyright (c) Facebook, Inc. and its affiliates.\n// Copyright (c) 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n//\n// Part of libcu++, the C++ Standard Library for your entire system,\n// under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _CUDA___CONCEPTS\n#define _CUDA___CONCEPTS\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#if _LIBCUDACXX_STD_VER > 11\n\n#define _LIBCUDACXX_PP_CAT_(_Xp, ...) _Xp##__VA_ARGS__\n#define _LIBCUDACXX_PP_CAT(_Xp, ...) _LIBCUDACXX_PP_CAT_(_Xp, __VA_ARGS__)\n\n#define _LIBCUDACXX_PP_CAT2_(_Xp, ...) _Xp##__VA_ARGS__\n#define _LIBCUDACXX_PP_CAT2(_Xp, ...) _LIBCUDACXX_PP_CAT2_(_Xp, __VA_ARGS__)\n\n#define _LIBCUDACXX_PP_CAT3_(_Xp, ...) _Xp##__VA_ARGS__\n#define _LIBCUDACXX_PP_CAT3(_Xp, ...) _LIBCUDACXX_PP_CAT3_(_Xp, __VA_ARGS__)\n\n#define _LIBCUDACXX_PP_CAT4_(_Xp, ...) _Xp##__VA_ARGS__\n#define _LIBCUDACXX_PP_CAT4(_Xp, ...) _LIBCUDACXX_PP_CAT4_(_Xp, __VA_ARGS__)\n\n#define _LIBCUDACXX_PP_EVAL_(_Xp, _ARGS) _Xp _ARGS\n#define _LIBCUDACXX_PP_EVAL(_Xp, ...) _LIBCUDACXX_PP_EVAL_(_Xp, (__VA_ARGS__))\n\n#define _LIBCUDACXX_PP_EVAL2_(_Xp, _ARGS) _Xp _ARGS\n#define _LIBCUDACXX_PP_EVAL2(_Xp, ...) _LIBCUDACXX_PP_EVAL2_(_Xp, (__VA_ARGS__))\n\n#define _LIBCUDACXX_PP_EXPAND(...) __VA_ARGS__\n#define _LIBCUDACXX_PP_EAT(...)\n\n#define _LIBCUDACXX_PP_CHECK(...)                                              \\\n  _LIBCUDACXX_PP_EXPAND(_LIBCUDACXX_PP_CHECK_N(__VA_ARGS__, 0, ))\n#define _LIBCUDACXX_PP_CHECK_N(_Xp, _Num, ...) _Num\n#define _LIBCUDACXX_PP_PROBE(_Xp) _Xp, 1,\n#define _LIBCUDACXX_PP_PROBE_N(_Xp, _Num) _Xp, _Num,\n\n#define _LIBCUDACXX_PP_IS_PAREN(_Xp)                                           \\\n  _LIBCUDACXX_PP_CHECK(_LIBCUDACXX_PP_IS_PAREN_PROBE _Xp)\n#define _LIBCUDACXX_PP_IS_PAREN_PROBE(...) _LIBCUDACXX_PP_PROBE(~)\n\n// The final _LIBCUDACXX_PP_EXPAND here is to avoid\n// https://stackoverflow.com/questions/5134523/msvc-doesnt-expand-va-args-correctly\n#define _LIBCUDACXX_PP_COUNT(...)                                              \\\n  _LIBCUDACXX_PP_EXPAND(_LIBCUDACXX_PP_COUNT_(                                 \\\n      __VA_ARGS__, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, \\\n      35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18,  \\\n      17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, ))            \\\n  /**/\n#define _LIBCUDACXX_PP_COUNT_(                                                 \\\n    _01, _02, _03, _04, _05, _06, _07, _08, _09, _10, _11, _12, _13, _14, _15, \\\n    _16, _17, _18, _19, _20, _21, _22, _23, _24, _25, _26, _27, _28, _29, _30, \\\n    _31, _32, _33, _34, _35, _36, _37, _38, _39, _40, _41, _42, _43, _44, _45, \\\n    _46, _47, _48, _49, _50, _Np, ...)                                         \\\n  _Np /**/\n\n#define _LIBCUDACXX_PP_IIF(_BIT) _LIBCUDACXX_PP_CAT_(_LIBCUDACXX_PP_IIF_, _BIT)\n#define _LIBCUDACXX_PP_IIF_0(_TRUE, ...) __VA_ARGS__\n#define _LIBCUDACXX_PP_IIF_1(_TRUE, ...) _TRUE\n\n#define _LIBCUDACXX_PP_LPAREN (\n\n#define _LIBCUDACXX_PP_NOT(_BIT) _LIBCUDACXX_PP_CAT_(_LIBCUDACXX_PP_NOT_, _BIT)\n#define _LIBCUDACXX_PP_NOT_0 1\n#define _LIBCUDACXX_PP_NOT_1 0\n\n#define _LIBCUDACXX_PP_EMPTY()\n#define _LIBCUDACXX_PP_COMMA() ,\n#define _LIBCUDACXX_PP_LBRACE() {\n#define _LIBCUDACXX_PP_RBRACE() }\n#define _LIBCUDACXX_PP_COMMA_IIF(_Xp)                                          \\\n  _LIBCUDACXX_PP_IIF(_Xp)(_LIBCUDACXX_PP_EMPTY, _LIBCUDACXX_PP_COMMA)() /**/\n\n#define _LIBCUDACXX_PP_FOR_EACH(_Mp, ...)                                      \\\n  _LIBCUDACXX_PP_FOR_EACH_N(_LIBCUDACXX_PP_COUNT(__VA_ARGS__), _Mp, __VA_ARGS__)\n#define _LIBCUDACXX_PP_FOR_EACH_N(_Np, _Mp, ...)                               \\\n  _LIBCUDACXX_PP_CAT2(_LIBCUDACXX_PP_FOR_EACH_, _Np)(_Mp, __VA_ARGS__)\n#define _LIBCUDACXX_PP_FOR_EACH_1(_Mp, _1) _Mp(_1)\n#define _LIBCUDACXX_PP_FOR_EACH_2(_Mp, _1, _2) _Mp(_1) _Mp(_2)\n#define _LIBCUDACXX_PP_FOR_EACH_3(_Mp, _1, _2, _3) _Mp(_1) _Mp(_2) _Mp(_3)\n#define _LIBCUDACXX_PP_FOR_EACH_4(_Mp, _1, _2, _3, _4)                         \\\n  _Mp(_1) _Mp(_2) _Mp(_3) _Mp(_4)\n#define _LIBCUDACXX_PP_FOR_EACH_5(_Mp, _1, _2, _3, _4, _5)                     \\\n  _Mp(_1) _Mp(_2) _Mp(_3) _Mp(_4) _Mp(_5)\n#define _LIBCUDACXX_PP_FOR_EACH_6(_Mp, _1, _2, _3, _4, _5, _6)                 \\\n  _Mp(_1) _Mp(_2) _Mp(_3) _Mp(_4) _Mp(_5) _Mp(_6)\n#define _LIBCUDACXX_PP_FOR_EACH_7(_Mp, _1, _2, _3, _4, _5, _6, _7)             \\\n  _Mp(_1) _Mp(_2) _Mp(_3) _Mp(_4) _Mp(_5) _Mp(_6) _Mp(_7)\n#define _LIBCUDACXX_PP_FOR_EACH_8(_Mp, _1, _2, _3, _4, _5, _6, _7, _8)         \\\n  _Mp(_1) _Mp(_2) _Mp(_3) _Mp(_4) _Mp(_5) _Mp(_6) _Mp(_7) _Mp(_8)\n\n#define _LIBCUDACXX_PP_PROBE_EMPTY_PROBE__LIBCUDACXX_PP_PROBE_EMPTY            \\\n  _LIBCUDACXX_PP_PROBE(~)\n\n#define _LIBCUDACXX_PP_PROBE_EMPTY()\n#define _LIBCUDACXX_PP_IS_NOT_EMPTY(...)                                       \\\n  _LIBCUDACXX_PP_EVAL(                                                         \\\n      _LIBCUDACXX_PP_CHECK,                                                    \\\n      _LIBCUDACXX_PP_CAT(_LIBCUDACXX_PP_PROBE_EMPTY_PROBE_,                    \\\n                         _LIBCUDACXX_PP_PROBE_EMPTY __VA_ARGS__()))            \\\n  /**/\n\n#define _LIBCUDACXX_PP_TAIL(_, ...) __VA_ARGS__\n\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_M0(_REQ)                             \\\n  _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_SELECT_(_REQ)(_REQ)\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_M1(_REQ) _LIBCUDACXX_PP_EXPAND _REQ\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_(...)                                \\\n  { _LIBCUDACXX_PP_FOR_EACH(_LIBCUDACXX_CONCEPT_FRAGMENT_REQS_M, __VA_ARGS__) }\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_SELECT_(_REQ)                        \\\n  _LIBCUDACXX_PP_CAT3(                                                         \\\n      _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_SELECT_,                               \\\n      _LIBCUDACXX_PP_EVAL(                                                     \\\n          _LIBCUDACXX_PP_CHECK,                                                \\\n          _LIBCUDACXX_PP_CAT3(_LIBCUDACXX_CONCEPT_FRAGMENT_REQS_SELECT_PROBE_, \\\n                              _REQ)))                                          \\\n  /**/\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_SELECT_PROBE_requires                \\\n  _LIBCUDACXX_PP_PROBE_N(~, 1)\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_SELECT_PROBE_noexcept                \\\n  _LIBCUDACXX_PP_PROBE_N(~, 2)\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_SELECT_PROBE_typename                \\\n  _LIBCUDACXX_PP_PROBE_N(~, 3)\n\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_SELECT_0 _LIBCUDACXX_PP_EXPAND\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_SELECT_1                             \\\n  _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_REQUIRES_OR_NOEXCEPT\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_SELECT_2                             \\\n  _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_REQUIRES_OR_NOEXCEPT\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_SELECT_3                             \\\n  _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_REQUIRES_OR_NOEXCEPT\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_REQUIRES_OR_NOEXCEPT(_REQ)           \\\n  _LIBCUDACXX_PP_CAT4(_LIBCUDACXX_CONCEPT_FRAGMENT_REQS_REQUIRES_, _REQ)\n#define _LIBCUDACXX_PP_EAT_TYPENAME_PROBE_typename _LIBCUDACXX_PP_PROBE(~)\n#define _LIBCUDACXX_PP_EAT_TYPENAME_SELECT_(_Xp, ...)                          \\\n  _LIBCUDACXX_PP_CAT3(                                                         \\\n      _LIBCUDACXX_PP_EAT_TYPENAME_SELECT_,                                     \\\n      _LIBCUDACXX_PP_EVAL(                                                     \\\n          _LIBCUDACXX_PP_CHECK,                                                \\\n          _LIBCUDACXX_PP_CAT3(_LIBCUDACXX_PP_EAT_TYPENAME_PROBE_, _Xp)))\n#define _LIBCUDACXX_PP_EAT_TYPENAME_(...)                                      \\\n  _LIBCUDACXX_PP_EVAL2(_LIBCUDACXX_PP_EAT_TYPENAME_SELECT_, __VA_ARGS__, )     \\\n  (__VA_ARGS__)\n#define _LIBCUDACXX_PP_EAT_TYPENAME_SELECT_0(...) __VA_ARGS__\n#define _LIBCUDACXX_PP_EAT_TYPENAME_SELECT_1(...)                              \\\n  _LIBCUDACXX_PP_CAT3(_LIBCUDACXX_PP_EAT_TYPENAME_, __VA_ARGS__)\n#define _LIBCUDACXX_PP_EAT_TYPENAME_typename\n\n#if (defined(__cpp_concepts) && _LIBCUDACXX_STD_VER >= 20) ||                  \\\n    defined(_LIBCUDACXX_DOXYGEN_INVOKED)\n\n#define _LIBCUDACXX_CONCEPT concept\n\n#define _LIBCUDACXX_CONCEPT_FRAGMENT(_NAME, ...)                               \\\n  concept _NAME =                                                              \\\n      _LIBCUDACXX_PP_CAT(_LIBCUDACXX_CONCEPT_FRAGMENT_REQS_, __VA_ARGS__)\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_requires(...)                        \\\n  requires(__VA_ARGS__) _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_M(_REQ)                              \\\n  _LIBCUDACXX_PP_CAT2(_LIBCUDACXX_CONCEPT_FRAGMENT_REQS_M,                     \\\n                      _LIBCUDACXX_PP_IS_PAREN(_REQ))                           \\\n  (_REQ);\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_REQUIRES_requires(...)               \\\n  requires __VA_ARGS__\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_REQUIRES_typename(...)               \\\n  typename _LIBCUDACXX_PP_EAT_TYPENAME_(__VA_ARGS__)\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_REQUIRES_noexcept(...)               \\\n  { __VA_ARGS__ }                                                              \\\n  noexcept\n\n#define _LIBCUDACXX_FRAGMENT(_NAME, ...) _NAME<__VA_ARGS__>\n\n#else\n\n#define _LIBCUDACXX_CONCEPT _LIBCUDACXX_INLINE_VAR constexpr bool\n\n#define _LIBCUDACXX_CONCEPT_FRAGMENT(_NAME, ...)                               \\\n  _LIBCUDACXX_INLINE_VISIBILITY auto _NAME##_LIBCUDACXX_CONCEPT_FRAGMENT_impl_ \\\n          _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_##__VA_ARGS__ > {}                 \\\n  template <typename... _As>                                                   \\\n  _LIBCUDACXX_INLINE_VISIBILITY char _NAME##_LIBCUDACXX_CONCEPT_FRAGMENT_(     \\\n      _Concept::_Tag<_As...> *,                                                \\\n      decltype(&_NAME##_LIBCUDACXX_CONCEPT_FRAGMENT_impl_<_As...>));           \\\n  _LIBCUDACXX_INLINE_VISIBILITY char(                                          \\\n      &_NAME##_LIBCUDACXX_CONCEPT_FRAGMENT_(...))[2] /**/\n#if defined(_MSC_VER) && !defined(__clang__)\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_TRUE(...)                                 \\\n  _Concept::_Is_true<decltype(_LIBCUDACXX_PP_FOR_EACH(                         \\\n      _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_M, __VA_ARGS__) void())>()\n#else\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_TRUE(...)                                 \\\n  !(decltype(_LIBCUDACXX_PP_FOR_EACH(_LIBCUDACXX_CONCEPT_FRAGMENT_REQS_M,      \\\n                                     __VA_ARGS__) void(),                      \\\n             false){})\n#endif\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_requires(...)                        \\\n  (__VA_ARGS__)->_Concept::_Enable_if_t < _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_2_\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_2_(...)                              \\\n  _LIBCUDACXX_CONCEPT_FRAGMENT_TRUE(__VA_ARGS__)\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_M(_REQ)                              \\\n  _LIBCUDACXX_PP_CAT2(_LIBCUDACXX_CONCEPT_FRAGMENT_REQS_M,                     \\\n                      _LIBCUDACXX_PP_IS_PAREN(_REQ))                           \\\n  (_REQ),\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_REQUIRES_requires(...)               \\\n  _Concept::_Requires<__VA_ARGS__>\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_REQUIRES_typename(...)               \\\n  static_cast<_Concept::_Tag<__VA_ARGS__> *>(nullptr)\n#if defined(_LIBCUDACXX_COMPILER_GCC)\n// GCC can't mangle noexcept expressions, so just check that the\n// expression is well-formed.\n// https://gcc.gnu.org/bugzilla/show_bug.cgi?id=70790\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_REQUIRES_noexcept(...) __VA_ARGS__\n#else\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_REQUIRES_noexcept(...)               \\\n  _Concept::_Requires<noexcept(__VA_ARGS__)>\n#endif\n\n#define _LIBCUDACXX_FRAGMENT(_NAME, ...)                                       \\\n  (1u == sizeof(_NAME##_LIBCUDACXX_CONCEPT_FRAGMENT_(                          \\\n             static_cast<_Concept::_Tag<__VA_ARGS__> *>(nullptr), nullptr)))\n\n#endif\n\n////////////////////////////////////////////////////////////////////////////////\n// _LIBCUDACXX_TEMPLATE\n// Usage:\n//   _LIBCUDACXX_TEMPLATE(typename A, typename _Bp)\n//     (requires Concept1<A> _LIBCUDACXX_AND Concept2<_Bp>)\n//   void foo(A a, _Bp b)\n//   {}\n#if (defined(__cpp_concepts) && _LIBCUDACXX_STD_VER >= 20)\n#define _LIBCUDACXX_TEMPLATE(...)                                              \\\n  template <__VA_ARGS__> _LIBCUDACXX_PP_EXPAND /**/\n#define _LIBCUDACXX_AND &&                     /**/\n#define _LIBCUDACXX_TRAILING_REQUIRES(...)                                     \\\n  -> __VA_ARGS__ _LIBCUDACXX_PP_EXPAND\n#else\n#define _LIBCUDACXX_TEMPLATE(...)                                              \\\n  template <__VA_ARGS__ _LIBCUDACXX_TEMPLATE_SFINAE_AUX_ /**/\n#define _LIBCUDACXX_AND                                                        \\\n  &&_LIBCUDACXX_true_, int > = 0, _Concept::_Enable_if_t < /**/\n#define _LIBCUDACXX_TRAILING_REQUIRES(...)                                     \\\n  -> _Concept::_Requires_t<__VA_ARGS__ _LIBCUDACXX_TRAILING_REQUIRES_AUX_\n#endif\n\n#define _LIBCUDACXX_TEMPLATE_SFINAE(...)                                       \\\n  template <__VA_ARGS__ _LIBCUDACXX_TEMPLATE_SFINAE_AUX_ /**/\n#define _LIBCUDACXX_TEMPLATE_SFINAE_AUX_(...)                                  \\\n  , bool _LIBCUDACXX_true_ = true,                                             \\\n         _Concept::_Enable_if_t <                                              \\\n                 _LIBCUDACXX_PP_CAT(_LIBCUDACXX_TEMPLATE_SFINAE_AUX_3_,        \\\n                                    __VA_ARGS__) &&                            \\\n             _LIBCUDACXX_true_,                                                \\\n         int > = 0 > /**/\n#define _LIBCUDACXX_TRAILING_REQUIRES_AUX_(...)                                \\\n  , _LIBCUDACXX_PP_CAT(_LIBCUDACXX_TEMPLATE_SFINAE_AUX_3_, __VA_ARGS__)> /**/\n#define _LIBCUDACXX_TEMPLATE_SFINAE_AUX_3_requires\n\nnamespace _Concept {\ntemplate <bool> struct _Select {};\n\ntemplate <> struct _Select<true> { template <class _Tp> using type = _Tp; };\n\ntemplate <bool _Bp, class _Tp = void>\nusing _Enable_if_t = typename _Select<_Bp>::template type<_Tp>;\n\ntemplate <class _Tp, bool _Bp>\nusing _Requires_t = typename _Select<_Bp>::template type<_Tp>;\n\ntemplate <typename...> struct _Tag;\ntemplate <class>\n_LIBCUDACXX_INLINE_VISIBILITY inline constexpr bool _Is_true() {\n  return true;\n}\n\n#if defined(_LIBCUDACXX_COMPILER_CLANG) || defined(_LIBCUDACXX_COMPILER_MSVC)\ntemplate <bool _Bp>\n_LIBCUDACXX_INLINE_VISIBILITY _Concept::_Enable_if_t<_Bp> _Requires() {}\n#else\ntemplate <bool _Bp, _Concept::_Enable_if_t<_Bp, int> = 0>\n_LIBCUDACXX_INLINE_VAR constexpr int _Requires = 0;\n#endif\n} // namespace _Concept\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n#endif //_CUDA___CONCEPTS\n", "../__concepts/arithmetic.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_ARITHMETIC_H\n#define _LIBCUDACXX___CONCEPTS_ARITHMETIC_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__type_traits/is_arithmetic.h\"\n#include \"../__type_traits/is_floating_point.h\"\n#include \"../__type_traits/is_integral.h\"\n#include \"../__type_traits/is_signed_integer.h\"\n#include \"../__type_traits/is_signed.h\"\n#include \"../__type_traits/is_unsigned_integer.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 11\n\n// [concepts.arithmetic], arithmetic concepts\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT integral = _LIBCUDACXX_TRAIT(is_integral, _Tp);\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT signed_integral = integral<_Tp> && _LIBCUDACXX_TRAIT(is_signed, _Tp);\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT unsigned_integral = integral<_Tp> && !signed_integral<_Tp>;\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT floating_point = _LIBCUDACXX_TRAIT(is_floating_point, _Tp);\n\n// Concept helpers for the internal type traits for the fundamental types.\ntemplate <class _Tp>\n_LIBCUDACXX_CONCEPT __libcpp_unsigned_integer = __libcpp_is_unsigned_integer<_Tp>::value;\ntemplate <class _Tp>\n_LIBCUDACXX_CONCEPT __libcpp_signed_integer = __libcpp_is_signed_integer<_Tp>::value;\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_ARITHMETIC_H\n", "../__concepts/assignable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_ASSIGNABLE_H\n#define _LIBCUDACXX___CONCEPTS_ASSIGNABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__concepts/common_reference_with.h\"\n#include \"../__concepts/same_as.h\"\n#include \"../__type_traits/is_reference.h\"\n#include \"../__type_traits/make_const_lvalue_ref.h\"\n#include \"../__utility/forward.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\n\n// [concept.assignable]\n\ntemplate<class _Lhs, class _Rhs>\nconcept assignable_from =\n  is_lvalue_reference_v<_Lhs> &&\n  common_reference_with<__make_const_lvalue_ref<_Lhs>, __make_const_lvalue_ref<_Rhs>> &&\n  requires (_Lhs __lhs, _Rhs&& __rhs) {\n    { __lhs = _CUDA_VSTD::forward<_Rhs>(__rhs) } -> same_as<_Lhs>;\n  };\n\n#elif _LIBCUDACXX_STD_VER > 11\n\ntemplate<class _Lhs, class _Rhs>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __assignable_from_,\n  requires(_Lhs __lhs, _Rhs&& __rhs)(\n    requires(_LIBCUDACXX_TRAIT(is_lvalue_reference, _Lhs)),\n    requires(common_reference_with<__make_const_lvalue_ref<_Lhs>, __make_const_lvalue_ref<_Rhs>>),\n    requires(same_as<_Lhs, decltype(__lhs = _CUDA_VSTD::forward<_Rhs>(__rhs))>)\n  ));\n\ntemplate<class _Lhs, class _Rhs>\n_LIBCUDACXX_CONCEPT assignable_from = _LIBCUDACXX_FRAGMENT(__assignable_from_, _Lhs, _Rhs);\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_ASSIGNABLE_H\n", "../__concepts/boolean_testable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_BOOLEAN_TESTABLE_H\n#define _LIBCUDACXX___CONCEPTS_BOOLEAN_TESTABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__concepts/convertible_to.h\"\n#include \"../__utility/forward.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\n\n// [concepts.booleantestable]\n\ntemplate<class _Tp>\nconcept __boolean_testable_impl = convertible_to<_Tp, bool>;\n\ntemplate<class _Tp>\nconcept __boolean_testable = __boolean_testable_impl<_Tp> && requires(_Tp&& __t) {\n  { !_CUDA_VSTD::forward<_Tp>(__t) } -> __boolean_testable_impl;\n};\n\n#elif _LIBCUDACXX_STD_VER > 11\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT __boolean_testable_impl = convertible_to<_Tp, bool>;\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __boolean_testable_,\n  requires(_Tp&& __t)(\n    requires(__boolean_testable_impl<_Tp>),\n    requires(__boolean_testable_impl<decltype(!_CUDA_VSTD::forward<_Tp>(__t))>)\n  ));\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT __boolean_testable = _LIBCUDACXX_FRAGMENT(__boolean_testable_, _Tp);\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_BOOLEAN_TESTABLE_H\n", "../__concepts/class_or_enum.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_CLASS_OR_ENUM_H\n#define _LIBCUDACXX___CONCEPTS_CLASS_OR_ENUM_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__type_traits/is_class.h\"\n#include \"../__type_traits/is_enum.h\"\n#include \"../__type_traits/is_union.h\"\n#include \"../__type_traits/remove_cvref.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 11\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT __class_or_enum = _LIBCUDACXX_TRAIT(is_class, _Tp) || _LIBCUDACXX_TRAIT(is_union, _Tp) || _LIBCUDACXX_TRAIT(is_enum, _Tp);\n\n// Work around Clang bug https://llvm.org/PR52970\n// TODO: remove this workaround once libc++ no longer has to support Clang 13 (it was fixed in Clang 14).\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT __workaround_52970 = _LIBCUDACXX_TRAIT(is_class, remove_cvref_t<_Tp>) || _LIBCUDACXX_TRAIT(is_union, remove_cvref_t<_Tp>);\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_CLASS_OR_ENUM_H\n", "../__concepts/common_reference_with.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_COMMON_REFERENCE_WITH_H\n#define _LIBCUDACXX___CONCEPTS_COMMON_REFERENCE_WITH_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__concepts/convertible_to.h\"\n#include \"../__concepts/same_as.h\"\n#include \"../__type_traits/common_reference.h\"\n#include \"../__type_traits/copy_cv.h\"\n#include \"../__type_traits/copy_cvref.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\n\n// [concept.commonref]\n\ntemplate<class _Tp, class _Up>\nconcept common_reference_with =\n  same_as<common_reference_t<_Tp, _Up>, common_reference_t<_Up, _Tp>> &&\n  convertible_to<_Tp, common_reference_t<_Tp, _Up>> &&\n  convertible_to<_Up, common_reference_t<_Tp, _Up>>;\n\n#elif _LIBCUDACXX_STD_VER > 11\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __common_reference_exists_,\n  requires()(\n    typename(common_reference_t<_Tp, _Up>),\n    typename(common_reference_t<_Up, _Tp>)\n  ));\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT _Common_reference_exists = _LIBCUDACXX_FRAGMENT(__common_reference_exists_, _Tp, _Up);\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __common_reference_with_,\n  requires()(\n    requires(_Common_reference_exists<_Tp, _Up>),\n    requires(same_as<common_reference_t<_Tp, _Up>, common_reference_t<_Up, _Tp>>),\n    requires(convertible_to<_Tp, common_reference_t<_Tp, _Up>>),\n    requires(convertible_to<_Up, common_reference_t<_Tp, _Up>>)\n  ));\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT common_reference_with = _LIBCUDACXX_FRAGMENT(__common_reference_with_, _Tp, _Up);\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_COMMON_REFERENCE_WITH_H\n", "../__concepts/constructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_CONSTRUCTIBLE_H\n#define _LIBCUDACXX___CONCEPTS_CONSTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__concepts/convertible_to.h\"\n#include \"../__concepts/destructible.h\"\n#include \"../__type_traits/add_lvalue_reference.h\"\n#include \"../__type_traits/is_constructible.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\n\n// [concept.constructible]\ntemplate<class _Tp, class... _Args>\nconcept constructible_from =\n    destructible<_Tp> && is_constructible_v<_Tp, _Args...>;\n\n// [concept.default.init]\ntemplate<class _Tp>\nconcept __default_initializable = requires { ::new _Tp; };\n\ntemplate<class _Tp>\nconcept default_initializable = constructible_from<_Tp> &&\n    requires { _Tp{}; } && __default_initializable<_Tp>;\n\n// [concept.moveconstructible]\ntemplate<class _Tp>\nconcept move_constructible =\n  constructible_from<_Tp, _Tp> && convertible_to<_Tp, _Tp>;\n\n// [concept.copyconstructible]\ntemplate<class _Tp>\nconcept copy_constructible =\n  move_constructible<_Tp> &&\n  constructible_from<_Tp, _Tp&> && convertible_to<_Tp&, _Tp> &&\n  constructible_from<_Tp, const _Tp&> && convertible_to<const _Tp&, _Tp> &&\n  constructible_from<_Tp, const _Tp> && convertible_to<const _Tp, _Tp>;\n\n#elif _LIBCUDACXX_STD_VER > 11\n\ntemplate<class _Tp, class... _Args>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __constructible_from_,\n  requires()(\n    requires(destructible<_Tp>),\n    requires(_LIBCUDACXX_TRAIT(is_constructible, _Tp, _Args...))\n  ));\n\ntemplate<class _Tp, class... _Args>\n_LIBCUDACXX_CONCEPT constructible_from = _LIBCUDACXX_FRAGMENT(__constructible_from_, _Tp, _Args...);\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __default_initializable_,\n  requires()(\n    (::new _Tp)\n  ));\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT __default_initializable = _LIBCUDACXX_FRAGMENT(__default_initializable_, _Tp);\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  _Default_initializable_,\n  requires(_Tp = _Tp{}) (\n    requires(constructible_from<_Tp>),\n    requires(__default_initializable<_Tp>)\n  ));\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT default_initializable = _LIBCUDACXX_FRAGMENT(_Default_initializable_, _Tp);\n\n// [concept.moveconstructible]\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __move_constructible_,\n  requires()(\n    requires(constructible_from<_Tp, _Tp>),\n    requires(convertible_to<_Tp, _Tp>)\n  ));\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT move_constructible = _LIBCUDACXX_FRAGMENT(__move_constructible_, _Tp);\n\n// [concept.copyconstructible]\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __copy_constructible_,\n  requires()(\n    requires(move_constructible<_Tp>),\n    requires(constructible_from<_Tp, add_lvalue_reference_t<_Tp>> && convertible_to<add_lvalue_reference_t<_Tp>, _Tp>),\n    requires(constructible_from<_Tp, const add_lvalue_reference_t<_Tp>> && convertible_to<const add_lvalue_reference_t<_Tp>, _Tp>),\n    requires(constructible_from<_Tp, const _Tp> && convertible_to<const _Tp, _Tp>)\n  ));\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT copy_constructible =  _LIBCUDACXX_FRAGMENT(__copy_constructible_, _Tp);\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_CONSTRUCTIBLE_H\n", "../__concepts/convertible_to.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_CONVERTIBLE_TO_H\n#define _LIBCUDACXX___CONCEPTS_CONVERTIBLE_TO_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__type_traits/is_convertible.h\"\n#include \"../__utility/declval.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\n\n// [concept.convertible]\n\ntemplate<class _From, class _To>\nconcept convertible_to =\n  is_convertible_v<_From, _To> &&\n  requires {\n    static_cast<_To>(_CUDA_VSTD::declval<_From>());\n  };\n\n#elif _LIBCUDACXX_STD_VER > 11\n\n#if defined(_LIBCUDACXX_COMPILER_MSVC)\n_LIBCUDACXX_NV_DIAG_SUPPRESS(1211) // nonstandard cast to array type ignored\n#endif // _LIBCUDACXX_COMPILER_MSVC\n\n// We cannot put this conversion check with the other constraint, as types with deleted operator will break here\ntemplate<class _From, class _To>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __test_conversion_,\n  requires()(\n    static_cast<_To>(_CUDA_VSTD::declval<_From>())\n  ));\n\ntemplate<class _From, class _To>\n_LIBCUDACXX_CONCEPT __test_conversion = _LIBCUDACXX_FRAGMENT(__test_conversion_, _From, _To);\n\ntemplate<class _From, class _To>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __convertible_to_,\n  requires()(\n    requires(_LIBCUDACXX_TRAIT(is_convertible, _From, _To)),\n    requires(__test_conversion<_From, _To>)\n  ));\n\ntemplate<class _From, class _To>\n_LIBCUDACXX_CONCEPT convertible_to = _LIBCUDACXX_FRAGMENT(__convertible_to_, _From, _To);\n\n#if defined(_LIBCUDACXX_COMPILER_MSVC)\n_LIBCUDACXX_NV_DIAG_DEFAULT(1211) // nonstandard cast to array type ignored\n#endif // _LIBCUDACXX_COMPILER_MSVC\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_CONVERTIBLE_TO_H\n", "../__concepts/copyable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_COPYABLE_H\n#define _LIBCUDACXX___CONCEPTS_COPYABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__concepts/assignable.h\"\n#include \"../__concepts/constructible.h\"\n#include \"../__concepts/movable.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\n\n// [concepts.object]\n\ntemplate<class _Tp>\nconcept copyable =\n  copy_constructible<_Tp> &&\n  movable<_Tp> &&\n  assignable_from<_Tp&, _Tp&> &&\n  assignable_from<_Tp&, const _Tp&> &&\n  assignable_from<_Tp&, const _Tp>;\n\n#elif _LIBCUDACXX_STD_VER > 11\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __copyable_,\n  requires()(\n    requires(copy_constructible<_Tp>),\n    requires(movable<_Tp>),\n    requires(assignable_from<_Tp&, _Tp&>),\n    requires(assignable_from<_Tp&, const _Tp&>),\n    requires(assignable_from<_Tp&, const _Tp>)\n  ));\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT copyable = _LIBCUDACXX_FRAGMENT(__copyable_,_Tp);\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_COPYABLE_H\n", "../__concepts/destructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_DESTRUCTIBLE_H\n#define _LIBCUDACXX___CONCEPTS_DESTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__type_traits/enable_if.h\"\n#include \"../__type_traits/is_destructible.h\"\n#include \"../__type_traits/is_object.h\"\n#include \"../__type_traits/is_nothrow_destructible.h\"\n#include \"../__type_traits/void_t.h\"\n#include \"../__utility/declval.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 11\n\n#if defined(_LIBCUDACXX_COMPILER_MSVC)\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT destructible = __is_nothrow_destructible(_Tp);\n\n#else // ^^^ _LIBCUDACXX_COMPILER_MSVC ^^^ / vvv !_LIBCUDACXX_COMPILER_MSVC vvv\n\ntemplate<class _Tp, class = void, class = void>\n_LIBCUDACXX_INLINE_VAR constexpr bool __destructible_impl = false;\n\ntemplate<class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool __destructible_impl<_Tp,\n                                   __enable_if_t<_LIBCUDACXX_TRAIT(is_object, _Tp)>,\n#if defined(_LIBCUDACXX_COMPILER_GCC)\n                                   __enable_if_t<_LIBCUDACXX_TRAIT(is_destructible, _Tp)>>\n#else // ^^^ defined(_LIBCUDACXX_COMPILER_GCC) ^^^ / vvv !_LIBCUDACXX_COMPILER_GCC vvv\n                                   __void_t<decltype(_CUDA_VSTD::declval<_Tp>().~_Tp())>>\n#endif // !_LIBCUDACXX_COMPILER_GCC\n                                   = noexcept(_CUDA_VSTD::declval<_Tp>().~_Tp());\n\ntemplate<class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool __destructible = __destructible_impl<_Tp>;\n\ntemplate<class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool __destructible<_Tp&> = true;\n\ntemplate<class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool __destructible<_Tp&&> = true;\n\ntemplate<class _Tp, size_t _Nm>\n_LIBCUDACXX_INLINE_VAR constexpr bool __destructible<_Tp[_Nm]> = __destructible<_Tp>;\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT destructible = __destructible<_Tp>;\n\n#endif // !_LIBCUDACXX_COMPILER_MSVC\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_DESTRUCTIBLE_H\n", "../__concepts/equality_comparable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_EQUALITY_COMPARABLE_H\n#define _LIBCUDACXX___CONCEPTS_EQUALITY_COMPARABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__concepts/boolean_testable.h\"\n#include \"../__concepts/common_reference_with.h\"\n#include \"../__type_traits/common_reference.h\"\n#include \"../__type_traits/make_const_lvalue_ref.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\n\n// [concept.equalitycomparable]\n\ntemplate<class _Tp, class _Up>\nconcept __weakly_equality_comparable_with =\n  requires(__make_const_lvalue_ref<_Tp> __t, __make_const_lvalue_ref<_Up> __u) {\n    { __t == __u } -> __boolean_testable;\n    { __t != __u } -> __boolean_testable;\n    { __u == __t } -> __boolean_testable;\n    { __u != __t } -> __boolean_testable;\n  };\n\ntemplate<class _Tp>\nconcept equality_comparable = __weakly_equality_comparable_with<_Tp, _Tp>;\n\ntemplate<class _Tp, class _Up>\nconcept equality_comparable_with =\n  equality_comparable<_Tp> && equality_comparable<_Up> &&\n  common_reference_with<__make_const_lvalue_ref<_Tp>, __make_const_lvalue_ref<_Up>> &&\n  equality_comparable<\n    common_reference_t<\n      __make_const_lvalue_ref<_Tp>,\n      __make_const_lvalue_ref<_Up>>> &&\n  __weakly_equality_comparable_with<_Tp, _Up>;\n\n#elif _LIBCUDACXX_STD_VER > 11\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __with_lvalue_reference_,\n  requires()(\n    typename(__make_const_lvalue_ref<_Tp>)\n  ));\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT _With_lvalue_reference = _LIBCUDACXX_FRAGMENT(__with_lvalue_reference_, _Tp);\n\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __weakly_equality_comparable_with_,\n  requires(__make_const_lvalue_ref<_Tp> __t, __make_const_lvalue_ref<_Up> __u) //\n  (requires(_With_lvalue_reference<_Tp>),\n   requires(_With_lvalue_reference<_Up>),\n   requires(__boolean_testable<decltype(__t == __u)>),\n   requires(__boolean_testable<decltype(__t != __u)>),\n   requires(__boolean_testable<decltype(__u == __t)>),\n   requires(__boolean_testable<decltype(__u != __t)>)));\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT __weakly_equality_comparable_with =\n  _LIBCUDACXX_FRAGMENT(__weakly_equality_comparable_with_, _Tp, _Up);\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT equality_comparable = __weakly_equality_comparable_with<_Tp, _Tp>;\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __equality_comparable_with_,\n  requires()(\n    requires(equality_comparable<_Tp>),\n    requires(equality_comparable<_Up>),\n    requires(common_reference_with<__make_const_lvalue_ref<_Tp>, __make_const_lvalue_ref<_Up>>),\n    requires(equality_comparable<\n    common_reference_t<\n      __make_const_lvalue_ref<_Tp>,\n      __make_const_lvalue_ref<_Up>>>),\n    requires(__weakly_equality_comparable_with<_Tp, _Up>)));\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT equality_comparable_with = _LIBCUDACXX_FRAGMENT(__equality_comparable_with_, _Tp, _Up);\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_EQUALITY_COMPARABLE_H\n", "../__concepts/invocable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_INVOCABLE_H\n#define _LIBCUDACXX___CONCEPTS_INVOCABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__functional/invoke.h\"\n#include \"../__utility/forward.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\n\n// [concept.invocable]\n\ntemplate<class _Fn, class... _Args>\nconcept invocable = requires(_Fn&& __fn, _Args&&... __args) {\n  _CUDA_VSTD::__invoke(_CUDA_VSTD::forward<_Fn>(__fn), _CUDA_VSTD::forward<_Args>(__args)...); // not required to be equality preserving\n};\n\n// [concept.regular.invocable]\n\ntemplate<class _Fn, class... _Args>\nconcept regular_invocable = invocable<_Fn, _Args...>;\n\ntemplate <class _Fun, class... _Args>\nconcept __invoke_constructible = requires(_Fun&& __fun, _Args&&... __args) {\n    static_cast<remove_cvref_t<invoke_result_t<_Fun, _Args...>>>(\n        _CUDA_VSTD::invoke(_CUDA_VSTD::forward<_Fun>(__fun), _CUDA_VSTD::forward<_Args>(__args)...));\n};\n\n#elif _LIBCUDACXX_STD_VER > 11\n\ntemplate<class _Fn, class... _Args>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  _Invocable_,\n  requires(_Fn&& __fn, _Args&&... __args)(\n    (_CUDA_VSTD::invoke(_CUDA_VSTD::forward<_Fn>(__fn), _CUDA_VSTD::forward<_Args>(__args)...))\n  ));\n\ntemplate<class _Fn, class... _Args>\n_LIBCUDACXX_CONCEPT invocable = _LIBCUDACXX_FRAGMENT(_Invocable_, _Fn, _Args...);\n\ntemplate<class _Fn, class... _Args>\n_LIBCUDACXX_CONCEPT regular_invocable = invocable<_Fn, _Args...>;\n\ntemplate <class _Fun, class... _Args>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __invoke_constructible_,\n  requires(_Fun&& __fun, _Args&&... __args)(\n    (static_cast<__remove_cvref_t<invoke_result_t<_Fun, _Args...>>>(\n        _CUDA_VSTD::invoke(_CUDA_VSTD::forward<_Fun>(__fun), _CUDA_VSTD::forward<_Args>(__args)...)))\n  ));\ntemplate <class _Fun, class... _Args>\n_LIBCUDACXX_CONCEPT __invoke_constructible = _LIBCUDACXX_FRAGMENT(__invoke_constructible_, _Fun, _Args...);\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_INVOCABLE_H\n", "../__concepts/movable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_MOVABLE_H\n#define _LIBCUDACXX___CONCEPTS_MOVABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__concepts/assignable.h\"\n#include \"../__concepts/constructible.h\"\n#include \"../__concepts/swappable.h\"\n#include \"../__type_traits/is_object.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\n\ntemplate<class _Tp>\nconcept movable =\n  is_object_v<_Tp> &&\n  move_constructible<_Tp>&&\n  assignable_from<_Tp&, _Tp> &&\n  swappable<_Tp>;\n\n#elif _LIBCUDACXX_STD_VER > 11\n\n// [concepts.object]\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  _Movable_,\n  requires()\n  (requires(is_object_v<_Tp>),\n   requires(move_constructible<_Tp>),\n   requires(assignable_from<_Tp&, _Tp>),\n   requires(swappable<_Tp>)));\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT movable = _LIBCUDACXX_FRAGMENT(_Movable_, _Tp);\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_MOVABLE_H\n", "../__concepts/predicate.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_PREDICATE_H\n#define _LIBCUDACXX___CONCEPTS_PREDICATE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__concepts/boolean_testable.h\"\n#include \"../__concepts/invocable.h\"\n#include \"../__functional/invoke.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\n\ntemplate<class _Fn, class... _Args>\nconcept predicate =\n  regular_invocable<_Fn, _Args...> && __boolean_testable<invoke_result_t<_Fn, _Args...>>;\n\n#elif _LIBCUDACXX_STD_VER > 11\n\n// [concept.predicate]\ntemplate<class _Fn, class... _Args>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  _Predicate_,\n  requires()\n  (requires(regular_invocable<_Fn, _Args...>),\n   requires(__boolean_testable<invoke_result_t<_Fn, _Args...>>)));\n\ntemplate<class _Fn, class... _Args>\n_LIBCUDACXX_CONCEPT predicate = _LIBCUDACXX_FRAGMENT(_Predicate_, _Fn, _Args...);\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_PREDICATE_H\n", "../__concepts/same_as.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_SAME_AS_H\n#define _LIBCUDACXX___CONCEPTS_SAME_AS_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__type_traits/is_same.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 11\n\n// [concept.same]\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT __same_as_impl = _IsSame<_Tp, _Up>::value;\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT same_as = __same_as_impl<_Tp, _Up> && __same_as_impl<_Up, _Tp>;\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_SAME_AS_H\n", "../__concepts/semiregular.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_SEMIREGULAR_H\n#define _LIBCUDACXX___CONCEPTS_SEMIREGULAR_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__concepts/constructible.h\"\n#include \"../__concepts/copyable.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\n\n// [concept.object]\n\ntemplate<class _Tp>\nconcept semiregular = copyable<_Tp> && default_initializable<_Tp>;\n\n#elif _LIBCUDACXX_STD_VER > 11\n\n// [concept.object]\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __semiregular_,\n  requires()(\n    requires(copyable<_Tp>),\n    requires(default_initializable<_Tp>)\n  ));\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT semiregular = _LIBCUDACXX_FRAGMENT(__semiregular_, _Tp);\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_SEMIREGULAR_H\n", "../__concepts/swappable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_SWAPPABLE_H\n#define _LIBCUDACXX___CONCEPTS_SWAPPABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__concepts/assignable.h\"\n#include \"../__concepts/class_or_enum.h\"\n#include \"../__concepts/common_reference_with.h\"\n#include \"../__concepts/constructible.h\"\n#include \"../__type_traits/extent.h\"\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_nothrow_move_assignable.h\"\n#include \"../__type_traits/is_nothrow_move_constructible.h\"\n#include \"../__type_traits/remove_cvref.h\"\n#include \"../__type_traits/type_identity.h\"\n#include \"../__type_traits/void_t.h\"\n#include \"../__utility/declval.h\"\n#include \"../__utility/exchange.h\"\n#include \"../__utility/forward.h\"\n#include \"../__utility/move.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n#if defined(_LIBCUDACXX_COMPILER_MSVC)\n_LIBCUDACXX_NV_DIAG_SUPPRESS(461) // nonstandard cast to array type ignored\n#endif // _LIBCUDACXX_COMPILER_MSVC\n\n#if _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_BEGIN_NAMESPACE_RANGES\n\n// [concept.swappable]\n\n_LIBCUDACXX_BEGIN_NAMESPACE_CPO(__swap)\n\n  template<class _Tp>\n  void swap(_Tp&, _Tp&) = delete;\n\n#if _LIBCUDACXX_STD_VER > 17\n  template<class _Tp, class _Up>\n  concept __unqualified_swappable_with =\n    (__class_or_enum<remove_cvref_t<_Tp>> || __class_or_enum<remove_cvref_t<_Up>>) &&\n    requires(_Tp&& __t, _Up&& __u) {\n      swap(_CUDA_VSTD::forward<_Tp>(__t), _CUDA_VSTD::forward<_Up>(__u));\n    };\n\n  template<class _Tp>\n  concept __exchangeable =\n    !__unqualified_swappable_with<_Tp&, _Tp&> &&\n    move_constructible<_Tp> &&\n    assignable_from<_Tp&, _Tp>;\n\n#else // ^^^ CXX20 ^^^ / vvv CXX17 vvv\n\n  template<class _Tp, class _Up>\n  _LIBCUDACXX_CONCEPT_FRAGMENT(\n    __unqualified_swappable_with_,\n    requires(_Tp&& __t, _Up&& __u)(\n      (swap(_CUDA_VSTD::forward<_Tp>(__t), _CUDA_VSTD::forward<_Up>(__u)))\n    ));\n\n  template<class _Tp, class _Up>\n  _LIBCUDACXX_CONCEPT __unqualified_swappable_with = _LIBCUDACXX_FRAGMENT(__unqualified_swappable_with_, _Tp, _Up);\n\n  template<class _Tp>\n  _LIBCUDACXX_CONCEPT_FRAGMENT(\n    __exchangeable_,\n    requires()(\n      requires(!__unqualified_swappable_with<_Tp&, _Tp&>),\n      requires(move_constructible<_Tp>),\n      requires(assignable_from<_Tp&, _Tp>)\n    ));\n\n  template<class _Tp>\n  _LIBCUDACXX_CONCEPT __exchangeable = _LIBCUDACXX_FRAGMENT(__exchangeable_, _Tp);\n#endif // _LIBCUDACXX_STD_VER < 20\n\n\n#if _LIBCUDACXX_STD_VER > 17 && !defined(_LIBCUDACXX_COMPILER_NVHPC) // nvbug4051640\n  struct __fn;\n\n_LIBCUDACXX_NV_DIAG_SUPPRESS(2642)\n  template<class _Tp, class _Up, size_t _Size>\n  concept __swappable_arrays =\n    !__unqualified_swappable_with<_Tp(&)[_Size], _Up(&)[_Size]> &&\n    extent_v<_Tp> == extent_v<_Up> &&\n    requires(_Tp(& __t)[_Size], _Up(& __u)[_Size], const __fn& __swap) {\n      __swap(__t[0], __u[0]);\n    };\n_LIBCUDACXX_NV_DIAG_DEFAULT(2642)\n\n#else\n  template<class _Tp, class _Up, size_t _Size, class = void>\n  _LIBCUDACXX_INLINE_VAR constexpr bool __swappable_arrays = false;\n#endif // _LIBCUDACXX_STD_VER < 20 || defined(_LIBCUDACXX_COMPILER_NVHPC)\n\n\n  template<class _Tp, class _Up, class = void>\n  _LIBCUDACXX_INLINE_VAR constexpr bool __noexcept_swappable_arrays = false;\n\n  struct __fn {\n    // 2.1   `S` is `(void)swap(E1, E2)`* if `E1` or `E2` has class or enumeration type and...\n    // *The name `swap` is used here unqualified.\n    _LIBCUDACXX_TEMPLATE(class _Tp, class _Up)\n      (requires __unqualified_swappable_with<_Tp, _Up>)\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr void operator()(_Tp&& __t, _Up&& __u) const\n      noexcept(noexcept(swap(_CUDA_VSTD::forward<_Tp>(__t), _CUDA_VSTD::forward<_Up>(__u))))\n    {\n      swap(_CUDA_VSTD::forward<_Tp>(__t), _CUDA_VSTD::forward<_Up>(__u));\n    }\n\n    // 2.2   Otherwise, if `E1` and `E2` are lvalues of array types with equal extent and...\n    _LIBCUDACXX_TEMPLATE(class _Tp, class _Up, size_t _Size)\n      (requires __swappable_arrays<_Tp, _Up, _Size>)\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr void operator()(_Tp(& __t)[_Size], _Up(& __u)[_Size]) const\n      noexcept(__noexcept_swappable_arrays<_Tp, _Up>)\n    {\n      // TODO(cjdb): replace with `_CUDA_VRANGES::swap_ranges`.\n      for (size_t __i = 0; __i < _Size; ++__i) {\n        (*this)(__t[__i], __u[__i]);\n      }\n    }\n\n    // 2.3   Otherwise, if `E1` and `E2` are lvalues of the same type `T` that models...\n    _LIBCUDACXX_TEMPLATE(class _Tp)\n      (requires __exchangeable<_Tp>)\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr void operator()(_Tp& __x, _Tp& __y) const\n      noexcept(_LIBCUDACXX_TRAIT(is_nothrow_move_constructible, _Tp) && _LIBCUDACXX_TRAIT(is_nothrow_move_assignable, _Tp))\n    {\n      __y = _CUDA_VSTD::exchange(__x, _CUDA_VSTD::move(__y));\n    }\n  };\n\n#if _LIBCUDACXX_STD_VER < 20 || defined(_LIBCUDACXX_COMPILER_NVHPC)\n  template<class _Tp, class _Up, class _Size>\n  _LIBCUDACXX_CONCEPT_FRAGMENT(\n    __swappable_arrays_,\n    requires(_Tp(& __t)[_Size::value], _Up(& __u)[_Size::value], const __fn& __swap)(\n      requires(!__unqualified_swappable_with<_Tp(&)[_Size::value], _Up(&)[_Size::value]>),\n      requires(_LIBCUDACXX_TRAIT(extent, _Tp) == _LIBCUDACXX_TRAIT(extent, _Up)),\n      (__swap(__t[0], __u[0]))\n    ));\n\n  template<class _Tp, class _Up, size_t _Size>\n  _LIBCUDACXX_INLINE_VAR constexpr bool __swappable_arrays<_Tp, _Up, _Size, void_t<type_identity_t<_Tp>>> =\n    _LIBCUDACXX_FRAGMENT(__swappable_arrays_, _Tp, _Up, _CUDA_VSTD::integral_constant<size_t, _Size>);\n#endif // _LIBCUDACXX_STD_VER < 20 || defined(_LIBCUDACXX_COMPILER_NVHPC)\n\n  template<class _Tp, class _Up>\n  _LIBCUDACXX_INLINE_VAR constexpr bool __noexcept_swappable_arrays<_Tp, _Up, void_t<type_identity_t<_Tp>>> =\n    noexcept(__swap::__fn{}(_CUDA_VSTD::declval<_Tp&>(), _CUDA_VSTD::declval<_Up&>()));\n\n_LIBCUDACXX_END_NAMESPACE_CPO\n\ninline namespace __cpo {\n  _LIBCUDACXX_CPO_ACCESSIBILITY auto swap = __swap::__fn{};\n} // namespace __cpo\n_LIBCUDACXX_END_NAMESPACE_RANGES\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\ntemplate<class _Tp>\nconcept swappable = requires(_Tp& __a, _Tp& __b) { _CUDA_VRANGES::swap(__a, __b); };\n\ntemplate<class _Tp, class _Up>\nconcept swappable_with =\n  common_reference_with<_Tp, _Up> &&\n  requires(_Tp&& __t, _Up&& __u) {\n    _CUDA_VRANGES::swap(_CUDA_VSTD::forward<_Tp>(__t), _CUDA_VSTD::forward<_Tp>(__t));\n    _CUDA_VRANGES::swap(_CUDA_VSTD::forward<_Up>(__u), _CUDA_VSTD::forward<_Up>(__u));\n    _CUDA_VRANGES::swap(_CUDA_VSTD::forward<_Tp>(__t), _CUDA_VSTD::forward<_Up>(__u));\n    _CUDA_VRANGES::swap(_CUDA_VSTD::forward<_Up>(__u), _CUDA_VSTD::forward<_Tp>(__t));\n  };\n#else\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __swappable_,\n  requires(_Tp& __a, _Tp& __b)(\n    (_CUDA_VRANGES::swap(__a, __b))\n  ));\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT swappable = _LIBCUDACXX_FRAGMENT(__swappable_, _Tp);\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __swappable_with_,\n  requires(_Tp&& __t, _Up&& __u)(\n    requires(common_reference_with<_Tp, _Up>),\n    (_CUDA_VRANGES::swap(_CUDA_VSTD::forward<_Tp>(__t), _CUDA_VSTD::forward<_Tp>(__t))),\n    (_CUDA_VRANGES::swap(_CUDA_VSTD::forward<_Up>(__u), _CUDA_VSTD::forward<_Up>(__u))),\n    (_CUDA_VRANGES::swap(_CUDA_VSTD::forward<_Tp>(__t), _CUDA_VSTD::forward<_Up>(__u))),\n    (_CUDA_VRANGES::swap(_CUDA_VSTD::forward<_Up>(__u), _CUDA_VSTD::forward<_Tp>(__t)))\n  ));\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT swappable_with = _LIBCUDACXX_FRAGMENT(__swappable_with_, _Tp, _Up);\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n#if defined(_LIBCUDACXX_COMPILER_MSVC)\n_LIBCUDACXX_NV_DIAG_DEFAULT(461) // nonstandard cast to array type ignored\n#endif // _LIBCUDACXX_COMPILER_MSVC\n\n\n#endif // _LIBCUDACXX___CONCEPTS_SWAPPABLE_H\n", "../__concepts/totally_ordered.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_TOTALLY_ORDERED_H\n#define _LIBCUDACXX___CONCEPTS_TOTALLY_ORDERED_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__concepts/boolean_testable.h\"\n#include \"../__concepts/equality_comparable.h\"\n#include \"../__type_traits/common_reference.h\"\n#include \"../__type_traits/make_const_lvalue_ref.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\n\n// [concept.totallyordered]\n\ntemplate<class _Tp, class _Up>\nconcept __partially_ordered_with =\n  requires(__make_const_lvalue_ref<_Tp> __t, __make_const_lvalue_ref<_Up> __u) {\n    { __t <  __u } -> __boolean_testable;\n    { __t >  __u } -> __boolean_testable;\n    { __t <= __u } -> __boolean_testable;\n    { __t >= __u } -> __boolean_testable;\n    { __u <  __t } -> __boolean_testable;\n    { __u >  __t } -> __boolean_testable;\n    { __u <= __t } -> __boolean_testable;\n    { __u >= __t } -> __boolean_testable;\n  };\n\ntemplate<class _Tp>\nconcept totally_ordered = equality_comparable<_Tp> && __partially_ordered_with<_Tp, _Tp>;\n\ntemplate<class _Tp, class _Up>\nconcept totally_ordered_with =\n  totally_ordered<_Tp> && totally_ordered<_Up> &&\n  equality_comparable_with<_Tp, _Up> &&\n  totally_ordered<\n    common_reference_t<\n      __make_const_lvalue_ref<_Tp>,\n      __make_const_lvalue_ref<_Up>>> &&\n  __partially_ordered_with<_Tp, _Up>;\n\n#elif _LIBCUDACXX_STD_VER > 11\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __partially_ordered_with_,\n  requires(__make_const_lvalue_ref<_Tp> __t, __make_const_lvalue_ref<_Up> __u) //\n  (requires(__boolean_testable<decltype(__t <  __u)>),\n   requires(__boolean_testable<decltype(__t >  __u)>),\n   requires(__boolean_testable<decltype(__t <= __u)>),\n   requires(__boolean_testable<decltype(__t >= __u)>),\n   requires(__boolean_testable<decltype(__u <  __t)>),\n   requires(__boolean_testable<decltype(__u >  __t)>),\n   requires(__boolean_testable<decltype(__u <= __t)>),\n   requires(__boolean_testable<decltype(__u >= __t)>)));\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT __partially_ordered_with = _LIBCUDACXX_FRAGMENT(__partially_ordered_with_, _Tp, _Up);\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __totally_ordered_,\n  requires()(\n    requires(equality_comparable<_Tp>),\n    requires(__partially_ordered_with<_Tp, _Tp>)\n  ));\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT totally_ordered = _LIBCUDACXX_FRAGMENT(__totally_ordered_, _Tp);\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __totally_ordered_with_,\n  requires()(\n    requires(totally_ordered<_Tp>),\n    requires(totally_ordered<_Up>),\n    requires(equality_comparable_with<_Tp, _Up>),\n    requires(totally_ordered<\n    common_reference_t<\n      __make_const_lvalue_ref<_Tp>,\n      __make_const_lvalue_ref<_Up>>>),\n    requires(__partially_ordered_with<_Tp, _Up>)));\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT totally_ordered_with = _LIBCUDACXX_FRAGMENT(__totally_ordered_with_, _Tp, _Up);;\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_TOTALLY_ORDERED_H\n", "../__functional/binary_function.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_BINARY_FUNCTION_H\n#define _LIBCUDACXX___FUNCTIONAL_BINARY_FUNCTION_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER <= 14 || defined(_LIBCUDACXX_ENABLE_CXX17_REMOVED_UNARY_BINARY_FUNCTION)\n\ntemplate <class _Arg1, class _Arg2, class _Result>\nstruct _LIBCUDACXX_TEMPLATE_VIS _LIBCUDACXX_DEPRECATED_IN_CXX11 binary_function\n{\n    typedef _Arg1   first_argument_type;\n    typedef _Arg2   second_argument_type;\n    typedef _Result result_type;\n};\n\n#endif // _LIBCUDACXX_STD_VER <= 14 || defined(_LIBCUDACXX_ENABLE_CXX17_REMOVED_UNARY_BINARY_FUNCTION)\n\ntemplate <class _Arg1, class _Arg2, class _Result> struct __binary_function_keep_layout_base {\n#if _LIBCUDACXX_STD_VER <= 17 || defined(_LIBCUDACXX_ENABLE_CXX20_REMOVED_BINDER_TYPEDEFS)\n  using first_argument_type _LIBCUDACXX_DEPRECATED_IN_CXX17 = _Arg1;\n  using second_argument_type _LIBCUDACXX_DEPRECATED_IN_CXX17 = _Arg2;\n  using result_type _LIBCUDACXX_DEPRECATED_IN_CXX17 = _Result;\n#endif\n};\n\n#if _LIBCUDACXX_STD_VER <= 14 || defined(_LIBCUDACXX_ENABLE_CXX17_REMOVED_UNARY_BINARY_FUNCTION)\n_LIBCUDACXX_DIAGNOSTIC_PUSH\n_LIBCUDACXX_CLANG_DIAGNOSTIC_IGNORED(\"-Wdeprecated-declarations\")\ntemplate <class _Arg1, class _Arg2, class _Result>\nusing __binary_function = binary_function<_Arg1, _Arg2, _Result>;\n_LIBCUDACXX_DIAGNOSTIC_POP\n#else\ntemplate <class _Arg1, class _Arg2, class _Result>\nusing __binary_function = __binary_function_keep_layout_base<_Arg1, _Arg2, _Result>;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FUNCTIONAL_BINARY_FUNCTION_H\n", "../__functional/identity.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_IDENTITY_H\n#define _LIBCUDACXX___FUNCTIONAL_IDENTITY_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__utility/forward.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\nstruct __identity {\n  template <class _Tp>\n  _LIBCUDACXX_NODISCARD_EXT _LIBCUDACXX_INLINE_VISIBILITY constexpr _Tp&& operator()(_Tp&& __t) const noexcept {\n    return _CUDA_VSTD::forward<_Tp>(__t);\n  }\n\n  using is_transparent = void;\n};\n\n#if _LIBCUDACXX_STD_VER > 11\n\nstruct identity {\n    template<class _Tp>\n    _LIBCUDACXX_NODISCARD_EXT _LIBCUDACXX_INLINE_VISIBILITY constexpr _Tp&& operator()(_Tp&& __t) const noexcept\n    {\n        return _CUDA_VSTD::forward<_Tp>(__t);\n    }\n\n    using is_transparent = void;\n};\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FUNCTIONAL_IDENTITY_H\n", "../__functional/invoke.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_INVOKE_H\n#define _LIBCUDACXX___FUNCTIONAL_INVOKE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_lvalue_reference.h\"\n#include \"../__type_traits/apply_cv.h\"\n#include \"../__type_traits/conditional.h\"\n#include \"../__type_traits/decay.h\"\n#include \"../__type_traits/enable_if.h\"\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_base_of.h\"\n#include \"../__type_traits/is_core_convertible.h\"\n#include \"../__type_traits/is_member_function_pointer.h\"\n#include \"../__type_traits/is_member_object_pointer.h\"\n#include \"../__type_traits/is_reference_wrapper.h\"\n#include \"../__type_traits/is_same.h\"\n#include \"../__type_traits/is_void.h\"\n#include \"../__type_traits/nat.h\"\n#include \"../__type_traits/remove_cv.h\"\n#include \"../__utility/declval.h\"\n#include \"../__utility/forward.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n// TODO: Disentangle the type traits and _CUDA_VSTD::invoke properly\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\nstruct __any\n{\n    _LIBCUDACXX_INLINE_VISIBILITY __any(...);\n};\n\ntemplate <class _MP, bool _IsMemberFunctionPtr, bool _IsMemberObjectPtr>\nstruct __member_pointer_traits_imp\n{\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param...), true, false>\n{\n    typedef _Class _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param..., ...), true, false>\n{\n    typedef _Class _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param..., ...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param...) const, true, false>\n{\n    typedef _Class const _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param..., ...) const, true, false>\n{\n    typedef _Class const _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param..., ...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param...) volatile, true, false>\n{\n    typedef _Class volatile _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param..., ...) volatile, true, false>\n{\n    typedef _Class volatile _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param..., ...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param...) const volatile, true, false>\n{\n    typedef _Class const volatile _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param..., ...) const volatile, true, false>\n{\n    typedef _Class const volatile _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param..., ...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param...) &, true, false>\n{\n    typedef _Class& _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param..., ...) &, true, false>\n{\n    typedef _Class& _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param..., ...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param...) const&, true, false>\n{\n    typedef _Class const& _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param..., ...) const&, true, false>\n{\n    typedef _Class const& _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param..., ...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param...) volatile&, true, false>\n{\n    typedef _Class volatile& _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param..., ...) volatile&, true, false>\n{\n    typedef _Class volatile& _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param..., ...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param...) const volatile&, true, false>\n{\n    typedef _Class const volatile& _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param..., ...) const volatile&, true, false>\n{\n    typedef _Class const volatile& _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param..., ...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param...) &&, true, false>\n{\n    typedef _Class&& _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param..., ...) &&, true, false>\n{\n    typedef _Class&& _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param..., ...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param...) const&&, true, false>\n{\n    typedef _Class const&& _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param..., ...) const&&, true, false>\n{\n    typedef _Class const&& _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param..., ...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param...) volatile&&, true, false>\n{\n    typedef _Class volatile&& _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param..., ...) volatile&&, true, false>\n{\n    typedef _Class volatile&& _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param..., ...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param...) const volatile&&, true, false>\n{\n    typedef _Class const volatile&& _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param..., ...) const volatile&&, true, false>\n{\n    typedef _Class const volatile&& _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param..., ...);\n};\n\ntemplate <class _Rp, class _Class>\nstruct __member_pointer_traits_imp<_Rp _Class::*, false, true>\n{\n    typedef _Class _ClassType;\n    typedef _Rp _ReturnType;\n};\n\ntemplate <class _MP>\nstruct __member_pointer_traits\n    : public __member_pointer_traits_imp<__remove_cv_t<_MP>,\n                    is_member_function_pointer<_MP>::value,\n                    is_member_object_pointer<_MP>::value>\n{\n//     typedef ... _ClassType;\n//     typedef ... _ReturnType;\n//     typedef ... _FnType;\n};\n\ntemplate <class _DecayedFp>\nstruct __member_pointer_class_type {};\n\ntemplate <class _Ret, class _ClassType>\nstruct __member_pointer_class_type<_Ret _ClassType::*> {\n  typedef _ClassType type;\n};\n\ntemplate <class _Fp, class _A0,\n         class _DecayFp = __decay_t<_Fp>,\n         class _DecayA0 = typename decay<_A0>::type,\n         class _ClassT = typename __member_pointer_class_type<_DecayFp>::type>\nusing __enable_if_bullet1 = __enable_if_t\n    <\n        is_member_function_pointer<_DecayFp>::value\n        && is_base_of<_ClassT, _DecayA0>::value\n    >;\n\ntemplate <class _Fp, class _A0,\n         class _DecayFp = __decay_t<_Fp>,\n         class _DecayA0 = typename decay<_A0>::type>\nusing __enable_if_bullet2 = __enable_if_t\n    <\n        is_member_function_pointer<_DecayFp>::value\n        && __is_reference_wrapper<_DecayA0>::value\n    >;\n\ntemplate <class _Fp, class _A0,\n         class _DecayFp = __decay_t<_Fp>,\n         class _DecayA0 = typename decay<_A0>::type,\n         class _ClassT = typename __member_pointer_class_type<_DecayFp>::type>\nusing __enable_if_bullet3 =__enable_if_t\n    <\n        is_member_function_pointer<_DecayFp>::value\n        && !is_base_of<_ClassT, _DecayA0>::value\n        && !__is_reference_wrapper<_DecayA0>::value\n    >;\n\ntemplate <class _Fp, class _A0,\n         class _DecayFp = __decay_t<_Fp>,\n         class _DecayA0 = typename decay<_A0>::type,\n         class _ClassT = typename __member_pointer_class_type<_DecayFp>::type>\nusing __enable_if_bullet4 = __enable_if_t\n    <\n        is_member_object_pointer<_DecayFp>::value\n        && is_base_of<_ClassT, _DecayA0>::value\n    >;\n\ntemplate <class _Fp, class _A0,\n         class _DecayFp = __decay_t<_Fp>,\n         class _DecayA0 = typename decay<_A0>::type>\nusing __enable_if_bullet5 = __enable_if_t\n    <\n        is_member_object_pointer<_DecayFp>::value\n        && __is_reference_wrapper<_DecayA0>::value\n    >;\n\ntemplate <class _Fp, class _A0,\n         class _DecayFp = __decay_t<_Fp>,\n         class _DecayA0 = typename decay<_A0>::type,\n         class _ClassT = typename __member_pointer_class_type<_DecayFp>::type>\nusing __enable_if_bullet6 = __enable_if_t\n    <\n        is_member_object_pointer<_DecayFp>::value\n        && !is_base_of<_ClassT, _DecayA0>::value\n        && !__is_reference_wrapper<_DecayA0>::value\n    >;\n\n// __invoke forward declarations\n\n// fall back - none of the bullets\n\ntemplate <class ..._Args>\n_LIBCUDACXX_INLINE_VISIBILITY __nat __invoke(__any, _Args&& ...__args);\n\n// bullets 1, 2 and 3\n\n_LIBCUDACXX_DISABLE_EXEC_CHECK\ntemplate <class _Fp, class _A0, class ..._Args,\n          class = __enable_if_bullet1<_Fp, _A0> >\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr decltype((_CUDA_VSTD::declval<_A0>().*_CUDA_VSTD::declval<_Fp>())(_CUDA_VSTD::declval<_Args>()...))\n__invoke(_Fp&& __f, _A0&& __a0, _Args&& ...__args)\n    noexcept(noexcept((static_cast<_A0&&>(__a0).*__f)(static_cast<_Args&&>(__args)...)))\n    { return           (static_cast<_A0&&>(__a0).*__f)(static_cast<_Args&&>(__args)...); }\n\n_LIBCUDACXX_DISABLE_EXEC_CHECK\ntemplate <class _Fp, class _A0, class ..._Args,\n          class = __enable_if_bullet2<_Fp, _A0> >\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr decltype((_CUDA_VSTD::declval<_A0>().get().*_CUDA_VSTD::declval<_Fp>())(_CUDA_VSTD::declval<_Args>()...))\n__invoke(_Fp&& __f, _A0&& __a0, _Args&& ...__args)\n    noexcept(noexcept((__a0.get().*__f)(static_cast<_Args&&>(__args)...)))\n    { return          (__a0.get().*__f)(static_cast<_Args&&>(__args)...); }\n\n_LIBCUDACXX_DISABLE_EXEC_CHECK\ntemplate <class _Fp, class _A0, class ..._Args,\n          class = __enable_if_bullet3<_Fp, _A0> >\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr decltype(((*_CUDA_VSTD::declval<_A0>()).*_CUDA_VSTD::declval<_Fp>())(_CUDA_VSTD::declval<_Args>()...))\n__invoke(_Fp&& __f, _A0&& __a0, _Args&& ...__args)\n    noexcept(noexcept(((*static_cast<_A0&&>(__a0)).*__f)(static_cast<_Args&&>(__args)...)))\n    { return          ((*static_cast<_A0&&>(__a0)).*__f)(static_cast<_Args&&>(__args)...); }\n\n// bullets 4, 5 and 6\n\n_LIBCUDACXX_DISABLE_EXEC_CHECK\ntemplate <class _Fp, class _A0,\n          class = __enable_if_bullet4<_Fp, _A0> >\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr decltype(_CUDA_VSTD::declval<_A0>().*_CUDA_VSTD::declval<_Fp>())\n__invoke(_Fp&& __f, _A0&& __a0)\n    noexcept(noexcept(static_cast<_A0&&>(__a0).*__f))\n    { return          static_cast<_A0&&>(__a0).*__f; }\n\n_LIBCUDACXX_DISABLE_EXEC_CHECK\ntemplate <class _Fp, class _A0,\n          class = __enable_if_bullet5<_Fp, _A0> >\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr decltype(_CUDA_VSTD::declval<_A0>().get().*_CUDA_VSTD::declval<_Fp>())\n__invoke(_Fp&& __f, _A0&& __a0)\n    noexcept(noexcept(__a0.get().*__f))\n    { return          __a0.get().*__f; }\n\n_LIBCUDACXX_DISABLE_EXEC_CHECK\ntemplate <class _Fp, class _A0,\n          class = __enable_if_bullet6<_Fp, _A0> >\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr decltype((*_CUDA_VSTD::declval<_A0>()).*_CUDA_VSTD::declval<_Fp>())\n__invoke(_Fp&& __f, _A0&& __a0)\n    noexcept(noexcept((*static_cast<_A0&&>(__a0)).*__f))\n    { return          (*static_cast<_A0&&>(__a0)).*__f; }\n\n// bullet 7\n\n_LIBCUDACXX_DISABLE_EXEC_CHECK\ntemplate <class _Fp, class ..._Args>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr decltype(_CUDA_VSTD::declval<_Fp>()(_CUDA_VSTD::declval<_Args>()...))\n__invoke(_Fp&& __f, _Args&& ...__args)\n    noexcept(noexcept(static_cast<_Fp&&>(__f)(static_cast<_Args&&>(__args)...)))\n    { return          static_cast<_Fp&&>(__f)(static_cast<_Args&&>(__args)...); }\n\n// __invokable\ntemplate <class _Ret, class _Fp, class ..._Args>\nstruct __invokable_r\n{\n  template <class _XFp, class ..._XArgs>\n  _LIBCUDACXX_INLINE_VISIBILITY\n  static decltype(_CUDA_VSTD::__invoke(_CUDA_VSTD::declval<_XFp>(), _CUDA_VSTD::declval<_XArgs>()...)) __try_call(int);\n\n  template <class _XFp, class ..._XArgs>\n  _LIBCUDACXX_INLINE_VISIBILITY\n  static __nat __try_call(...);\n\n  // FIXME: Check that _Ret, _Fp, and _Args... are all complete types, cv void,\n  // or incomplete array types as required by the standard.\n  using _Result = decltype(__try_call<_Fp, _Args...>(0));\n\n  using type = __conditional_t<\n      _IsNotSame<_Result, __nat>::value,\n      __conditional_t<is_void<_Ret>::value, true_type, __is_core_convertible<_Result, _Ret> >,\n      false_type>;\n  static const bool value = type::value;\n};\ntemplate <class _Fp, class ..._Args>\nusing __invokable = __invokable_r<void, _Fp, _Args...>;\n\ntemplate <bool _IsInvokable, bool _IsCVVoid, class _Ret, class _Fp, class ..._Args>\nstruct __nothrow_invokable_r_imp {\n  static const bool value = false;\n};\n\ntemplate <class _Ret, class _Fp, class ..._Args>\nstruct __nothrow_invokable_r_imp<true, false, _Ret, _Fp, _Args...>\n{\n    typedef __nothrow_invokable_r_imp _ThisT;\n\n    template <class _Tp>\n    _LIBCUDACXX_INLINE_VISIBILITY\n    static void __test_noexcept(_Tp) noexcept;\n\n    static const bool value = noexcept(_ThisT::__test_noexcept<_Ret>(\n        _CUDA_VSTD::__invoke(declval<_Fp>(), _CUDA_VSTD::declval<_Args>()...)));\n};\n\ntemplate <class _Ret, class _Fp, class ..._Args>\nstruct __nothrow_invokable_r_imp<true, true, _Ret, _Fp, _Args...>\n{\n    static const bool value = noexcept(\n        _CUDA_VSTD::__invoke(_CUDA_VSTD::declval<_Fp>(), _CUDA_VSTD::declval<_Args>()...));\n};\n\ntemplate <class _Ret, class _Fp, class ..._Args>\nusing __nothrow_invokable_r =\n    __nothrow_invokable_r_imp<\n            __invokable_r<_Ret, _Fp, _Args...>::value,\n            is_void<_Ret>::value,\n            _Ret, _Fp, _Args...\n    >;\n\ntemplate <class _Fp, class ..._Args>\nusing __nothrow_invokable =\n    __nothrow_invokable_r_imp<\n            __invokable<_Fp, _Args...>::value,\n            true, void, _Fp, _Args...\n    >;\n\ntemplate <class _Fp, class ..._Args>\nstruct __invoke_of\n    : public enable_if<\n        __invokable<_Fp, _Args...>::value,\n        typename __invokable_r<void, _Fp, _Args...>::_Result>\n{\n#if defined(__NVCC__) && defined(__CUDACC_EXTENDED_LAMBDA__) && \\\n   !defined(__CUDA_ARCH__)\n  static_assert(!__nv_is_extended_device_lambda_closure_type(_Fp),\n                \"Attempt to use an extended __device__ lambda in a context \"\n                \"that requires querying its return type in host code. Use a \"\n                \"named function object, a __host__ __device__ lambda, or \"\n                \"cuda::proclaim_return_type instead.\");\n#endif\n};\n\ntemplate <class _Ret, bool = is_void<_Ret>::value>\nstruct __invoke_void_return_wrapper\n{\n    template <class ..._Args>\n    _LIBCUDACXX_INLINE_VISIBILITY static _Ret __call(_Args&&... __args) {\n        return _CUDA_VSTD::__invoke(_CUDA_VSTD::forward<_Args>(__args)...);\n    }\n};\n\ntemplate <class _Ret>\nstruct __invoke_void_return_wrapper<_Ret, true>\n{\n    template <class ..._Args>\n    _LIBCUDACXX_INLINE_VISIBILITY static void __call(_Args&&... __args) {\n        _CUDA_VSTD::__invoke(_CUDA_VSTD::forward<_Args>(__args)...);\n    }\n};\n\n#if _LIBCUDACXX_STD_VER > 11\n\n// is_invocable\n\ntemplate <class _Fn, class ..._Args>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_invocable\n    : integral_constant<bool, __invokable<_Fn, _Args...>::value> {};\n\ntemplate <class _Ret, class _Fn, class ..._Args>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_invocable_r\n    : integral_constant<bool, __invokable_r<_Ret, _Fn, _Args...>::value> {};\n\ntemplate <class _Fn, class ..._Args>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_invocable_v = is_invocable<_Fn, _Args...>::value;\n\ntemplate <class _Ret, class _Fn, class ..._Args>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_invocable_r_v = is_invocable_r<_Ret, _Fn, _Args...>::value;\n\n// is_nothrow_invocable\n\ntemplate <class _Fn, class ..._Args>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_invocable\n    : integral_constant<bool, __nothrow_invokable<_Fn, _Args...>::value> {};\n\ntemplate <class _Ret, class _Fn, class ..._Args>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_invocable_r\n    : integral_constant<bool, __nothrow_invokable_r<_Ret, _Fn, _Args...>::value> {};\n\ntemplate <class _Fn, class ..._Args>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_invocable_v = is_nothrow_invocable<_Fn, _Args...>::value;\n\ntemplate <class _Ret, class _Fn, class ..._Args>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_invocable_r_v = is_nothrow_invocable_r<_Ret, _Fn, _Args...>::value;\n\ntemplate <class _Fn, class... _Args>\nstruct _LIBCUDACXX_TEMPLATE_VIS invoke_result\n    : __invoke_of<_Fn, _Args...>\n{\n};\n\ntemplate <class _Fn, class... _Args>\nusing invoke_result_t = typename invoke_result<_Fn, _Args...>::type;\n\ntemplate <class _Fn, class ..._Args>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr invoke_result_t<_Fn, _Args...>\ninvoke(_Fn&& __f, _Args&&... __args)\n    noexcept(is_nothrow_invocable_v<_Fn, _Args...>)\n{\n    return _CUDA_VSTD::__invoke(_CUDA_VSTD::forward<_Fn>(__f), _CUDA_VSTD::forward<_Args>(__args)...);\n}\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FUNCTIONAL_INVOKE_H\n", "../__functional/operations.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_OPERATIONS_H\n#define _LIBCUDACXX___FUNCTIONAL_OPERATIONS_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__functional/binary_function.h\"\n#include \"../__functional/unary_function.h\"\n#include \"../__utility/forward.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n// Arithmetic operations\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS plus\n    : __binary_function<_Tp, _Tp, _Tp>\n{\n    typedef _Tp __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator()(const _Tp& __x, const _Tp& __y) const\n        {return __x + __y;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(plus);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS plus<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _T1, class _T2>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_T1&& __t, _T2&& __u) const\n        noexcept(noexcept(_CUDA_VSTD::forward<_T1>(__t) + _CUDA_VSTD::forward<_T2>(__u)))\n        -> decltype(      _CUDA_VSTD::forward<_T1>(__t) + _CUDA_VSTD::forward<_T2>(__u))\n        { return          _CUDA_VSTD::forward<_T1>(__t) + _CUDA_VSTD::forward<_T2>(__u); }\n    typedef void is_transparent;\n};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS minus\n    : __binary_function<_Tp, _Tp, _Tp>\n{\n    typedef _Tp __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator()(const _Tp& __x, const _Tp& __y) const\n        {return __x - __y;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(minus);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS minus<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _T1, class _T2>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_T1&& __t, _T2&& __u) const\n        noexcept(noexcept(_CUDA_VSTD::forward<_T1>(__t) - _CUDA_VSTD::forward<_T2>(__u)))\n        -> decltype(      _CUDA_VSTD::forward<_T1>(__t) - _CUDA_VSTD::forward<_T2>(__u))\n        { return          _CUDA_VSTD::forward<_T1>(__t) - _CUDA_VSTD::forward<_T2>(__u); }\n    typedef void is_transparent;\n};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS multiplies\n    : __binary_function<_Tp, _Tp, _Tp>\n{\n    typedef _Tp __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator()(const _Tp& __x, const _Tp& __y) const\n        {return __x * __y;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(multiplies);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS multiplies<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _T1, class _T2>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_T1&& __t, _T2&& __u) const\n        noexcept(noexcept(_CUDA_VSTD::forward<_T1>(__t) * _CUDA_VSTD::forward<_T2>(__u)))\n        -> decltype(      _CUDA_VSTD::forward<_T1>(__t) * _CUDA_VSTD::forward<_T2>(__u))\n        { return          _CUDA_VSTD::forward<_T1>(__t) * _CUDA_VSTD::forward<_T2>(__u); }\n    typedef void is_transparent;\n};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS divides\n    : __binary_function<_Tp, _Tp, _Tp>\n{\n    typedef _Tp __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator()(const _Tp& __x, const _Tp& __y) const\n        {return __x / __y;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(divides);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS divides<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _T1, class _T2>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_T1&& __t, _T2&& __u) const\n        noexcept(noexcept(_CUDA_VSTD::forward<_T1>(__t) / _CUDA_VSTD::forward<_T2>(__u)))\n        -> decltype(      _CUDA_VSTD::forward<_T1>(__t) / _CUDA_VSTD::forward<_T2>(__u))\n        { return          _CUDA_VSTD::forward<_T1>(__t) / _CUDA_VSTD::forward<_T2>(__u); }\n    typedef void is_transparent;\n};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS modulus\n    : __binary_function<_Tp, _Tp, _Tp>\n{\n    typedef _Tp __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator()(const _Tp& __x, const _Tp& __y) const\n        {return __x % __y;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(modulus);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS modulus<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _T1, class _T2>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_T1&& __t, _T2&& __u) const\n        noexcept(noexcept(_CUDA_VSTD::forward<_T1>(__t) % _CUDA_VSTD::forward<_T2>(__u)))\n        -> decltype(      _CUDA_VSTD::forward<_T1>(__t) % _CUDA_VSTD::forward<_T2>(__u))\n        { return          _CUDA_VSTD::forward<_T1>(__t) % _CUDA_VSTD::forward<_T2>(__u); }\n    typedef void is_transparent;\n};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS negate\n    : __unary_function<_Tp, _Tp>\n{\n    typedef _Tp __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator()(const _Tp& __x) const\n        {return -__x;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(negate);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS negate<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _Tp>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_Tp&& __x) const\n        noexcept(noexcept(- _CUDA_VSTD::forward<_Tp>(__x)))\n        -> decltype(      - _CUDA_VSTD::forward<_Tp>(__x))\n        { return          - _CUDA_VSTD::forward<_Tp>(__x); }\n    typedef void is_transparent;\n};\n#endif\n\n// Bitwise operations\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS bit_and\n    : __binary_function<_Tp, _Tp, _Tp>\n{\n    typedef _Tp __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator()(const _Tp& __x, const _Tp& __y) const\n        {return __x & __y;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(bit_and);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS bit_and<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _T1, class _T2>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_T1&& __t, _T2&& __u) const\n        noexcept(noexcept(_CUDA_VSTD::forward<_T1>(__t) & _CUDA_VSTD::forward<_T2>(__u)))\n        -> decltype(      _CUDA_VSTD::forward<_T1>(__t) & _CUDA_VSTD::forward<_T2>(__u))\n        { return          _CUDA_VSTD::forward<_T1>(__t) & _CUDA_VSTD::forward<_T2>(__u); }\n    typedef void is_transparent;\n};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\nstruct _LIBCUDACXX_TEMPLATE_VIS bit_not\n    : __unary_function<_Tp, _Tp>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator()(const _Tp& __x) const\n        {return ~__x;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(bit_not);\n\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS bit_not<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _Tp>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_Tp&& __x) const\n        noexcept(noexcept(~_CUDA_VSTD::forward<_Tp>(__x)))\n        -> decltype(      ~_CUDA_VSTD::forward<_Tp>(__x))\n        { return          ~_CUDA_VSTD::forward<_Tp>(__x); }\n    typedef void is_transparent;\n};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS bit_or\n    : __binary_function<_Tp, _Tp, _Tp>\n{\n    typedef _Tp __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator()(const _Tp& __x, const _Tp& __y) const\n        {return __x | __y;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(bit_or);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS bit_or<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _T1, class _T2>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_T1&& __t, _T2&& __u) const\n        noexcept(noexcept(_CUDA_VSTD::forward<_T1>(__t) | _CUDA_VSTD::forward<_T2>(__u)))\n        -> decltype(      _CUDA_VSTD::forward<_T1>(__t) | _CUDA_VSTD::forward<_T2>(__u))\n        { return          _CUDA_VSTD::forward<_T1>(__t) | _CUDA_VSTD::forward<_T2>(__u); }\n    typedef void is_transparent;\n};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS bit_xor\n    : __binary_function<_Tp, _Tp, _Tp>\n{\n    typedef _Tp __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator()(const _Tp& __x, const _Tp& __y) const\n        {return __x ^ __y;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(bit_xor);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS bit_xor<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _T1, class _T2>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_T1&& __t, _T2&& __u) const\n        noexcept(noexcept(_CUDA_VSTD::forward<_T1>(__t) ^ _CUDA_VSTD::forward<_T2>(__u)))\n        -> decltype(      _CUDA_VSTD::forward<_T1>(__t) ^ _CUDA_VSTD::forward<_T2>(__u))\n        { return          _CUDA_VSTD::forward<_T1>(__t) ^ _CUDA_VSTD::forward<_T2>(__u); }\n    typedef void is_transparent;\n};\n#endif\n\n// Comparison operations\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS equal_to\n    : __binary_function<_Tp, _Tp, bool>\n{\n    typedef bool __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    bool operator()(const _Tp& __x, const _Tp& __y) const\n        {return __x == __y;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(equal_to);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS equal_to<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _T1, class _T2>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_T1&& __t, _T2&& __u) const\n        noexcept(noexcept(_CUDA_VSTD::forward<_T1>(__t) == _CUDA_VSTD::forward<_T2>(__u)))\n        -> decltype(      _CUDA_VSTD::forward<_T1>(__t) == _CUDA_VSTD::forward<_T2>(__u))\n        { return          _CUDA_VSTD::forward<_T1>(__t) == _CUDA_VSTD::forward<_T2>(__u); }\n    typedef void is_transparent;\n};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS not_equal_to\n    : __binary_function<_Tp, _Tp, bool>\n{\n    typedef bool __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    bool operator()(const _Tp& __x, const _Tp& __y) const\n        {return __x != __y;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(not_equal_to);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS not_equal_to<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _T1, class _T2>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_T1&& __t, _T2&& __u) const\n        noexcept(noexcept(_CUDA_VSTD::forward<_T1>(__t) != _CUDA_VSTD::forward<_T2>(__u)))\n        -> decltype(      _CUDA_VSTD::forward<_T1>(__t) != _CUDA_VSTD::forward<_T2>(__u))\n        { return          _CUDA_VSTD::forward<_T1>(__t) != _CUDA_VSTD::forward<_T2>(__u); }\n    typedef void is_transparent;\n};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS less\n    : __binary_function<_Tp, _Tp, bool>\n{\n    typedef bool __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    bool operator()(const _Tp& __x, const _Tp& __y) const\n        {return __x < __y;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(less);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS less<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _T1, class _T2>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_T1&& __t, _T2&& __u) const\n        noexcept(noexcept(_CUDA_VSTD::forward<_T1>(__t) < _CUDA_VSTD::forward<_T2>(__u)))\n        -> decltype(      _CUDA_VSTD::forward<_T1>(__t) < _CUDA_VSTD::forward<_T2>(__u))\n        { return          _CUDA_VSTD::forward<_T1>(__t) < _CUDA_VSTD::forward<_T2>(__u); }\n    typedef void is_transparent;\n};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS less_equal\n    : __binary_function<_Tp, _Tp, bool>\n{\n    typedef bool __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    bool operator()(const _Tp& __x, const _Tp& __y) const\n        {return __x <= __y;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(less_equal);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS less_equal<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _T1, class _T2>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_T1&& __t, _T2&& __u) const\n        noexcept(noexcept(_CUDA_VSTD::forward<_T1>(__t) <= _CUDA_VSTD::forward<_T2>(__u)))\n        -> decltype(      _CUDA_VSTD::forward<_T1>(__t) <= _CUDA_VSTD::forward<_T2>(__u))\n        { return          _CUDA_VSTD::forward<_T1>(__t) <= _CUDA_VSTD::forward<_T2>(__u); }\n    typedef void is_transparent;\n};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS greater_equal\n    : __binary_function<_Tp, _Tp, bool>\n{\n    typedef bool __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    bool operator()(const _Tp& __x, const _Tp& __y) const\n        {return __x >= __y;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(greater_equal);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS greater_equal<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _T1, class _T2>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_T1&& __t, _T2&& __u) const\n        noexcept(noexcept(_CUDA_VSTD::forward<_T1>(__t) >= _CUDA_VSTD::forward<_T2>(__u)))\n        -> decltype(      _CUDA_VSTD::forward<_T1>(__t) >= _CUDA_VSTD::forward<_T2>(__u))\n        { return          _CUDA_VSTD::forward<_T1>(__t) >= _CUDA_VSTD::forward<_T2>(__u); }\n    typedef void is_transparent;\n};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS greater\n    : __binary_function<_Tp, _Tp, bool>\n{\n    typedef bool __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    bool operator()(const _Tp& __x, const _Tp& __y) const\n        {return __x > __y;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(greater);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS greater<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _T1, class _T2>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_T1&& __t, _T2&& __u) const\n        noexcept(noexcept(_CUDA_VSTD::forward<_T1>(__t) > _CUDA_VSTD::forward<_T2>(__u)))\n        -> decltype(      _CUDA_VSTD::forward<_T1>(__t) > _CUDA_VSTD::forward<_T2>(__u))\n        { return          _CUDA_VSTD::forward<_T1>(__t) > _CUDA_VSTD::forward<_T2>(__u); }\n    typedef void is_transparent;\n};\n#endif\n\n// Logical operations\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS logical_and\n    : __binary_function<_Tp, _Tp, bool>\n{\n    typedef bool __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    bool operator()(const _Tp& __x, const _Tp& __y) const\n        {return __x && __y;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(logical_and);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS logical_and<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _T1, class _T2>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_T1&& __t, _T2&& __u) const\n        noexcept(noexcept(_CUDA_VSTD::forward<_T1>(__t) && _CUDA_VSTD::forward<_T2>(__u)))\n        -> decltype(      _CUDA_VSTD::forward<_T1>(__t) && _CUDA_VSTD::forward<_T2>(__u))\n        { return          _CUDA_VSTD::forward<_T1>(__t) && _CUDA_VSTD::forward<_T2>(__u); }\n    typedef void is_transparent;\n};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS logical_not\n    : __unary_function<_Tp, bool>\n{\n    typedef bool __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    bool operator()(const _Tp& __x) const\n        {return !__x;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(logical_not);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS logical_not<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _Tp>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_Tp&& __x) const\n        noexcept(noexcept(!_CUDA_VSTD::forward<_Tp>(__x)))\n        -> decltype(      !_CUDA_VSTD::forward<_Tp>(__x))\n        { return          !_CUDA_VSTD::forward<_Tp>(__x); }\n    typedef void is_transparent;\n};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS logical_or\n    : __binary_function<_Tp, _Tp, bool>\n{\n    typedef bool __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    bool operator()(const _Tp& __x, const _Tp& __y) const\n        {return __x || __y;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(logical_or);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS logical_or<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _T1, class _T2>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_T1&& __t, _T2&& __u) const\n        noexcept(noexcept(_CUDA_VSTD::forward<_T1>(__t) || _CUDA_VSTD::forward<_T2>(__u)))\n        -> decltype(      _CUDA_VSTD::forward<_T1>(__t) || _CUDA_VSTD::forward<_T2>(__u))\n        { return          _CUDA_VSTD::forward<_T1>(__t) || _CUDA_VSTD::forward<_T2>(__u); }\n    typedef void is_transparent;\n};\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FUNCTIONAL_OPERATIONS_H\n", "../__functional/perfect_forward.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_PERFECT_FORWARD_H\n#define _LIBCUDACXX___FUNCTIONAL_PERFECT_FORWARD_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__functional/invoke.h\"\n#include \"../__type_traits/enable_if.h\"\n#include \"../__type_traits/is_constructible.h\"\n#include \"../__type_traits/is_nothrow_constructible.h\"\n#include \"../__utility/declval.h\"\n#include \"../__utility/forward.h\"\n#include \"../__utility/integer_sequence.h\"\n#include \"../__utility/move.h\"\n\n#include \"../tuple\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 14\n\ntemplate <class _Op, class _Indices, class... _BoundArgs>\nstruct __perfect_forward_impl;\n\ntemplate <class _Op, size_t... _Idx, class... _BoundArgs>\nstruct __perfect_forward_impl<_Op, index_sequence<_Idx...>, _BoundArgs...> {\nprivate:\n  tuple<_BoundArgs...> __bound_args_;\n\npublic:\n  _LIBCUDACXX_TEMPLATE(class... _Args)\n    (requires is_constructible_v<tuple<_BoundArgs...>, _Args&&...>)\n  _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY\n  explicit constexpr __perfect_forward_impl(_Args&&... __bound_args)\n    noexcept(is_nothrow_constructible_v<tuple<_BoundArgs...>, _Args&&...>)\n    : __bound_args_(_CUDA_VSTD::forward<_Args>(__bound_args)...) {}\n\n  __perfect_forward_impl(__perfect_forward_impl const&) = default;\n  __perfect_forward_impl(__perfect_forward_impl&&) = default;\n\n  __perfect_forward_impl& operator=(__perfect_forward_impl const&) = default;\n  __perfect_forward_impl& operator=(__perfect_forward_impl&&) = default;\n\n  _LIBCUDACXX_TEMPLATE(class... _Args)\n    (requires is_invocable_v<_Op, _BoundArgs&..., _Args...>)\n  _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY constexpr auto operator()(_Args&&... __args) &\n    noexcept(noexcept(_Op()(_CUDA_VSTD::get<_Idx>(__bound_args_)..., _CUDA_VSTD::forward<_Args>(__args)...)))\n    -> decltype(      _Op()(_CUDA_VSTD::get<_Idx>(__bound_args_)..., _CUDA_VSTD::forward<_Args>(__args)...))\n    { return          _Op()(_CUDA_VSTD::get<_Idx>(__bound_args_)..., _CUDA_VSTD::forward<_Args>(__args)...); }\n\n  _LIBCUDACXX_TEMPLATE(class... _Args)\n    (requires (!is_invocable_v<_Op, _BoundArgs&..., _Args...>))\n  _LIBCUDACXX_INLINE_VISIBILITY auto operator()(_Args&&...) & = delete;\n\n  _LIBCUDACXX_TEMPLATE(class... _Args)\n    (requires is_invocable_v<_Op, _BoundArgs const&..., _Args...>)\n  _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY constexpr auto operator()(_Args&&... __args) const&\n    noexcept(noexcept(_Op()(_CUDA_VSTD::get<_Idx>(__bound_args_)..., _CUDA_VSTD::forward<_Args>(__args)...)))\n    -> decltype(      _Op()(_CUDA_VSTD::get<_Idx>(__bound_args_)..., _CUDA_VSTD::forward<_Args>(__args)...))\n    { return          _Op()(_CUDA_VSTD::get<_Idx>(__bound_args_)..., _CUDA_VSTD::forward<_Args>(__args)...); }\n\n  _LIBCUDACXX_TEMPLATE(class... _Args)\n    (requires (!is_invocable_v<_Op, _BoundArgs const&..., _Args...>))\n  _LIBCUDACXX_INLINE_VISIBILITY auto operator()(_Args&&...) const& = delete;\n\n  _LIBCUDACXX_TEMPLATE(class... _Args)\n    (requires is_invocable_v<_Op, _BoundArgs..., _Args...>)\n  _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY constexpr auto operator()(_Args&&... __args) &&\n    noexcept(noexcept(_Op()(_CUDA_VSTD::get<_Idx>(_CUDA_VSTD::move(__bound_args_))..., _CUDA_VSTD::forward<_Args>(__args)...)))\n    -> decltype(      _Op()(_CUDA_VSTD::get<_Idx>(_CUDA_VSTD::move(__bound_args_))..., _CUDA_VSTD::forward<_Args>(__args)...))\n    { return          _Op()(_CUDA_VSTD::get<_Idx>(_CUDA_VSTD::move(__bound_args_))..., _CUDA_VSTD::forward<_Args>(__args)...); }\n\n  _LIBCUDACXX_TEMPLATE(class... _Args)\n    (requires (!is_invocable_v<_Op, _BoundArgs..., _Args...>))\n  _LIBCUDACXX_INLINE_VISIBILITY auto operator()(_Args&&...) && = delete;\n\n  _LIBCUDACXX_TEMPLATE(class... _Args)\n    (requires is_invocable_v<_Op, _BoundArgs const..., _Args...>)\n  _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY constexpr auto operator()(_Args&&... __args) const&&\n    noexcept(noexcept(_Op()(_CUDA_VSTD::get<_Idx>(_CUDA_VSTD::move(__bound_args_))..., _CUDA_VSTD::forward<_Args>(__args)...)))\n    -> decltype(      _Op()(_CUDA_VSTD::get<_Idx>(_CUDA_VSTD::move(__bound_args_))..., _CUDA_VSTD::forward<_Args>(__args)...))\n    { return          _Op()(_CUDA_VSTD::get<_Idx>(_CUDA_VSTD::move(__bound_args_))..., _CUDA_VSTD::forward<_Args>(__args)...); }\n\n  _LIBCUDACXX_TEMPLATE(class... _Args)\n    (requires (!is_invocable_v<_Op, _BoundArgs const..., _Args...>))\n  _LIBCUDACXX_INLINE_VISIBILITY auto operator()(_Args&&...) const&& = delete;\n};\n\n// __perfect_forward implements a perfect-forwarding call wrapper as explained in [func.require].\ntemplate <class _Op, class ..._Args>\nusing __perfect_forward = __perfect_forward_impl<_Op, index_sequence_for<_Args...>, _Args...>;\n\n#endif // _LIBCUDACXX_STD_VER > 14\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FUNCTIONAL_PERFECT_FORWARD_H\n", "../__functional/unary_function.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_UNARY_FUNCTION_H\n#define _LIBCUDACXX___FUNCTIONAL_UNARY_FUNCTION_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER <= 14 || defined(_LIBCUDACXX_ENABLE_CXX17_REMOVED_UNARY_BINARY_FUNCTION)\n\ntemplate <class _Arg, class _Result>\nstruct _LIBCUDACXX_TEMPLATE_VIS _LIBCUDACXX_DEPRECATED_IN_CXX11 unary_function\n{\n    typedef _Arg    argument_type;\n    typedef _Result result_type;\n};\n\n#endif // _LIBCUDACXX_STD_VER <= 14\n\ntemplate <class _Arg, class _Result> struct __unary_function_keep_layout_base {\n#if _LIBCUDACXX_STD_VER <= 17 || defined(_LIBCUDACXX_ENABLE_CXX20_REMOVED_BINDER_TYPEDEFS)\n  using argument_type _LIBCUDACXX_DEPRECATED_IN_CXX17 = _Arg;\n  using result_type _LIBCUDACXX_DEPRECATED_IN_CXX17 = _Result;\n#endif\n};\n\n#if _LIBCUDACXX_STD_VER <= 14 || defined(_LIBCUDACXX_ENABLE_CXX17_REMOVED_UNARY_BINARY_FUNCTION)\n_LIBCUDACXX_DIAGNOSTIC_PUSH\n_LIBCUDACXX_CLANG_DIAGNOSTIC_IGNORED(\"-Wdeprecated-declarations\")\ntemplate <class _Arg, class _Result>\nusing __unary_function = unary_function<_Arg, _Result>;\n_LIBCUDACXX_DIAGNOSTIC_POP\n#else\ntemplate <class _Arg, class _Result>\nusing __unary_function = __unary_function_keep_layout_base<_Arg, _Result>;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FUNCTIONAL_UNARY_FUNCTION_H\n", "../__functional/unwrap_ref.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_UNWRAP_REF_H\n#define _LIBCUDACXX___FUNCTIONAL_UNWRAP_REF_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp>\nstruct __unwrap_reference { typedef _LIBCUDACXX_NODEBUG_TYPE _Tp type; };\n\ntemplate <class _Tp>\nclass reference_wrapper;\n\ntemplate <class _Tp>\nstruct __unwrap_reference<reference_wrapper<_Tp> > { typedef _LIBCUDACXX_NODEBUG_TYPE _Tp& type; };\n\ntemplate <class _Tp>\nstruct decay;\n\n#if _LIBCUDACXX_STD_VER > 17\ntemplate <class _Tp>\nstruct unwrap_reference : __unwrap_reference<_Tp> { };\n\ntemplate <class _Tp>\nusing unwrap_reference_t = typename unwrap_reference<_Tp>::type;\n\ntemplate <class _Tp>\nstruct unwrap_ref_decay : unwrap_reference<typename decay<_Tp>::type> { };\n\ntemplate <class _Tp>\nusing unwrap_ref_decay_t = typename unwrap_ref_decay<_Tp>::type;\n#endif // _LIBCUDACXX_STD_VER > 17\n\ntemplate <class _Tp>\nstruct __unwrap_ref_decay\n#if _LIBCUDACXX_STD_VER > 17\n    : unwrap_ref_decay<_Tp>\n#else\n    : __unwrap_reference<typename decay<_Tp>::type>\n#endif\n{ };\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FUNCTIONAL_UNWRAP_REF_H\n", "../__functional/weak_result_type.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_WEAK_RESULT_TYPE_H\n#define _LIBCUDACXX___FUNCTIONAL_WEAK_RESULT_TYPE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__functional/binary_function.h\"\n#include \"../__functional/invoke.h\"\n#include \"../__functional/unary_function.h\"\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_same.h\"\n#include \"../__utility/declval.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp>\nstruct __has_result_type\n{\nprivate:\n    template <class _Up> _LIBCUDACXX_INLINE_VISIBILITY static false_type __test(...);\n    template <class _Up> _LIBCUDACXX_INLINE_VISIBILITY static true_type __test(typename _Up::result_type* = 0);\npublic:\n    static const bool value = decltype(__test<_Tp>(0))::value;\n};\n\n// __weak_result_type\n\ntemplate <class _Tp>\nstruct __derives_from_unary_function\n{\nprivate:\n    struct __two {char __lx; char __lxx;};\n    static _LIBCUDACXX_INLINE_VISIBILITY __two __test(...);\n    template <class _Ap, class _Rp>\n        static _LIBCUDACXX_INLINE_VISIBILITY __unary_function<_Ap, _Rp>\n        __test(const volatile __unary_function<_Ap, _Rp>*);\n\npublic:\n    static const bool value = !is_same<decltype(__test((_Tp*)0)), __two>::value;\n    typedef decltype(__test((_Tp*)0)) type;\n};\n\ntemplate <class _Tp>\nstruct __derives_from_binary_function\n{\nprivate:\n    struct __two {char __lx; char __lxx;};\n    static __two _LIBCUDACXX_INLINE_VISIBILITY __test(...);\n    template <class _A1, class _A2, class _Rp>\n        static _LIBCUDACXX_INLINE_VISIBILITY __binary_function<_A1, _A2, _Rp>\n        __test(const volatile __binary_function<_A1, _A2, _Rp>*);\n\npublic:\n    static const bool value = !is_same<decltype(__test((_Tp*)0)), __two>::value;\n    typedef decltype(__test((_Tp*)0)) type;\n};\n\ntemplate <class _Tp, bool = __derives_from_unary_function<_Tp>::value>\nstruct __maybe_derive_from_unary_function  // bool is true\n    : public __derives_from_unary_function<_Tp>::type\n{\n};\n\ntemplate <class _Tp>\nstruct __maybe_derive_from_unary_function<_Tp, false>\n{\n};\n\ntemplate <class _Tp, bool = __derives_from_binary_function<_Tp>::value>\nstruct __maybe_derive_from_binary_function  // bool is true\n    : public __derives_from_binary_function<_Tp>::type\n{\n};\n\ntemplate <class _Tp>\nstruct __maybe_derive_from_binary_function<_Tp, false>\n{\n};\n\ntemplate <class _Tp, bool = __has_result_type<_Tp>::value>\nstruct __weak_result_type_imp // bool is true\n    : public __maybe_derive_from_unary_function<_Tp>,\n      public __maybe_derive_from_binary_function<_Tp>\n{\n#if _LIBCUDACXX_STD_VER <= 17 || defined(_LIBCUDACXX_ENABLE_CXX20_REMOVED_BINDER_TYPEDEFS)\n    using result_type _LIBCUDACXX_NODEBUG_TYPE _LIBCUDACXX_DEPRECATED_IN_CXX17 = typename _Tp::result_type;\n#endif\n};\n\ntemplate <class _Tp>\nstruct __weak_result_type_imp<_Tp, false>\n    : public __maybe_derive_from_unary_function<_Tp>,\n      public __maybe_derive_from_binary_function<_Tp>\n{\n};\n\ntemplate <class _Tp>\nstruct __weak_result_type\n    : public __weak_result_type_imp<_Tp>\n{\n};\n\n// 0 argument case\n\ntemplate <class _Rp>\nstruct __weak_result_type<_Rp ()>\n{\n#if _LIBCUDACXX_STD_VER <= 17 || defined(_LIBCUDACXX_ENABLE_CXX20_REMOVED_BINDER_TYPEDEFS)\n    using result_type _LIBCUDACXX_NODEBUG_TYPE _LIBCUDACXX_DEPRECATED_IN_CXX17 = _Rp;\n#endif\n};\n\ntemplate <class _Rp>\nstruct __weak_result_type<_Rp (&)()>\n{\n#if _LIBCUDACXX_STD_VER <= 17 || defined(_LIBCUDACXX_ENABLE_CXX20_REMOVED_BINDER_TYPEDEFS)\n    using result_type _LIBCUDACXX_NODEBUG_TYPE _LIBCUDACXX_DEPRECATED_IN_CXX17 = _Rp;\n#endif\n};\n\ntemplate <class _Rp>\nstruct __weak_result_type<_Rp (*)()>\n{\n#if _LIBCUDACXX_STD_VER <= 17 || defined(_LIBCUDACXX_ENABLE_CXX20_REMOVED_BINDER_TYPEDEFS)\n    using result_type _LIBCUDACXX_NODEBUG_TYPE _LIBCUDACXX_DEPRECATED_IN_CXX17 = _Rp;\n#endif\n};\n\n// 1 argument case\n\ntemplate <class _Rp, class _A1>\nstruct __weak_result_type<_Rp (_A1)>\n    : public __unary_function<_A1, _Rp>\n{\n};\n\ntemplate <class _Rp, class _A1>\nstruct __weak_result_type<_Rp (&)(_A1)>\n    : public __unary_function<_A1, _Rp>\n{\n};\n\ntemplate <class _Rp, class _A1>\nstruct __weak_result_type<_Rp (*)(_A1)>\n    : public __unary_function<_A1, _Rp>\n{\n};\n\ntemplate <class _Rp, class _Cp>\nstruct __weak_result_type<_Rp (_Cp::*)()>\n    : public __unary_function<_Cp*, _Rp>\n{\n};\n\ntemplate <class _Rp, class _Cp>\nstruct __weak_result_type<_Rp (_Cp::*)() const>\n    : public __unary_function<const _Cp*, _Rp>\n{\n};\n\ntemplate <class _Rp, class _Cp>\nstruct __weak_result_type<_Rp (_Cp::*)() volatile>\n    : public __unary_function<volatile _Cp*, _Rp>\n{\n};\n\ntemplate <class _Rp, class _Cp>\nstruct __weak_result_type<_Rp (_Cp::*)() const volatile>\n    : public __unary_function<const volatile _Cp*, _Rp>\n{\n};\n\n// 2 argument case\n\ntemplate <class _Rp, class _A1, class _A2>\nstruct __weak_result_type<_Rp (_A1, _A2)>\n    : public __binary_function<_A1, _A2, _Rp>\n{\n};\n\ntemplate <class _Rp, class _A1, class _A2>\nstruct __weak_result_type<_Rp (*)(_A1, _A2)>\n    : public __binary_function<_A1, _A2, _Rp>\n{\n};\n\ntemplate <class _Rp, class _A1, class _A2>\nstruct __weak_result_type<_Rp (&)(_A1, _A2)>\n    : public __binary_function<_A1, _A2, _Rp>\n{\n};\n\ntemplate <class _Rp, class _Cp, class _A1>\nstruct __weak_result_type<_Rp (_Cp::*)(_A1)>\n    : public __binary_function<_Cp*, _A1, _Rp>\n{\n};\n\ntemplate <class _Rp, class _Cp, class _A1>\nstruct __weak_result_type<_Rp (_Cp::*)(_A1) const>\n    : public __binary_function<const _Cp*, _A1, _Rp>\n{\n};\n\ntemplate <class _Rp, class _Cp, class _A1>\nstruct __weak_result_type<_Rp (_Cp::*)(_A1) volatile>\n    : public __binary_function<volatile _Cp*, _A1, _Rp>\n{\n};\n\ntemplate <class _Rp, class _Cp, class _A1>\nstruct __weak_result_type<_Rp (_Cp::*)(_A1) const volatile>\n    : public __binary_function<const volatile _Cp*, _A1, _Rp>\n{\n};\n\n// 3 or more arguments\n\ntemplate <class _Rp, class _A1, class _A2, class _A3, class ..._A4>\nstruct __weak_result_type<_Rp (_A1, _A2, _A3, _A4...)>\n{\n#if _LIBCUDACXX_STD_VER <= 17 || defined(_LIBCUDACXX_ENABLE_CXX20_REMOVED_BINDER_TYPEDEFS)\n    using result_type _LIBCUDACXX_NODEBUG_TYPE _LIBCUDACXX_DEPRECATED_IN_CXX17 = _Rp;\n#endif\n};\n\ntemplate <class _Rp, class _A1, class _A2, class _A3, class ..._A4>\nstruct __weak_result_type<_Rp (&)(_A1, _A2, _A3, _A4...)>\n{\n#if _LIBCUDACXX_STD_VER <= 17 || defined(_LIBCUDACXX_ENABLE_CXX20_REMOVED_BINDER_TYPEDEFS)\n    using result_type _LIBCUDACXX_NODEBUG_TYPE _LIBCUDACXX_DEPRECATED_IN_CXX17 = _Rp;\n#endif\n};\n\ntemplate <class _Rp, class _A1, class _A2, class _A3, class ..._A4>\nstruct __weak_result_type<_Rp (*)(_A1, _A2, _A3, _A4...)>\n{\n#if _LIBCUDACXX_STD_VER <= 17 || defined(_LIBCUDACXX_ENABLE_CXX20_REMOVED_BINDER_TYPEDEFS)\n    using result_type _LIBCUDACXX_NODEBUG_TYPE _LIBCUDACXX_DEPRECATED_IN_CXX17 = _Rp;\n#endif\n};\n\ntemplate <class _Rp, class _Cp, class _A1, class _A2, class ..._A3>\nstruct __weak_result_type<_Rp (_Cp::*)(_A1, _A2, _A3...)>\n{\n#if _LIBCUDACXX_STD_VER <= 17 || defined(_LIBCUDACXX_ENABLE_CXX20_REMOVED_BINDER_TYPEDEFS)\n    using result_type _LIBCUDACXX_NODEBUG_TYPE _LIBCUDACXX_DEPRECATED_IN_CXX17 = _Rp;\n#endif\n};\n\ntemplate <class _Rp, class _Cp, class _A1, class _A2, class ..._A3>\nstruct __weak_result_type<_Rp (_Cp::*)(_A1, _A2, _A3...) const>\n{\n#if _LIBCUDACXX_STD_VER <= 17 || defined(_LIBCUDACXX_ENABLE_CXX20_REMOVED_BINDER_TYPEDEFS)\n    using result_type _LIBCUDACXX_NODEBUG_TYPE _LIBCUDACXX_DEPRECATED_IN_CXX17 = _Rp;\n#endif\n};\n\ntemplate <class _Rp, class _Cp, class _A1, class _A2, class ..._A3>\nstruct __weak_result_type<_Rp (_Cp::*)(_A1, _A2, _A3...) volatile>\n{\n#if _LIBCUDACXX_STD_VER <= 17 || defined(_LIBCUDACXX_ENABLE_CXX20_REMOVED_BINDER_TYPEDEFS)\n    using result_type _LIBCUDACXX_NODEBUG_TYPE _LIBCUDACXX_DEPRECATED_IN_CXX17 = _Rp;\n#endif\n};\n\ntemplate <class _Rp, class _Cp, class _A1, class _A2, class ..._A3>\nstruct __weak_result_type<_Rp (_Cp::*)(_A1, _A2, _A3...) const volatile>\n{\n#if _LIBCUDACXX_STD_VER <= 17 || defined(_LIBCUDACXX_ENABLE_CXX20_REMOVED_BINDER_TYPEDEFS)\n    using result_type _LIBCUDACXX_NODEBUG_TYPE _LIBCUDACXX_DEPRECATED_IN_CXX17 = _Rp;\n#endif\n};\n\ntemplate <class _Tp, class ..._Args>\nstruct __invoke_return\n{\n    typedef decltype(_CUDA_VSTD::__invoke(declval<_Tp>(), declval<_Args>()...)) type;\n};\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FUNCTIONAL_WEAK_RESULT_TYPE_H\n", "../__fwd/array.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===---------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===---------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FWD_ARRAY_H\n#define _LIBCUDACXX___FWD_ARRAY_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp, size_t _Size>\nstruct _LIBCUDACXX_TEMPLATE_VIS array;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FWD_ARRAY_H\n", "../__fwd/get.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FWD_GET_H\n#define _LIBCUDACXX___FWD_GET_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__fwd/array.h\"\n#include \"../__fwd/pair.h\"\n#include \"../__fwd/tuple.h\"\n#include \"../__tuple_dir/tuple_element.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <size_t _Ip, class ..._Tp>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\ntypename tuple_element<_Ip, tuple<_Tp...> >::type&\nget(tuple<_Tp...>&) noexcept;\n\ntemplate <size_t _Ip, class ..._Tp>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nconst typename tuple_element<_Ip, tuple<_Tp...> >::type&\nget(const tuple<_Tp...>&) noexcept;\n\ntemplate <size_t _Ip, class ..._Tp>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\ntypename tuple_element<_Ip, tuple<_Tp...> >::type&&\nget(tuple<_Tp...>&&) noexcept;\n\ntemplate <size_t _Ip, class ..._Tp>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nconst typename tuple_element<_Ip, tuple<_Tp...> >::type&&\nget(const tuple<_Tp...>&&) noexcept;\n\ntemplate <size_t _Ip, class _T1, class _T2>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\ntypename tuple_element<_Ip, pair<_T1, _T2> >::type&\nget(pair<_T1, _T2>&) noexcept;\n\ntemplate <size_t _Ip, class _T1, class _T2>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nconst typename tuple_element<_Ip, pair<_T1, _T2> >::type&\nget(const pair<_T1, _T2>&) noexcept;\n\ntemplate <size_t _Ip, class _T1, class _T2>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\ntypename tuple_element<_Ip, pair<_T1, _T2> >::type&&\nget(pair<_T1, _T2>&&) noexcept;\n\ntemplate <size_t _Ip, class _T1, class _T2>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nconst typename tuple_element<_Ip, pair<_T1, _T2> >::type&&\nget(const pair<_T1, _T2>&&) noexcept;\n\ntemplate <size_t _Ip, class _Tp, size_t _Size>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n_Tp&\nget(array<_Tp, _Size>&) noexcept;\n\ntemplate <size_t _Ip, class _Tp, size_t _Size>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nconst _Tp&\nget(const array<_Tp, _Size>&) noexcept;\n\ntemplate <size_t _Ip, class _Tp, size_t _Size>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n_Tp&&\nget(array<_Tp, _Size>&&) noexcept;\n\ntemplate <size_t _Ip, class _Tp, size_t _Size>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nconst _Tp&&\nget(const array<_Tp, _Size>&&) noexcept;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FWD_GET_H\n", "../__fwd/hash.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===---------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===---------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FWD_HASH_H\n#define _LIBCUDACXX___FWD_HASH_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class>\nstruct _LIBCUDACXX_TEMPLATE_VIS hash;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FWD_HASH_H\n", "../__fwd/memory_resource.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FWD_MEMORY_RESOURCE_H\n#define _LIBCUDACXX___FWD_MEMORY_RESOURCE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\nnamespace pmr {\ntemplate <class _ValueType>\nclass _LIBCUDACXX_TEMPLATE_VIS polymorphic_allocator;\n} // namespace pmr\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FWD_MEMORY_RESOURCE_H\n", "../__fwd/pair.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===---------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===---------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FWD_PAIR_H\n#define _LIBCUDACXX___FWD_PAIR_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class, class>\nstruct _LIBCUDACXX_TEMPLATE_VIS pair;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FWD_PAIR_H\n", "../__fwd/tuple.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===---------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===---------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FWD_TUPLE_H\n#define _LIBCUDACXX___FWD_TUPLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class...>\nclass _LIBCUDACXX_TEMPLATE_VIS tuple;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FWD_TUPLE_H\n", "../__iterator/access.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___ITERATOR_ACCESS_H\n#define _LIBCUDACXX___ITERATOR_ACCESS_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp, size_t _Np>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n_Tp*\nbegin(_Tp (&__array)[_Np])\n{\n    return __array;\n}\n\ntemplate <class _Tp, size_t _Np>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n_Tp*\nend(_Tp (&__array)[_Np])\n{\n    return __array + _Np;\n}\n\ntemplate <class _Cp>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX14\nauto\nbegin(_Cp& __c) -> decltype(__c.begin())\n{\n    return __c.begin();\n}\n\ntemplate <class _Cp>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX14\nauto\nbegin(const _Cp& __c) -> decltype(__c.begin())\n{\n    return __c.begin();\n}\n\ntemplate <class _Cp>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX14\nauto\nend(_Cp& __c) -> decltype(__c.end())\n{\n    return __c.end();\n}\n\ntemplate <class _Cp>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX14\nauto\nend(const _Cp& __c) -> decltype(__c.end())\n{\n    return __c.end();\n}\n\n#if _LIBCUDACXX_STD_VER > 11\n\ntemplate <class _Cp>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nauto cbegin(const _Cp& __c) -> decltype(_CUDA_VSTD::begin(__c))\n{\n    return _CUDA_VSTD::begin(__c);\n}\n\ntemplate <class _Cp>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nauto cend(const _Cp& __c) -> decltype(_CUDA_VSTD::end(__c))\n{\n    return _CUDA_VSTD::end(__c);\n}\n\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___ITERATOR_ACCESS_H\n", "../__iterator/incrementable_traits.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___ITERATOR_INCREMENTABLE_TRAITS_H\n#define _LIBCUDACXX___ITERATOR_INCREMENTABLE_TRAITS_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__concepts/arithmetic.h\"\n#include \"../__type_traits/conditional.h\"\n#include \"../__type_traits/enable_if.h\"\n#include \"../__type_traits/is_const.h\"\n#include \"../__type_traits/is_pointer.h\"\n#include \"../__type_traits/is_primary_template.h\"\n#include \"../__type_traits/make_signed.h\"\n#include \"../__type_traits/remove_cvref.h\"\n#include \"../__type_traits/void_t.h\"\n#include \"../__utility/declval.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\n\n// [incrementable.traits]\ntemplate<class> struct incrementable_traits {};\n\ntemplate<class _Tp>\nrequires is_object_v<_Tp>\nstruct incrementable_traits<_Tp*> {\n  using difference_type = ptrdiff_t;\n};\n\ntemplate<class _Ip>\nstruct incrementable_traits<const _Ip> : incrementable_traits<_Ip> {};\n\ntemplate<class _Tp>\nconcept __has_member_difference_type = requires { typename _Tp::difference_type; };\n\ntemplate<__has_member_difference_type _Tp>\nstruct incrementable_traits<_Tp> {\n  using difference_type = typename _Tp::difference_type;\n};\n\ntemplate<class _Tp>\nconcept __has_integral_minus =\n  requires(const _Tp& __x, const _Tp& __y) {\n    { __x - __y } -> integral;\n  };\n\ntemplate<__has_integral_minus _Tp>\nrequires (!__has_member_difference_type<_Tp>)\nstruct incrementable_traits<_Tp> {\n  using difference_type = make_signed_t<decltype(declval<_Tp>() - declval<_Tp>())>;\n};\n\ntemplate <class>\nstruct _LIBCUDACXX_TEMPLATE_VIS iterator_traits;\n\n// Let `RI` be `remove_cvref_t<I>`. The type `iter_difference_t<I>` denotes\n// `incrementable_traits<RI>::difference_type` if `iterator_traits<RI>` names a specialization\n// generated from the primary template, and `iterator_traits<RI>::difference_type` otherwise.\ntemplate <class _Ip>\nusing iter_difference_t = typename conditional_t<__is_primary_template<iterator_traits<remove_cvref_t<_Ip>>>::value,\n                                                 incrementable_traits<remove_cvref_t<_Ip> >,\n                                                 iterator_traits<remove_cvref_t<_Ip> > >::difference_type;\n\n#elif _LIBCUDACXX_STD_VER > 14\n\n// [incrementable.traits]\ntemplate<class, class = void> struct incrementable_traits {};\n\ntemplate<class _Tp>\nstruct incrementable_traits<_Tp*, enable_if_t<_LIBCUDACXX_TRAIT(is_object, _Tp)>> {\n  using difference_type = ptrdiff_t;\n};\n\ntemplate<class _Ip>\nstruct incrementable_traits<const _Ip> : incrementable_traits<_Ip> {};\n\ntemplate<class _Tp, class = void>\n_LIBCUDACXX_INLINE_VAR constexpr bool __has_member_difference_type = false;\n\ntemplate<class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool __has_member_difference_type<_Tp, void_t<typename _Tp::difference_type>> = true;\n\ntemplate<class _Tp,class = void, class = void>\n_LIBCUDACXX_INLINE_VAR constexpr bool __has_integral_minus = false;\n\n// In C++17 we get issues trying to bind void* to a const& so special case it here\ntemplate<class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool __has_integral_minus<_Tp, enable_if_t<!same_as<_Tp, void*>>,\n                                                                void_t<decltype(_CUDA_VSTD::declval<const _Tp&>() - _CUDA_VSTD::declval<const _Tp&>())>>\n  = integral<decltype(_CUDA_VSTD::declval<const _Tp&>() - _CUDA_VSTD::declval<const _Tp&>())>;\n\ntemplate <class _Tp>\nstruct incrementable_traits<_Tp, enable_if_t<!_LIBCUDACXX_TRAIT(is_pointer, _Tp) && !_LIBCUDACXX_TRAIT(is_const, _Tp) &&\n                                              __has_member_difference_type<_Tp>>> {\n  using difference_type = typename _Tp::difference_type;\n};\n\ntemplate <class _Tp>\nstruct incrementable_traits<_Tp, enable_if_t<!_LIBCUDACXX_TRAIT(is_pointer, _Tp) && !_LIBCUDACXX_TRAIT(is_const, _Tp) &&\n                                             !__has_member_difference_type<_Tp> && __has_integral_minus<_Tp>>> {\n  using difference_type = make_signed_t<decltype(declval<_Tp>() - declval<_Tp>())>;\n};\n\ntemplate <class, class = void>\nstruct _LIBCUDACXX_TEMPLATE_VIS iterator_traits;\n\n// Let `RI` be `remove_cvref_t<I>`. The type `iter_difference_t<I>` denotes\n// `incrementable_traits<RI>::difference_type` if `iterator_traits<RI>` names a specialization\n// generated from the primary template, and `iterator_traits<RI>::difference_type` otherwise.\ntemplate <class _Ip>\nusing iter_difference_t = typename conditional_t<__is_primary_template<iterator_traits<remove_cvref_t<_Ip>>>::value,\n                                                 incrementable_traits<remove_cvref_t<_Ip> >,\n                                                 iterator_traits<remove_cvref_t<_Ip> > >::difference_type;\n\n#endif // _LIBCUDACXX_STD_VER > 14\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___ITERATOR_INCREMENTABLE_TRAITS_H\n", "../__iterator/iterator_traits.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___ITERATOR_ITERATOR_TRAITS_H\n#define _LIBCUDACXX___ITERATOR_ITERATOR_TRAITS_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__concepts/arithmetic.h\"\n#include \"../__concepts/constructible.h\"\n#include \"../__concepts/convertible_to.h\"\n#include \"../__concepts/copyable.h\"\n#include \"../__concepts/equality_comparable.h\"\n#include \"../__concepts/same_as.h\"\n#include \"../__concepts/totally_ordered.h\"\n#include \"../__fwd/pair.h\"\n#include \"../__iterator/incrementable_traits.h\"\n#include \"../__iterator/readable_traits.h\"\n#include \"../__type_traits/add_const.h\"\n#include \"../__type_traits/is_convertible.h\"\n#include \"../__type_traits/is_primary_template.h\"\n#include \"../__type_traits/remove_cv.h\"\n#include \"../__type_traits/void_t.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\n\ntemplate <class _Tp>\nusing __with_reference = _Tp&;\n\ntemplate <class _Tp>\nconcept __can_reference = requires {\n  typename __with_reference<_Tp>;\n};\n\ntemplate <class _Tp>\nconcept __dereferenceable = requires(_Tp& __t) {\n  { *__t } -> __can_reference; // not required to be equality-preserving\n};\n\n// [iterator.traits]\ntemplate<__dereferenceable _Tp>\nusing iter_reference_t = decltype(*declval<_Tp&>());\n\ntemplate <class>\nstruct _LIBCUDACXX_TEMPLATE_VIS iterator_traits;\n\n#elif _LIBCUDACXX_STD_VER > 14\n\ntemplate <class _Tp>\nusing __with_reference = _Tp&;\n\ntemplate <class _Tp>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __can_reference_,\n  requires() //\n  (typename(__with_reference<_Tp>)));\n\ntemplate <class _Tp>\n_LIBCUDACXX_CONCEPT __can_reference = _LIBCUDACXX_FRAGMENT(__can_reference_, _Tp);\n\n#if defined(__clang__)\n_Pragma(\"clang diagnostic push\")\n_Pragma(\"clang diagnostic ignored \\\"-Wvoid-ptr-dereference\\\"\")\n#endif\ntemplate <class _Tp>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __dereferenceable_,\n  requires(_Tp& __t)(\n    requires(__can_reference<decltype(*__t)>)\n  ));\n#if defined(__clang__)\n_Pragma(\"clang diagnostic pop\")\n#endif\n\ntemplate <class _Tp>\n_LIBCUDACXX_CONCEPT __dereferenceable = _LIBCUDACXX_FRAGMENT(__dereferenceable_, _Tp);\n\n// [iterator.traits]\ntemplate<class _Tp>\nusing iter_reference_t = enable_if_t<__dereferenceable<_Tp>, decltype(*_CUDA_VSTD::declval<_Tp&>())>;\n\ntemplate <class, class>\nstruct _LIBCUDACXX_TEMPLATE_VIS iterator_traits;\n#else\ntemplate <class>\nstruct _LIBCUDACXX_TEMPLATE_VIS iterator_traits;\n#endif // _LIBCUDACXX_STD_VER > 11\n\nstruct _LIBCUDACXX_TEMPLATE_VIS input_iterator_tag {};\nstruct _LIBCUDACXX_TEMPLATE_VIS output_iterator_tag {};\nstruct _LIBCUDACXX_TEMPLATE_VIS forward_iterator_tag       : public input_iterator_tag {};\nstruct _LIBCUDACXX_TEMPLATE_VIS bidirectional_iterator_tag : public forward_iterator_tag {};\nstruct _LIBCUDACXX_TEMPLATE_VIS random_access_iterator_tag : public bidirectional_iterator_tag {};\n#if _LIBCUDACXX_STD_VER > 11\nstruct _LIBCUDACXX_TEMPLATE_VIS contiguous_iterator_tag    : public random_access_iterator_tag {};\n#endif\n\ntemplate <class _Iter>\nstruct __iter_traits_cache {\n  using type = _If<\n    __is_primary_template<iterator_traits<_Iter>>::value,\n    _Iter,\n    iterator_traits<_Iter>\n  >;\n};\ntemplate <class _Iter>\nusing _ITER_TRAITS = typename __iter_traits_cache<_Iter>::type;\n\nstruct __iter_concept_concept_test {\n  template <class _Iter>\n  using _Apply = typename _ITER_TRAITS<_Iter>::iterator_concept;\n};\nstruct __iter_concept_category_test {\n  template <class _Iter>\n  using _Apply = typename _ITER_TRAITS<_Iter>::iterator_category;\n};\nstruct __iter_concept_random_fallback {\n  template <class _Iter>\n  using _Apply = __enable_if_t<\n                          __is_primary_template<iterator_traits<_Iter>>::value,\n                          random_access_iterator_tag\n                        >;\n};\n\ntemplate <class _Iter, class _Tester> struct __test_iter_concept\n    : _IsValidExpansion<_Tester::template _Apply, _Iter>,\n      _Tester\n{\n};\n\ntemplate <class _Iter>\nstruct __iter_concept_cache {\n  using type = _Or<\n    __test_iter_concept<_Iter, __iter_concept_concept_test>,\n    __test_iter_concept<_Iter, __iter_concept_category_test>,\n    __test_iter_concept<_Iter, __iter_concept_random_fallback>\n  >;\n};\n\ntemplate <class _Iter>\nusing _ITER_CONCEPT = typename __iter_concept_cache<_Iter>::type::template _Apply<_Iter>;\n\n\ntemplate <class _Tp>\nstruct __has_iterator_typedefs\n{\nprivate:\n    template <class _Up>\n    _LIBCUDACXX_INLINE_VISIBILITY static false_type __test(...);\n    template <class _Up>\n    _LIBCUDACXX_INLINE_VISIBILITY static true_type __test(__void_t<typename _Up::iterator_category>* = nullptr,\n                                                          __void_t<typename _Up::difference_type>* = nullptr,\n                                                          __void_t<typename _Up::value_type>* = nullptr,\n                                                          __void_t<typename _Up::reference>* = nullptr,\n                                                          __void_t<typename _Up::pointer>* = nullptr);\npublic:\n    static const bool value = decltype(__test<_Tp>(0,0,0,0,0))::value;\n};\n\ntemplate <class _Tp>\nstruct __has_iterator_category\n{\nprivate:\n    template <class _Up> _LIBCUDACXX_INLINE_VISIBILITY static false_type __test(...);\n    template <class _Up> _LIBCUDACXX_INLINE_VISIBILITY static true_type __test(typename _Up::iterator_category* = nullptr);\npublic:\n    static const bool value = decltype(__test<_Tp>(nullptr))::value;\n};\n\ntemplate <class _Tp>\nstruct __has_iterator_concept\n{\nprivate:\n    template <class _Up> _LIBCUDACXX_INLINE_VISIBILITY static false_type __test(...);\n    template <class _Up> _LIBCUDACXX_INLINE_VISIBILITY static true_type __test(typename _Up::iterator_concept* = nullptr);\npublic:\n    static const bool value = decltype(__test<_Tp>(nullptr))::value;\n};\n\n#if _LIBCUDACXX_STD_VER > 17\n\n// The `cpp17-*-iterator` exposition-only concepts have very similar names to the `Cpp17*Iterator` named requirements\n// from `[iterator.cpp17]`. To avoid confusion between the two, the exposition-only concepts have been banished to\n// a \"detail\" namespace indicating they have a niche use-case.\nnamespace __iterator_traits_detail {\ntemplate<class _Ip>\nconcept __cpp17_iterator =\n  requires(_Ip __i) {\n    {   *__i } -> __can_reference;\n    {  ++__i } -> same_as<_Ip&>;\n    { *__i++ } -> __can_reference;\n  } &&\n  copyable<_Ip>;\n\ntemplate<class _Ip>\nconcept __cpp17_input_iterator =\n  __cpp17_iterator<_Ip> &&\n  equality_comparable<_Ip> &&\n  requires(_Ip __i) {\n    typename incrementable_traits<_Ip>::difference_type;\n    typename indirectly_readable_traits<_Ip>::value_type;\n    typename common_reference_t<iter_reference_t<_Ip>&&,\n                                typename indirectly_readable_traits<_Ip>::value_type&>;\n    typename common_reference_t<decltype(*__i++)&&,\n                                typename indirectly_readable_traits<_Ip>::value_type&>;\n    requires signed_integral<typename incrementable_traits<_Ip>::difference_type>;\n  };\n\ntemplate<class _Ip>\nconcept __cpp17_forward_iterator =\n  __cpp17_input_iterator<_Ip> &&\n  constructible_from<_Ip> &&\n  is_lvalue_reference_v<iter_reference_t<_Ip>> &&\n  same_as<remove_cvref_t<iter_reference_t<_Ip>>,\n          typename indirectly_readable_traits<_Ip>::value_type> &&\n  requires(_Ip __i) {\n    {  __i++ } -> convertible_to<_Ip const&>;\n    { *__i++ } -> same_as<iter_reference_t<_Ip>>;\n  };\n\ntemplate<class _Ip>\nconcept __cpp17_bidirectional_iterator =\n  __cpp17_forward_iterator<_Ip> &&\n  requires(_Ip __i) {\n    {  --__i } -> same_as<_Ip&>;\n    {  __i-- } -> convertible_to<_Ip const&>;\n    { *__i-- } -> same_as<iter_reference_t<_Ip>>;\n  };\n\ntemplate<class _Ip>\nconcept __cpp17_random_access_iterator =\n  __cpp17_bidirectional_iterator<_Ip> &&\n  totally_ordered<_Ip> &&\n  requires(_Ip __i, typename incrementable_traits<_Ip>::difference_type __n) {\n    { __i += __n } -> same_as<_Ip&>;\n    { __i -= __n } -> same_as<_Ip&>;\n    { __i +  __n } -> same_as<_Ip>;\n    { __n +  __i } -> same_as<_Ip>;\n    { __i -  __n } -> same_as<_Ip>;\n    { __i -  __i } -> same_as<decltype(__n)>; // NOLINT(misc-redundant-expression) ; This is llvm.org/PR54114\n    {  __i[__n]  } -> convertible_to<iter_reference_t<_Ip>>;\n  };\n} // namespace __iterator_traits_detail\n\ntemplate<class _Ip>\nconcept __has_member_reference = requires { typename _Ip::reference; };\n\ntemplate<class _Ip>\nconcept __has_member_pointer = requires { typename _Ip::pointer; };\n\ntemplate<class _Ip>\nconcept __has_member_iterator_category = requires { typename _Ip::iterator_category; };\n\ntemplate<class _Ip>\nconcept __specifies_members = requires {\n    typename _Ip::value_type;\n    typename _Ip::difference_type;\n    requires __has_member_reference<_Ip>;\n    requires __has_member_iterator_category<_Ip>;\n  };\n\ntemplate<class>\nstruct __iterator_traits_member_pointer_or_void {\n  using type = void;\n};\n\ntemplate<__has_member_pointer _Tp>\nstruct __iterator_traits_member_pointer_or_void<_Tp> {\n  using type = typename _Tp::pointer;\n};\n\ntemplate<class _Tp>\nconcept __cpp17_iterator_missing_members =\n  !__specifies_members<_Tp> &&\n  __iterator_traits_detail::__cpp17_iterator<_Tp>;\n\ntemplate<class _Tp>\nconcept __cpp17_input_iterator_missing_members =\n  __cpp17_iterator_missing_members<_Tp> &&\n  __iterator_traits_detail::__cpp17_input_iterator<_Tp>;\n\n// Otherwise, `pointer` names `void`.\ntemplate<class>\nstruct __iterator_traits_member_pointer_or_arrow_or_void { using type = void; };\n\n// [iterator.traits]/3.2.1\n// If the qualified-id `I::pointer` is valid and denotes a type, `pointer` names that type.\ntemplate<__has_member_pointer _Ip>\nstruct __iterator_traits_member_pointer_or_arrow_or_void<_Ip> { using type = typename _Ip::pointer; };\n\n// Otherwise, if `decltype(declval<I&>().operator->())` is well-formed, then `pointer` names that\n// type.\ntemplate<class _Ip>\n  requires requires(_Ip& __i) { __i.operator->(); } && (!__has_member_pointer<_Ip>)\nstruct __iterator_traits_member_pointer_or_arrow_or_void<_Ip> {\n  using type = decltype(declval<_Ip&>().operator->());\n};\n\n// Otherwise, `reference` names `iter-reference-t<I>`.\ntemplate<class _Ip>\nstruct __iterator_traits_member_reference { using type = iter_reference_t<_Ip>; };\n\n// [iterator.traits]/3.2.2\n// If the qualified-id `I::reference` is valid and denotes a type, `reference` names that type.\ntemplate<__has_member_reference _Ip>\nstruct __iterator_traits_member_reference<_Ip> { using type = typename _Ip::reference; };\n\n// [iterator.traits]/3.2.3.4\n// input_iterator_tag\ntemplate<class _Ip>\nstruct __deduce_iterator_category {\n  using type = input_iterator_tag;\n};\n\n// [iterator.traits]/3.2.3.1\n// `random_access_iterator_tag` if `I` satisfies `cpp17-random-access-iterator`, or otherwise\ntemplate<__iterator_traits_detail::__cpp17_random_access_iterator _Ip>\nstruct __deduce_iterator_category<_Ip> {\n  using type = random_access_iterator_tag;\n};\n\n// [iterator.traits]/3.2.3.2\n// `bidirectional_iterator_tag` if `I` satisfies `cpp17-bidirectional-iterator`, or otherwise\ntemplate<__iterator_traits_detail::__cpp17_bidirectional_iterator _Ip>\n  requires (!__iterator_traits_detail::__cpp17_random_access_iterator<_Ip>) // nvbug 3885350\nstruct __deduce_iterator_category<_Ip> {\n  using type = bidirectional_iterator_tag;\n};\n\n// [iterator.traits]/3.2.3.3\n// `forward_iterator_tag` if `I` satisfies `cpp17-forward-iterator`, or otherwise\ntemplate<__iterator_traits_detail::__cpp17_forward_iterator _Ip>\n  requires (!__iterator_traits_detail::__cpp17_bidirectional_iterator<_Ip>) // nvbug 3885350\nstruct __deduce_iterator_category<_Ip> {\n  using type = forward_iterator_tag;\n};\n\ntemplate<class _Ip>\nstruct __iterator_traits_iterator_category : __deduce_iterator_category<_Ip> {};\n\n// [iterator.traits]/3.2.3\n// If the qualified-id `I::iterator-category` is valid and denotes a type, `iterator-category` names\n// that type.\ntemplate<__has_member_iterator_category _Ip>\nstruct __iterator_traits_iterator_category<_Ip> {\n  using type = typename _Ip::iterator_category;\n};\n\n// otherwise, it names void.\ntemplate<class>\nstruct __iterator_traits_difference_type { using type = void; };\n\n// If the qualified-id `incrementable_traits<I>::difference_type` is valid and denotes a type, then\n// `difference_type` names that type;\ntemplate<class _Ip>\nrequires requires { typename incrementable_traits<_Ip>::difference_type; }\nstruct __iterator_traits_difference_type<_Ip> {\n  using type = typename incrementable_traits<_Ip>::difference_type;\n};\n\n// [iterator.traits]/3.4\n// Otherwise, `iterator_traits<I>` has no members by any of the above names.\ntemplate<class>\nstruct __iterator_traits {};\n\n// [iterator.traits]/3.1\n// If `I` has valid ([temp.deduct]) member types `difference-type`, `value-type`, `reference`, and\n// `iterator-category`, then `iterator-traits<I>` has the following publicly accessible members:\ntemplate<__specifies_members _Ip>\nstruct __iterator_traits<_Ip> {\n  using iterator_category  = typename _Ip::iterator_category;\n  using value_type         = typename _Ip::value_type;\n  using difference_type    = typename _Ip::difference_type;\n  using pointer            = typename __iterator_traits_member_pointer_or_void<_Ip>::type;\n  using reference          = typename _Ip::reference;\n};\n\n// [iterator.traits]/3.2\n// Otherwise, if `I` satisfies the exposition-only concept `cpp17-input-iterator`,\n// `iterator-traits<I>` has the following publicly accessible members:\ntemplate<__cpp17_input_iterator_missing_members _Ip>\nstruct __iterator_traits<_Ip> {\n  using iterator_category = typename __iterator_traits_iterator_category<_Ip>::type;\n  using value_type        = typename indirectly_readable_traits<_Ip>::value_type;\n  using difference_type   = typename incrementable_traits<_Ip>::difference_type;\n  using pointer           = typename __iterator_traits_member_pointer_or_arrow_or_void<_Ip>::type;\n  using reference         = typename __iterator_traits_member_reference<_Ip>::type;\n};\n\n// Otherwise, if `I` satisfies the exposition-only concept `cpp17-iterator`, then\n// `iterator_traits<I>` has the following publicly accessible members:\ntemplate<__cpp17_iterator_missing_members _Ip>\n  requires (!__cpp17_input_iterator_missing_members<_Ip>) // nvbug 3885350\nstruct __iterator_traits<_Ip> {\n  using iterator_category = output_iterator_tag;\n  using value_type        = void;\n  using difference_type   = typename __iterator_traits_difference_type<_Ip>::type;\n  using pointer           = void;\n  using reference         = void;\n};\n\ntemplate<class _Ip>\nstruct _LIBCUDACXX_TEMPLATE_VIS iterator_traits : __iterator_traits<_Ip> {\n  using __primary_template = iterator_traits;\n};\n\n#elif _LIBCUDACXX_STD_VER > 14\n\n// The `cpp17-*-iterator` exposition-only concepts have very similar names to the `Cpp17*Iterator` named requirements\n// from `[iterator.cpp17]`. To avoid confusion between the two, the exposition-only concepts have been banished to\n// a \"detail\" namespace indicating they have a niche use-case.\nnamespace __iterator_traits_detail {\ntemplate <class _Ip>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __cpp17_iterator_,\n  requires(_Ip __i) //\n  (requires(__can_reference<decltype(*__i)>),\n   requires(same_as<_Ip&, decltype(++__i)>),\n   requires(__can_reference<decltype(*__i++)>),\n   requires(copyable<_Ip>)));\n\ntemplate <class _Ip>\n_LIBCUDACXX_CONCEPT __cpp17_iterator = _LIBCUDACXX_FRAGMENT(__cpp17_iterator_, _Ip);\n\ntemplate<class _Ip>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __cpp17_input_iterator_,\n  requires(_Ip __i)(\n    typename(common_reference_t<iter_reference_t<_Ip>&&, typename indirectly_readable_traits<_Ip>::value_type&>),\n    typename(common_reference_t<decltype(*__i++)&&, typename indirectly_readable_traits<_Ip>::value_type&>),\n    requires(__cpp17_iterator<_Ip>),\n    requires(equality_comparable<_Ip>),\n    requires(__has_member_difference_type<incrementable_traits<_Ip>>),\n    requires(__has_member_value_type<indirectly_readable_traits<_Ip>>),\n    requires(signed_integral<typename incrementable_traits<_Ip>::difference_type>)\n  ));\n\ntemplate<class _Ip>\n_LIBCUDACXX_CONCEPT __cpp17_input_iterator = _LIBCUDACXX_FRAGMENT(__cpp17_input_iterator_, _Ip);\n\ntemplate<class _Ip>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __cpp17_forward_iterator_,\n  requires(_Ip __i)(\n    requires(__cpp17_input_iterator<_Ip>),\n    requires(convertible_to<decltype(__i++), _Ip const&>),\n    requires(same_as<iter_reference_t<_Ip>, decltype(*__i++)>),\n    requires(constructible_from<_Ip>),\n    requires(_LIBCUDACXX_TRAIT(is_lvalue_reference, iter_reference_t<_Ip>)),\n    requires(same_as<remove_cvref_t<iter_reference_t<_Ip>>, typename indirectly_readable_traits<_Ip>::value_type>)\n  ));\n\ntemplate<class _Ip>\n_LIBCUDACXX_CONCEPT __cpp17_forward_iterator = _LIBCUDACXX_FRAGMENT(__cpp17_forward_iterator_, _Ip);\n\ntemplate<class _Ip>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __cpp17_bidirectional_iterator_,\n  requires(_Ip __i)(\n    requires(__cpp17_forward_iterator<_Ip>),\n    requires(same_as<_Ip&, decltype(--__i)>),\n    requires(convertible_to<decltype(__i--), _Ip const&>),\n    requires(same_as<iter_reference_t<_Ip>, decltype(*__i--)>)\n  ));\n\ntemplate<class _Ip>\n_LIBCUDACXX_CONCEPT __cpp17_bidirectional_iterator = _LIBCUDACXX_FRAGMENT(__cpp17_bidirectional_iterator_, _Ip);\n\ntemplate<class _Ip>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __cpp17_random_access_iterator_,\n  requires(_Ip __i, typename incrementable_traits<_Ip>::difference_type __n) //\n  (requires(same_as<_Ip&, decltype(__i += __n)>),\n   requires(same_as<_Ip&, decltype(__i -= __n)>),\n   requires(same_as<_Ip, decltype(__i +  __n)>),\n   requires(same_as<_Ip, decltype(__n +  __i)>),\n   requires(same_as<_Ip, decltype(__i -  __n)>),\n   requires(same_as<decltype(__n), decltype(__i -  __i)>),\n   requires(convertible_to<decltype(__i[__n]), iter_reference_t<_Ip>>)));\n\ntemplate<class _Ip>\n_LIBCUDACXX_CONCEPT __cpp17_random_access_iterator =\n  __cpp17_bidirectional_iterator<_Ip> &&\n  totally_ordered<_Ip> &&\n  _LIBCUDACXX_FRAGMENT(__cpp17_random_access_iterator_, _Ip);\n} // namespace __iterator_traits_detail\n\ntemplate<class, class = void>\n_LIBCUDACXX_INLINE_VAR constexpr bool __has_member_reference = false;\n\ntemplate<class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool __has_member_reference<_Tp, void_t<typename _Tp::reference>> = true;\n\ntemplate<class, class = void>\n_LIBCUDACXX_INLINE_VAR constexpr bool __has_member_pointer = false;\n\ntemplate<class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool __has_member_pointer<_Tp, void_t<typename _Tp::pointer>> = true;\n\ntemplate<class, class = void>\n_LIBCUDACXX_INLINE_VAR constexpr bool __has_member_iterator_category = false;\n\ntemplate<class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool __has_member_iterator_category<_Tp, void_t<typename _Tp::iterator_category>> = true;\n\ntemplate<class _Ip>\n_LIBCUDACXX_CONCEPT __specifies_members =\n    __has_member_value_type<_Ip> &&\n    __has_member_difference_type<_Ip> &&\n    __has_member_reference<_Ip> &&\n    __has_member_iterator_category<_Ip>;\n\ntemplate<class, class = void>\nstruct __iterator_traits_member_pointer_or_void {\n  using type = void;\n};\n\ntemplate<class _Tp>\nstruct __iterator_traits_member_pointer_or_void<_Tp, enable_if_t<__has_member_pointer<_Tp>>> {\n  using type = typename _Tp::pointer;\n};\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT __cpp17_iterator_missing_members =\n  !__specifies_members<_Tp> &&\n  __iterator_traits_detail::__cpp17_iterator<_Tp>;\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT __cpp17_input_iterator_missing_members =\n  __cpp17_iterator_missing_members<_Tp> &&\n  __iterator_traits_detail::__cpp17_input_iterator<_Tp>;\n\n// Otherwise, `pointer` names `void`.\ntemplate<class, class = void>\nstruct __iterator_traits_member_pointer_or_arrow_or_void { using type = void; };\n\n// [iterator.traits]/3.2.1\n// If the qualified-id `I::pointer` is valid and denotes a type, `pointer` names that type.\ntemplate<class _Ip>\nstruct __iterator_traits_member_pointer_or_arrow_or_void<_Ip, enable_if_t<__has_member_pointer<_Ip>>>\n{ using type = typename _Ip::pointer; };\n\ntemplate<class _Ip>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __has_operator_arrow_,\n  requires(_Ip& __i) //\n  (__i.operator->()));\n\ntemplate<class _Ip>\n_LIBCUDACXX_CONCEPT __has_operator_arrow = _LIBCUDACXX_FRAGMENT(__has_operator_arrow_, _Ip);\n\n// Otherwise, if `decltype(declval<I&>().operator->())` is well-formed, then `pointer` names that\n// type.\ntemplate<class _Ip>\nstruct __iterator_traits_member_pointer_or_arrow_or_void<_Ip,\n  enable_if_t<__has_operator_arrow<_Ip> && !__has_member_pointer<_Ip>>> {\n  using type = decltype(declval<_Ip&>().operator->());\n};\n\n// Otherwise, `reference` names `iter-reference-t<I>`.\ntemplate<class _Ip, class = void>\nstruct __iterator_traits_member_reference { using type = iter_reference_t<_Ip>; };\n\n// [iterator.traits]/3.2.2\n// If the qualified-id `I::reference` is valid and denotes a type, `reference` names that type.\ntemplate<class _Ip>\nstruct __iterator_traits_member_reference<_Ip, enable_if_t<__has_member_reference<_Ip>>>\n{ using type = typename _Ip::reference; };\n\n// [iterator.traits]/3.2.3.4\n// input_iterator_tag\ntemplate<class _Ip, class = void>\nstruct __deduce_iterator_category {\n  using type = input_iterator_tag;\n};\n\n// [iterator.traits]/3.2.3.1\n// `random_access_iterator_tag` if `I` satisfies `cpp17-random-access-iterator`, or otherwise\ntemplate<class _Ip>\nstruct __deduce_iterator_category<_Ip, enable_if_t<\n  __iterator_traits_detail::__cpp17_random_access_iterator<_Ip>>> {\n  using type = random_access_iterator_tag;\n};\n\n// [iterator.traits]/3.2.3.2\n// `bidirectional_iterator_tag` if `I` satisfies `cpp17-bidirectional-iterator`, or otherwise\ntemplate<class _Ip>\nstruct __deduce_iterator_category<_Ip, enable_if_t<\n  !__iterator_traits_detail::__cpp17_random_access_iterator<_Ip> &&\n  __iterator_traits_detail::__cpp17_bidirectional_iterator<_Ip>>> {\n  using type = bidirectional_iterator_tag;\n};\n\n// [iterator.traits]/3.2.3.3\n// `forward_iterator_tag` if `I` satisfies `cpp17-forward-iterator`, or otherwise\ntemplate<class _Ip>\nstruct __deduce_iterator_category<_Ip, enable_if_t<\n  !__iterator_traits_detail::__cpp17_bidirectional_iterator<_Ip> &&\n  __iterator_traits_detail::__cpp17_forward_iterator<_Ip>>> {\n  using type = forward_iterator_tag;\n};\n\ntemplate<class _Ip, class = void>\nstruct __iterator_traits_iterator_category : __deduce_iterator_category<_Ip> {};\n\n// [iterator.traits]/3.2.3\n// If the qualified-id `I::iterator-category` is valid and denotes a type, `iterator-category` names\n// that type.\ntemplate<class _Ip>\nstruct __iterator_traits_iterator_category<_Ip, enable_if_t<\n  __has_member_iterator_category<_Ip>>> {\n  using type = typename _Ip::iterator_category;\n};\n\n// otherwise, it names void.\ntemplate<class, class = void>\nstruct __iterator_traits_difference_type { using type = void; };\n\n// If the qualified-id `incrementable_traits<I>::difference_type` is valid and denotes a type, then\n// `difference_type` names that type;\ntemplate<class _Ip>\nstruct __iterator_traits_difference_type<_Ip, void_t<\n  typename incrementable_traits<_Ip>::difference_type>> {\n  using type = typename incrementable_traits<_Ip>::difference_type;\n};\n\n// [iterator.traits]/3.4\n// Otherwise, `iterator_traits<I>` has no members by any of the above names.\ntemplate<class, class = void>\nstruct __iterator_traits {};\n\n// [iterator.traits]/3.1\n// If `I` has valid ([temp.deduct]) member types `difference-type`, `value-type`, `reference`, and\n// `iterator-category`, then `iterator-traits<I>` has the following publicly accessible members:\ntemplate<class _Ip>\nstruct __iterator_traits<_Ip, enable_if_t<__specifies_members<_Ip>>> {\n  using iterator_category  = typename _Ip::iterator_category;\n  using value_type         = typename _Ip::value_type;\n  using difference_type    = typename _Ip::difference_type;\n  using pointer            = typename __iterator_traits_member_pointer_or_void<_Ip>::type;\n  using reference          = typename _Ip::reference;\n};\n\n// [iterator.traits]/3.2\n// Otherwise, if `I` satisfies the exposition-only concept `cpp17-input-iterator`,\n// `iterator-traits<I>` has the following publicly accessible members:\ntemplate<class _Ip>\nstruct __iterator_traits<_Ip, enable_if_t<!__specifies_members<_Ip> &&\n                                           __cpp17_input_iterator_missing_members<_Ip>>> {\n  using iterator_category = typename __iterator_traits_iterator_category<_Ip>::type;\n  using value_type        = typename indirectly_readable_traits<_Ip>::value_type;\n  using difference_type   = typename incrementable_traits<_Ip>::difference_type;\n  using pointer           = typename __iterator_traits_member_pointer_or_arrow_or_void<_Ip>::type;\n  using reference         = typename __iterator_traits_member_reference<_Ip>::type;\n};\n\n// Otherwise, if `I` satisfies the exposition-only concept `cpp17-iterator`, then\n// `iterator_traits<I>` has the following publicly accessible members:\ntemplate<class _Ip>\nstruct __iterator_traits<_Ip, enable_if_t<!__specifies_members<_Ip> &&\n                                          !__cpp17_input_iterator_missing_members<_Ip> &&\n                                           __cpp17_iterator_missing_members<_Ip>>> {\n  using iterator_category = output_iterator_tag;\n  using value_type        = void;\n  using difference_type   = typename __iterator_traits_difference_type<_Ip>::type;\n  using pointer           = void;\n  using reference         = void;\n};\n\ntemplate<class _Ip, class>\nstruct _LIBCUDACXX_TEMPLATE_VIS iterator_traits : __iterator_traits<_Ip> {\n  using __primary_template = iterator_traits;\n};\n\n#else // _LIBCUDACXX_STD_VER > 11\n\ntemplate <class _Iter, bool> struct __iterator_traits {};\n\ntemplate <class _Iter, bool> struct __iterator_traits_impl {};\n\ntemplate <class _Iter>\nstruct __iterator_traits_impl<_Iter, true>\n{\n    typedef typename _Iter::difference_type   difference_type;\n    typedef typename _Iter::value_type        value_type;\n    typedef typename _Iter::pointer           pointer;\n    typedef typename _Iter::reference         reference;\n    typedef typename _Iter::iterator_category iterator_category;\n};\n\ntemplate <class _Iter>\nstruct __iterator_traits<_Iter, true>\n    :  __iterator_traits_impl\n      <\n        _Iter,\n        is_convertible<typename _Iter::iterator_category, input_iterator_tag>::value ||\n        is_convertible<typename _Iter::iterator_category, output_iterator_tag>::value\n      >\n{};\n\n// iterator_traits<Iterator> will only have the nested types if Iterator::iterator_category\n//    exists.  Else iterator_traits<Iterator> will be an empty class.  This is a\n//    conforming extension which allows some programs to compile and behave as\n//    the client expects instead of failing at compile time.\n\ntemplate <class _Iter>\nstruct _LIBCUDACXX_TEMPLATE_VIS iterator_traits\n    : __iterator_traits<_Iter, __has_iterator_typedefs<_Iter>::value> {\n\n  using __primary_template = iterator_traits;\n};\n#endif // _LIBCUDACXX_STD_VER < 17\n\ntemplate<class _Tp>\n#if _LIBCUDACXX_STD_VER > 17\nrequires is_object_v<_Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS iterator_traits<_Tp*>\n{\n    typedef ptrdiff_t difference_type;\n    typedef __remove_cv_t<_Tp> value_type;\n    typedef _Tp* pointer;\n    typedef typename add_lvalue_reference<_Tp>::type reference;\n    typedef random_access_iterator_tag iterator_category;\n#if _LIBCUDACXX_STD_VER > 14\n    typedef contiguous_iterator_tag    iterator_concept;\n#endif\n};\n\ntemplate <class _Tp, class _Up, bool = __has_iterator_category<iterator_traits<_Tp> >::value>\nstruct __has_iterator_category_convertible_to\n    : is_convertible<typename iterator_traits<_Tp>::iterator_category, _Up>\n{};\n\ntemplate <class _Tp, class _Up>\nstruct __has_iterator_category_convertible_to<_Tp, _Up, false> : false_type {};\n\ntemplate <class _Tp, class _Up, bool = __has_iterator_concept<_Tp>::value>\nstruct __has_iterator_concept_convertible_to\n    : is_convertible<typename _Tp::iterator_concept, _Up>\n{};\n\ntemplate <class _Tp, class _Up>\nstruct __has_iterator_concept_convertible_to<_Tp, _Up, false> : false_type {};\n\ntemplate <class _Tp>\nstruct __is_cpp17_input_iterator : public __has_iterator_category_convertible_to<_Tp, input_iterator_tag> {};\n\ntemplate <class _Tp>\nstruct __is_cpp17_forward_iterator : public __has_iterator_category_convertible_to<_Tp, forward_iterator_tag> {};\n\ntemplate <class _Tp>\nstruct __is_cpp17_bidirectional_iterator : public __has_iterator_category_convertible_to<_Tp, bidirectional_iterator_tag> {};\n\ntemplate <class _Tp>\nstruct __is_cpp17_random_access_iterator : public __has_iterator_category_convertible_to<_Tp, random_access_iterator_tag> {};\n\n// __is_cpp17_contiguous_iterator determines if an iterator is known by\n// libc++ to be contiguous, either because it advertises itself as such\n// (in C++20) or because it is a pointer type or a known trivial wrapper\n// around a (possibly fancy) pointer type, such as __wrap_iter<T*>.\n// Such iterators receive special \"contiguous\" optimizations in\n// std::copy and std::sort.\n//\n#if _LIBCUDACXX_STD_VER > 14\ntemplate <class _Tp>\nstruct __is_cpp17_contiguous_iterator : _Or<\n    __has_iterator_category_convertible_to<_Tp, contiguous_iterator_tag>,\n    __has_iterator_concept_convertible_to<_Tp, contiguous_iterator_tag>\n> {};\n#else\ntemplate <class _Tp>\nstruct __is_cpp17_contiguous_iterator : false_type {};\n#endif\n\n// Any native pointer which is an iterator is also a contiguous iterator.\ntemplate <class _Up>\nstruct __is_cpp17_contiguous_iterator<_Up*> : true_type {};\n\n\ntemplate <class _Iter>\nclass __wrap_iter;\n\ntemplate <class _Tp>\nstruct __is_exactly_cpp17_input_iterator\n    : public integral_constant<bool,\n         __has_iterator_category_convertible_to<_Tp, input_iterator_tag>::value &&\n        !__has_iterator_category_convertible_to<_Tp, forward_iterator_tag>::value> {};\n\ntemplate <class _Tp>\nstruct __is_exactly_cpp17_forward_iterator\n    : public integral_constant<bool,\n         __has_iterator_category_convertible_to<_Tp, forward_iterator_tag>::value &&\n        !__has_iterator_category_convertible_to<_Tp, bidirectional_iterator_tag>::value> {};\n\ntemplate <class _Tp>\nstruct __is_exactly_cpp17_bidirectional_iterator\n    : public integral_constant<bool,\n         __has_iterator_category_convertible_to<_Tp, bidirectional_iterator_tag>::value &&\n        !__has_iterator_category_convertible_to<_Tp, random_access_iterator_tag>::value> {};\n\ntemplate<class _InputIterator>\nusing __iter_value_type = typename iterator_traits<_InputIterator>::value_type;\n\ntemplate<class _InputIterator>\nusing __iter_key_type = typename remove_const<typename iterator_traits<_InputIterator>::value_type::first_type>::type;\n\ntemplate<class _InputIterator>\nusing __iter_mapped_type = typename iterator_traits<_InputIterator>::value_type::second_type;\n\ntemplate<class _InputIterator>\nusing __iter_to_alloc_type = pair<\n    typename add_const<typename iterator_traits<_InputIterator>::value_type::first_type>::type,\n    typename iterator_traits<_InputIterator>::value_type::second_type>;\n\ntemplate <class _Iter>\nusing __iterator_category_type = typename iterator_traits<_Iter>::iterator_category;\n\ntemplate <class _Iter>\nusing __iterator_pointer_type = typename iterator_traits<_Iter>::pointer;\n\ntemplate <class _Iter>\nusing __iter_diff_t = typename iterator_traits<_Iter>::difference_type;\n\ntemplate<class _InputIterator>\nusing __iter_value_type = typename iterator_traits<_InputIterator>::value_type;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___ITERATOR_ITERATOR_TRAITS_H\n", "../__iterator/readable_traits.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___ITERATOR_READABLE_TRAITS_H\n#define _LIBCUDACXX___ITERATOR_READABLE_TRAITS_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/is_primary_template.h\"\n#include \"../__type_traits/conditional.h\"\n#include \"../__type_traits/enable_if.h\"\n#include \"../__type_traits/is_array.h\"\n#include \"../__type_traits/is_const.h\"\n#include \"../__type_traits/is_object.h\"\n#include \"../__type_traits/is_primary_template.h\"\n#include \"../__type_traits/remove_cv.h\"\n#include \"../__type_traits/remove_cvref.h\"\n#include \"../__type_traits/remove_extent.h\"\n#include \"../__type_traits/void_t.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\n\n// [readable.traits]\ntemplate<class> struct __cond_value_type {};\n\ntemplate<class _Tp>\nrequires is_object_v<_Tp>\nstruct __cond_value_type<_Tp> { using value_type = remove_cv_t<_Tp>; };\n\ntemplate<class _Tp>\nconcept __has_member_value_type = requires { typename _Tp::value_type; };\n\ntemplate<class _Tp>\nconcept __has_member_element_type = requires { typename _Tp::element_type; };\n\ntemplate<class> struct indirectly_readable_traits {};\n\ntemplate<class _Ip>\nrequires is_array_v<_Ip>\nstruct indirectly_readable_traits<_Ip> {\n  using value_type = remove_cv_t<remove_extent_t<_Ip>>;\n};\n\ntemplate<class _Ip>\nstruct indirectly_readable_traits<const _Ip> : indirectly_readable_traits<_Ip> {};\n\ntemplate<class _Tp>\nstruct indirectly_readable_traits<_Tp*> : __cond_value_type<_Tp> {};\n\ntemplate<__has_member_value_type _Tp>\nstruct indirectly_readable_traits<_Tp>\n  : __cond_value_type<typename _Tp::value_type> {};\n\ntemplate<__has_member_element_type _Tp>\nstruct indirectly_readable_traits<_Tp>\n  : __cond_value_type<typename _Tp::element_type> {};\n\ntemplate<__has_member_value_type _Tp>\n  requires __has_member_element_type<_Tp>\nstruct indirectly_readable_traits<_Tp> {};\n\ntemplate<__has_member_value_type _Tp>\n  requires __has_member_element_type<_Tp> &&\n           same_as<remove_cv_t<typename _Tp::element_type>,\n                   remove_cv_t<typename _Tp::value_type>>\nstruct indirectly_readable_traits<_Tp>\n  : __cond_value_type<typename _Tp::value_type> {};\n\ntemplate <class>\nstruct _LIBCUDACXX_TEMPLATE_VIS iterator_traits;\n\n// Let `RI` be `remove_cvref_t<I>`. The type `iter_value_t<I>` denotes\n// `indirectly_readable_traits<RI>::value_type` if `iterator_traits<RI>` names a specialization\n// generated from the primary template, and `iterator_traits<RI>::value_type` otherwise.\ntemplate <class _Ip>\nusing iter_value_t = typename conditional_t<__is_primary_template<iterator_traits<remove_cvref_t<_Ip>>>::value,\n                                            indirectly_readable_traits<remove_cvref_t<_Ip> >,\n                                            iterator_traits<remove_cvref_t<_Ip> > >::value_type;\n\n#elif _LIBCUDACXX_STD_VER > 14\n\n// [readable.traits]\ntemplate<class, class = void> struct __cond_value_type {};\n\ntemplate<class _Tp>\nstruct __cond_value_type<_Tp, enable_if_t<_LIBCUDACXX_TRAIT(is_object, _Tp)>> { using value_type = remove_cv_t<_Tp>; };\n\ntemplate<class _Tp, class = void>\n_LIBCUDACXX_INLINE_VAR constexpr bool __has_member_value_type = false;\n\ntemplate<class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool __has_member_value_type<_Tp, void_t<typename _Tp::value_type>> = true;\n\ntemplate<class _Tp, class = void>\n_LIBCUDACXX_INLINE_VAR constexpr bool __has_member_element_type = false;\n\ntemplate<class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool __has_member_element_type<_Tp, void_t<typename _Tp::element_type>> = true;\n\ntemplate<class, class = void> struct indirectly_readable_traits {};\n\ntemplate <class _Ip>\nstruct indirectly_readable_traits<_Ip, enable_if_t<!_LIBCUDACXX_TRAIT(is_const,_Ip) && _LIBCUDACXX_TRAIT(is_array, _Ip)>> {\n  using value_type = remove_cv_t<remove_extent_t<_Ip>>;\n};\n\ntemplate<class _Ip>\nstruct indirectly_readable_traits<const _Ip>\n  : indirectly_readable_traits<_Ip> {};\n\ntemplate<class _Tp>\nstruct indirectly_readable_traits<_Tp*>\n  : __cond_value_type<_Tp> {};\n\ntemplate<class _Tp>\nstruct indirectly_readable_traits<_Tp, enable_if_t<!_LIBCUDACXX_TRAIT(is_const, _Tp) && __has_member_value_type<_Tp> && !__has_member_element_type<_Tp>>>\n  : __cond_value_type<typename _Tp::value_type> {};\n\ntemplate<class _Tp>\nstruct indirectly_readable_traits<_Tp, enable_if_t<!_LIBCUDACXX_TRAIT(is_const, _Tp) && !__has_member_value_type<_Tp> && __has_member_element_type<_Tp>>>\n  : __cond_value_type<typename _Tp::element_type> {};\n\ntemplate<class _Tp>\nstruct indirectly_readable_traits<_Tp, enable_if_t<!_LIBCUDACXX_TRAIT(is_const, _Tp) && __has_member_value_type<_Tp> && __has_member_element_type<_Tp> &&\n                                                   same_as<remove_cv_t<typename _Tp::element_type>,\n                                                           remove_cv_t<typename _Tp::value_type>>>>\n  : __cond_value_type<typename _Tp::value_type> {};\n\ntemplate <class, class>\nstruct _LIBCUDACXX_TEMPLATE_VIS iterator_traits;\n\n// Let `RI` be `remove_cvref_t<I>`. The type `iter_value_t<I>` denotes\n// `indirectly_readable_traits<RI>::value_type` if `iterator_traits<RI>` names a specialization\n// generated from the primary template, and `iterator_traits<RI>::value_type` otherwise.\ntemplate <class _Ip>\nusing iter_value_t = typename conditional_t<__is_primary_template<iterator_traits<remove_cvref_t<_Ip>>>::value,\n                                            indirectly_readable_traits<remove_cvref_t<_Ip> >,\n                                            iterator_traits<remove_cvref_t<_Ip> > >::value_type;\n\n#endif // _LIBCUDACXX_STD_VER > 14\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___ITERATOR_READABLE_TRAITS_H\n", "../__memory/addressof.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___MEMORY_ADDRESSOF_H\n#define _LIBCUDACXX___MEMORY_ADDRESSOF_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n// addressof\n// NVCXX has the builtin defined but did not mark it as supported\n#if defined(_LIBCUDACXX_ADDRESSOF)\n\ntemplate <class _Tp>\ninline _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n_LIBCUDACXX_NO_CFI _LIBCUDACXX_INLINE_VISIBILITY\n_Tp*\naddressof(_Tp& __x) noexcept\n{\n    return __builtin_addressof(__x);\n}\n\n#else\n\ntemplate <class _Tp>\ninline _LIBCUDACXX_NO_CFI _LIBCUDACXX_INLINE_VISIBILITY\n_Tp*\naddressof(_Tp& __x) noexcept\n{\n  return reinterpret_cast<_Tp *>(\n      const_cast<char *>(&reinterpret_cast<const volatile char &>(__x)));\n}\n\n#endif // defined(_LIBCUDACXX_ADDRESSOF)\n\n#if defined(_LIBCUDACXX_HAS_OBJC_ARC) && !defined(_LIBCUDACXX_PREDEFINED_OBJC_ARC_ADDRESSOF)\n// Objective-C++ Automatic Reference Counting uses qualified pointers\n// that require special addressof() signatures. When\n// _LIBCUDACXX_PREDEFINED_OBJC_ARC_ADDRESSOF is defined, the compiler\n// itself is providing these definitions. Otherwise, we provide them.\ntemplate <class _Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY\n__strong _Tp*\naddressof(__strong _Tp& __x) noexcept\n{\n  return &__x;\n}\n\n#ifdef _LIBCUDACXX_HAS_OBJC_ARC_WEAK\ntemplate <class _Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY\n__weak _Tp*\naddressof(__weak _Tp& __x) noexcept\n{\n  return &__x;\n}\n#endif\n\ntemplate <class _Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY\n__autoreleasing _Tp*\naddressof(__autoreleasing _Tp& __x) noexcept\n{\n  return &__x;\n}\n\ntemplate <class _Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY\n__unsafe_unretained _Tp*\naddressof(__unsafe_unretained _Tp& __x) noexcept\n{\n  return &__x;\n}\n#endif\n\ntemplate <class _Tp> _Tp* addressof(const _Tp&&) noexcept = delete;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___MEMORY_ADDRESSOF_H\n", "../__memory/voidify.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___MEMORY_VOIDIFY_H\n#define _LIBCUDACXX___MEMORY_VOIDIFY_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__memory/addressof.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <typename _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17 void* __voidify(_Tp& __from) {\n  // Cast away cv-qualifiers to allow modifying elements of a range through const iterators.\n  return const_cast<void*>(static_cast<const volatile void*>(_CUDA_VSTD::addressof(__from)));\n}\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___MEMORY_VOIDIFY_H\n", "../__tuple_dir/apply_cv.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TUPLE_APPLY_CV_H\n#define _LIBCUDACXX___TUPLE_APPLY_CV_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/is_const.h\"\n#include \"../__type_traits/is_reference.h\"\n#include \"../__type_traits/is_volatile.h\"\n#include \"../__type_traits/remove_reference.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <bool _ApplyLV, bool _ApplyConst, bool _ApplyVolatile>\nstruct __apply_cv_mf;\ntemplate <>\nstruct __apply_cv_mf<false, false, false> {\n  template <class _Tp> using __apply = _Tp;\n};\ntemplate <>\nstruct __apply_cv_mf<false, true, false> {\n  template <class _Tp> using __apply _LIBCUDACXX_NODEBUG_TYPE = const _Tp;\n};\ntemplate <>\nstruct __apply_cv_mf<false, false, true> {\n  template <class _Tp> using __apply _LIBCUDACXX_NODEBUG_TYPE = volatile _Tp;\n};\ntemplate <>\nstruct __apply_cv_mf<false, true, true> {\n  template <class _Tp> using __apply _LIBCUDACXX_NODEBUG_TYPE = const volatile _Tp;\n};\ntemplate <>\nstruct __apply_cv_mf<true, false, false> {\n  template <class _Tp> using __apply _LIBCUDACXX_NODEBUG_TYPE = _Tp&;\n};\ntemplate <>\nstruct __apply_cv_mf<true, true, false> {\n  template <class _Tp> using __apply _LIBCUDACXX_NODEBUG_TYPE = const _Tp&;\n};\ntemplate <>\nstruct __apply_cv_mf<true, false, true> {\n  template <class _Tp> using __apply _LIBCUDACXX_NODEBUG_TYPE = volatile _Tp&;\n};\ntemplate <>\nstruct __apply_cv_mf<true, true, true> {\n  template <class _Tp> using __apply _LIBCUDACXX_NODEBUG_TYPE = const volatile _Tp&;\n};\ntemplate <class _Tp, class _RawTp = __libcpp_remove_reference_t<_Tp> >\nusing __apply_cv_t _LIBCUDACXX_NODEBUG_TYPE = __apply_cv_mf<\n    is_lvalue_reference<_Tp>::value,\n    is_const<_RawTp>::value,\n    is_volatile<_RawTp>::value>;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TUPLE_APPLY_CV_H\n", "../__tuple_dir/make_tuple_types.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TUPLE_MAKE_TUPLE_TYPES_H\n#define _LIBCUDACXX___TUPLE_MAKE_TUPLE_TYPES_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__fwd/array.h\"\n#include \"../__fwd/tuple.h\"\n#include \"../__tuple_dir/apply_cv.h\"\n#include \"../__tuple_dir/tuple_element.h\"\n#include \"../__tuple_dir/tuple_indices.h\"\n#include \"../__tuple_dir/tuple_size.h\"\n#include \"../__tuple_dir/tuple_types.h\"\n#include \"../__type_traits/remove_cv.h\"\n#include \"../__type_traits/remove_reference.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n// __make_tuple_types<_Tuple<_Types...>, _Ep, _Sp>::type is a\n// __tuple_types<_Types...> using only those _Types in the range [_Sp, _Ep).\n// _Sp defaults to 0 and _Ep defaults to tuple_size<_Tuple>.  If _Tuple is a\n// lvalue_reference type, then __tuple_types<_Types&...> is the result.\n\ntemplate <class _TupleTypes, class _TupleIndices>\nstruct __make_tuple_types_flat;\n\ntemplate <template <class...> class _Tuple, class ..._Types, size_t ..._Idx>\nstruct __make_tuple_types_flat<_Tuple<_Types...>, __tuple_indices<_Idx...>> {\n  // Specialization for pair, tuple, and __tuple_types\n  template <class _Tp, class _ApplyFn = __apply_cv_t<_Tp>>\n  using __apply_quals _LIBCUDACXX_NODEBUG_TYPE = __tuple_types<\n      typename _ApplyFn::template __apply<__type_pack_element<_Idx, _Types...>>...\n    >;\n};\n\ntemplate <class _Vt, size_t _Np, size_t ..._Idx>\nstruct __make_tuple_types_flat<array<_Vt, _Np>, __tuple_indices<_Idx...>> {\n  template <size_t>\n  using __value_type = _Vt;\n  template <class _Tp, class _ApplyFn = __apply_cv_t<_Tp>>\n  using __apply_quals = __tuple_types<\n      typename _ApplyFn::template __apply<__value_type<_Idx>>...\n    >;\n};\n\ntemplate <class _Tp, size_t _Ep = tuple_size<__libcpp_remove_reference_t<_Tp> >::value,\n          size_t _Sp = 0,\n          bool _SameSize = (_Ep == tuple_size<__libcpp_remove_reference_t<_Tp> >::value)>\nstruct __make_tuple_types\n{\n    static_assert(_Sp <= _Ep, \"__make_tuple_types input error\");\n    using _RawTp = __remove_cv_t<__libcpp_remove_reference_t<_Tp>>;\n    using _Maker = __make_tuple_types_flat<_RawTp, typename __make_tuple_indices<_Ep, _Sp>::type>;\n    using type = typename _Maker::template __apply_quals<_Tp>;\n};\n\ntemplate <class ..._Types, size_t _Ep>\nstruct __make_tuple_types<tuple<_Types...>, _Ep, 0, true> {\n  typedef _LIBCUDACXX_NODEBUG_TYPE __tuple_types<_Types...> type;\n};\n\ntemplate <class ..._Types, size_t _Ep>\nstruct __make_tuple_types<__tuple_types<_Types...>, _Ep, 0, true> {\n  typedef _LIBCUDACXX_NODEBUG_TYPE __tuple_types<_Types...> type;\n};\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TUPLE_MAKE_TUPLE_TYPES_H\n", "../__tuple_dir/sfinae_helpers.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TUPLE_SFINAE_HELPERS_H\n#define _LIBCUDACXX___TUPLE_SFINAE_HELPERS_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__fwd/tuple.h\"\n#include \"../__tuple_dir/make_tuple_types.h\"\n#include \"../__tuple_dir/tuple_element.h\"\n#include \"../__tuple_dir/tuple_like.h\"\n#include \"../__tuple_dir/tuple_size.h\"\n#include \"../__tuple_dir/tuple_types.h\"\n#include \"../__type_traits/enable_if.h\"\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_assignable.h\"\n#include \"../__type_traits/is_constructible.h\"\n#include \"../__type_traits/is_convertible.h\"\n#include \"../__type_traits/is_same.h\"\n#include \"../__type_traits/remove_cvref.h\"\n#include \"../__type_traits/remove_reference.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <bool ..._Preds>\nstruct __all_dummy;\n\ntemplate <bool ..._Pred>\nusing __all = _IsSame<__all_dummy<_Pred...>, __all_dummy<((void)_Pred, true)...>>;\n\nstruct __tuple_sfinae_base {\n  template <class, class>\n  struct __test_size : false_type {};\n\n  template <class ..._Tp, class ..._Up>\n  struct __test_size<__tuple_types<_Tp...>, __tuple_types<_Up...>>\n    : _BoolConstant<sizeof...(_Tp) == sizeof...(_Up)> {};\n\n  template <template <class, class...> class,\n            class _Tp, class _Up, bool = __test_size<_Tp, _Up>::value>\n  struct __test : false_type {};\n\n  template <template <class, class...> class _Trait,\n            class ..._LArgs, class ..._RArgs>\n  struct __test<_Trait, __tuple_types<_LArgs...>, __tuple_types<_RArgs...>, true>\n      : __all<_Trait<_LArgs, _RArgs>::value...> {};\n\n  template <class _FromArgs, class _ToArgs>\n  using __constructible = __test<is_constructible, _ToArgs, _FromArgs>;\n  template <class _FromArgs, class _ToArgs>\n  using __convertible = __test<is_convertible, _FromArgs, _ToArgs>;\n  template <class _FromArgs, class _ToArgs>\n  using __assignable = __test<is_assignable, _ToArgs, _FromArgs>;\n};\n\n// __tuple_convertible\n\ntemplate <class _Tp, class _Up, bool = __tuple_like<__libcpp_remove_reference_t<_Tp>>::value,\n                                bool = __tuple_like<_Up>::value>\nstruct __tuple_convertible\n    : public false_type {};\n\ntemplate <class _Tp, class _Up>\nstruct __tuple_convertible<_Tp, _Up, true, true>\n    : public __tuple_sfinae_base::__convertible<\n      typename __make_tuple_types<_Tp>::type\n    , typename __make_tuple_types<_Up>::type\n    >\n{};\n\n// __tuple_constructible\n\ntemplate <class _Tp, class _Up, bool = __tuple_like<__libcpp_remove_reference_t<_Tp>>::value,\n                                bool = __tuple_like<_Up>::value>\nstruct __tuple_constructible\n    : public false_type {};\n\ntemplate <class _Tp, class _Up>\nstruct __tuple_constructible<_Tp, _Up, true, true>\n    : public __tuple_sfinae_base::__constructible<\n      typename __make_tuple_types<_Tp>::type\n    , typename __make_tuple_types<_Up>::type\n    >\n{};\n\n// __tuple_assignable\n\ntemplate <class _Tp, class _Up, bool = __tuple_like<__libcpp_remove_reference_t<_Tp>>::value,\n                                bool = __tuple_like<_Up>::value>\nstruct __tuple_assignable\n    : public false_type {};\n\ntemplate <class _Tp, class _Up>\nstruct __tuple_assignable<_Tp, _Up, true, true>\n    : public __tuple_sfinae_base::__assignable<\n      typename __make_tuple_types<_Tp>::type\n    , typename __make_tuple_types<_Up&>::type\n    >\n{};\n\n\ntemplate <size_t _Ip, class ..._Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS tuple_element<_Ip, tuple<_Tp...> >\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE typename tuple_element<_Ip, __tuple_types<_Tp...> >::type type;\n};\n\ntemplate <bool _IsTuple, class _SizeTrait, size_t _Expected>\nstruct __tuple_like_with_size_imp : false_type {};\n\ntemplate <class _SizeTrait, size_t _Expected>\nstruct __tuple_like_with_size_imp<true, _SizeTrait, _Expected>\n    : integral_constant<bool, _SizeTrait::value == _Expected> {};\n\ntemplate <class _Tuple, size_t _ExpectedSize,\n          class _RawTuple = __remove_cvref_t<_Tuple>>\nusing __tuple_like_with_size _LIBCUDACXX_NODEBUG_TYPE = __tuple_like_with_size_imp<\n                                   __tuple_like<_RawTuple>::value,\n                                   tuple_size<_RawTuple>, _ExpectedSize\n                              >;\n\nstruct _LIBCUDACXX_TYPE_VIS __check_tuple_constructor_fail {\n    template <int&...>\n    using __enable_explicit_default = false_type;\n    template <int&...>\n    using __enable_implicit_default = false_type;\n    template <class ...>\n    using __enable_explicit = false_type;\n    template <class ...>\n    using __enable_implicit = false_type;\n    template <class ...>\n    using __enable_assign = false_type;\n};\n\n#if _LIBCUDACXX_STD_VER > 11\n\ntemplate <bool _CanCopy, bool _CanMove>\nstruct __sfinae_ctor_base {};\ntemplate <>\nstruct __sfinae_ctor_base<false, false> {\n  __sfinae_ctor_base() = default;\n  __sfinae_ctor_base(__sfinae_ctor_base const&) = delete;\n  __sfinae_ctor_base(__sfinae_ctor_base &&) = delete;\n  __sfinae_ctor_base& operator=(__sfinae_ctor_base const&) = default;\n  __sfinae_ctor_base& operator=(__sfinae_ctor_base&&) = default;\n};\ntemplate <>\nstruct __sfinae_ctor_base<true, false> {\n  __sfinae_ctor_base() = default;\n  __sfinae_ctor_base(__sfinae_ctor_base const&) = default;\n  __sfinae_ctor_base(__sfinae_ctor_base &&) = delete;\n  __sfinae_ctor_base& operator=(__sfinae_ctor_base const&) = default;\n  __sfinae_ctor_base& operator=(__sfinae_ctor_base&&) = default;\n};\ntemplate <>\nstruct __sfinae_ctor_base<false, true> {\n  __sfinae_ctor_base() = default;\n  __sfinae_ctor_base(__sfinae_ctor_base const&) = delete;\n  __sfinae_ctor_base(__sfinae_ctor_base &&) = default;\n  __sfinae_ctor_base& operator=(__sfinae_ctor_base const&) = default;\n  __sfinae_ctor_base& operator=(__sfinae_ctor_base&&) = default;\n};\n\ntemplate <bool _CanCopy, bool _CanMove>\nstruct __sfinae_assign_base {};\ntemplate <>\nstruct __sfinae_assign_base<false, false> {\n  __sfinae_assign_base() = default;\n  __sfinae_assign_base(__sfinae_assign_base const&) = default;\n  __sfinae_assign_base(__sfinae_assign_base &&) = default;\n  __sfinae_assign_base& operator=(__sfinae_assign_base const&) = delete;\n  __sfinae_assign_base& operator=(__sfinae_assign_base&&) = delete;\n};\ntemplate <>\nstruct __sfinae_assign_base<true, false> {\n  __sfinae_assign_base() = default;\n  __sfinae_assign_base(__sfinae_assign_base const&) = default;\n  __sfinae_assign_base(__sfinae_assign_base &&) = default;\n  __sfinae_assign_base& operator=(__sfinae_assign_base const&) = default;\n  __sfinae_assign_base& operator=(__sfinae_assign_base&&) = delete;\n};\ntemplate <>\nstruct __sfinae_assign_base<false, true> {\n  __sfinae_assign_base() = default;\n  __sfinae_assign_base(__sfinae_assign_base const&) = default;\n  __sfinae_assign_base(__sfinae_assign_base &&) = default;\n  __sfinae_assign_base& operator=(__sfinae_assign_base const&) = delete;\n  __sfinae_assign_base& operator=(__sfinae_assign_base&&) = default;\n};\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TUPLE_SFINAE_HELPERS_H\n", "../__tuple_dir/structured_bindings.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TUPLE_STRUCTURED_BINDINGS_H\n#define _LIBCUDACXX___TUPLE_STRUCTURED_BINDINGS_H\n\n#ifdef __cuda_std__\n\n#if defined(_LIBCUDACXX_COMPILER_CLANG)\n_Pragma(\"clang diagnostic push\")\n_Pragma(\"clang diagnostic ignored \\\"-Wmismatched-tags\\\"\")\n#endif // _LIBCUDACXX_COMPILER_CLANG\n\n#if !defined(__CUDACC_RTC__)\n// Fetch utility to get primary template for ::std::tuple_size necessary for the specialization of\n// ::std::tuple_size<cuda::std::tuple> to enable structured bindings.\n// See https://github.com/NVIDIA/libcudacxx/issues/316\n#include <utility>\n#endif\n\n#include \"../__fwd/array.h\"\n#include \"../__fwd/pair.h\"\n#include \"../__fwd/tuple.h\"\n#include \"../__tuple_dir/tuple_element.h\"\n#include \"../__tuple_dir/tuple_size.h\"\n#include \"../__type_traits/integral_constant.h\"\n\n// This is a workaround for the fact that structured bindings require that the specializations of\n// `tuple_size` and `tuple_element` reside in namespace std (https://eel.is/c++draft/dcl.struct.bind#4).\n// See https://github.com/NVIDIA/libcudacxx/issues/316 for a short discussion\n#if _LIBCUDACXX_STD_VER > 14\nnamespace std {\n#if defined(__CUDACC_RTC__)\n    template <class... _Tp>\n    struct tuple_size;\n\n    template<size_t _Ip, class... _Tp>\n    struct tuple_element;\n#endif\n\n    template <class _Tp, size_t _Size>\n    struct tuple_size<_CUDA_VSTD::array<_Tp, _Size>>\n      : _CUDA_VSTD::tuple_size<_CUDA_VSTD::array<_Tp, _Size>>\n    {};\n\n    template <class _Tp, size_t _Size>\n    struct tuple_size<const _CUDA_VSTD::array<_Tp, _Size>>\n      : _CUDA_VSTD::tuple_size<_CUDA_VSTD::array<_Tp, _Size>>\n    {};\n\n    template <class _Tp, size_t _Size>\n    struct tuple_size<volatile _CUDA_VSTD::array<_Tp, _Size>>\n      : _CUDA_VSTD::tuple_size<_CUDA_VSTD::array<_Tp, _Size>>\n    {};\n\n    template <class _Tp, size_t _Size>\n    struct tuple_size<const volatile _CUDA_VSTD::array<_Tp, _Size>>\n      : _CUDA_VSTD::tuple_size<_CUDA_VSTD::array<_Tp, _Size>>\n    {};\n\n    template<size_t _Ip, class _Tp, size_t _Size>\n    struct tuple_element<_Ip, _CUDA_VSTD::array<_Tp, _Size>>\n      : _CUDA_VSTD::tuple_element<_Ip, _CUDA_VSTD::array<_Tp, _Size>>\n    {};\n\n    template<size_t _Ip, class _Tp, size_t _Size>\n    struct tuple_element<_Ip, const _CUDA_VSTD::array<_Tp, _Size>>\n      : _CUDA_VSTD::tuple_element<_Ip, const _CUDA_VSTD::array<_Tp, _Size>>\n    {};\n\n    template<size_t _Ip, class _Tp, size_t _Size>\n    struct tuple_element<_Ip, volatile _CUDA_VSTD::array<_Tp, _Size>>\n      : _CUDA_VSTD::tuple_element<_Ip, volatile _CUDA_VSTD::array<_Tp, _Size>>\n    {};\n\n    template<size_t _Ip, class _Tp, size_t _Size>\n    struct tuple_element<_Ip, const volatile _CUDA_VSTD::array<_Tp, _Size>>\n      : _CUDA_VSTD::tuple_element<_Ip, const volatile _CUDA_VSTD::array<_Tp, _Size>>\n    {};\n\n    template <class _Tp, class _Up>\n    struct tuple_size<_CUDA_VSTD::pair<_Tp, _Up>>\n      : _CUDA_VSTD::tuple_size<_CUDA_VSTD::pair<_Tp, _Up>>\n    {};\n\n    template <class _Tp, class _Up>\n    struct tuple_size<const _CUDA_VSTD::pair<_Tp, _Up>>\n      : _CUDA_VSTD::tuple_size<_CUDA_VSTD::pair<_Tp, _Up>>\n    {};\n\n    template <class _Tp, class _Up>\n    struct tuple_size<volatile _CUDA_VSTD::pair<_Tp, _Up>>\n      : _CUDA_VSTD::tuple_size<_CUDA_VSTD::pair<_Tp, _Up>>\n    {};\n\n    template <class _Tp, class _Up>\n    struct tuple_size<const volatile _CUDA_VSTD::pair<_Tp, _Up>>\n      : _CUDA_VSTD::tuple_size<_CUDA_VSTD::pair<_Tp, _Up>>\n    {};\n\n    template<size_t _Ip, class _Tp, class _Up>\n    struct tuple_element<_Ip, _CUDA_VSTD::pair<_Tp, _Up>>\n      : _CUDA_VSTD::tuple_element<_Ip, _CUDA_VSTD::pair<_Tp, _Up>>\n    {};\n\n    template<size_t _Ip, class _Tp, class _Up>\n    struct tuple_element<_Ip, const _CUDA_VSTD::pair<_Tp, _Up>>\n      : _CUDA_VSTD::tuple_element<_Ip, const _CUDA_VSTD::pair<_Tp, _Up>>\n    {};\n\n    template<size_t _Ip, class _Tp, class _Up>\n    struct tuple_element<_Ip, volatile _CUDA_VSTD::pair<_Tp, _Up>>\n      : _CUDA_VSTD::tuple_element<_Ip, volatile _CUDA_VSTD::pair<_Tp, _Up>>\n    {};\n\n    template<size_t _Ip, class _Tp, class _Up>\n    struct tuple_element<_Ip, const volatile _CUDA_VSTD::pair<_Tp, _Up>>\n      : _CUDA_VSTD::tuple_element<_Ip, const volatile _CUDA_VSTD::pair<_Tp, _Up>>\n    {};\n\n    template <class... _Tp>\n    struct tuple_size<_CUDA_VSTD::tuple<_Tp...>>\n      : _CUDA_VSTD::tuple_size<_CUDA_VSTD::tuple<_Tp...>>\n    {};\n\n    template <class... _Tp>\n    struct tuple_size<const _CUDA_VSTD::tuple<_Tp...>>\n      : _CUDA_VSTD::tuple_size<_CUDA_VSTD::tuple<_Tp...>>\n    {};\n\n    template <class... _Tp>\n    struct tuple_size<volatile _CUDA_VSTD::tuple<_Tp...>>\n      : _CUDA_VSTD::tuple_size<_CUDA_VSTD::tuple<_Tp...>>\n    {};\n\n    template <class... _Tp>\n    struct tuple_size<const volatile _CUDA_VSTD::tuple<_Tp...>>\n      : _CUDA_VSTD::tuple_size<_CUDA_VSTD::tuple<_Tp...>>\n    {};\n\n    template<size_t _Ip, class... _Tp>\n    struct tuple_element<_Ip, _CUDA_VSTD::tuple<_Tp...>>\n      : _CUDA_VSTD::tuple_element<_Ip, _CUDA_VSTD::tuple<_Tp...>>\n    {};\n\n    template<size_t _Ip, class... _Tp>\n    struct tuple_element<_Ip, const _CUDA_VSTD::tuple<_Tp...>>\n      : _CUDA_VSTD::tuple_element<_Ip, const _CUDA_VSTD::tuple<_Tp...>>\n    {};\n\n    template<size_t _Ip, class... _Tp>\n    struct tuple_element<_Ip, volatile _CUDA_VSTD::tuple<_Tp...>>\n      : _CUDA_VSTD::tuple_element<_Ip, volatile _CUDA_VSTD::tuple<_Tp...>>\n    {};\n\n    template<size_t _Ip, class... _Tp>\n    struct tuple_element<_Ip, const volatile _CUDA_VSTD::tuple<_Tp...>>\n      : _CUDA_VSTD::tuple_element<_Ip, const volatile _CUDA_VSTD::tuple<_Tp...>>\n    {};\n}\n#endif // _LIBCUDACXX_STD_VER > 14\n\n#if defined(_LIBCUDACXX_COMPILER_CLANG)\n_Pragma(\"clang diagnostic pop\")\n# endif // _LIBCUDACXX_COMPILER_CLANG\n\n#endif // __cuda_std__\n\n#endif // _LIBCUDACXX___TUPLE_STRUCTURED_BINDINGS_H\n", "../__tuple_dir/tuple_element.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TUPLE_TUPLE_ELEMENT_H\n#define _LIBCUDACXX___TUPLE_TUPLE_ELEMENT_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__tuple_dir/tuple_indices.h\"\n#include \"../__tuple_dir/tuple_types.h\"\n#include \"../__type_traits/add_const.h\"\n#include \"../__type_traits/add_cv.h\"\n#include \"../__type_traits/add_volatile.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <size_t _Ip, class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS tuple_element;\n\ntemplate <size_t _Ip, class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS tuple_element<_Ip, const _Tp>\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE typename add_const<typename tuple_element<_Ip, _Tp>::type>::type type;\n};\n\ntemplate <size_t _Ip, class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS tuple_element<_Ip, volatile _Tp>\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE typename add_volatile<typename tuple_element<_Ip, _Tp>::type>::type type;\n};\n\ntemplate <size_t _Ip, class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS tuple_element<_Ip, const volatile _Tp>\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE typename add_cv<typename tuple_element<_Ip, _Tp>::type>::type type;\n};\n\n#ifdef _LIBCUDACXX_COMPILER_MSVC\n\nnamespace __indexer_detail {\n\ntemplate <size_t _Idx, class ..._Types>\nstruct _nth_of;\n\ntemplate <class _Head, class ..._Tail>\nstruct _nth_of<0, _Head, _Tail...> {\n    using type = _Head;\n};\n\ntemplate <size_t _Idx, class _Head, class ..._Tail>\nstruct _nth_of<_Idx, _Head, _Tail...> {\n    using type = typename _nth_of<_Idx-1, _Tail...>::type;\n};\n\ntemplate <size_t _Idx, class ..._Types>\nstruct nth_of {\n    static_assert(_Idx < sizeof...(_Types), \"\");\n    using _impl = _nth_of<_Idx, _Types...>;\n    using type = typename _impl::type;\n};\n\n} // namespace __indexer_detail\n\ntemplate <size_t _Idx, class ..._Types>\nusing __type_pack_element _LIBCUDACXX_NODEBUG_TYPE = typename __indexer_detail::nth_of<_Idx, _Types...>::type;\n\n#elif !__has_builtin(__type_pack_element)\n\nnamespace __indexer_detail {\n\ntemplate <size_t _Idx, class _Tp>\nstruct __indexed { using type _LIBCUDACXX_NODEBUG_TYPE = _Tp; };\n\ntemplate <class _Types, class _Indexes> struct __indexer;\n\ntemplate <class ..._Types, size_t ..._Idx>\nstruct __indexer<__tuple_types<_Types...>, __tuple_indices<_Idx...>>\n    : __indexed<_Idx, _Types>...\n{};\n\ntemplate <size_t _Idx, class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n__indexed<_Idx, _Tp> __at_index(__indexed<_Idx, _Tp> const&);\n\n} // namespace __indexer_detail\n\ntemplate <size_t _Idx, class ..._Types>\nusing __type_pack_element _LIBCUDACXX_NODEBUG_TYPE = typename decltype(\n    __indexer_detail::__at_index<_Idx>(\n        __indexer_detail::__indexer<\n            __tuple_types<_Types...>,\n            typename __make_tuple_indices<sizeof...(_Types)>::type\n        >{})\n  )::type;\n#endif\n\ntemplate <size_t _Ip, class ..._Types>\nstruct _LIBCUDACXX_TEMPLATE_VIS tuple_element<_Ip, __tuple_types<_Types...> >\n{\n    static_assert(_Ip < sizeof...(_Types), \"tuple_element index out of range\");\n    typedef _LIBCUDACXX_NODEBUG_TYPE __type_pack_element<_Ip, _Types...> type;\n};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <size_t _Ip, class ..._Tp>\nusing tuple_element_t _LIBCUDACXX_NODEBUG_TYPE = typename tuple_element <_Ip, _Tp...>::type;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TUPLE_TUPLE_ELEMENT_H\n", "../__tuple_dir/tuple_indices.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TUPLE_MAKE_TUPLE_INDICES_H\n#define _LIBCUDACXX___TUPLE_MAKE_TUPLE_INDICES_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__utility/integer_sequence.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <size_t _Ep, size_t _Sp = 0>\nstruct __make_tuple_indices\n{\n    static_assert(_Sp <= _Ep, \"__make_tuple_indices input error\");\n    typedef __make_indices_imp<_Ep, _Sp> type;\n};\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TUPLE_MAKE_TUPLE_INDICES_H\n", "../__tuple_dir/tuple_like.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TUPLE_TUPLE_LIKE_H\n#define _LIBCUDACXX___TUPLE_TUPLE_LIKE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__fwd/array.h\"\n#include \"../__fwd/pair.h\"\n#include \"../__fwd/tuple.h\"\n#include \"../__tuple_dir/tuple_types.h\"\n#include \"../__type_traits/integral_constant.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp> struct __tuple_like : false_type {};\n\ntemplate <class _Tp> struct __tuple_like<const _Tp> : public __tuple_like<_Tp> {};\ntemplate <class _Tp> struct __tuple_like<volatile _Tp> : public __tuple_like<_Tp> {};\ntemplate <class _Tp> struct __tuple_like<const volatile _Tp> : public __tuple_like<_Tp> {};\n\ntemplate <class... _Tp> struct __tuple_like<tuple<_Tp...> > : true_type {};\n\ntemplate <class _T1, class _T2> struct __tuple_like<pair<_T1, _T2> > : true_type {};\n\ntemplate <class _Tp, size_t _Size> struct __tuple_like<array<_Tp, _Size> > : true_type {};\n\ntemplate <class... _Tp> struct __tuple_like<__tuple_types<_Tp...> > : true_type {};\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TUPLE_TUPLE_LIKE_H\n", "../__tuple_dir/tuple_size.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TUPLE_TUPLE_SIZE_H\n#define _LIBCUDACXX___TUPLE_TUPLE_SIZE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__fwd/tuple.h\"\n#include \"../__tuple_dir/tuple_types.h\"\n#include \"../__type_traits/enable_if.h\"\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_const.h\"\n#include \"../__type_traits/is_volatile.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS tuple_size;\n\ntemplate <class _Tp, class...>\nusing __enable_if_tuple_size_imp = _Tp;\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS tuple_size<__enable_if_tuple_size_imp<\n    const _Tp,\n    __enable_if_t<!is_volatile<_Tp>::value>,\n    integral_constant<size_t, sizeof(tuple_size<_Tp>)>>>\n    : public integral_constant<size_t, tuple_size<_Tp>::value> {};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS tuple_size<__enable_if_tuple_size_imp<\n    volatile _Tp,\n    __enable_if_t<!is_const<_Tp>::value>,\n    integral_constant<size_t, sizeof(tuple_size<_Tp>)>>>\n    : public integral_constant<size_t, tuple_size<_Tp>::value> {};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS tuple_size<__enable_if_tuple_size_imp<\n    const volatile _Tp,\n    integral_constant<size_t, sizeof(tuple_size<_Tp>)>>>\n    : public integral_constant<size_t, tuple_size<_Tp>::value> {};\n\ntemplate <class ..._Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS tuple_size<tuple<_Tp...> >\n    : public integral_constant<size_t, sizeof...(_Tp)>\n{\n};\n\ntemplate <class ..._Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS tuple_size<__tuple_types<_Tp...> >\n    : public integral_constant<size_t, sizeof...(_Tp)>\n{\n};\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TUPLE_TUPLE_SIZE_H\n", "../__tuple_dir/tuple_types.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TUPLE_TUPLE_TYPES_H\n#define _LIBCUDACXX___TUPLE_TUPLE_TYPES_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class ..._Tp> struct __tuple_types {};\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TUPLE_TUPLE_TYPES_H\n", "../__type_traits/add_const.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_ADD_CONST_H\n#define _LIBCUDACXX___TYPE_TRAITS_ADD_CONST_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS add_const {\n  typedef _LIBCUDACXX_NODEBUG_TYPE const _Tp type;\n};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> using add_const_t = typename add_const<_Tp>::type;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_ADD_CONST_H\n", "../__type_traits/add_cv.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_ADD_CV_H\n#define _LIBCUDACXX___TYPE_TRAITS_ADD_CV_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS add_cv {\n  typedef _LIBCUDACXX_NODEBUG_TYPE const volatile _Tp type;\n};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> using add_cv_t = typename add_cv<_Tp>::type;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_ADD_CV_H\n", "../__type_traits/add_lvalue_reference.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_ADD_LVALUE_REFERENCE_H\n#define _LIBCUDACXX___TYPE_TRAITS_ADD_LVALUE_REFERENCE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/is_referenceable.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_ADD_LVALUE_REFERENCE) && !defined(_LIBCUDACXX_USE_ADD_LVALUE_REFERENCE_FALLBACK)\n\ntemplate <class _Tp>\nusing __add_lvalue_reference_t = _LIBCUDACXX_ADD_LVALUE_REFERENCE(_Tp);\n\n#else\n\ntemplate <class _Tp, bool = __libcpp_is_referenceable<_Tp>::value>\nstruct __add_lvalue_reference_impl {\n  typedef _LIBCUDACXX_NODEBUG_TYPE _Tp type;\n};\ntemplate <class _Tp >\nstruct __add_lvalue_reference_impl<_Tp, true> {\n  typedef _LIBCUDACXX_NODEBUG_TYPE _Tp& type;\n};\n\ntemplate <class _Tp>\nusing __add_lvalue_reference_t = typename __add_lvalue_reference_impl<_Tp>::type;\n\n#endif // defined(_LIBCUDACXX_ADD_LVALUE_REFERENCE) && !defined(_LIBCUDACXX_USE_ADD_LVALUE_REFERENCE_FALLBACK)\n\ntemplate <class _Tp>\nstruct add_lvalue_reference {\n  using type _LIBCUDACXX_NODEBUG_TYPE = __add_lvalue_reference_t<_Tp>;\n};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> using add_lvalue_reference_t = __add_lvalue_reference_t<_Tp>;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_ADD_LVALUE_REFERENCE_H\n", "../__type_traits/add_pointer.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_ADD_POINTER_H\n#define _LIBCUDACXX___TYPE_TRAITS_ADD_POINTER_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/is_referenceable.h\"\n#include \"../__type_traits/is_same.h\"\n#include \"../__type_traits/is_void.h\"\n#include \"../__type_traits/remove_cv.h\"\n#include \"../__type_traits/remove_reference.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_ADD_POINTER) && !defined(_LIBCUDACXX_USE_ADD_POINTER_FALLBACK)\n\ntemplate <class _Tp>\nusing __add_pointer_t = _LIBCUDACXX_ADD_POINTER(_Tp);\n\n#else\ntemplate <class _Tp,\n          bool = __libcpp_is_referenceable<_Tp>::value || is_void<_Tp>::value>\nstruct __add_pointer_impl {\n  typedef _LIBCUDACXX_NODEBUG_TYPE __libcpp_remove_reference_t<_Tp>* type;\n};\ntemplate <class _Tp> struct __add_pointer_impl<_Tp, false>\n    {typedef _LIBCUDACXX_NODEBUG_TYPE _Tp type;};\n\ntemplate <class _Tp>\nusing __add_pointer_t = typename __add_pointer_impl<_Tp>::type;\n\n#endif // defined(_LIBCUDACXX_ADD_POINTER) && !defined(_LIBCUDACXX_USE_ADD_POINTER_FALLBACK)\n\ntemplate <class _Tp>\nstruct add_pointer {\n  using type _LIBCUDACXX_NODEBUG_TYPE = __add_pointer_t<_Tp>;\n};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> using add_pointer_t = __add_pointer_t<_Tp>;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_ADD_POINTER_H\n", "../__type_traits/add_rvalue_reference.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_ADD_RVALUE_REFERENCE_H\n#define _LIBCUDACXX___TYPE_TRAITS_ADD_RVALUE_REFERENCE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/is_referenceable.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_ADD_RVALUE_REFERENCE) && !defined(_LIBCUDACXX_USE_ADD_RVALUE_REFERENCE_FALLBACK)\n\ntemplate <class _Tp>\nusing __add_rvalue_reference_t = _LIBCUDACXX_ADD_RVALUE_REFERENCE(_Tp);\n\n#else\n\ntemplate <class _Tp, bool = __libcpp_is_referenceable<_Tp>::value>\nstruct __add_rvalue_reference_impl {\n  typedef _LIBCUDACXX_NODEBUG_TYPE _Tp type;\n};\ntemplate <class _Tp >\nstruct __add_rvalue_reference_impl<_Tp, true> {\n  typedef _LIBCUDACXX_NODEBUG_TYPE _Tp&& type;\n};\n\ntemplate <class _Tp>\nusing __add_rvalue_reference_t = typename __add_rvalue_reference_impl<_Tp>::type;\n\n#endif // defined(_LIBCUDACXX_ADD_RVALUE_REFERENCE) && !defined(_LIBCUDACXX_USE_ADD_RVALUE_REFERENCE_FALLBACK)\n\ntemplate <class _Tp>\nstruct add_rvalue_reference {\n  using type = __add_rvalue_reference_t<_Tp>;\n};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp>\nusing add_rvalue_reference_t = __add_rvalue_reference_t<_Tp>;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_ADD_RVALUE_REFERENCE_H\n", "../__type_traits/add_volatile.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_ADD_VOLATILE_H\n#define _LIBCUDACXX___TYPE_TRAITS_ADD_VOLATILE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS add_volatile {\n  typedef _LIBCUDACXX_NODEBUG_TYPE volatile _Tp type;\n};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> using add_volatile_t = typename add_volatile<_Tp>::type;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_ADD_VOLATILE_H\n", "../__type_traits/aligned_storage.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_ALIGNED_STORAGE_H\n#define _LIBCUDACXX___TYPE_TRAITS_ALIGNED_STORAGE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/conditional.h\"\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/nat.h\"\n#include \"../__type_traits/type_list.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp>\nstruct __align_type\n{\n    static const size_t value = _LIBCUDACXX_PREFERRED_ALIGNOF(_Tp);\n    typedef _Tp type;\n};\n\nstruct __struct_double {long double __lx;};\nstruct __struct_double4 {double __lx[4];};\n\ntypedef\n    __type_list<__align_type<unsigned char>,\n    __type_list<__align_type<unsigned short>,\n    __type_list<__align_type<unsigned int>,\n    __type_list<__align_type<unsigned long>,\n    __type_list<__align_type<unsigned long long>,\n    __type_list<__align_type<double>,\n    __type_list<__align_type<long double>,\n    __type_list<__align_type<__struct_double>,\n    __type_list<__align_type<__struct_double4>,\n    __type_list<__align_type<int*>,\n    __nat\n    > > > > > > > > > > __all_types;\n\ntemplate <size_t _Align>\nstruct _ALIGNAS(_Align) __fallback_overaligned {};\n\ntemplate <class _TL, size_t _Align> struct __find_pod;\n\ntemplate <class _Hp, size_t _Align>\nstruct __find_pod<__type_list<_Hp, __nat>, _Align>\n{\n    typedef __conditional_t<_Align == _Hp::value, typename _Hp::type, __fallback_overaligned<_Align> > type;\n};\n\ntemplate <class _Hp, class _Tp, size_t _Align>\nstruct __find_pod<__type_list<_Hp, _Tp>, _Align>\n{\n    typedef __conditional_t<_Align == _Hp::value, typename _Hp::type, typename __find_pod<_Tp, _Align>::type> type;\n};\n\ntemplate <class _TL, size_t _Len> struct __find_max_align;\n\ntemplate <class _Hp, size_t _Len>\nstruct __find_max_align<__type_list<_Hp, __nat>, _Len> : public integral_constant<size_t, _Hp::value> {};\n\ntemplate <size_t _Len, size_t _A1, size_t _A2>\nstruct __select_align\n{\nprivate:\n    static const size_t __min = _A2 < _A1 ? _A2 : _A1;\n    static const size_t __max = _A1 < _A2 ? _A2 : _A1;\npublic:\n    static const size_t value = _Len < __max ? __min : __max;\n};\n\ntemplate <class _Hp, class _Tp, size_t _Len>\nstruct __find_max_align<__type_list<_Hp, _Tp>, _Len>\n    : public integral_constant<size_t, __select_align<_Len, _Hp::value, __find_max_align<_Tp, _Len>::value>::value> {};\n\ntemplate <size_t _Len, size_t _Align = __find_max_align<__all_types, _Len>::value>\nstruct _LIBCUDACXX_TEMPLATE_VIS aligned_storage\n{\n    typedef typename __find_pod<__all_types, _Align>::type _Aligner;\n    union type\n    {\n        _Aligner __align;\n        unsigned char __data[(_Len + _Align - 1)/_Align * _Align];\n    };\n};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <size_t _Len, size_t _Align = __find_max_align<__all_types, _Len>::value>\n    using aligned_storage_t = typename aligned_storage<_Len, _Align>::type;\n#endif\n\n#define _CREATE_ALIGNED_STORAGE_SPECIALIZATION(n) \\\ntemplate <size_t _Len>\\\nstruct _LIBCUDACXX_TEMPLATE_VIS aligned_storage<_Len, n>\\\n{\\\n    struct _ALIGNAS(n) type\\\n    {\\\n        unsigned char __lx[(_Len + n - 1)/n * n];\\\n    };\\\n}\n\n_CREATE_ALIGNED_STORAGE_SPECIALIZATION(0x1);\n_CREATE_ALIGNED_STORAGE_SPECIALIZATION(0x2);\n_CREATE_ALIGNED_STORAGE_SPECIALIZATION(0x4);\n_CREATE_ALIGNED_STORAGE_SPECIALIZATION(0x8);\n_CREATE_ALIGNED_STORAGE_SPECIALIZATION(0x10);\n_CREATE_ALIGNED_STORAGE_SPECIALIZATION(0x20);\n_CREATE_ALIGNED_STORAGE_SPECIALIZATION(0x40);\n_CREATE_ALIGNED_STORAGE_SPECIALIZATION(0x80);\n_CREATE_ALIGNED_STORAGE_SPECIALIZATION(0x100);\n_CREATE_ALIGNED_STORAGE_SPECIALIZATION(0x200);\n_CREATE_ALIGNED_STORAGE_SPECIALIZATION(0x400);\n_CREATE_ALIGNED_STORAGE_SPECIALIZATION(0x800);\n_CREATE_ALIGNED_STORAGE_SPECIALIZATION(0x1000);\n_CREATE_ALIGNED_STORAGE_SPECIALIZATION(0x2000);\n// PE/COFF does not support alignment beyond 8192 (=0x2000)\n#if !defined(_LIBCUDACXX_OBJECT_FORMAT_COFF)\n_CREATE_ALIGNED_STORAGE_SPECIALIZATION(0x4000);\n#endif // !defined(_LIBCUDACXX_OBJECT_FORMAT_COFF)\n\n#undef _CREATE_ALIGNED_STORAGE_SPECIALIZATION\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_ALIGNED_STORAGE_H\n", "../__type_traits/apply_cv.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_APPLY_CV_H\n#define _LIBCUDACXX___TYPE_TRAITS_APPLY_CV_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/is_const.h\"\n#include \"../__type_traits/is_volatile.h\"\n#include \"../__type_traits/remove_reference.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp, class _Up, bool = is_const<__libcpp_remove_reference_t<_Tp> >::value,\n                             bool = is_volatile<__libcpp_remove_reference_t<_Tp> >::value>\nstruct __apply_cv\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE _Up type;\n};\n\ntemplate <class _Tp, class _Up>\nstruct __apply_cv<_Tp, _Up, true, false>\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE const _Up type;\n};\n\ntemplate <class _Tp, class _Up>\nstruct __apply_cv<_Tp, _Up, false, true>\n{\n    typedef volatile _Up type;\n};\n\ntemplate <class _Tp, class _Up>\nstruct __apply_cv<_Tp, _Up, true, true>\n{\n    typedef const volatile _Up type;\n};\n\ntemplate <class _Tp, class _Up>\nstruct __apply_cv<_Tp&, _Up, false, false>\n{\n    typedef _Up& type;\n};\n\ntemplate <class _Tp, class _Up>\nstruct __apply_cv<_Tp&, _Up, true, false>\n{\n    typedef const _Up& type;\n};\n\ntemplate <class _Tp, class _Up>\nstruct __apply_cv<_Tp&, _Up, false, true>\n{\n    typedef volatile _Up& type;\n};\n\ntemplate <class _Tp, class _Up>\nstruct __apply_cv<_Tp&, _Up, true, true>\n{\n    typedef const volatile _Up& type;\n};\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_APPLY_CV_H\n", "../__type_traits/common_reference.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n// SPDX-FileCopyrightText: Copyright (c) Microsoft Corporation.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_COMMON_REFERENCE_H\n#define _LIBCUDACXX___TYPE_TRAITS_COMMON_REFERENCE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/common_type.h\"\n#include \"../__type_traits/conditional.h\"\n#include \"../__type_traits/copy_cv.h\"\n#include \"../__type_traits/copy_cvref.h\"\n#include \"../__type_traits/disjunction.h\"\n#include \"../__type_traits/enable_if.h\"\n#include \"../__type_traits/is_array.h\"\n#include \"../__type_traits/is_convertible.h\"\n#include \"../__type_traits/is_reference.h\"\n#include \"../__type_traits/is_same.h\"\n#include \"../__type_traits/is_scalar.h\"\n#include \"../__type_traits/remove_reference.h\"\n#include \"../__type_traits/remove_cvref.h\"\n#include \"../__type_traits/void_t.h\"\n#include \"../__utility/declval.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n// common_reference\n#if _LIBCUDACXX_STD_VER > 11\n\n// Let COND_RES(X, Y) be:\n#ifdef _LIBCUDACXX_COMPILER_MSVC // Workaround for DevCom-1627396\ntemplate <class _Tp>\n_Tp __returns_exactly() noexcept; // not defined\n\ntemplate <class _Xp, class _Yp>\nusing __cond_res_if_right = decltype(false ? __returns_exactly<_Xp>() : __returns_exactly<_Yp>());\n\ntemplate <class _Tp, class _Up, class = void>\nstruct __cond_res_workaround {};\n\ntemplate <class _Tp, class _Up>\nstruct __cond_res_workaround<_Tp, _Up, void_t<__cond_res_if_right<_Tp, _Up>>> {\n    using _RTp = remove_cvref_t<_Tp>;\n    using type = conditional_t<is_same_v<_RTp, remove_cvref_t<_Up>> &&\n                               (is_scalar_v<_RTp> || is_array_v<_RTp>) &&\n                               ((is_lvalue_reference_v<_Tp> && is_rvalue_reference_v<_Up>) || (is_rvalue_reference_v<_Tp> && is_lvalue_reference_v<_Up>)),\n                 decay_t<__copy_cv_t<remove_reference_t<_Tp>, remove_reference_t<_Up>>>, __cond_res_if_right<_Tp, _Up>>;\n};\n\ntemplate <class _Xp, class _Yp>\nusing __cond_res = typename __cond_res_workaround<_Xp, _Yp>::type;\n#else // ^^^ MSVC ^^^ / vvv !MSVC vvv\ntemplate <class _Xp, class _Yp>\nusing __cond_res =\n    decltype(false ? _CUDA_VSTD::declval<_Xp(&)()>()() : _CUDA_VSTD::declval<_Yp(&)()>()());\n#endif // !MSVC\n\n// Let `XREF(A)` denote a unary alias template `T` such that `T<U>` denotes the same type as `U`\n// with the addition of `A`'s cv and reference qualifiers, for a non-reference cv-unqualified type\n// `U`.\n// [Note: `XREF(A)` is `__xref<A>::template __apply`]\ntemplate <class _Tp>\nstruct __xref {\n  template<class _Up>\n  using __apply = __copy_cvref_t<_Tp, _Up>;\n};\n\n// Given types A and B, let X be remove_reference_t<A>, let Y be remove_reference_t<B>,\n// and let COMMON-REF(A, B) be:\ntemplate<class _Ap, class _Bp, class = void>\nstruct __common_ref;\n\ntemplate<class _Xp, class _Yp>\nusing __common_ref_t = typename __common_ref<_Xp, _Yp>::__type;\n\ntemplate<class _Xp, class _Yp>\nusing __cv_cond_res = __cond_res<__copy_cv_t<_Xp, _Yp>&, __copy_cv_t<_Yp, _Xp>&>;\n\n\n//    If A and B are both lvalue reference types, COMMON-REF(A, B) is\n//    COND-RES(COPYCV(X, Y)&, COPYCV(Y, X)&) if that type exists and is a reference type.\ntemplate<class _Ap, class _Bp>\nstruct __common_ref<_Ap&, _Bp&, enable_if_t<is_reference_v<__cv_cond_res<_Ap, _Bp>>>>\n{\n    using __type = __cv_cond_res<_Ap, _Bp>;\n};\n\n//    Otherwise, let C be remove_reference_t<COMMON-REF(X&, Y&)>&&. ...\ntemplate <class _Xp, class _Yp>\nusing __common_ref_C = remove_reference_t<__common_ref_t<_Xp&, _Yp&>>&&;\n\n\n//    .... If A and B are both rvalue reference types, C is well-formed, and\n//    is_convertible_v<A, C> && is_convertible_v<B, C> is true, then COMMON-REF(A, B) is C.\ntemplate<class _Ap, class _Bp, class = void>\nstruct __common_ref_rr {};\n\ntemplate<class _Ap, class _Bp>\nstruct __common_ref_rr<_Ap&&, _Bp&&, enable_if_t<\n                            is_convertible_v<_Ap&&, __common_ref_C<_Ap, _Bp>>\n                         && is_convertible_v<_Bp&&, __common_ref_C<_Ap, _Bp>>>>\n{\n    using __type = __common_ref_C<_Ap, _Bp>;\n};\n\ntemplate<class _Ap, class _Bp>\nstruct __common_ref<_Ap&&, _Bp&&> : __common_ref_rr<_Ap&&, _Bp&&> {};\n\n//    Otherwise, let D be COMMON-REF(const X&, Y&). ...\ntemplate <class _Tp, class _Up>\nusing __common_ref_D = __common_ref_t<const _Tp&, _Up&>;\n\n//    ... If A is an rvalue reference and B is an lvalue reference and D is well-formed and\n//    is_convertible_v<A, D> is true, then COMMON-REF(A, B) is D.\ntemplate<class _Ap, class _Bp, class = void>\nstruct __common_ref_lr {};\n\ntemplate<class _Ap, class _Bp>\nstruct __common_ref_lr<_Ap&&, _Bp&, enable_if_t<is_convertible_v<_Ap&&, __common_ref_D<_Ap, _Bp>>>>\n{\n    using __type = __common_ref_D<_Ap, _Bp>;\n};\n\ntemplate<class _Ap, class _Bp>\nstruct __common_ref<_Ap&&, _Bp&> : __common_ref_lr<_Ap&&, _Bp&> {};\n\n//    Otherwise, if A is an lvalue reference and B is an rvalue reference, then\n//    COMMON-REF(A, B) is COMMON-REF(B, A).\ntemplate<class _Ap, class _Bp>\nstruct __common_ref<_Ap&, _Bp&&> : __common_ref_lr<_Bp&&, _Ap&> {};\n\n//    Otherwise, COMMON-REF(A, B) is ill-formed.\ntemplate<class _Ap, class _Bp, class>\nstruct __common_ref {};\n\n// Note C: For the common_reference trait applied to a parameter pack [...]\n\ntemplate <class...>\nstruct common_reference;\n\ntemplate <class... _Types>\nusing common_reference_t = typename common_reference<_Types...>::type;\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate<class, class, class = void>\n_LIBCUDACXX_INLINE_VAR constexpr bool __has_common_reference = false;\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_INLINE_VAR constexpr bool __has_common_reference<_Tp, _Up, void_t<common_reference_t<_Tp, _Up>>> = true;\n#endif  // _LIBCUDACXX_STD_VER > 11\n\n// bullet 1 - sizeof...(T) == 0\ntemplate<>\nstruct common_reference<> {};\n\n// bullet 2 - sizeof...(T) == 1\ntemplate <class _Tp>\nstruct common_reference<_Tp>\n{\n    using type = _Tp;\n};\n\n// bullet 3 - sizeof...(T) == 2\ntemplate <class _Tp, class _Up, class = void> struct __common_reference_sub_bullet3;\ntemplate <class _Tp, class _Up, class = void> struct __common_reference_sub_bullet2\n    : __common_reference_sub_bullet3<_Tp, _Up> {};\ntemplate <class _Tp, class _Up, class = void> struct __common_reference_sub_bullet1\n    : __common_reference_sub_bullet2<_Tp, _Up> {};\n\n// sub-bullet 1 - If T1 and T2 are reference types and COMMON-REF(T1, T2) is well-formed, then\n// the member typedef `type` denotes that type.\ntemplate <class _Tp, class _Up> struct common_reference<_Tp, _Up> : __common_reference_sub_bullet1<_Tp, _Up> {};\n\ntemplate <class _Tp, class _Up>\nstruct __common_reference_sub_bullet1<_Tp, _Up, void_t<__common_ref_t<_Tp, _Up>,\n    enable_if_t<is_reference_v<_Tp> && is_reference_v<_Up>>>>\n{\n    using type = __common_ref_t<_Tp, _Up>;\n};\n\n// sub-bullet 2 - Otherwise, if basic_common_reference<remove_cvref_t<T1>, remove_cvref_t<T2>, XREF(T1), XREF(T2)>::type\n// is well-formed, then the member typedef `type` denotes that type.\ntemplate <class, class, template <class> class, template <class> class> struct basic_common_reference {};\n\ntemplate <class _Tp, class _Up>\nusing __basic_common_reference_t = typename basic_common_reference<\n    remove_cvref_t<_Tp>, remove_cvref_t<_Up>,\n    __xref<_Tp>::template __apply, __xref<_Up>::template __apply>::type;\n\ntemplate <class _Tp, class _Up>\nstruct __common_reference_sub_bullet2<_Tp, _Up, void_t<__basic_common_reference_t<_Tp, _Up>>>\n{\n    using type = __basic_common_reference_t<_Tp, _Up>;\n};\n\n// sub-bullet 3 - Otherwise, if COND-RES(T1, T2) is well-formed,\n// then the member typedef `type` denotes that type.\ntemplate <class _Tp, class _Up>\nstruct __common_reference_sub_bullet3<_Tp, _Up, void_t<__cond_res<_Tp, _Up>>>\n{\n    using type = __cond_res<_Tp, _Up>;\n};\n\n\n// sub-bullet 4 & 5 - Otherwise, if common_type_t<T1, T2> is well-formed,\n//                    then the member typedef `type` denotes that type.\n//                  - Otherwise, there shall be no member `type`.\ntemplate <class _Tp, class _Up, class> struct __common_reference_sub_bullet3 : common_type<_Tp, _Up> {};\n\n// bullet 4 - If there is such a type `C`, the member typedef type shall denote the same type, if\n//            any, as `common_reference_t<C, Rest...>`.\ntemplate <class _Tp, class _Up, class _Vp, class... _Rest>\nstruct common_reference<_Tp, _Up, _Vp, void_t<common_reference_t<_Tp, _Up>>, _Rest...>\n    : common_reference<common_reference_t<_Tp, _Up>, _Vp, _Rest...>\n{};\n\n// bullet 5 - Otherwise, there shall be no member `type`.\ntemplate <class...> struct common_reference {};\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_COMMON_REFERENCE_H\n", "../__type_traits/common_type.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_COMMON_TYPE_H\n#define _LIBCUDACXX___TYPE_TRAITS_COMMON_TYPE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/decay.h\"\n#include \"../__type_traits/is_same.h\"\n#include \"../__type_traits/remove_cvref.h\"\n#include \"../__type_traits/void_t.h\"\n#include \"../__utility/declval.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class... _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS common_type;\n\ntemplate <class ..._Tp>\nusing __common_type_t = typename common_type<_Tp...>::type;\n\n// Let COND_RES(X, Y) be:\ntemplate <class _Tp, class _Up>\nusing __cond_type = decltype(false ? declval<_Tp>() : declval<_Up>());\n\n#if _LIBCUDACXX_STD_VER > 17\ntemplate <class _Tp, class _Up, class = void>\nstruct __common_type3 {};\n\n// sub-bullet 4 - \"if COND_RES(CREF(D1), CREF(D2)) denotes a type...\"\ntemplate <class _Tp, class _Up>\nstruct __common_type3<_Tp, _Up, void_t<__cond_type<const _Tp&, const _Up&>>>\n{\n    using type = remove_cvref_t<__cond_type<const _Tp&, const _Up&>>;\n};\n\ntemplate <class _Tp, class _Up, class = void>\nstruct __common_type2_imp : __common_type3<_Tp, _Up> {};\n#else\ntemplate <class _Tp, class _Up, class = void>\nstruct __common_type2_imp {};\n#endif\n\n// sub-bullet 3 - \"if decay_t<decltype(false ? declval<D1>() : declval<D2>())> ...\"\ntemplate <class _Tp, class _Up>\nstruct __common_type2_imp<_Tp, _Up, __void_t<__cond_type<_Tp, _Up>>>\n{\n  typedef _LIBCUDACXX_NODEBUG_TYPE __decay_t<__cond_type<_Tp, _Up>> type;\n};\n\ntemplate <class, class = void>\nstruct __common_type_impl {};\n\ntemplate <class... _Tp>\nstruct __common_types;\n\ntemplate <class _Tp, class _Up>\nstruct __common_type_impl<\n    __common_types<_Tp, _Up>, __void_t<__common_type_t<_Tp, _Up>> >\n{\n  typedef __common_type_t<_Tp, _Up> type;\n};\n\ntemplate <class _Tp, class _Up, class _Vp, class... _Rest>\nstruct __common_type_impl<__common_types<_Tp, _Up, _Vp, _Rest...>, __void_t<__common_type_t<_Tp, _Up>> >\n    : __common_type_impl<__common_types<__common_type_t<_Tp, _Up>, _Vp, _Rest...>> {};\n\n// bullet 1 - sizeof...(Tp) == 0\n\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS common_type<> {};\n\n// bullet 2 - sizeof...(Tp) == 1\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS common_type<_Tp>\n    : public common_type<_Tp, _Tp> {};\n\n// bullet 3 - sizeof...(Tp) == 2\n\n// sub-bullet 1 - \"If is_same_v<T1, D1> is false or ...\"\ntemplate <class _Tp, class _Up, class _D1 = __decay_t<_Tp>, class _D2 = __decay_t<_Up>>\nstruct __common_type2 : common_type<_D1, _D2> {};\n\ntemplate <class _Tp, class _Up>\nstruct __common_type2<_Tp, _Up, _Tp, _Up> : __common_type2_imp<_Tp, _Up> {};\n\ntemplate <class _Tp, class _Up>\nstruct _LIBCUDACXX_TEMPLATE_VIS common_type<_Tp, _Up>\n    : __common_type2<_Tp, _Up> {};\n\n// bullet 4 - sizeof...(Tp) > 2\n\ntemplate <class _Tp, class _Up, class _Vp, class... _Rest>\nstruct _LIBCUDACXX_TEMPLATE_VIS common_type<_Tp, _Up, _Vp, _Rest...>\n    : __common_type_impl<__common_types<_Tp, _Up, _Vp, _Rest...> > {};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class ..._Tp> using common_type_t = typename common_type<_Tp...>::type;\n\ntemplate<class, class, class = void>\n_LIBCUDACXX_INLINE_VAR constexpr bool __has_common_type = false;\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_INLINE_VAR constexpr bool __has_common_type<_Tp, _Up, void_t<common_type_t<_Tp, _Up>>> = true;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_COMMON_TYPE_H\n", "../__type_traits/conditional.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_CONDITIONAL_H\n#define _LIBCUDACXX___TYPE_TRAITS_CONDITIONAL_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <bool>\nstruct _IfImpl;\n\ntemplate <>\nstruct _IfImpl<true> {\n  template <class _IfRes, class _ElseRes>\n  using _Select _LIBCUDACXX_NODEBUG_TYPE = _IfRes;\n};\n\ntemplate <>\nstruct _IfImpl<false> {\n  template <class _IfRes, class _ElseRes>\n  using _Select _LIBCUDACXX_NODEBUG_TYPE = _ElseRes;\n};\n\ntemplate <bool _Cond, class _IfRes, class _ElseRes>\nusing _If _LIBCUDACXX_NODEBUG_TYPE = typename _IfImpl<_Cond>::template _Select<_IfRes, _ElseRes>;\n\ntemplate <bool _Bp, class _If, class _Then>\n    struct _LIBCUDACXX_TEMPLATE_VIS conditional {typedef _If type;};\ntemplate <class _If, class _Then>\n    struct _LIBCUDACXX_TEMPLATE_VIS conditional<false, _If, _Then> {typedef _Then type;};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <bool _Bp, class _IfRes, class _ElseRes>\nusing conditional_t = typename conditional<_Bp, _IfRes, _ElseRes>::type;\n#endif\n\n// Helper so we can use \"conditional_t\" in all language versions.\ntemplate <bool _Bp, class _If, class _Then> using __conditional_t = typename conditional<_Bp, _If, _Then>::type;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_CONDITIONAL_H\n", "../__type_traits/conjunction.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_CONJUNCTION_H\n#define _LIBCUDACXX___TYPE_TRAITS_CONJUNCTION_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/conditional.h\"\n#include \"../__type_traits/enable_if.h\"\n#include \"../__type_traits/integral_constant.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class...>\nusing __expand_to_true = true_type;\n\ntemplate <class... _Pred>\n_LIBCUDACXX_INLINE_VISIBILITY __expand_to_true<__enable_if_t<_Pred::value>...> __and_helper(int);\n\ntemplate <class...>\n_LIBCUDACXX_INLINE_VISIBILITY false_type __and_helper(...);\n\n// _And always performs lazy evaluation of its arguments.\n//\n// However, `_And<_Pred...>` itself will evaluate its result immediately (without having to\n// be instantiated) since it is an alias, unlike `conjunction<_Pred...>`, which is a struct.\n// If you want to defer the evaluation of `_And<_Pred...>` itself, use `_Lazy<_And, _Pred...>`.\ntemplate <class... _Pred>\nusing _And _LIBCUDACXX_NODEBUG_TYPE = decltype(__and_helper<_Pred...>(0));\n\n#if _LIBCUDACXX_STD_VER > 11\n\ntemplate <class...>\nstruct conjunction : true_type {};\n\ntemplate <class _Arg>\nstruct conjunction<_Arg> : _Arg {};\n\ntemplate <class _Arg, class... _Args>\nstruct conjunction<_Arg, _Args...> : conditional_t<!bool(_Arg::value), _Arg, conjunction<_Args...>> {};\n\ntemplate <class... _Args>\n_LIBCUDACXX_INLINE_VAR constexpr bool conjunction_v = conjunction<_Args...>::value;\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_CONJUNCTION_H\n", "../__type_traits/copy_cv.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_COPY_CV_H\n#define _LIBCUDACXX___TYPE_TRAITS_COPY_CV_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_const.h\"\n#include \"../__type_traits/add_cv.h\"\n#include \"../__type_traits/add_volatile.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n// Let COPYCV(FROM, TO) be an alias for type TO with the addition of FROM's\n// top-level cv-qualifiers.\ntemplate <class _From, class _To>\nstruct __copy_cv\n{\n    using type = _To;\n};\n\ntemplate <class _From, class _To>\nstruct __copy_cv<const _From, _To>\n{\n    using type = typename add_const<_To>::type;\n};\n\ntemplate <class _From, class _To>\nstruct __copy_cv<volatile _From, _To>\n{\n    using type = typename add_volatile<_To>::type;\n};\n\ntemplate <class _From, class _To>\nstruct __copy_cv<const volatile _From, _To>\n{\n    using type = typename add_cv<_To>::type;\n};\n\ntemplate <class _From, class _To>\nusing __copy_cv_t = typename __copy_cv<_From, _To>::type;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_COPY_CV_H\n", "../__type_traits/copy_cvref.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_COPY_CVREF_H\n#define _LIBCUDACXX___TYPE_TRAITS_COPY_CVREF_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_lvalue_reference.h\"\n#include \"../__type_traits/add_rvalue_reference.h\"\n#include \"../__type_traits/copy_cv.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _From, class _To>\nstruct __copy_cvref\n{\n    using type = __copy_cv_t<_From, _To>;\n};\n\ntemplate <class _From, class _To>\nstruct __copy_cvref<_From&, _To>\n{\n    using type = __add_lvalue_reference_t<__copy_cv_t<_From, _To> >;\n};\n\ntemplate <class _From, class _To>\nstruct __copy_cvref<_From&&, _To>\n{\n    using type = __add_rvalue_reference_t<__copy_cv_t<_From, _To> >;\n};\n\ntemplate <class _From, class _To>\nusing __copy_cvref_t = typename __copy_cvref<_From, _To>::type;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_COPY_CVREF_H\n", "../__type_traits/decay.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_DECAY_H\n#define _LIBCUDACXX___TYPE_TRAITS_DECAY_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_pointer.h\"\n#include \"../__type_traits/conditional.h\"\n#include \"../__type_traits/is_array.h\"\n#include \"../__type_traits/is_function.h\"\n#include \"../__type_traits/is_referenceable.h\"\n#include \"../__type_traits/remove_cv.h\"\n#include \"../__type_traits/remove_extent.h\"\n#include \"../__type_traits/remove_reference.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_DECAY) && !defined(_LIBCUDACXX_USE_DECAY_FALLBACK)\ntemplate <class _Tp>\nstruct decay {\n  using type _LIBCUDACXX_NODEBUG_TYPE = _LIBCUDACXX_DECAY(_Tp);\n};\n\n#else\n\ntemplate <class _Up, bool>\nstruct __decay_impl {\n    typedef _LIBCUDACXX_NODEBUG_TYPE __remove_cv_t<_Up> type;\n};\n\ntemplate <class _Up>\nstruct __decay_impl<_Up, true> {\npublic:\n    typedef _LIBCUDACXX_NODEBUG_TYPE __conditional_t\n                     <\n                         is_array<_Up>::value,\n                         __remove_extent_t<_Up>*,\n                         __conditional_t\n                         <\n                              is_function<_Up>::value,\n                              __add_pointer_t<_Up>,\n                              __remove_cv_t<_Up>\n                         >\n                     > type;\n};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS decay\n{\nprivate:\n    typedef _LIBCUDACXX_NODEBUG_TYPE __libcpp_remove_reference_t<_Tp> _Up;\npublic:\n  typedef _LIBCUDACXX_NODEBUG_TYPE typename __decay_impl<_Up, __libcpp_is_referenceable<_Up>::value>::type type;\n};\n#endif // defined(_LIBCUDACXX_DECAY) && !defined(_LIBCUDACXX_USE_DECAY_FALLBACK)\n\ntemplate <class _Tp> using __decay_t = typename decay<_Tp>::type;\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> using decay_t = typename decay<_Tp>::type;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_DECAY_H\n", "../__type_traits/disjunction.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_DISJUNCTION_H\n#define _LIBCUDACXX___TYPE_TRAITS_DISJUNCTION_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <bool>\nstruct _OrImpl;\n\ntemplate <>\nstruct _OrImpl<true> {\n  template <class _Res, class _First, class... _Rest>\n  using _Result _LIBCUDACXX_NODEBUG_TYPE =\n      typename _OrImpl<!bool(_First::value) && sizeof...(_Rest) != 0>::template _Result<_First, _Rest...>;\n};\n\ntemplate <>\nstruct _OrImpl<false> {\n  template <class _Res, class...>\n  using _Result = _Res;\n};\n\n// _Or always performs lazy evaluation of its arguments.\n//\n// However, `_Or<_Pred...>` itself will evaluate its result immediately (without having to\n// be instantiated) since it is an alias, unlike `disjunction<_Pred...>`, which is a struct.\n// If you want to defer the evaluation of `_Or<_Pred...>` itself, use `_Lazy<_Or, _Pred...>`\n// or `disjunction<_Pred...>` directly.\ntemplate <class... _Args>\nusing _Or _LIBCUDACXX_NODEBUG_TYPE = typename _OrImpl<sizeof...(_Args) != 0>::template _Result<false_type, _Args...>;\n\n#if _LIBCUDACXX_STD_VER > 11\n\n#ifdef _LIBCUDACXX_COMPILER_MSVC\ntemplate <class... _Args>\nstruct disjunction : false_type {};\n\ntemplate <class _First, class... _Rest>\nstruct disjunction<_First, _Rest...> : _OrImpl<true>::template _Result<false_type, _First, _Rest...> {};\n#else\ntemplate <class... _Args>\nstruct disjunction : _Or<_Args...> {};\n#endif // !MSVC\n\ntemplate <class... _Args>\n_LIBCUDACXX_INLINE_VAR constexpr bool disjunction_v = _Or<_Args...>::value;\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_DISJUNCTION_H\n", "../__type_traits/enable_if.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_ENABLE_IF_H\n#define _LIBCUDACXX___TYPE_TRAITS_ENABLE_IF_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <bool, class _Tp = void> struct _LIBCUDACXX_TEMPLATE_VIS enable_if {};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS enable_if<true, _Tp> {typedef _Tp type;};\n\ntemplate <bool _Bp, class _Tp = void> using __enable_if_t _LIBCUDACXX_NODEBUG_TYPE = typename enable_if<_Bp, _Tp>::type;\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <bool _Bp, class _Tp = void> using enable_if_t = typename enable_if<_Bp, _Tp>::type;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_ENABLE_IF_H\n", "../__type_traits/extent.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_EXTENT_H\n#define _LIBCUDACXX___TYPE_TRAITS_EXTENT_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_ARRAY_EXTENT) && !defined(_LIBCUDACXX_USE_ARRAY_EXTENT_FALLBACK)\n\ntemplate<class _Tp, size_t _Dim = 0>\nstruct _LIBCUDACXX_TEMPLATE_VIS extent\n    : integral_constant<size_t, _LIBCUDACXX_ARRAY_EXTENT(_Tp, _Dim)> { };\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, unsigned _Ip = 0>\n_LIBCUDACXX_INLINE_VAR constexpr size_t extent_v = _LIBCUDACXX_ARRAY_EXTENT(_Tp, _Ip);\n#endif\n\n#else\n\ntemplate <class _Tp, unsigned _Ip = 0> struct _LIBCUDACXX_TEMPLATE_VIS extent\n    : public integral_constant<size_t, 0> {};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS extent<_Tp[], 0>\n    : public integral_constant<size_t, 0> {};\ntemplate <class _Tp, unsigned _Ip> struct _LIBCUDACXX_TEMPLATE_VIS extent<_Tp[], _Ip>\n    : public integral_constant<size_t, extent<_Tp, _Ip-1>::value> {};\ntemplate <class _Tp, size_t _Np> struct _LIBCUDACXX_TEMPLATE_VIS extent<_Tp[_Np], 0>\n    : public integral_constant<size_t, _Np> {};\ntemplate <class _Tp, size_t _Np, unsigned _Ip> struct _LIBCUDACXX_TEMPLATE_VIS extent<_Tp[_Np], _Ip>\n    : public integral_constant<size_t, extent<_Tp, _Ip-1>::value> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, unsigned _Ip = 0>\n_LIBCUDACXX_INLINE_VAR constexpr size_t extent_v = extent<_Tp, _Ip>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_ARRAY_EXTENT) && !defined(_LIBCUDACXX_USE_ARRAY_EXTENT_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_EXTENT_H\n", "../__type_traits/integral_constant.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_INTEGRAL_CONSTANT_H\n#define _LIBCUDACXX___TYPE_TRAITS_INTEGRAL_CONSTANT_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp, _Tp __v>\nstruct _LIBCUDACXX_TEMPLATE_VIS integral_constant\n{\n  static constexpr const _Tp      value = __v;\n  typedef _Tp               value_type;\n  typedef integral_constant type;\n  _LIBCUDACXX_INLINE_VISIBILITY\n  constexpr operator value_type() const noexcept {return value;}\n#if _LIBCUDACXX_STD_VER > 11\n  _LIBCUDACXX_INLINE_VISIBILITY\n  constexpr value_type operator ()() const noexcept {return value;}\n#endif\n};\n\ntemplate <class _Tp, _Tp __v>\nconstexpr const _Tp integral_constant<_Tp, __v>::value;\n\ntypedef integral_constant<bool, true>  true_type;\ntypedef integral_constant<bool, false> false_type;\n\ntemplate <bool _Val>\nusing _BoolConstant _LIBCUDACXX_NODEBUG_TYPE = integral_constant<bool, _Val>;\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <bool __b>\nusing bool_constant = integral_constant<bool, __b>;\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\n#define _LIBCUDACXX_BOOL_CONSTANT(__b) bool_constant<(__b)>\n#else\n#define _LIBCUDACXX_BOOL_CONSTANT(__b) integral_constant<bool,(__b)>\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_INTEGRAL_CONSTANT_H\n", "../__type_traits/is_arithmetic.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_ARITHMETIC_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_ARITHMETIC_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_floating_point.h\"\n#include \"../__type_traits/is_integral.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_arithmetic\n    : public integral_constant<bool, is_integral<_Tp>::value      ||\n                                     is_floating_point<_Tp>::value> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_arithmetic_v = is_arithmetic<_Tp>::value;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_ARITHMETIC_H\n", "../__type_traits/is_array.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_ARRAY_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_ARRAY_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n// TODO: Clang incorrectly reports that __is_array is true for T[0].\n//       Re-enable the branch once https://llvm.org/PR54705 is fixed.\n#if defined(_LIBCUDACXX_IS_ARRAY) && !defined(_LIBCUDACXX_USE_IS_ARRAY_FALLBACK)\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_array\n    : public integral_constant<bool, _LIBCUDACXX_IS_ARRAY(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_array_v = _LIBCUDACXX_IS_ARRAY(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_array\n    : public false_type {};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_array<_Tp[]>\n    : public true_type {};\ntemplate <class _Tp, size_t _Np> struct _LIBCUDACXX_TEMPLATE_VIS is_array<_Tp[_Np]>\n    : public true_type {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_array_v = is_array<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_ARRAY) && !defined(_LIBCUDACXX_USE_IS_ARRAY_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_ARRAY_H\n", "../__type_traits/is_assignable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_ASSIGNABLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_ASSIGNABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_void.h\"\n#include \"../__utility/declval.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate<typename, typename _Tp> struct __select_2nd { typedef _LIBCUDACXX_NODEBUG_TYPE _Tp type; };\n\n#if defined(_LIBCUDACXX_IS_ASSIGNABLE) && !defined(_LIBCUDACXX_USE_IS_ASSIGNABLE_FALLBACK)\n\ntemplate <class _T1, class _T2> struct _LIBCUDACXX_TEMPLATE_VIS is_assignable\n    : public integral_constant<bool, _LIBCUDACXX_IS_ASSIGNABLE(_T1, _T2)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _T1, class _T2>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_assignable_v = _LIBCUDACXX_IS_ASSIGNABLE(_T1, _T2);\n#endif\n\n#else\n\ntemplate <class _Tp, class _Arg>\n_LIBCUDACXX_INLINE_VISIBILITY\ntypename __select_2nd<decltype((_CUDA_VSTD::declval<_Tp>() = _CUDA_VSTD::declval<_Arg>())), true_type>::type\n__is_assignable_test(int);\n\ntemplate <class, class>\n_LIBCUDACXX_INLINE_VISIBILITY\nfalse_type __is_assignable_test(...);\n\ntemplate <class _Tp, class _Arg, bool = is_void<_Tp>::value || is_void<_Arg>::value>\nstruct __is_assignable_imp\n    : public decltype((_CUDA_VSTD::__is_assignable_test<_Tp, _Arg>(0))) {};\n\ntemplate <class _Tp, class _Arg>\nstruct __is_assignable_imp<_Tp, _Arg, true>\n    : public false_type\n{\n};\n\ntemplate <class _Tp, class _Arg>\nstruct is_assignable\n    : public __is_assignable_imp<_Tp, _Arg> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, class _Arg>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_assignable_v = is_assignable<_Tp, _Arg>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_ASSIGNABLE) && !defined(_LIBCUDACXX_USE_IS_ASSIGNABLE_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_ASSIGNABLE_H\n", "../__type_traits/is_base_of.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_BASE_OF_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_BASE_OF_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_BASE_OF) && !defined(_LIBCUDACXX_USE_IS_BASE_OF_FALLBACK)\n\ntemplate <class _Bp, class _Dp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_base_of\n    : public integral_constant<bool, _LIBCUDACXX_IS_BASE_OF(_Bp, _Dp)> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Bp, class _Dp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_base_of_v = _LIBCUDACXX_IS_BASE_OF(_Bp, _Dp);\n#endif\n\n#else  // defined(_LIBCUDACXX_IS_BASE_OF) && !defined(_LIBCUDACXX_USE_IS_BASE_OF_FALLBACK)\n\nnamespace __is_base_of_imp\n{\ntemplate <class _Tp>\nstruct _Dst\n{\n    _Dst(const volatile _Tp &);\n};\ntemplate <class _Tp>\nstruct _Src\n{\n    operator const volatile _Tp &();\n    template <class _Up> operator const _Dst<_Up> &();\n};\ntemplate <size_t> struct __one { typedef char type; };\ntemplate <class _Bp, class _Dp> typename __one<sizeof(_Dst<_Bp>(declval<_Src<_Dp> >()))>::type __test(int);\ntemplate <class _Bp, class _Dp> __two __test(...);\n}\n\ntemplate <class _Bp, class _Dp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_base_of\n    : public integral_constant<bool, is_class<_Bp>::value &&\n                                     sizeof(__is_base_of_imp::__test<_Bp, _Dp>(0)) == 2> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Bp, class _Dp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_base_of_v = is_base_of<_Bp, _Dp>::value;\n#endif\n\n#endif  // defined(_LIBCUDACXX_IS_BASE_OF) && !defined(_LIBCUDACXX_USE_IS_BASE_OF_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_BASE_OF_H\n", "../__type_traits/is_class.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_CLASS_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_CLASS_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_union.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\nstruct __two {char __lx[2];};\n\n#if defined(_LIBCUDACXX_IS_CLASS) && !defined(_LIBCUDACXX_USE_IS_CLASS_FALLBACK)\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_class\n    : public integral_constant<bool, _LIBCUDACXX_IS_CLASS(_Tp)> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_class_v = _LIBCUDACXX_IS_CLASS(_Tp);\n#endif\n\n#else\n\nnamespace __is_class_imp\n{\ntemplate <class _Tp> char  __test(int _Tp::*);\ntemplate <class _Tp> __two __test(...);\n}\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_class\n    : public integral_constant<bool, sizeof(__is_class_imp::__test<_Tp>(0)) == 1 && !is_union<_Tp>::value> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_class_v = is_class<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_CLASS) && !defined(_LIBCUDACXX_USE_IS_CLASS_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_CLASS_H\n", "../__type_traits/is_const.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_CONST_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_CONST_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_CONST) && !defined(_LIBCUDACXX_USE_IS_CONST_FALLBACK)\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_const\n    : public integral_constant<bool, _LIBCUDACXX_IS_CONST(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_const_v = _LIBCUDACXX_IS_CONST(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_const            : public false_type {};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_const<_Tp const> : public true_type {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_const_v = is_const<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_CONST) && !defined(_LIBCUDACXX_USE_IS_CONST_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_CONST_H\n", "../__type_traits/is_constant_evaluated.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_CONSTANT_EVALUATED_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_CONSTANT_EVALUATED_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_CONSTANT_EVALUATED)\n#if defined(__cuda_std__) || _LIBCUDACXX_STD_VER > 17\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr bool is_constant_evaluated() noexcept {\n  return _LIBCUDACXX_IS_CONSTANT_EVALUATED();\n}\n#endif\n\ninline constexpr _LIBCUDACXX_INLINE_VISIBILITY\nbool __libcpp_is_constant_evaluated() noexcept { return _LIBCUDACXX_IS_CONSTANT_EVALUATED(); }\n#else\ninline constexpr _LIBCUDACXX_INLINE_VISIBILITY\nbool __libcpp_is_constant_evaluated() noexcept { return false; }\n#endif // defined(_LIBCUDACXX_IS_CONSTANT_EVALUATED)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_CONSTANT_EVALUATED_H\n", "../__type_traits/is_constructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_IS_CONSTRUCTIBLE_H\n#define _LIBCUDACXX___TYPE_IS_CONSTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/conjunction.h\"\n#include \"../__type_traits/disjunction.h\"\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_base_of.h\"\n#include \"../__type_traits/is_destructible.h\"\n#include \"../__type_traits/is_reference.h\"\n#include \"../__type_traits/is_same.h\"\n#include \"../__type_traits/is_void.h\"\n#include \"../__type_traits/negation.h\"\n#include \"../__type_traits/remove_cvref.h\"\n#include \"../__utility/declval.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n\nnamespace __is_construct\n{\nstruct __nat {};\n}\n\n// FIXME: This logic isn't awesome.\n#if (!defined(_LIBCUDACXX_IS_CONSTRUCTIBLE) || \\\n    defined(_LIBCUDACXX_TESTING_FALLBACK_IS_CONSTRUCTIBLE) || \\\n    defined(_LIBCUDACXX_USE_IS_CONSTRUCTIBLE_FALLBACK))\n\ntemplate <class _Tp, class... _Args>\nstruct __libcpp_is_constructible;\n\ntemplate <class _To, class _From>\nstruct __is_invalid_base_to_derived_cast {\n  static_assert(is_reference<_To>::value, \"Wrong specialization\");\n  using _RawFrom = __remove_cvref_t<_From>;\n  using _RawTo = __remove_cvref_t<_To>;\n  static const bool value = _And<\n        _IsNotSame<_RawFrom, _RawTo>,\n        is_base_of<_RawFrom, _RawTo>,\n        _Not<__libcpp_is_constructible<_RawTo, _From>>\n  >::value;\n};\n\ntemplate <class _To, class _From>\nstruct __is_invalid_lvalue_to_rvalue_cast : false_type {\n  static_assert(is_reference<_To>::value, \"Wrong specialization\");\n};\n\ntemplate <class _ToRef, class _FromRef>\nstruct __is_invalid_lvalue_to_rvalue_cast<_ToRef&&, _FromRef&> {\n  using _RawFrom = __remove_cvref_t<_FromRef>;\n  using _RawTo = __remove_cvref_t<_ToRef>;\n  static const bool value = _And<\n      _Not<is_function<_RawTo>>,\n      _Or<\n        _IsSame<_RawFrom, _RawTo>,\n        is_base_of<_RawTo, _RawFrom>>\n    >::value;\n};\n\nstruct __is_constructible_helper\n{\n    template <class _To>\n    _LIBCUDACXX_INLINE_VISIBILITY\n    static void __eat(_To);\n\n    // This overload is needed to work around a Clang bug that disallows\n    // static_cast<T&&>(e) for non-reference-compatible types.\n    // Example: static_cast<int&&>(declval<double>());\n    // NOTE: The static_cast implementation below is required to support\n    //  classes with explicit conversion operators.\n    template <class _To, class _From,\n              class = decltype(__eat<_To>(_CUDA_VSTD::declval<_From>()))>\n    _LIBCUDACXX_INLINE_VISIBILITY\n    static true_type __test_cast(int);\n\n    template <class _To, class _From,\n              class = decltype(static_cast<_To>(_CUDA_VSTD::declval<_From>()))>\n    _LIBCUDACXX_INLINE_VISIBILITY\n    static integral_constant<bool,\n        !__is_invalid_base_to_derived_cast<_To, _From>::value &&\n        !__is_invalid_lvalue_to_rvalue_cast<_To, _From>::value\n    > __test_cast(long);\n\n    template <class, class>\n    _LIBCUDACXX_INLINE_VISIBILITY\n    static false_type __test_cast(...);\n\n    template <class _Tp, class ..._Args,\n        class = decltype(_Tp(_CUDA_VSTD::declval<_Args>()...))>\n    _LIBCUDACXX_INLINE_VISIBILITY\n    static true_type __test_nary(int);\n    template <class _Tp, class...>\n    _LIBCUDACXX_INLINE_VISIBILITY\n    static false_type __test_nary(...);\n\n    template <class _Tp, class _A0, class = decltype(::new _Tp(_CUDA_VSTD::declval<_A0>()))>\n    _LIBCUDACXX_INLINE_VISIBILITY\n    static is_destructible<_Tp> __test_unary(int);\n    template <class, class>\n    _LIBCUDACXX_INLINE_VISIBILITY\n    static false_type __test_unary(...);\n};\n\ntemplate <class _Tp, bool = is_void<_Tp>::value>\nstruct __is_default_constructible\n    : decltype(__is_constructible_helper::__test_nary<_Tp>(0))\n{};\n\ntemplate <class _Tp>\nstruct __is_default_constructible<_Tp, true> : false_type {};\n\ntemplate <class _Tp>\nstruct __is_default_constructible<_Tp[], false> : false_type {};\n\ntemplate <class _Tp, size_t _Nx>\nstruct __is_default_constructible<_Tp[_Nx], false>\n    : __is_default_constructible<__remove_all_extents_t<_Tp>>  {};\n\ntemplate <class _Tp, class... _Args>\nstruct __libcpp_is_constructible\n{\n  static_assert(sizeof...(_Args) > 1, \"Wrong specialization\");\n  typedef decltype(__is_constructible_helper::__test_nary<_Tp, _Args...>(0))\n      type;\n};\n\ntemplate <class _Tp>\nstruct __libcpp_is_constructible<_Tp> : __is_default_constructible<_Tp> {};\n\ntemplate <class _Tp, class _A0>\nstruct __libcpp_is_constructible<_Tp, _A0>\n    : public decltype(__is_constructible_helper::__test_unary<_Tp, _A0>(0))\n{};\n\ntemplate <class _Tp, class _A0>\nstruct __libcpp_is_constructible<_Tp&, _A0>\n    : public decltype(__is_constructible_helper::\n    __test_cast<_Tp&, _A0>(0))\n{};\n\ntemplate <class _Tp, class _A0>\nstruct __libcpp_is_constructible<_Tp&&, _A0>\n    : public decltype(__is_constructible_helper::\n    __test_cast<_Tp&&, _A0>(0))\n{};\n\n#endif\n\n#if defined(_LIBCUDACXX_IS_CONSTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_CONSTRUCTIBLE_FALLBACK)\ntemplate <class _Tp, class ..._Args>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_constructible\n    : public integral_constant<bool, _LIBCUDACXX_IS_CONSTRUCTIBLE(_Tp, _Args...)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, class ..._Args>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_constructible_v = _LIBCUDACXX_IS_CONSTRUCTIBLE(_Tp, _Args...);\n#endif\n\n#else\ntemplate <class _Tp, class... _Args>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_constructible\n    : public __libcpp_is_constructible<_Tp, _Args...>::type {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, class ..._Args>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_constructible_v = is_constructible<_Tp, _Args...>::value;\n#endif\n\n#endif  // defined(_LIBCUDACXX_IS_CONSTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_CONSTRUCTIBLE_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_IS_CONSTRUCTIBLE_H\n", "../__type_traits/is_convertible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n// SPDX-FileCopyrightText: Copyright (c) Microsoft Corporation.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_CONVERTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_CONVERTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_array.h\"\n#include \"../__type_traits/is_function.h\"\n#include \"../__type_traits/is_void.h\"\n#include \"../__type_traits/remove_reference.h\"\n#include \"../__utility/declval.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_CONVERTIBLE_TO) && !defined(_LIBCUDACXX_USE_IS_CONVERTIBLE_FALLBACK)\n\ntemplate <class _T1, class _T2> struct _LIBCUDACXX_TEMPLATE_VIS is_convertible\n    : public integral_constant<bool, _LIBCUDACXX_IS_CONVERTIBLE_TO(_T1, _T2)> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _T1, class _T2>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_convertible_v = _LIBCUDACXX_IS_CONVERTIBLE_TO(_T1, _T2);\n#endif\n\n#ifdef _LIBCUDACXX_COMPILER_MSVC // Workaround for DevCom-1627396\ntemplate <class _Ty>\nstruct is_convertible<_Ty&, volatile _Ty&> : true_type {};\n\ntemplate <class _Ty>\nstruct is_convertible<volatile _Ty&, volatile _Ty&> : true_type {};\n\ntemplate <class _Ty>\nstruct is_convertible<_Ty&, const volatile _Ty&> : true_type {};\n\ntemplate <class _Ty>\nstruct is_convertible<volatile _Ty&, const volatile _Ty&> : true_type {};\n\ntemplate <class _Ty>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_convertible_v<_Ty&, volatile _Ty&> = true;\n\ntemplate <class _Ty>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_convertible_v<volatile _Ty&, volatile _Ty&> = true;\n\ntemplate <class _Ty>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_convertible_v<_Ty&, const volatile _Ty&> = true;\n\ntemplate <class _Ty>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_convertible_v<volatile _Ty&, const volatile _Ty&> = true;\n#endif // _LIBCUDACXX_COMPILER_MSVC\n\n#else  // __has_builtin(__is_convertible_to) && !defined(_LIBCUDACXX_USE_IS_CONVERTIBLE_FALLBACK)\n\nnamespace __is_convertible_imp\n{\n\n_LIBCUDACXX_NV_DIAG_SUPPRESS(3013) // a volatile function parameter is deprecated\ntemplate <class _Tp> _LIBCUDACXX_INLINE_VISIBILITY void  __test_convert(_Tp);\n_LIBCUDACXX_NV_DIAG_DEFAULT(3013) // a volatile function parameter is deprecated\n\ntemplate <class _From, class _To, class = void>\nstruct __is_convertible_test : public false_type {};\n\ntemplate <class _From, class _To>\nstruct __is_convertible_test<_From, _To,\n    decltype(_CUDA_VSTD::__is_convertible_imp::__test_convert<_To>(_CUDA_VSTD::declval<_From>()))> : public true_type\n{};\n\ntemplate <class _Tp, bool _IsArray =    is_array<_Tp>::value,\n                     bool _IsFunction = is_function<_Tp>::value,\n                     bool _IsVoid =     is_void<_Tp>::value>\n                     struct __is_array_function_or_void                          {enum {value = 0};};\ntemplate <class _Tp> struct __is_array_function_or_void<_Tp, true, false, false> {enum {value = 1};};\ntemplate <class _Tp> struct __is_array_function_or_void<_Tp, false, true, false> {enum {value = 2};};\ntemplate <class _Tp> struct __is_array_function_or_void<_Tp, false, false, true> {enum {value = 3};};\n}\n\ntemplate <class _Tp,\n    unsigned = __is_convertible_imp::__is_array_function_or_void<__libcpp_remove_reference_t<_Tp>>::value>\nstruct __is_convertible_check\n{\n    static const size_t __v = 0;\n};\n\ntemplate <class _Tp>\nstruct __is_convertible_check<_Tp, 0>\n{\n    static const size_t __v = sizeof(_Tp);\n};\n\ntemplate <class _T1, class _T2,\n    unsigned _T1_is_array_function_or_void = __is_convertible_imp::__is_array_function_or_void<_T1>::value,\n    unsigned _T2_is_array_function_or_void = __is_convertible_imp::__is_array_function_or_void<_T2>::value>\nstruct __is_convertible_fallback\n    : public integral_constant<bool,\n        __is_convertible_imp::__is_convertible_test<_T1, _T2>::value\n    >\n{};\n\ntemplate <class _T1, class _T2> struct __is_convertible_fallback<_T1, _T2, 0, 1> : public false_type {};\ntemplate <class _T1, class _T2> struct __is_convertible_fallback<_T1, _T2, 1, 1> : public false_type {};\ntemplate <class _T1, class _T2> struct __is_convertible_fallback<_T1, _T2, 2, 1> : public false_type {};\ntemplate <class _T1, class _T2> struct __is_convertible_fallback<_T1, _T2, 3, 1> : public false_type {};\n\ntemplate <class _T1, class _T2> struct __is_convertible_fallback<_T1, _T2, 0, 2> : public false_type {};\ntemplate <class _T1, class _T2> struct __is_convertible_fallback<_T1, _T2, 1, 2> : public false_type {};\ntemplate <class _T1, class _T2> struct __is_convertible_fallback<_T1, _T2, 2, 2> : public false_type {};\ntemplate <class _T1, class _T2> struct __is_convertible_fallback<_T1, _T2, 3, 2> : public false_type {};\n\ntemplate <class _T1, class _T2> struct __is_convertible_fallback<_T1, _T2, 0, 3> : public false_type {};\ntemplate <class _T1, class _T2> struct __is_convertible_fallback<_T1, _T2, 1, 3> : public false_type {};\ntemplate <class _T1, class _T2> struct __is_convertible_fallback<_T1, _T2, 2, 3> : public false_type {};\ntemplate <class _T1, class _T2> struct __is_convertible_fallback<_T1, _T2, 3, 3> : public true_type {};\n\ntemplate <class _T1, class _T2> struct _LIBCUDACXX_TEMPLATE_VIS is_convertible\n    : public __is_convertible_fallback<_T1, _T2>\n{\n    static const size_t __complete_check1 = __is_convertible_check<_T1>::__v;\n    static const size_t __complete_check2 = __is_convertible_check<_T2>::__v;\n};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _From, class _To>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_convertible_v = is_convertible<_From, _To>::value;\n#endif\n\n#endif // __has_builtin(__is_convertible_to) && !defined(_LIBCUDACXX_USE_IS_CONVERTIBLE_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_CONVERTIBLE_H\n", "../__type_traits/is_copy_assignable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_COPY_ASSIGNABLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_COPY_ASSIGNABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_const.h\"\n#include \"../__type_traits/add_lvalue_reference.h\"\n#include \"../__type_traits/is_assignable.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_copy_assignable\n    : public is_assignable<__add_lvalue_reference_t<_Tp>,\n                           __add_lvalue_reference_t<typename add_const<_Tp>::type>> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_copy_assignable_v = is_copy_assignable<_Tp>::value;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_COPY_ASSIGNABLE_H\n", "../__type_traits/is_copy_constructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_COPY_CONSTRUCTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_COPY_CONSTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_const.h\"\n#include \"../__type_traits/add_lvalue_reference.h\"\n#include \"../__type_traits/is_constructible.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_copy_constructible\n    : public is_constructible<_Tp, __add_lvalue_reference_t<typename add_const<_Tp>::type>> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_copy_constructible_v = is_copy_constructible<_Tp>::value;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_COPY_CONSTRUCTIBLE_H\n", "../__type_traits/is_core_convertible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_CORE_CONVERTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_CORE_CONVERTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n// [conv.general]/3 says \"E is convertible to T\" whenever \"T t=E;\" is well-formed.\n// We can't test for that, but we can test implicit convertibility by passing it\n// to a function. Notice that __is_core_convertible<void,void> is false,\n// and __is_core_convertible<immovable-type,immovable-type> is true in C++17 and later.\n\ntemplate <class _Tp, class _Up, class = void>\nstruct __is_core_convertible : public false_type {};\n\ntemplate <class _Tp, class _Up>\nstruct __is_core_convertible<_Tp, _Up, decltype(\n    static_cast<void(*)(_Up)>(0) ( static_cast<_Tp(*)()>(0)() )\n)> : public true_type {};\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_CORE_CONVERTIBLE_H\n", "../__type_traits/is_default_constructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_DEFAULT_CONSTRUCTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_DEFAULT_CONSTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/is_constructible.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_default_constructible\n    : public is_constructible<_Tp>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_default_constructible_v\n    = is_constructible_v<_Tp>;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_DEFAULT_CONSTRUCTIBLE_H\n", "../__type_traits/is_destructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_DESTRUCTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_DESTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_function.h\"\n#include \"../__type_traits/is_reference.h\"\n#include \"../__type_traits/remove_all_extents.h\"\n#include \"../__utility/declval.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_DESTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_DESTRUCTIBLE_FALLBACK)\n\ntemplate<class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_destructible\n   : public integral_constant<bool, _LIBCUDACXX_IS_DESTRUCTIBLE(_Tp)> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_destructible_v = _LIBCUDACXX_IS_DESTRUCTIBLE(_Tp);\n#endif\n\n#else // __has_builtin(__is_destructible)\n\n//  if it's a reference, return true\n//  if it's a function, return false\n//  if it's   void,     return false\n//  if it's an array of unknown bound, return false\n//  Otherwise, return \"declval<_Up&>().~_Up()\" is well-formed\n//    where _Up is remove_all_extents<_Tp>::type\n\ntemplate <class>\nstruct __is_destructible_apply { typedef int type; };\n\ntemplate <typename _Tp>\nstruct __is_destructor_wellformed {\n    template <typename _Tp1>\n    _LIBCUDACXX_INLINE_VISIBILITY static true_type  __test (\n        typename __is_destructible_apply<decltype(_CUDA_VSTD::declval<_Tp1&>().~_Tp1())>::type\n    );\n\n    template <typename _Tp1>\n    _LIBCUDACXX_INLINE_VISIBILITY static false_type __test (...);\n\n    static const bool value = decltype(__test<_Tp>(12))::value;\n};\n\ntemplate <class _Tp, bool>\nstruct __destructible_imp;\n\ntemplate <class _Tp>\nstruct __destructible_imp<_Tp, false>\n   : public integral_constant<bool,\n        __is_destructor_wellformed<__remove_all_extents_t<_Tp> >::value> {};\n\ntemplate <class _Tp>\nstruct __destructible_imp<_Tp, true>\n    : public true_type {};\n\ntemplate <class _Tp, bool>\nstruct __destructible_false;\n\ntemplate <class _Tp>\nstruct __destructible_false<_Tp, false> : public __destructible_imp<_Tp, is_reference<_Tp>::value> {};\n\ntemplate <class _Tp>\nstruct __destructible_false<_Tp, true> : public false_type {};\n\ntemplate <class _Tp>\nstruct is_destructible : public __destructible_false<_Tp, is_function<_Tp>::value> {};\n\ntemplate <class _Tp>\nstruct is_destructible<_Tp[]> : public false_type {};\n\ntemplate <>\nstruct is_destructible<void> : public false_type {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_destructible_v = is_destructible<_Tp>::value;\n#endif\n\n#endif // __has_builtin(__is_destructible)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_DESTRUCTIBLE_H\n", "../__type_traits/is_enum.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_ENUM_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_ENUM_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_array.h\"\n#include \"../__type_traits/is_class.h\"\n#include \"../__type_traits/is_floating_point.h\"\n#include \"../__type_traits/is_function.h\"\n#include \"../__type_traits/is_integral.h\"\n#include \"../__type_traits/is_member_pointer.h\"\n#include \"../__type_traits/is_pointer.h\"\n#include \"../__type_traits/is_reference.h\"\n#include \"../__type_traits/is_union.h\"\n#include \"../__type_traits/is_void.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_ENUM) && !defined(_LIBCUDACXX_USE_IS_ENUM_FALLBACK)\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_enum\n    : public integral_constant<bool, _LIBCUDACXX_IS_ENUM(_Tp)> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_enum_v = _LIBCUDACXX_IS_ENUM(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_enum\n    : public integral_constant<bool, !is_void<_Tp>::value             &&\n                                     !is_integral<_Tp>::value         &&\n                                     !is_floating_point<_Tp>::value   &&\n                                     !is_array<_Tp>::value            &&\n                                     !is_pointer<_Tp>::value          &&\n                                     !is_reference<_Tp>::value        &&\n                                     !is_member_pointer<_Tp>::value   &&\n                                     !is_union<_Tp>::value            &&\n                                     !is_class<_Tp>::value            &&\n                                     !is_function<_Tp>::value         > {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_enum_v\n    = is_enum<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_ENUM) && !defined(_LIBCUDACXX_USE_IS_ENUM_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_ENUM_H\n", "../__type_traits/is_floating_point.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_FLOATING_POINT_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_FLOATING_POINT_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/remove_cv.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp> struct __libcpp_is_floating_point              : public false_type {};\ntemplate <>          struct __libcpp_is_floating_point<float>       : public true_type {};\ntemplate <>          struct __libcpp_is_floating_point<double>      : public true_type {};\ntemplate <>          struct __libcpp_is_floating_point<long double> : public true_type {};\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_floating_point\n    : public __libcpp_is_floating_point<__remove_cv_t<_Tp> > {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_floating_point_v = is_floating_point<_Tp>::value;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_FLOATING_POINT_H\n", "../__type_traits/is_function.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_FUNCTIONAL_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_FUNCTIONAL_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_const.h\"\n#include \"../__type_traits/is_reference.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_FUNCTION) && !defined(_LIBCUDACXX_USE_IS_FUNCTION_FALLBACK)\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_function : integral_constant<bool, _LIBCUDACXX_IS_FUNCTION(_Tp)> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_function_v = _LIBCUDACXX_IS_FUNCTION(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_function\n    : public integral_constant<bool, !(is_reference<_Tp>::value || is_const<const _Tp>::value)> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_function_v = is_function<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_FUNCTION) && !defined(_LIBCUDACXX_USE_IS_FUNCTION_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_FUNCTIONAL_H\n", "../__type_traits/is_fundamental.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_FUNDAMENTAL_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_FUNDAMENTAL_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_arithmetic.h\"\n#include \"../__type_traits/is_null_pointer.h\"\n#include \"../__type_traits/is_void.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_FUNDAMENTAL) && !defined(_LIBCUDACXX_USE_IS_FUNDAMENTAL_FALLBACK)\n\ntemplate<class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_fundamental\n    : public integral_constant<bool, _LIBCUDACXX_IS_FUNDAMENTAL(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_fundamental_v = _LIBCUDACXX_IS_FUNDAMENTAL(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_fundamental\n    : public integral_constant<bool, is_void<_Tp>::value        ||\n                                     __is_nullptr_t<_Tp>::value ||\n                                     is_arithmetic<_Tp>::value> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_fundamental_v = is_fundamental<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_FUNDAMENTAL) && !defined(_LIBCUDACXX_USE_IS_FUNDAMENTAL_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_FUNDAMENTAL_H\n", "../__type_traits/is_implicitly_default_constructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_IMPLICITLY_DEFAULT_CONSTRUCTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_IMPLICITLY_DEFAULT_CONSTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_default_constructible.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n// First of all, we can't implement this check in C++03 mode because the {}\n// default initialization syntax isn't valid.\n// Second, we implement the trait in a funny manner with two defaulted template\n// arguments to workaround Clang's PR43454.\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY void __test_implicit_default_constructible(_Tp);\n\ntemplate <class _Tp, class = void, class = typename is_default_constructible<_Tp>::type>\nstruct __is_implicitly_default_constructible\n    : false_type\n{ };\n\ntemplate <class _Tp>\nstruct __is_implicitly_default_constructible<_Tp, decltype(__test_implicit_default_constructible<_Tp const&>({})), true_type>\n    : true_type\n{ };\n\ntemplate <class _Tp>\nstruct __is_implicitly_default_constructible<_Tp, decltype(__test_implicit_default_constructible<_Tp const&>({})), false_type>\n    : false_type\n{ };\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_IMPLICITLY_DEFAULT_CONSTRUCTIBLE_H\n", "../__type_traits/is_integral.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_INTEGRAL_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_INTEGRAL_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/remove_cv.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_INTEGRAL) && !defined(_LIBCUDACXX_USE_IS_INTEGRAL_FALLBACK)\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_integral\n    : public integral_constant<bool, _LIBCUDACXX_IS_INTEGRAL(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_integral_v = _LIBCUDACXX_IS_INTEGRAL(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct __libcpp_is_integral                     : public false_type {};\ntemplate <>          struct __libcpp_is_integral<bool>               : public true_type {};\ntemplate <>          struct __libcpp_is_integral<char>               : public true_type {};\ntemplate <>          struct __libcpp_is_integral<signed char>        : public true_type {};\ntemplate <>          struct __libcpp_is_integral<unsigned char>      : public true_type {};\ntemplate <>          struct __libcpp_is_integral<wchar_t>            : public true_type {};\n#ifndef _LIBCUDACXX_NO_HAS_CHAR8_T\ntemplate <>          struct __libcpp_is_integral<char8_t>            : public true_type {};\n#endif\n#ifndef _LIBCUDACXX_HAS_NO_UNICODE_CHARS\ntemplate <>          struct __libcpp_is_integral<char16_t>           : public true_type {};\ntemplate <>          struct __libcpp_is_integral<char32_t>           : public true_type {};\n#endif  // _LIBCUDACXX_HAS_NO_UNICODE_CHARS\ntemplate <>          struct __libcpp_is_integral<short>              : public true_type {};\ntemplate <>          struct __libcpp_is_integral<unsigned short>     : public true_type {};\ntemplate <>          struct __libcpp_is_integral<int>                : public true_type {};\ntemplate <>          struct __libcpp_is_integral<unsigned int>       : public true_type {};\ntemplate <>          struct __libcpp_is_integral<long>               : public true_type {};\ntemplate <>          struct __libcpp_is_integral<unsigned long>      : public true_type {};\ntemplate <>          struct __libcpp_is_integral<long long>          : public true_type {};\ntemplate <>          struct __libcpp_is_integral<unsigned long long> : public true_type {};\n#ifndef _LIBCUDACXX_HAS_NO_INT128\ntemplate <>          struct __libcpp_is_integral<__int128_t>         : public true_type {};\ntemplate <>          struct __libcpp_is_integral<__uint128_t>        : public true_type {};\n#endif\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_integral\n    : public integral_constant<bool, __libcpp_is_integral<__remove_cv_t<_Tp> >::value>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_integral_v = is_integral<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_INTEGRAL) && !defined(_LIBCUDACXX_USE_IS_INTEGRAL_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_INTEGRAL_H\n", "../__type_traits/is_member_function_pointer.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_MEMBER_FUNCTION_POINTER_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_MEMBER_FUNCTION_POINTER_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_function.h\"\n#include \"../__type_traits/remove_cv.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_MEMBER_FUNCTION_POINTER) && !defined(_LIBCUDACXX_USE_IS_MEMBER_FUNCTION_POINTER_FALLBACK)\n\ntemplate<class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_member_function_pointer\n    : public integral_constant<bool, _LIBCUDACXX_IS_MEMBER_FUNCTION_POINTER(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_member_function_pointer_v = _LIBCUDACXX_IS_MEMBER_FUNCTION_POINTER(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct __libcpp_is_member_pointer {\n  enum {\n    __is_member = false,\n    __is_func = false,\n    __is_obj = false\n  };\n};\ntemplate <class _Tp, class _Up> struct __libcpp_is_member_pointer<_Tp _Up::*> {\n  enum {\n    __is_member = true,\n    __is_func = is_function<_Tp>::value,\n    __is_obj = !__is_func,\n  };\n};\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_member_function_pointer\n    : public integral_constant<bool, __libcpp_is_member_pointer<__remove_cv_t<_Tp> >::__is_func >\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_member_function_pointer_v = is_member_function_pointer<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_MEMBER_FUNCTION_POINTER) && !defined(_LIBCUDACXX_USE_IS_MEMBER_FUNCTION_POINTER_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_MEMBER_FUNCTION_POINTER_H\n", "../__type_traits/is_member_object_pointer.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_MEMBER_OBJECT_POINTER_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_MEMBER_OBJECT_POINTER_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_member_function_pointer.h\"\n#include \"../__type_traits/remove_cv.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_MEMBER_OBJECT_POINTER) && !defined(_LIBCUDACXX_USE_IS_MEMBER_OBJECT_POINTER_FALLBACK)\n\ntemplate<class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_member_object_pointer\n    : public integral_constant<bool, _LIBCUDACXX_IS_MEMBER_OBJECT_POINTER(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_member_object_pointer_v = _LIBCUDACXX_IS_MEMBER_OBJECT_POINTER(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_member_object_pointer\n    : public integral_constant<bool, __libcpp_is_member_pointer<__remove_cv_t<_Tp> >::__is_obj >\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_member_object_pointer_v = is_member_object_pointer<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_MEMBER_OBJECT_POINTER) && !defined(_LIBCUDACXX_USE_IS_MEMBER_OBJECT_POINTER_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_MEMBER_FUNCTION_POINTER_H\n", "../__type_traits/is_member_pointer.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_MEMBER_POINTER_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_MEMBER_POINTER_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_member_function_pointer.h\"\n#include \"../__type_traits/remove_cv.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_MEMBER_POINTER) && !defined(_LIBCUDACXX_USE_IS_MEMBER_POINTER_FALLBACK)\n\ntemplate<class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_member_pointer\n    : public integral_constant<bool, _LIBCUDACXX_IS_MEMBER_POINTER(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_member_pointer_v = _LIBCUDACXX_IS_MEMBER_POINTER(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_member_pointer\n    : public integral_constant<bool, __libcpp_is_member_pointer<__remove_cv_t<_Tp> >::__is_member >\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_member_pointer_v = is_member_pointer<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_MEMBER_POINTER) && !defined(_LIBCUDACXX_USE_IS_MEMBER_POINTER_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_MEMBER_POINTER_H\n", "../__type_traits/is_move_assignable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_MOVE_ASSIGNABLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_MOVE_ASSIGNABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_lvalue_reference.h\"\n#include \"../__type_traits/add_rvalue_reference.h\"\n#include \"../__type_traits/is_assignable.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_move_assignable\n    : public is_assignable<__add_lvalue_reference_t<_Tp>, __add_rvalue_reference_t<_Tp>> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_move_assignable_v = is_move_assignable<_Tp>::value;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_MOVE_ASSIGNABLE_H\n", "../__type_traits/is_move_constructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_MOVE_CONSTRUCTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_MOVE_CONSTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_lvalue_reference.h\"\n#include \"../__type_traits/add_rvalue_reference.h\"\n#include \"../__type_traits/is_constructible.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_move_constructible\n    : public is_constructible<_Tp, __add_rvalue_reference_t<_Tp>>\n{ };\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_move_constructible_v = is_move_constructible<_Tp>::value;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_MOVE_CONSTRUCTIBLE_H\n", "../__type_traits/is_nothrow_assignable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_ASSIGNABLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_ASSIGNABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_assignable.h\"\n#include \"../__type_traits/is_scalar.h\"\n#include \"../__utility/declval.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_NOTHROW_ASSIGNABLE) && !defined(_LIBCUDACXX_USE_IS_NOTHROW_ASSIGNABLE_FALLBACK)\n\ntemplate <class _Tp, class _Arg>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_assignable\n    : public integral_constant<bool, _LIBCUDACXX_IS_NOTHROW_ASSIGNABLE(_Tp, _Arg)> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, class _Arg>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_assignable_v = _LIBCUDACXX_IS_NOTHROW_ASSIGNABLE(_Tp, _Arg);\n#endif\n\n#elif !defined(_LIBCUDACXX_HAS_NO_NOEXCEPT) && !defined(_LIBCUDACXX_HAS_NO_NOEXCEPT_SFINAE)\n\ntemplate <bool, class _Tp, class _Arg> struct __libcpp_is_nothrow_assignable;\n\ntemplate <class _Tp, class _Arg>\nstruct __libcpp_is_nothrow_assignable<false, _Tp, _Arg>\n    : public false_type\n{ };\n\ntemplate <class _Tp, class _Arg>\nstruct __libcpp_is_nothrow_assignable<true, _Tp, _Arg>\n    : public integral_constant<bool, noexcept(_CUDA_VSTD::declval<_Tp>() = _CUDA_VSTD::declval<_Arg>()) >\n{ };\n\ntemplate <class _Tp, class _Arg>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_assignable\n    : public __libcpp_is_nothrow_assignable<is_assignable<_Tp, _Arg>::value, _Tp, _Arg>\n{ };\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, class _Arg>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_assignable_v = is_nothrow_assignable<_Tp, _Arg>::value;\n#endif\n\n#else\n\ntemplate <class _Tp, class _Arg>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_assignable\n    : public false_type {};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_assignable<_Tp&, _Tp>\n#if defined(_LIBCUDACXX_HAS_NOTHROW_ASSIGN) && !defined(_LIBCUDACXX_USE_HAS_NOTHROW_ASSIGN_FALLBACK)\n    : integral_constant<bool, _LIBCUDACXX_HAS_NOTHROW_ASSIGN(_Tp)> {};\n#else\n    : integral_constant<bool, is_scalar<_Tp>::value> {};\n#endif // defined(_LIBCUDACXX_HAS_NOTHROW_ASSIGN) && !defined(_LIBCUDACXX_USE_HAS_NOTHROW_ASSIGN_FALLBACK)\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_assignable<_Tp&, _Tp&>\n#if defined(_LIBCUDACXX_HAS_NOTHROW_ASSIGN) && !defined(_LIBCUDACXX_USE_HAS_NOTHROW_ASSIGN_FALLBACK)\n    : integral_constant<bool, _LIBCUDACXX_HAS_NOTHROW_ASSIGN(_Tp)> {};\n#else\n    : integral_constant<bool, is_scalar<_Tp>::value> {};\n#endif // defined(_LIBCUDACXX_HAS_NOTHROW_ASSIGN) && !defined(_LIBCUDACXX_USE_HAS_NOTHROW_ASSIGN_FALLBACK)\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_assignable<_Tp&, const _Tp&>\n#if defined(_LIBCUDACXX_HAS_NOTHROW_ASSIGN) && !defined(_LIBCUDACXX_USE_HAS_NOTHROW_ASSIGN_FALLBACK)\n    : integral_constant<bool, _LIBCUDACXX_HAS_NOTHROW_ASSIGN(_Tp)> {};\n#else\n    : integral_constant<bool, is_scalar<_Tp>::value> {};\n#endif // defined(_LIBCUDACXX_HAS_NOTHROW_ASSIGN) && !defined(_LIBCUDACXX_USE_HAS_NOTHROW_ASSIGN_FALLBACK)\n\n#ifndef _LIBCUDACXX_HAS_NO_RVALUE_REFERENCES\n\ntemplate <class _Tp>\nstruct is_nothrow_assignable<_Tp&, _Tp&&>\n#if defined(_LIBCUDACXX_HAS_NOTHROW_ASSIGN) && !defined(_LIBCUDACXX_USE_HAS_NOTHROW_ASSIGN_FALLBACK)\n    : integral_constant<bool, _LIBCUDACXX_HAS_NOTHROW_ASSIGN(_Tp)> {};\n#else\n    : integral_constant<bool, is_scalar<_Tp>::value> {};\n#endif // defined(_LIBCUDACXX_HAS_NOTHROW_ASSIGN) && !defined(_LIBCUDACXX_USE_HAS_NOTHROW_ASSIGN_FALLBACK)\n\n#endif // _LIBCUDACXX_HAS_NO_RVALUE_REFERENCES\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, class _Arg>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_assignable_v = is_nothrow_assignable<_Tp, _Arg>::value;\n#endif\n\n#endif // !defined(_LIBCUDACXX_HAS_NO_NOEXCEPT)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_ASSIGNABLE_H\n", "../__type_traits/is_nothrow_constructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_CONSTRUCTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_CONSTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_constructible.h\"\n#include \"../__type_traits/is_scalar.h\"\n#include \"../__utility/declval.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_NOTHROW_CONSTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_NOTHROW_CONSTRUCTIBLE_FALLBACK)\n\ntemplate <class _Tp, class... _Args>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_constructible\n    : public integral_constant<bool, _LIBCUDACXX_IS_NOTHROW_CONSTRUCTIBLE(_Tp, _Args...)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, class ..._Args>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_constructible_v = _LIBCUDACXX_IS_NOTHROW_CONSTRUCTIBLE(_Tp, _Args...);\n#endif\n\n#else\n\n#if !defined(_LIBCUDACXX_HAS_NO_NOEXCEPT)\n\ntemplate <bool, bool, class _Tp, class... _Args> struct __libcpp_is_nothrow_constructible;\n\ntemplate <class _Tp, class... _Args>\nstruct __libcpp_is_nothrow_constructible</*is constructible*/true, /*is reference*/false, _Tp, _Args...>\n    : public integral_constant<bool, noexcept(_Tp(_CUDA_VSTD::declval<_Args>()...))>\n{\n};\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY void __implicit_conversion_to(_Tp) noexcept { }\n\ntemplate <class _Tp, class _Arg>\nstruct __libcpp_is_nothrow_constructible</*is constructible*/true, /*is reference*/true, _Tp, _Arg>\n    : public integral_constant<bool, noexcept(__implicit_conversion_to<_Tp>(_CUDA_VSTD::declval<_Arg>()))>\n{\n};\n\ntemplate <class _Tp, bool _IsReference, class... _Args>\nstruct __libcpp_is_nothrow_constructible</*is constructible*/false, _IsReference, _Tp, _Args...>\n    : public false_type\n{\n};\n\ntemplate <class _Tp, class... _Args>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_constructible\n    : __libcpp_is_nothrow_constructible<is_constructible<_Tp, _Args...>::value, is_reference<_Tp>::value, _Tp, _Args...>\n{\n};\n\ntemplate <class _Tp, size_t _Ns>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_constructible<_Tp[_Ns]>\n    : __libcpp_is_nothrow_constructible<is_constructible<_Tp>::value, is_reference<_Tp>::value, _Tp>\n{\n};\n\n#else\n\ntemplate <class _Tp, class... _Args>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_constructible\n    : false_type\n{\n};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_constructible<_Tp>\n#if defined(_LIBCUDACXX_HAS_NOTHROW_CONSTRUCTOR) && !defined(_LIBCUDACXX_USE_HAS_NOTHROW_CONSTRUCTOR_FALLBACK)\n    : integral_constant<bool, _LIBCUDACXX_HAS_NOTHROW_CONSTRUCTOR(_Tp)>\n#else\n    : integral_constant<bool, is_scalar<_Tp>::value>\n#endif // defined(_LIBCUDACXX_HAS_NOTHROW_CONSTRUCTOR) && !defined(_LIBCUDACXX_USE_HAS_NOTHROW_CONSTRUCTOR_FALLBACK)\n{\n};\n\ntemplate <class _Tp>\n#ifndef _LIBCUDACXX_HAS_NO_RVALUE_REFERENCES\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_constructible<_Tp, _Tp&&>\n#else\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_constructible<_Tp, _Tp>\n#endif\n#if defined(_LIBCUDACXX_HAS_NOTHROW_COPY) && !defined(_LIBCUDACXX_USE_HAS_NOTHROW_COPY_FALLBACK)\n    : integral_constant<bool, _LIBCUDACXX_HAS_NOTHROW_COPY(_Tp)>\n#else\n    : integral_constant<bool, is_scalar<_Tp>::value>\n#endif // defined(_LIBCUDACXX_HAS_NOTHROW_COPY) && !defined(_LIBCUDACXX_USE_HAS_NOTHROW_COPY_FALLBACK)\n{\n};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_constructible<_Tp, const _Tp&>\n#if defined(_LIBCUDACXX_HAS_NOTHROW_COPY) && !defined(_LIBCUDACXX_USE_HAS_NOTHROW_COPY_FALLBACK)\n    : integral_constant<bool, _LIBCUDACXX_HAS_NOTHROW_COPY(_Tp)>\n#else\n    : integral_constant<bool, is_scalar<_Tp>::value>\n#endif // defined(_LIBCUDACXX_HAS_NOTHROW_COPY) && !defined(_LIBCUDACXX_USE_HAS_NOTHROW_COPY_FALLBACK)\n{\n};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_constructible<_Tp, _Tp&>\n#if defined(_LIBCUDACXX_HAS_NOTHROW_COPY) && !defined(_LIBCUDACXX_USE_HAS_NOTHROW_COPY_FALLBACK)\n    : integral_constant<bool, _LIBCUDACXX_HAS_NOTHROW_COPY(_Tp)>\n#else\n    : integral_constant<bool, is_scalar<_Tp>::value>\n#endif // defined(_LIBCUDACXX_HAS_NOTHROW_COPY) && !defined(_LIBCUDACXX_USE_HAS_NOTHROW_COPY_FALLBACK)\n{\n};\n\n#endif // !defined(_LIBCUDACXX_HAS_NO_NOEXCEPT)\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, class ..._Args>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_constructible_v = is_nothrow_constructible<_Tp, _Args...>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_NOTHROW_CONSTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_NOTHROW_CONSTRUCTIBLE_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_CONSTRUCTIBLE_H\n", "../__type_traits/is_nothrow_copy_assignable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_COPY_ASSIGNABLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_COPY_ASSIGNABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_const.h\"\n#include \"../__type_traits/add_lvalue_reference.h\"\n#include \"../__type_traits/is_nothrow_assignable.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_NOTHROW_ASSIGNABLE) && !defined(_LIBCUDACXX_USE_IS_NOTHROW_ASSIGNABLE_FALLBACK)\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_copy_assignable\n    : public integral_constant<bool, _LIBCUDACXX_IS_NOTHROW_ASSIGNABLE(__add_lvalue_reference_t<_Tp>,\n                                                                       __add_lvalue_reference_t<typename add_const<_Tp>::type>)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_copy_assignable_v =\n    _LIBCUDACXX_IS_NOTHROW_ASSIGNABLE(__add_lvalue_reference_t<_Tp>,\n                                      __add_lvalue_reference_t<typename add_const<_Tp>::type>);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_copy_assignable\n    : public is_nothrow_assignable<__add_lvalue_reference_t<_Tp>,\n                                   __add_lvalue_reference_t<typename add_const<_Tp>::type>> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_copy_assignable_v = is_nothrow_copy_assignable<_Tp>::value;\n#endif\n\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_COPY_ASSIGNABLE_H\n", "../__type_traits/is_nothrow_copy_constructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_COPY_CONSTRUCTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_COPY_CONSTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_const.h\"\n#include \"../__type_traits/add_lvalue_reference.h\"\n#include \"../__type_traits/is_nothrow_constructible.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_copy_constructible\n    : public is_nothrow_constructible<_Tp, __add_lvalue_reference_t<typename add_const<_Tp>::type>> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_copy_constructible_v\n    = is_nothrow_copy_constructible<_Tp>::value;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_COPY_CONSTRUCTIBLE_H\n", "../__type_traits/is_nothrow_default_constructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_DEFAULT_CONSTRUCTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_DEFAULT_CONSTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/is_nothrow_constructible.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_NOTHROW_CONSTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_NOTHROW_CONSTRUCTIBLE_FALLBACK)\n\ntemplate <class _Tp, class... _Args>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_default_constructible\n    : public integral_constant<bool, _LIBCUDACXX_IS_NOTHROW_CONSTRUCTIBLE(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, class ..._Args>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_default_constructible_v = _LIBCUDACXX_IS_NOTHROW_CONSTRUCTIBLE(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_default_constructible\n    : public is_nothrow_constructible<_Tp>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_default_constructible_v = is_nothrow_constructible<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_NOTHROW_CONSTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_NOTHROW_CONSTRUCTIBLE_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_DEFAULT_CONSTRUCTIBLE_H\n", "../__type_traits/is_nothrow_destructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_DESTRUCTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_DESTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_destructible.h\"\n#include \"../__type_traits/is_reference.h\"\n#include \"../__type_traits/is_scalar.h\"\n#include \"../__type_traits/remove_all_extents.h\"\n#include \"../__utility/declval.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n// is_nothrow_destructible\n\n#if defined(_LIBCUDACXX_IS_NOTHROW_DESTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_NOTHROW_DESTRUCTIBLE_FALLBACK)\n\ntemplate <class _Tp>\nstruct is_nothrow_destructible\n   : public integral_constant<bool, _LIBCUDACXX_IS_NOTHROW_DESTRUCTIBLE(_Tp)> {};\n\n#elif !defined(_LIBCUDACXX_HAS_NO_NOEXCEPT)\n\ntemplate <bool, class _Tp> struct __libcpp_is_nothrow_destructible;\n\ntemplate <class _Tp>\nstruct __libcpp_is_nothrow_destructible<false, _Tp>\n    : public false_type\n{\n};\n\ntemplate <class _Tp>\nstruct __libcpp_is_nothrow_destructible<true, _Tp>\n    : public integral_constant<bool, noexcept(_CUDA_VSTD::declval<_Tp>().~_Tp()) >\n{\n};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_destructible\n    : public __libcpp_is_nothrow_destructible<is_destructible<_Tp>::value, _Tp>\n{\n};\n\ntemplate <class _Tp, size_t _Ns>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_destructible<_Tp[_Ns]>\n    : public is_nothrow_destructible<_Tp>\n{\n};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_destructible<_Tp&>\n    : public true_type\n{\n};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_destructible<_Tp&&>\n    : public true_type\n{\n};\n\n#else\n\ntemplate <class _Tp> struct __libcpp_nothrow_destructor\n    : public integral_constant<bool, is_scalar<_Tp>::value ||\n                                     is_reference<_Tp>::value> {};\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_destructible\n    : public __libcpp_nothrow_destructor<__remove_all_extents_t<_Tp>> {};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_destructible<_Tp[]>\n    : public false_type {};\n\n#endif // defined(_LIBCUDACXX_IS_NOTHROW_DESTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_NOTHROW_DESTRUCTIBLE_FALLBACK)\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_destructible_v\n    = is_nothrow_destructible<_Tp>::value;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_DESTRUCTIBLE_H\n", "../__type_traits/is_nothrow_move_assignable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_MOVE_ASSIGNABLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_MOVE_ASSIGNABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_lvalue_reference.h\"\n#include \"../__type_traits/add_rvalue_reference.h\"\n#include \"../__type_traits/is_nothrow_assignable.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_NOTHROW_ASSIGNABLE) && !defined(_LIBCUDACXX_USE_IS_NOTHROW_ASSIGNABLE_FALLBACK)\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_move_assignable\n    : public integral_constant<bool, _LIBCUDACXX_IS_NOTHROW_ASSIGNABLE(__add_lvalue_reference_t<_Tp>,\n                                                                       __add_rvalue_reference_t<_Tp>)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_move_assignable_v =\n    _LIBCUDACXX_IS_NOTHROW_ASSIGNABLE(__add_lvalue_reference_t<_Tp>, __add_rvalue_reference_t<_Tp>);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_move_assignable\n    : public is_nothrow_assignable<__add_lvalue_reference_t<_Tp>,\n                                   __add_rvalue_reference_t<_Tp>>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_move_assignable_v = is_nothrow_move_assignable<_Tp>::value;\n#endif\n\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_MOVE_ASSIGNABLE_H\n", "../__type_traits/is_nothrow_move_constructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_MOVE_CONSTRUCTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_MOVE_CONSTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_rvalue_reference.h\"\n#include \"../__type_traits/is_nothrow_constructible.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_move_constructible\n    : public is_nothrow_constructible<_Tp, __add_rvalue_reference_t<_Tp>>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_move_constructible_v\n    = is_nothrow_move_constructible<_Tp>::value;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_MOVE_CONSTRUCTIBLE_H\n", "../__type_traits/is_null_pointer.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_NULL_POINTER_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_NULL_POINTER_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/remove_cv.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp> struct __is_nullptr_t_impl       : public false_type {};\ntemplate <>          struct __is_nullptr_t_impl<nullptr_t> : public true_type {};\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS __is_nullptr_t\n    : public __is_nullptr_t_impl<__remove_cv_t<_Tp> > {};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_null_pointer\n    : public __is_nullptr_t_impl<__remove_cv_t<_Tp> > {};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_null_pointer_v = is_null_pointer<_Tp>::value;\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_NULL_POINTER_H\n", "../__type_traits/is_object.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_OBJECT_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_OBJECT_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_array.h\"\n#include \"../__type_traits/is_class.h\"\n#include \"../__type_traits/is_scalar.h\"\n#include \"../__type_traits/is_union.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_OBJECT) && !defined(_LIBCUDACXX_USE_IS_OBJECT_FALLBACK)\n\ntemplate<class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_object\n    : public integral_constant<bool, _LIBCUDACXX_IS_OBJECT(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_object_v = _LIBCUDACXX_IS_OBJECT(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_object\n    : public integral_constant<bool, is_scalar<_Tp>::value ||\n                                     is_array<_Tp>::value  ||\n                                     is_union<_Tp>::value  ||\n                                     is_class<_Tp>::value  > {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_object_v = is_object<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_OBJECT) && !defined(_LIBCUDACXX_USE_IS_OBJECT_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_OBJECT_H\n", "../__type_traits/is_pointer.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_POINTER_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_POINTER_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/remove_cv.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_POINTER) && !defined(_LIBCUDACXX_USE_IS_POINTER_FALLBACK)\n\ntemplate<class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_pointer\n    : public integral_constant<bool, _LIBCUDACXX_IS_POINTER(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_pointer_v = _LIBCUDACXX_IS_POINTER(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct __libcpp_is_pointer       : public false_type {};\ntemplate <class _Tp> struct __libcpp_is_pointer<_Tp*> : public true_type {};\n\ntemplate <class _Tp> struct __libcpp_remove_objc_qualifiers { typedef _Tp type; };\n#if defined(_LIBCUDACXX_HAS_OBJC_ARC)\ntemplate <class _Tp> struct __libcpp_remove_objc_qualifiers<_Tp __strong> { typedef _Tp type; };\ntemplate <class _Tp> struct __libcpp_remove_objc_qualifiers<_Tp __weak> { typedef _Tp type; };\ntemplate <class _Tp> struct __libcpp_remove_objc_qualifiers<_Tp __autoreleasing> { typedef _Tp type; };\ntemplate <class _Tp> struct __libcpp_remove_objc_qualifiers<_Tp __unsafe_unretained> { typedef _Tp type; };\n#endif\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_pointer\n    : public __libcpp_is_pointer<typename __libcpp_remove_objc_qualifiers<__remove_cv_t<_Tp> >::type> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_pointer_v = is_pointer<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_POINTER) && !defined(_LIBCUDACXX_USE_IS_POINTER_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_POINTER_H\n", "../__type_traits/is_primary_template.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_PRIMARY_TEMPLATE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_PRIMARY_TEMPLATE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/enable_if.h\"\n#include \"../__type_traits/is_same.h\"\n#include \"../__type_traits/is_valid_expansion.h\"\n#include \"../__type_traits/void_t.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_COMPILER_MSVC)\ntemplate<class _Tp, class = void>\nstruct __is_primary_template : false_type {};\n\ntemplate<class _Tp>\nstruct __is_primary_template<_Tp, void_t<typename _Tp::__primary_template>>\n  : public is_same<_Tp, typename _Tp::__primary_template> {};\n\n#else // ^^^ _LIBCUDACXX_COMPILER_MSVC ^^^ / vvv !_LIBCUDACXX_COMPILER_MSVC vvv\n\ntemplate <class _Tp>\nusing __test_for_primary_template = __enable_if_t<\n    _IsSame<_Tp, typename _Tp::__primary_template>::value\n  >;\ntemplate <class _Tp>\nusing __is_primary_template = _IsValidExpansion<\n    __test_for_primary_template, _Tp\n  >;\n#endif // !_LIBCUDACXX_COMPILER_MSVC\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_PRIMARY_TEMPLATE_H\n", "../__type_traits/is_reference.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_REFERENCE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_REFERENCE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_LVALUE_REFERENCE) && !defined(_LIBCUDACXX_USE_IS_LVALUE_REFERENCE_FALLBACK) && \\\n    defined(_LIBCUDACXX_IS_RVALUE_REFERENCE) && !defined(_LIBCUDACXX_USE_IS_RVALUE_REFERENCE_FALLBACK) && \\\n    defined(_LIBCUDACXX_IS_REFERENCE) && !defined(_LIBCUDACXX_USE_IS_REFERENCE_FALLBACK)\n\ntemplate<class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_lvalue_reference\n    : public integral_constant<bool, _LIBCUDACXX_IS_LVALUE_REFERENCE(_Tp)>\n    {};\n\ntemplate<class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_rvalue_reference\n    : public integral_constant<bool, _LIBCUDACXX_IS_RVALUE_REFERENCE(_Tp)>\n    {};\n\ntemplate<class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_reference\n    : public integral_constant<bool, _LIBCUDACXX_IS_REFERENCE(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_lvalue_reference_v = _LIBCUDACXX_IS_LVALUE_REFERENCE(_Tp);\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_rvalue_reference_v = _LIBCUDACXX_IS_RVALUE_REFERENCE(_Tp);\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_reference_v = _LIBCUDACXX_IS_REFERENCE(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_lvalue_reference       : public false_type {};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_lvalue_reference<_Tp&> : public true_type {};\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_rvalue_reference        : public false_type {};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_rvalue_reference<_Tp&&> : public true_type {};\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_reference        : public false_type {};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_reference<_Tp&>  : public true_type {};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_reference<_Tp&&> : public true_type {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_lvalue_reference_v = is_lvalue_reference<_Tp>::value;\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_rvalue_reference_v = is_rvalue_reference<_Tp>::value;\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_reference_v = is_reference<_Tp>::value;\n#endif\n\n#endif // __has_builtin(__is_lvalue_reference) && etc...\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_REFERENCE_H\n", "../__type_traits/is_reference_wrapper.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_REFERENCE_WRAPPER_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_REFERENCE_WRAPPER_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/remove_cv.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp> class _LIBCUDACXX_TEMPLATE_VIS reference_wrapper;\n\ntemplate <class _Tp> struct __is_reference_wrapper_impl : public false_type {};\ntemplate <class _Tp> struct __is_reference_wrapper_impl<reference_wrapper<_Tp> > : public true_type {};\ntemplate <class _Tp> struct __is_reference_wrapper\n    : public __is_reference_wrapper_impl<__remove_cv_t<_Tp> > {};\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_ENABLE_IF_H\n", "../__type_traits/is_referenceable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_REFERENCEABLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_REFERENCEABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_same.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_REFERENCEABLE) && !defined(_LIBCUDACXX_USE_IS_REFERENCEABLE_FALLBACK)\n\ntemplate <class _Tp>\nstruct __libcpp_is_referenceable\n  : public integral_constant<bool, _LIBCUDACXX_IS_REFERENCEABLE(_Tp)>\n  {};\n\n#else\nstruct __libcpp_is_referenceable_impl {\n  template <class _Tp>\n  _LIBCUDACXX_INLINE_VISIBILITY static _Tp& __test(int);\n  template <class _Tp>\n  _LIBCUDACXX_INLINE_VISIBILITY static false_type __test(...);\n};\n\ntemplate <class _Tp>\nstruct __libcpp_is_referenceable\n    : integral_constant<bool, _IsNotSame<decltype(__libcpp_is_referenceable_impl::__test<_Tp>(0)), false_type>::value> {\n};\n#endif // defined(_LIBCUDACXX_IS_REFERENCEABLE) && !defined(_LIBCUDACXX_USE_IS_REFERENCEABLE_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_REFERENCEABLE_H\n", "../__type_traits/is_same.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_SAME_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_SAME_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_SAME) && !defined(_LIBCUDACXX_USE_IS_SAME_FALLBACK)\n\ntemplate <class _Tp, class _Up>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_same : _BoolConstant<_LIBCUDACXX_IS_SAME(_Tp, _Up)> { };\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, class _Up>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_same_v = _LIBCUDACXX_IS_SAME(_Tp, _Up);\n#endif\n\n// _IsSame<T,U> has the same effect as is_same<T,U> but instantiates fewer types:\n// is_same<A,B> and is_same<C,D> are guaranteed to be different types, but\n// _IsSame<A,B> and _IsSame<C,D> are the same type (namely, false_type).\n// Neither GCC nor Clang can mangle the __is_same builtin, so _IsSame\n// mustn't be directly used anywhere that contributes to name-mangling\n// (such as in a dependent return type).\n\ntemplate <class _Tp, class _Up>\nusing _IsSame = _BoolConstant<_LIBCUDACXX_IS_SAME(_Tp, _Up)>;\n\ntemplate <class _Tp, class _Up>\nusing _IsNotSame = _BoolConstant<!_LIBCUDACXX_IS_SAME(_Tp, _Up)>;\n\n#else\n\ntemplate <class _Tp, class _Up> struct _LIBCUDACXX_TEMPLATE_VIS is_same           : public false_type {};\ntemplate <class _Tp>            struct _LIBCUDACXX_TEMPLATE_VIS is_same<_Tp, _Tp> : public true_type {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, class _Up>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_same_v = is_same<_Tp, _Up>::value;\n#endif\n\n// _IsSame<T,U> has the same effect as is_same<T,U> but instantiates fewer types:\n// is_same<A,B> and is_same<C,D> are guaranteed to be different types, but\n// _IsSame<A,B> and _IsSame<C,D> are the same type (namely, false_type).\n// Neither GCC nor Clang can mangle the __is_same builtin, so _IsSame\n// mustn't be directly used anywhere that contributes to name-mangling\n// (such as in a dependent return type).\n\ntemplate <class _Tp, class _Up>\nusing _IsSame = _BoolConstant<is_same<_Tp, _Up>::value>;\n\ntemplate <class _Tp, class _Up>\nusing _IsNotSame = _BoolConstant<!is_same<_Tp, _Up>::value>;\n\n#endif // defined(_LIBCUDACXX_IS_SAME) && !defined(_LIBCUDACXX_USE_IS_SAME_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_SAME_H\n", "../__type_traits/is_scalar.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_SCALAR_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_SCALAR_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_arithmetic.h\"\n#include \"../__type_traits/is_enum.h\"\n#include \"../__type_traits/is_member_pointer.h\"\n#include \"../__type_traits/is_null_pointer.h\"\n#include \"../__type_traits/is_pointer.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_SCALAR) && !defined(_LIBCUDACXX_USE_IS_SCALAR_FALLBACK)\n\ntemplate<class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_scalar\n    : public integral_constant<bool, _LIBCUDACXX_IS_SCALAR(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_scalar_v = _LIBCUDACXX_IS_SCALAR(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct __is_block : false_type {};\n#if defined(_LIBCUDACXX_HAS_EXTENSION_BLOCKS)\ntemplate <class _Rp, class ..._Args> struct __is_block<_Rp (^)(_Args...)> : true_type {};\n#endif\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_scalar\n    : public integral_constant<bool, is_arithmetic<_Tp>::value     ||\n                                     is_member_pointer<_Tp>::value ||\n                                     is_pointer<_Tp>::value        ||\n                                     __is_nullptr_t<_Tp>::value    ||\n                                     __is_block<_Tp>::value        ||\n                                     is_enum<_Tp>::value           > {};\n\ntemplate <> struct _LIBCUDACXX_TEMPLATE_VIS is_scalar<nullptr_t> : public true_type {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_scalar_v = is_scalar<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_SCALAR) && !defined(_LIBCUDACXX_USE_IS_SCALAR_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_SCALAR_H\n", "../__type_traits/is_signed.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_SIGNED_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_SIGNED_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_arithmetic.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_SIGNED) && !defined(_LIBCUDACXX_USE_IS_SIGNED_FALLBACK)\n\ntemplate<class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_signed\n    : public integral_constant<bool, _LIBCUDACXX_IS_SIGNED(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_signed_v = _LIBCUDACXX_IS_SIGNED(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp, bool = is_integral<_Tp>::value>\nstruct __libcpp_is_signed_impl : public _BoolConstant<(_Tp(-1) < _Tp(0))> {};\n\ntemplate <class _Tp>\nstruct __libcpp_is_signed_impl<_Tp, false> : public true_type {};  // floating point\n\ntemplate <class _Tp, bool = is_arithmetic<_Tp>::value>\nstruct __libcpp_is_signed : public __libcpp_is_signed_impl<_Tp> {};\n\ntemplate <class _Tp> struct __libcpp_is_signed<_Tp, false> : public false_type {};\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_signed : public __libcpp_is_signed<_Tp> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_signed_v = is_signed<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_SIGNED) && !defined(_LIBCUDACXX_USE_IS_SIGNED_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_SIGNED_H\n", "../__type_traits/is_signed_integer.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_SIGNED_INTEGER_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_SIGNED_INTEGER_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp> struct __libcpp_is_signed_integer : public false_type {};\ntemplate <> struct __libcpp_is_signed_integer<signed char>      : public true_type {};\ntemplate <> struct __libcpp_is_signed_integer<signed short>     : public true_type {};\ntemplate <> struct __libcpp_is_signed_integer<signed int>       : public true_type {};\ntemplate <> struct __libcpp_is_signed_integer<signed long>      : public true_type {};\ntemplate <> struct __libcpp_is_signed_integer<signed long long> : public true_type {};\n#ifndef _LIBCUDACXX_HAS_NO_INT128\ntemplate <> struct __libcpp_is_signed_integer<__int128_t>       : public true_type {};\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_SIGNED_INTEGER_H\n", "../__type_traits/is_standard_layout.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_STANDARD_LAYOUT_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_STANDARD_LAYOUT_H\n\n#ifndef __cuda_std__\n#include <__config>\n#include \"../__type_traits/remove_all_extents.h\"\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_STANDARD_LAYOUT) && !defined(_LIBCUDACXX_USE_IS_STANDARD_LAYOUT_FALLBACK)\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_standard_layout\n    : public integral_constant<bool, _LIBCUDACXX_IS_STANDARD_LAYOUT(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_standard_layout_v = _LIBCUDACXX_IS_STANDARD_LAYOUT(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_standard_layout\n    : integral_constant<bool, is_scalar<__remove_all_extents_t<_Tp>>::value>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_standard_layout_v\n    = is_standard_layout<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_STANDARD_LAYOUT) && !defined(_LIBCUDACXX_USE_IS_STANDARD_LAYOUT_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_STANDARD_LAYOUT_H\n", "../__type_traits/is_swappable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_SWAPPABLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_SWAPPABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_lvalue_reference.h\"\n#include \"../__type_traits/conditional.h\"\n#include \"../__type_traits/enable_if.h\"\n#include \"../__type_traits/is_move_assignable.h\"\n#include \"../__type_traits/is_move_constructible.h\"\n#include \"../__type_traits/is_nothrow_move_assignable.h\"\n#include \"../__type_traits/is_nothrow_move_constructible.h\"\n#include \"../__type_traits/is_referenceable.h\"\n#include \"../__type_traits/is_same.h\"\n#include \"../__type_traits/is_void.h\"\n#include \"../__type_traits/nat.h\"\n#include \"../__utility/declval.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp> struct __is_swappable;\ntemplate <class _Tp> struct __is_nothrow_swappable;\n\ntemplate <class _Tp>\nusing __swap_result_t = __enable_if_t<_LIBCUDACXX_TRAIT(is_move_constructible, _Tp)\n                                   && _LIBCUDACXX_TRAIT(is_move_assignable, _Tp)>;\n\ntemplate <class _Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n__swap_result_t<_Tp>\nswap(_Tp& __x, _Tp& __y) noexcept(_LIBCUDACXX_TRAIT(is_nothrow_move_constructible, _Tp)\n                               && _LIBCUDACXX_TRAIT(is_nothrow_move_assignable, _Tp));\n\ntemplate<class _Tp, size_t _Np>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n__enable_if_t<__is_swappable<_Tp>::value>\nswap(_Tp (&__a)[_Np], _Tp (&__b)[_Np]) noexcept(__is_nothrow_swappable<_Tp>::value);\n\nnamespace __detail\n{\n// ALL generic swap overloads MUST already have a declaration available at this point.\n\ntemplate <class _Tp, class _Up = _Tp,\n          bool _NotVoid = !_LIBCUDACXX_TRAIT(is_void, _Tp) && !_LIBCUDACXX_TRAIT(is_void, _Up)>\nstruct __swappable_with\n{\n    template <class _LHS, class _RHS>\n    _LIBCUDACXX_INLINE_VISIBILITY static decltype(swap(_CUDA_VSTD::declval<_LHS>(), _CUDA_VSTD::declval<_RHS>()))\n    __test_swap(int);\n    template <class, class>\n    _LIBCUDACXX_INLINE_VISIBILITY static __nat __test_swap(long);\n\n    // Extra parens are needed for the C++03 definition of decltype.\n    typedef decltype((__test_swap<_Tp, _Up>(0))) __swap1;\n    typedef decltype((__test_swap<_Up, _Tp>(0))) __swap2;\n\n    static const bool value = _IsNotSame<__swap1, __nat>::value\n                           && _IsNotSame<__swap2, __nat>::value;\n};\n\ntemplate <class _Tp, class _Up>\nstruct __swappable_with<_Tp, _Up,  false> : false_type {};\n\ntemplate <class _Tp, class _Up = _Tp, bool _Swappable = __swappable_with<_Tp, _Up>::value>\nstruct __nothrow_swappable_with {\n  static const bool value =\n      noexcept(swap(_CUDA_VSTD::declval<_Tp>(), _CUDA_VSTD::declval<_Up>()))\n  &&  noexcept(swap(_CUDA_VSTD::declval<_Up>(), _CUDA_VSTD::declval<_Tp>()));\n};\n\ntemplate <class _Tp, class _Up>\nstruct __nothrow_swappable_with<_Tp, _Up, false> : false_type {};\n\n} // namespace __detail\n\ntemplate <class _Tp>\nstruct __is_swappable\n    : public integral_constant<bool, __detail::__swappable_with<_Tp&>::value>\n{\n};\n\ntemplate <class _Tp>\nstruct __is_nothrow_swappable\n    : public integral_constant<bool, __detail::__nothrow_swappable_with<_Tp&>::value>\n{\n};\n\n#if _LIBCUDACXX_STD_VER > 11\n\ntemplate <class _Tp, class _Up>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_swappable_with\n    : public integral_constant<bool, __detail::__swappable_with<_Tp, _Up>::value>\n{\n};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_swappable\n    : public __conditional_t<\n        __libcpp_is_referenceable<_Tp>::value,\n        is_swappable_with<\n            __add_lvalue_reference_t<_Tp>,\n            __add_lvalue_reference_t<_Tp> >,\n        false_type\n    >\n{\n};\n\ntemplate <class _Tp, class _Up>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_swappable_with\n    : public integral_constant<bool, __detail::__nothrow_swappable_with<_Tp, _Up>::value>\n{\n};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_swappable\n    : public __conditional_t<\n        __libcpp_is_referenceable<_Tp>::value,\n        is_nothrow_swappable_with<\n            __add_lvalue_reference_t<_Tp>,\n            __add_lvalue_reference_t<_Tp> >,\n        false_type\n    >\n{\n};\n\ntemplate <class _Tp, class _Up>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_swappable_with_v = is_swappable_with<_Tp, _Up>::value;\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_swappable_v = is_swappable<_Tp>::value;\n\ntemplate <class _Tp, class _Up>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_swappable_with_v = is_nothrow_swappable_with<_Tp, _Up>::value;\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_swappable_v = is_nothrow_swappable<_Tp>::value;\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_SWAPPABLE_H\n", "../__type_traits/is_trivial.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIAL_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIAL_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_trivially_copyable.h\"\n#include \"../__type_traits/is_trivially_default_constructible.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_TRIVIAL) && !defined(_LIBCUDACXX_USE_IS_TRIVIAL_FALLBACK)\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_trivial\n    : public integral_constant<bool, _LIBCUDACXX_IS_TRIVIAL(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivial_v = _LIBCUDACXX_IS_TRIVIAL(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_trivial\n    : public integral_constant<bool, is_trivially_copyable<_Tp>::value &&\n                                     is_trivially_default_constructible<_Tp>::value>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivial_v\n    = is_trivial<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_TRIVIAL) && !defined(_LIBCUDACXX_USE_IS_TRIVIAL_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIAL_H\n", "../__type_traits/is_trivially_assignable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_ASSIGNABLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_ASSIGNABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_scalar.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_TRIVIALLY_ASSIGNABLE) && !defined(_LIBCUDACXX_USE_IS_TRIVIALLY_ASSIGNABLE_FALLBACK)\n\ntemplate <class _Tp, class _Arg>\nstruct is_trivially_assignable\n    : integral_constant<bool, _LIBCUDACXX_IS_TRIVIALLY_ASSIGNABLE(_Tp, _Arg)>\n{ };\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, class _Arg>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivially_assignable_v = _LIBCUDACXX_IS_TRIVIALLY_ASSIGNABLE(_Tp, _Arg);\n#endif\n\n#else\n\ntemplate <class _Tp, class _Arg>\nstruct is_trivially_assignable\n    : public false_type {};\n\ntemplate <class _Tp>\nstruct is_trivially_assignable<_Tp&, _Tp>\n    : integral_constant<bool, is_scalar<_Tp>::value> {};\n\ntemplate <class _Tp>\nstruct is_trivially_assignable<_Tp&, _Tp&>\n    : integral_constant<bool, is_scalar<_Tp>::value> {};\n\ntemplate <class _Tp>\nstruct is_trivially_assignable<_Tp&, const _Tp&>\n    : integral_constant<bool, is_scalar<_Tp>::value> {};\n\ntemplate <class _Tp>\nstruct is_trivially_assignable<_Tp&, _Tp&&>\n    : integral_constant<bool, is_scalar<_Tp>::value> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, class _Arg>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivially_assignable_v\n    = is_trivially_assignable<_Tp, _Arg>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_TRIVIALLY_ASSIGNABLE) && !defined(_LIBCUDACXX_USE_IS_TRIVIALLY_ASSIGNABLE_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_ASSIGNABLE_H\n", "../__type_traits/is_trivially_constructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_CONSTRUCTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_CONSTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_scalar.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_TRIVIALLY_CONSTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_TRIVIALLY_CONSTRUCTIBLE_FALLBACK)\n\ntemplate <class _Tp, class... _Args>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_trivially_constructible\n    : public integral_constant<bool, _LIBCUDACXX_IS_TRIVIALLY_CONSTRUCTIBLE(_Tp, _Args...)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, class... _Args>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivially_constructible_v =\n    _LIBCUDACXX_IS_TRIVIALLY_CONSTRUCTIBLE(_Tp, _Args...);\n#endif\n\n#else\n\ntemplate <class _Tp, class... _Args>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_trivially_constructible\n    : false_type\n{\n};\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_trivially_constructible<_Tp>\n#if defined(_LIBCUDACXX_HAS_TRIVIAL_CONSTRUCTOR) && !defined(_LIBCUDACXX_USE_HAS_TRIVIAL_CONSTRUCTOR_FALLBACK)\n    : integral_constant<bool, _LIBCUDACXX_HAS_TRIVIAL_CONSTRUCTOR(_Tp)>\n#else\n    : integral_constant<bool, is_scalar<_Tp>::value>\n#endif // defined(_LIBCUDACXX_HAS_TRIVIAL_CONSTRUCTOR) && !defined(_LIBCUDACXX_USE_HAS_TRIVIAL_CONSTRUCTOR_FALLBACK)\n{\n};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_trivially_constructible<_Tp, _Tp&&>\n    : integral_constant<bool, is_scalar<_Tp>::value>\n{\n};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_trivially_constructible<_Tp, const _Tp&>\n    : integral_constant<bool, is_scalar<_Tp>::value>\n{\n};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_trivially_constructible<_Tp, _Tp&>\n    : integral_constant<bool, is_scalar<_Tp>::value>\n{\n};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, class... _Args>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivially_constructible_v\n    = is_trivially_constructible<_Tp, _Args...>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_TRIVIALLY_CONSTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_TRIVIALLY_CONSTRUCTIBLE_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_CONSTRUCTIBLE_H\n", "../__type_traits/is_trivially_copy_assignable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_COPY_ASSIGNABLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_COPY_ASSIGNABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_const.h\"\n#include \"../__type_traits/add_lvalue_reference.h\"\n#include \"../__type_traits/is_trivially_assignable.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_TRIVIALLY_ASSIGNABLE) && !defined(_LIBCUDACXX_USE_IS_TRIVIALLY_ASSIGNABLE_FALLBACK)\n\ntemplate <class _Tp>\nstruct is_trivially_copy_assignable\n    : public integral_constant<bool,\n        _LIBCUDACXX_IS_TRIVIALLY_ASSIGNABLE(__add_lvalue_reference_t<_Tp>,\n                                            __add_lvalue_reference_t<typename add_const<_Tp>::type>)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivially_copy_assignable_v =\n    _LIBCUDACXX_IS_TRIVIALLY_ASSIGNABLE(__add_lvalue_reference_t<_Tp>,\n                                        __add_lvalue_reference_t<typename add_const<_Tp>::type>);\n#endif\n\n#else\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_trivially_copy_assignable\n    : public is_trivially_assignable<__add_lvalue_reference_t<_Tp>,\n                                     __add_lvalue_reference_t<typename add_const<_Tp>::type>>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivially_copy_assignable_v = is_trivially_copy_assignable<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_TRIVIALLY_ASSIGNABLE) && !defined(_LIBCUDACXX_USE_IS_TRIVIALLY_ASSIGNABLE_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_COPY_ASSIGNABLE_H\n", "../__type_traits/is_trivially_copy_constructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_COPY_CONSTRUCTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_COPY_CONSTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_const.h\"\n#include \"../__type_traits/add_lvalue_reference.h\"\n#include \"../__type_traits/is_trivially_constructible.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_TRIVIALLY_CONSTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_TRIVIALLY_CONSTRUCTIBLE_FALLBACK)\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_trivially_copy_constructible\n    : public integral_constant<bool,\n        _LIBCUDACXX_IS_TRIVIALLY_CONSTRUCTIBLE(_Tp, __add_lvalue_reference_t<typename add_const<_Tp>::type>)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivially_copy_constructible_v =\n    _LIBCUDACXX_IS_TRIVIALLY_CONSTRUCTIBLE(_Tp, __add_lvalue_reference_t<typename add_const<_Tp>::type>);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_trivially_copy_constructible\n    : public is_trivially_constructible<_Tp, __add_lvalue_reference_t<typename add_const<_Tp>::type>>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivially_copy_constructible_v = is_trivially_copy_constructible<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_TRIVIALLY_CONSTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_TRIVIALLY_CONSTRUCTIBLE_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_COPY_CONSTRUCTIBLE_H\n", "../__type_traits/is_trivially_copyable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_COPYABLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_COPYABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_scalar.h\"\n#include \"../__type_traits/remove_all_extents.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_TRIVIALLY_COPYABLE) && !defined(_LIBCUDACXX_USE_IS_TRIVIALLY_COPYABLE_FALLBACK)\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_trivially_copyable\n    : public integral_constant<bool, _LIBCUDACXX_IS_TRIVIALLY_COPYABLE(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivially_copyable_v = _LIBCUDACXX_IS_TRIVIALLY_COPYABLE(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_trivially_copyable\n    : integral_constant<bool, is_scalar<__remove_all_extents_t<_Tp>::type>::value>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivially_copyable_v\n    = is_trivially_copyable<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_TRIVIALLY_COPYABLE) && !defined(_LIBCUDACXX_USE_IS_TRIVIALLY_COPYABLE_FALLBACK)\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_COPYABLE_H\n", "../__type_traits/is_trivially_default_constructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_DEFAULT_CONSTRUCTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_DEFAULT_CONSTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/is_trivially_constructible.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_TRIVIALLY_CONSTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_TRIVIALLY_CONSTRUCTIBLE_FALLBACK)\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_trivially_default_constructible\n    : public integral_constant<bool, _LIBCUDACXX_IS_TRIVIALLY_CONSTRUCTIBLE(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivially_default_constructible_v = _LIBCUDACXX_IS_TRIVIALLY_CONSTRUCTIBLE(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_trivially_default_constructible\n    : public is_trivially_constructible<_Tp>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivially_default_constructible_v\n    = is_trivially_default_constructible<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_TRIVIALLY_CONSTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_TRIVIALLY_CONSTRUCTIBLE_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_DEFAULT_CONSTRUCTIBLE_H\n", "../__type_traits/is_trivially_destructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_DESTRUCTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_DESTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_destructible.h\"\n#include \"../__type_traits/is_reference.h\"\n#include \"../__type_traits/is_scalar.h\"\n#include \"../__type_traits/remove_all_extents.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_TRIVIALLY_DESTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_TRIVIALLY_DESTRUCTIBLE_FALLBACK)\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_trivially_destructible\n    : public integral_constant<bool, _LIBCUDACXX_IS_TRIVIALLY_DESTRUCTIBLE(_Tp)> {};\n\n#elif defined(_LIBCUDACXX_HAS_TRIVIAL_DESTRUCTOR) && !defined(_LIBCUDACXX_USE_HAS_TRIVIAL_DESTRUCTOR_FALLBACK)\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_trivially_destructible\n    : public integral_constant<bool, is_destructible<_Tp>::value && _LIBCUDACXX_HAS_TRIVIAL_DESTRUCTOR(_Tp)> {};\n\n#else\n\ntemplate <class _Tp> struct __libcpp_trivial_destructor\n    : public integral_constant<bool, is_scalar<_Tp>::value ||\n                                     is_reference<_Tp>::value> {};\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_trivially_destructible\n    : public __libcpp_trivial_destructor<__remove_all_extents_t<_Tp>> {};\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_trivially_destructible<_Tp[]>\n    : public false_type {};\n\n#endif // defined(_LIBCUDACXX_HAS_TRIVIAL_DESTRUCTOR) && !defined(_LIBCUDACXX_USE_HAS_TRIVIAL_DESTRUCTOR_FALLBACK)\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivially_destructible_v = is_trivially_destructible<_Tp>::value;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_DESTRUCTIBLE_H\n", "../__type_traits/is_trivially_move_assignable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_MOVE_ASSIGNABLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_MOVE_ASSIGNABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_lvalue_reference.h\"\n#include \"../__type_traits/add_rvalue_reference.h\"\n#include \"../__type_traits/is_trivially_assignable.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_TRIVIALLY_ASSIGNABLE) && !defined(_LIBCUDACXX_USE_IS_TRIVIALLY_ASSIGNABLE_FALLBACK)\n\ntemplate <class _Tp>\nstruct is_trivially_move_assignable\n    : public integral_constant<bool,\n        _LIBCUDACXX_IS_TRIVIALLY_ASSIGNABLE(__add_lvalue_reference_t<_Tp>, __add_rvalue_reference_t<_Tp>)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivially_move_assignable_v =\n    _LIBCUDACXX_IS_TRIVIALLY_ASSIGNABLE(__add_lvalue_reference_t<_Tp>, __add_rvalue_reference_t<_Tp>);\n#endif\n\n#else\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_trivially_move_assignable\n    : public is_trivially_assignable<__add_lvalue_reference_t<_Tp>, __add_rvalue_reference_t<_Tp>>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivially_move_assignable_v = is_trivially_move_assignable<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_TRIVIALLY_ASSIGNABLE) && !defined(_LIBCUDACXX_USE_IS_TRIVIALLY_ASSIGNABLE_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_MOVE_ASSIGNABLE_H\n", "../__type_traits/is_union.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_UNION_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_UNION_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/remove_cv.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_UNION) && !defined(_LIBCUDACXX_USE_IS_UNION_FALLBACK)\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_union\n    : public integral_constant<bool, _LIBCUDACXX_IS_UNION(_Tp)> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_union_v = _LIBCUDACXX_IS_UNION(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct __libcpp_union : public false_type {};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_union\n    : public __libcpp_union<__remove_cv_t<_Tp>> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_union_v = is_union<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_UNION) && !defined(_LIBCUDACXX_USE_IS_UNION_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_UNION_H\n", "../__type_traits/is_unsigned.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_UNSIGNED_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_UNSIGNED_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_arithmetic.h\"\n#include \"../__type_traits/is_integral.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n// Before AppleClang 14, __is_unsigned returned true for enums with signed underlying type.\n#if defined(_LIBCUDACXX_IS_UNSIGNED) && !defined(_LIBCUDACXX_USE_IS_UNSIGNED_FALLBACK) && !(defined(_LIBCUDACXX_APPLE_CLANG_VER) && _LIBCUDACXX_APPLE_CLANG_VER < 1400)\n\ntemplate<class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_unsigned\n    : public integral_constant<bool, _LIBCUDACXX_IS_UNSIGNED(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_unsigned_v = _LIBCUDACXX_IS_UNSIGNED(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp, bool = is_integral<_Tp>::value>\nstruct __libcpp_is_unsigned_impl : public _BoolConstant<(_Tp(0) < _Tp(-1))> {};\n\ntemplate <class _Tp>\nstruct __libcpp_is_unsigned_impl<_Tp, false> : public false_type {};  // floating point\n\ntemplate <class _Tp, bool = is_arithmetic<_Tp>::value>\nstruct __libcpp_is_unsigned : public __libcpp_is_unsigned_impl<_Tp> {};\n\ntemplate <class _Tp> struct __libcpp_is_unsigned<_Tp, false> : public false_type {};\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_unsigned : public __libcpp_is_unsigned<_Tp> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_unsigned_v = is_unsigned<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_UNSIGNED) && !defined(_LIBCUDACXX_USE_IS_UNSIGNED_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_UNSIGNED_H\n", "../__type_traits/is_unsigned_integer.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_UNSIGNED_INTEGER_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_UNSIGNED_INTEGER_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp> struct __libcpp_is_unsigned_integer : public false_type {};\ntemplate <> struct __libcpp_is_unsigned_integer<unsigned char>      : public true_type {};\ntemplate <> struct __libcpp_is_unsigned_integer<unsigned short>     : public true_type {};\ntemplate <> struct __libcpp_is_unsigned_integer<unsigned int>       : public true_type {};\ntemplate <> struct __libcpp_is_unsigned_integer<unsigned long>      : public true_type {};\ntemplate <> struct __libcpp_is_unsigned_integer<unsigned long long> : public true_type {};\n#ifndef _LIBCUDACXX_HAS_NO_INT128\ntemplate <> struct __libcpp_is_unsigned_integer<__uint128_t>        : public true_type {};\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_UNSIGNED_INTEGER_H\n", "../__type_traits/is_valid_expansion.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_VALID_EXPANSION_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_VALID_EXPANSION_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <template <class...> class _Templ, class ..._Args, class = _Templ<_Args...> >\n_LIBCUDACXX_INLINE_VISIBILITY true_type __sfinae_test_impl(int);\ntemplate <template <class...> class, class ...>\n_LIBCUDACXX_INLINE_VISIBILITY false_type __sfinae_test_impl(...);\n\ntemplate <template <class ...> class _Templ, class ..._Args>\nusing _IsValidExpansion _LIBCUDACXX_NODEBUG_TYPE = decltype(_CUDA_VSTD::__sfinae_test_impl<_Templ, _Args...>(0));\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_VALID_EXPANSION_H\n", "../__type_traits/is_void.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_VOID_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_VOID_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_same.h\"\n#include \"../__type_traits/remove_cvref.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_VOID) && !defined(_LIBCUDACXX_USE_IS_VOID_FALLBACK)\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_void\n    : integral_constant<bool, _LIBCUDACXX_IS_VOID(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_void_v = __is_void(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_void\n    : public is_same<__remove_cv_t<_Tp>, void> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_void_v = is_void<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_VOID) && !defined(_LIBCUDACXX_USE_IS_VOID_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_VOID_H\n", "../__type_traits/is_volatile.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_VOLATILE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_VOLATILE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_VOLATILE) && !defined(_LIBCUDACXX_USE_IS_VOLATILE_FALLBACK)\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_volatile :\n    : public integral_constant<bool, _LIBCUDACXX_IS_VOLATILE(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_volatile_v = _LIBCUDACXX_IS_VOLATILE(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_volatile               : public false_type {};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_volatile<_Tp volatile> : public true_type {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_volatile_v = is_volatile<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_VOLATILE) && !defined(_LIBCUDACXX_USE_IS_VOLATILE_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_VOLATILE_H\n", "../__type_traits/lazy.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_LAZY_H\n#define _LIBCUDACXX___TYPE_TRAITS_LAZY_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <template <class...> class _Func, class ..._Args>\nstruct _Lazy : _Func<_Args...> {};\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_LAZY_H\n", "../__type_traits/make_const_lvalue_ref.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_MAKE_CONST_LVALUE_REF_H\n#define _LIBCUDACXX___TYPE_TRAITS_MAKE_CONST_LVALUE_REF_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/remove_reference.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate<class _Tp>\nusing __make_const_lvalue_ref = const __libcpp_remove_reference_t<_Tp>&;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_MAKE_CONST_LVALUE_REF_H\n", "../__type_traits/make_signed.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_MAKE_SIGNED_H\n#define _LIBCUDACXX___TYPE_TRAITS_MAKE_SIGNED_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/apply_cv.h\"\n#include \"../__type_traits/is_enum.h\"\n#include \"../__type_traits/is_integral.h\"\n#include \"../__type_traits/nat.h\"\n#include \"../__type_traits/remove_cv.h\"\n#include \"../__type_traits/type_list.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_MAKE_SIGNED) && !defined(_LIBCUDACXX_USE_MAKE_SIGNED_FALLBACK)\n\ntemplate <class _Tp>\nusing __make_signed_t = _LIBCUDACXX_MAKE_SIGNED(_Tp);\n\n#else\ntypedef\n    __type_list<signed char,\n    __type_list<signed short,\n    __type_list<signed int,\n    __type_list<signed long,\n    __type_list<signed long long,\n#  ifndef _LIBCUDACXX_HAS_NO_INT128\n    __type_list<__int128_t,\n#  endif\n    __nat\n#  ifndef _LIBCUDACXX_HAS_NO_INT128\n    >\n#  endif\n    > > > > > __signed_types;\n\ntemplate <class _Tp, bool = is_integral<_Tp>::value || is_enum<_Tp>::value>\nstruct __make_signed_impl {};\n\ntemplate <class _Tp>\nstruct __make_signed_impl<_Tp, true>\n{\n    typedef typename __find_first<__signed_types, sizeof(_Tp)>::type type;\n};\n\ntemplate <> struct __make_signed_impl<bool,               true> {};\ntemplate <> struct __make_signed_impl<  signed short,     true> {typedef short     type;};\ntemplate <> struct __make_signed_impl<unsigned short,     true> {typedef short     type;};\ntemplate <> struct __make_signed_impl<  signed int,       true> {typedef int       type;};\ntemplate <> struct __make_signed_impl<unsigned int,       true> {typedef int       type;};\ntemplate <> struct __make_signed_impl<  signed long,      true> {typedef long      type;};\ntemplate <> struct __make_signed_impl<unsigned long,      true> {typedef long      type;};\ntemplate <> struct __make_signed_impl<  signed long long, true> {typedef long long type;};\ntemplate <> struct __make_signed_impl<unsigned long long, true> {typedef long long type;};\n#  ifndef _LIBCUDACXX_HAS_NO_INT128\ntemplate <> struct __make_signed_impl<__int128_t,         true> {typedef __int128_t type;};\ntemplate <> struct __make_signed_impl<__uint128_t,        true> {typedef __int128_t type;};\n#  endif\n\ntemplate <class _Tp>\nusing __make_signed_t = typename __apply_cv<_Tp, typename __make_signed_impl<__remove_cv_t<_Tp> >::type>::type;\n\n#endif // defined(_LIBCUDACXX_MAKE_SIGNED) && !defined(_LIBCUDACXX_USE_MAKE_SIGNED_FALLBACK)\n\ntemplate <class _Tp>\nstruct make_signed {\n  using type _LIBCUDACXX_NODEBUG_TYPE = __make_signed_t<_Tp>;\n};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> using make_signed_t = __make_signed_t<_Tp>;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_MAKE_SIGNED_H\n", "../__type_traits/make_unsigned.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_MAKE_UNSIGNED_H\n#define _LIBCUDACXX___TYPE_TRAITS_MAKE_UNSIGNED_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/apply_cv.h\"\n#include \"../__type_traits/conditional.h\"\n#include \"../__type_traits/is_enum.h\"\n#include \"../__type_traits/is_integral.h\"\n#include \"../__type_traits/is_unsigned.h\"\n#include \"../__type_traits/nat.h\"\n#include \"../__type_traits/remove_cv.h\"\n#include \"../__type_traits/type_list.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_MAKE_UNSIGNED) && !defined(_LIBCUDACXX_USE_MAKE_UNSIGNED_FALLBACK)\n\ntemplate <class _Tp>\nusing __make_unsigned_t = _LIBCUDACXX_MAKE_UNSIGNED(_Tp);\n\n#else\ntypedef\n    __type_list<unsigned char,\n    __type_list<unsigned short,\n    __type_list<unsigned int,\n    __type_list<unsigned long,\n    __type_list<unsigned long long,\n#  ifndef _LIBCUDACXX_HAS_NO_INT128\n    __type_list<__uint128_t,\n#  endif\n    __nat\n#  ifndef _LIBCUDACXX_HAS_NO_INT128\n    >\n#  endif\n    > > > > > __unsigned_types;\n\ntemplate <class _Tp, bool = is_integral<_Tp>::value || is_enum<_Tp>::value>\nstruct __make_unsigned_impl {};\n\ntemplate <class _Tp>\nstruct __make_unsigned_impl<_Tp, true>\n{\n    typedef typename __find_first<__unsigned_types, sizeof(_Tp)>::type type;\n};\n\ntemplate <> struct __make_unsigned_impl<bool,               true> {};\ntemplate <> struct __make_unsigned_impl<  signed short,     true> {typedef unsigned short     type;};\ntemplate <> struct __make_unsigned_impl<unsigned short,     true> {typedef unsigned short     type;};\ntemplate <> struct __make_unsigned_impl<  signed int,       true> {typedef unsigned int       type;};\ntemplate <> struct __make_unsigned_impl<unsigned int,       true> {typedef unsigned int       type;};\ntemplate <> struct __make_unsigned_impl<  signed long,      true> {typedef unsigned long      type;};\ntemplate <> struct __make_unsigned_impl<unsigned long,      true> {typedef unsigned long      type;};\ntemplate <> struct __make_unsigned_impl<  signed long long, true> {typedef unsigned long long type;};\ntemplate <> struct __make_unsigned_impl<unsigned long long, true> {typedef unsigned long long type;};\n#  ifndef _LIBCUDACXX_HAS_NO_INT128\ntemplate <> struct __make_unsigned_impl<__int128_t,         true> {typedef __uint128_t        type;};\ntemplate <> struct __make_unsigned_impl<__uint128_t,        true> {typedef __uint128_t        type;};\n#  endif\n\ntemplate <class _Tp>\nusing __make_unsigned_t = typename __apply_cv<_Tp, typename __make_unsigned_impl<__remove_cv_t<_Tp> >::type>::type;\n\n#endif // defined(_LIBCUDACXX_MAKE_UNSIGNED) && !defined(_LIBCUDACXX_USE_MAKE_UNSIGNED_FALLBACK)\n\ntemplate <class _Tp>\nstruct make_unsigned {\n  using type _LIBCUDACXX_NODEBUG_TYPE = __make_unsigned_t<_Tp>;\n};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> using make_unsigned_t = __make_unsigned_t<_Tp>;\n#endif\n\ntemplate <class _Tp>\n_LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY constexpr\n__make_unsigned_t<_Tp> __to_unsigned_like(_Tp __x) noexcept {\n    return static_cast<__make_unsigned_t<_Tp> >(__x);\n}\n\ntemplate <class _Tp, class _Up>\nusing __copy_unsigned_t = __conditional_t<is_unsigned<_Tp>::value, __make_unsigned_t<_Up>, _Up>;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_MAKE_UNSIGNED_H\n", "../__type_traits/nat.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_NAT_H\n#define _LIBCUDACXX___TYPE_TRAITS_NAT_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\nstruct __nat\n{\n    __nat() = delete;\n    __nat(const __nat&) = delete;\n    __nat& operator=(const __nat&) = delete;\n    ~__nat() = delete;\n};\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_NAT_H\n", "../__type_traits/negation.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_NEGATION_H\n#define _LIBCUDACXX___TYPE_TRAITS_NEGATION_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Pred>\nstruct _Not : _BoolConstant<!_Pred::value> {};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp>\nstruct negation : _Not<_Tp> {};\ntemplate<class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool negation_v = !_Tp::value;\n#endif // _LIBCUDACXX_STD_VER > 14\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_NEGATION_H\n", "../__type_traits/remove_all_extents.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_REMOVE_ALL_EXTENTS_H\n#define _LIBCUDACXX___TYPE_TRAITS_REMOVE_ALL_EXTENTS_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_REMOVE_ALL_EXTENTS) && !defined(_LIBCUDACXX_USE_REMOVE_ALL_EXTENTS_FALLBACK)\ntemplate <class _Tp>\nstruct remove_all_extents {\n  using type _LIBCUDACXX_NODEBUG_TYPE = _LIBCUDACXX_REMOVE_ALL_EXTENTS(_Tp);\n};\n\ntemplate <class _Tp>\nusing __remove_all_extents_t = _LIBCUDACXX_REMOVE_ALL_EXTENTS(_Tp);\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS remove_all_extents\n    {typedef _Tp type;};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS remove_all_extents<_Tp[]>\n    {typedef typename remove_all_extents<_Tp>::type type;};\ntemplate <class _Tp, size_t _Np> struct _LIBCUDACXX_TEMPLATE_VIS remove_all_extents<_Tp[_Np]>\n    {typedef typename remove_all_extents<_Tp>::type type;};\n\ntemplate <class _Tp>\nusing __remove_all_extents_t = typename remove_all_extents<_Tp>::type;\n\n#endif // defined(_LIBCUDACXX_REMOVE_ALL_EXTENTS) && !defined(_LIBCUDACXX_USE_REMOVE_ALL_EXTENTS_FALLBACK)\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> using remove_all_extents_t = __remove_all_extents_t<_Tp>;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_REMOVE_ALL_EXTENTS_H\n", "../__type_traits/remove_const.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_REMOVE_CONST_H\n#define _LIBCUDACXX___TYPE_TRAITS_REMOVE_CONST_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_REMOVE_CONST) && !defined(_LIBCUDACXX_USE_REMOVE_CONST_FALLBACK)\ntemplate <class _Tp>\nstruct remove_const {\n  using type _LIBCUDACXX_NODEBUG_TYPE = _LIBCUDACXX_REMOVE_CONST(_Tp);\n};\n\ntemplate <class _Tp>\nusing __remove_const_t = _LIBCUDACXX_REMOVE_CONST(_Tp);\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS remove_const            {typedef _Tp type;};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS remove_const<const _Tp> {typedef _Tp type;};\n\ntemplate <class _Tp>\nusing __remove_const_t = typename remove_const<_Tp>::type;\n\n#endif // defined(_LIBCUDACXX_REMOVE_CONST) && !defined(_LIBCUDACXX_USE_REMOVE_CONST_FALLBACK)\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> using remove_const_t = __remove_const_t<_Tp>;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_REMOVE_CONST_H\n", "../__type_traits/remove_const_ref.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_REMOVE_CONST_REF_H\n#define _LIBCUDACXX___TYPE_TRAITS_REMOVE_CONST_REF_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/remove_const.h\"\n#include \"../__type_traits/remove_reference.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp>\nusing __remove_const_ref_t = __remove_const_t<__libcpp_remove_reference_t<_Tp> >;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_REMOVE_CONST_REF_H\n", "../__type_traits/remove_cv.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_REMOVE_CV_H\n#define _LIBCUDACXX___TYPE_TRAITS_REMOVE_CV_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/remove_const.h\"\n#include \"../__type_traits/remove_volatile.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_REMOVE_CV) && !defined(_LIBCUDACXX_USE_REMOVE_CV_FALLBACK)\ntemplate <class _Tp>\nstruct remove_cv {\n  using type _LIBCUDACXX_NODEBUG_TYPE = _LIBCUDACXX_REMOVE_CV(_Tp);\n};\n\ntemplate <class _Tp>\nusing __remove_cv_t = _LIBCUDACXX_REMOVE_CV(_Tp);\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS remove_cv\n{typedef __remove_volatile_t<__remove_const_t<_Tp> > type;};\n\ntemplate <class _Tp>\nusing __remove_cv_t = __remove_volatile_t<__remove_const_t<_Tp> >;\n\n#endif // defined(_LIBCUDACXX_REMOVE_CV) && !defined(_LIBCUDACXX_USE_REMOVE_CV_FALLBACK)\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> using remove_cv_t = __remove_cv_t<_Tp>;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_REMOVE_CV_H\n", "../__type_traits/remove_cvref.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_REMOVE_CVREF_H\n#define _LIBCUDACXX___TYPE_TRAITS_REMOVE_CVREF_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/is_same.h\"\n#include \"../__type_traits/remove_cv.h\"\n#include \"../__type_traits/remove_reference.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_REMOVE_CVREF) && !defined(_LIBCUDACXX_USE_REMOVE_CVREF_FALLBACK)\n\ntemplate <class _Tp>\nusing __remove_cvref_t _LIBCUDACXX_NODEBUG_TYPE = _LIBCUDACXX_REMOVE_CVREF(_Tp);\n\n#else\n\ntemplate <class _Tp>\nusing __remove_cvref_t _LIBCUDACXX_NODEBUG_TYPE = __remove_cv_t<__libcpp_remove_reference_t<_Tp> >;\n\n#endif // defined(_LIBCUDACXX_REMOVE_CVREF) && !defined(_LIBCUDACXX_USE_REMOVE_CVREF_FALLBACK)\n\ntemplate <class _Tp, class _Up>\nstruct __is_same_uncvref : _IsSame<__remove_cvref_t<_Tp>, __remove_cvref_t<_Up> > {};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp>\nstruct remove_cvref {\n    using type _LIBCUDACXX_NODEBUG_TYPE = __remove_cvref_t<_Tp>;\n};\n\ntemplate <class _Tp> using remove_cvref_t = __remove_cvref_t<_Tp>;\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_REMOVE_CVREF_H\n", "../__type_traits/remove_extent.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_REMOVE_EXTENT_H\n#define _LIBCUDACXX___TYPE_TRAITS_REMOVE_EXTENT_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_REMOVE_EXTENT) && !defined(_LIBCUDACXX_USE_REMOVE_EXTENT_FALLBACK)\ntemplate <class _Tp>\nstruct remove_extent {\n  using type _LIBCUDACXX_NODEBUG_TYPE = _LIBCUDACXX_REMOVE_EXTENT(_Tp);\n};\n\ntemplate <class _Tp>\nusing __remove_extent_t = _LIBCUDACXX_REMOVE_EXTENT(_Tp);\n\n#else\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS remove_extent\n    {typedef _Tp type;};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS remove_extent<_Tp[]>\n    {typedef _Tp type;};\ntemplate <class _Tp, size_t _Np> struct _LIBCUDACXX_TEMPLATE_VIS remove_extent<_Tp[_Np]>\n    {typedef _Tp type;};\n\ntemplate <class _Tp>\nusing __remove_extent_t = typename remove_extent<_Tp>::type;\n\n#endif // defined(_LIBCUDACXX_REMOVE_EXTENT) && !defined(_LIBCUDACXX_USE_REMOVE_EXTENT_FALLBACK)\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> using remove_extent_t = __remove_extent_t<_Tp>;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_REMOVE_EXTENT_H\n", "../__type_traits/remove_reference.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_REMOVE_REFERENCE_H\n#define _LIBCUDACXX___TYPE_TRAITS_REMOVE_REFERENCE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_REMOVE_REFERENCE_T) && !defined(_LIBCUDACXX_USE_REMOVE_REFERENCE_T_FALLBACK)\ntemplate <class _Tp>\nstruct remove_reference {\n  using type _LIBCUDACXX_NODEBUG_TYPE = _LIBCUDACXX_REMOVE_REFERENCE_T(_Tp);\n};\n\ntemplate <class _Tp>\nusing __libcpp_remove_reference_t = _LIBCUDACXX_REMOVE_REFERENCE_T(_Tp);\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS remove_reference        {typedef _LIBCUDACXX_NODEBUG_TYPE _Tp type;};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS remove_reference<_Tp&>  {typedef _LIBCUDACXX_NODEBUG_TYPE _Tp type;};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS remove_reference<_Tp&&> {typedef _LIBCUDACXX_NODEBUG_TYPE _Tp type;};\n\ntemplate <class _Tp>\nusing __libcpp_remove_reference_t = typename remove_reference<_Tp>::type;\n\n#endif // defined(_LIBCUDACXX_REMOVE_REFERENCE_T) && !defined(_LIBCUDACXX_USE_REMOVE_REFERENCE_T_FALLBACK)\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> using remove_reference_t = __libcpp_remove_reference_t<_Tp>;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_REMOVE_REFERENCE_H\n", "../__type_traits/remove_volatile.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_REMOVE_VOLATILE_H\n#define _LIBCUDACXX___TYPE_TRAITS_REMOVE_VOLATILE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_REMOVE_VOLATILE) && !defined(_LIBCUDACXX_USE_REMOVE_VOLATILE_FALLBACK)\ntemplate <class _Tp>\nstruct remove_volatile {\n  using type _LIBCUDACXX_NODEBUG_TYPE = _LIBCUDACXX_REMOVE_VOLATILE(_Tp);\n};\n\ntemplate <class _Tp>\nusing __remove_volatile_t = _LIBCUDACXX_REMOVE_VOLATILE(_Tp);\n\n#else\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS remove_volatile               {typedef _Tp type;};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS remove_volatile<volatile _Tp> {typedef _Tp type;};\n\ntemplate <class _Tp>\nusing __remove_volatile_t = typename remove_volatile<_Tp>::type;\n\n#endif // defined(_LIBCUDACXX_REMOVE_VOLATILE) && !defined(_LIBCUDACXX_USE_REMOVE_VOLATILE_FALLBACK)\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> using remove_volatile_t = __remove_volatile_t<_Tp>;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_REMOVE_VOLATILE_H\n", "../__type_traits/type_identity.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_TYPE_IDENTITY_H\n#define _LIBCUDACXX___TYPE_TRAITS_TYPE_IDENTITY_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp>\nstruct __type_identity { typedef _Tp type; };\n\ntemplate <class _Tp>\nusing __type_identity_t _LIBCUDACXX_NODEBUG_TYPE = typename __type_identity<_Tp>::type;\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate<class _Tp> struct type_identity { typedef _Tp type; };\ntemplate<class _Tp> using type_identity_t = typename type_identity<_Tp>::type;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_TYPE_IDENTITY_H\n", "../__type_traits/type_list.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_TYPE_LIST_H\n#define _LIBCUDACXX___TYPE_TRAITS_TYPE_LIST_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Hp, class _Tp>\nstruct __type_list\n{\n    typedef _Hp _Head;\n    typedef _Tp _Tail;\n};\n\ntemplate <class _TypeList, size_t _Size, bool = _Size <= sizeof(typename _TypeList::_Head)> struct __find_first;\n\ntemplate <class _Hp, class _Tp, size_t _Size>\nstruct __find_first<__type_list<_Hp, _Tp>, _Size, true>\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE _Hp type;\n};\n\ntemplate <class _Hp, class _Tp, size_t _Size>\nstruct __find_first<__type_list<_Hp, _Tp>, _Size, false>\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE typename __find_first<_Tp, _Size>::type type;\n};\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_TYPE_LIST_H\n", "../__type_traits/underlying_type.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_UNDERLYING_TYPE_H\n#define _LIBCUDACXX___TYPE_TRAITS_UNDERLYING_TYPE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/is_enum.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_UNDERLYING_TYPE) && !defined(_LIBCUDACXX_USE_UNDERLYING_TYPE_FALLBACK)\n\ntemplate <class _Tp, bool = is_enum<_Tp>::value> struct __underlying_type_impl;\n\ntemplate <class _Tp>\nstruct __underlying_type_impl<_Tp, false> {};\n\ntemplate <class _Tp>\nstruct __underlying_type_impl<_Tp, true>\n{\n    typedef _LIBCUDACXX_UNDERLYING_TYPE(_Tp) type;\n};\n\ntemplate <class _Tp>\nstruct underlying_type : __underlying_type_impl<_Tp, is_enum<_Tp>::value> {};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> using underlying_type_t = typename underlying_type<_Tp>::type;\n#endif\n\n#else\n\ntemplate <class _Tp, bool _Support = false>\nstruct underlying_type\n{\n    static_assert(_Support, \"The underyling_type trait requires compiler \"\n                            \"support. Either no such support exists or \"\n                            \"libc++ does not know how to use it.\");\n};\n\n#endif // defined(_LIBCUDACXX_UNDERLYING_TYPE) && !defined(_LIBCUDACXX_USE_UNDERLYING_TYPE_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_UNDERLYING_TYPE_H\n", "../__type_traits/void_t.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_VOID_T_H\n#define _LIBCUDACXX___TYPE_TRAITS_VOID_T_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class...> using void_t = void;\n#endif\n\ntemplate <class...>\nusing __void_t = void;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_VOID_T_H\n", "../__utility/declval.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___UTILITY_DECLVAL_H\n#define _LIBCUDACXX___UTILITY_DECLVAL_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n// Suppress deprecation notice for volatile-qualified return type resulting\n// from volatile-qualified types _Tp.\n_LIBCUDACXX_SUPPRESS_DEPRECATED_PUSH\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY _Tp&& __declval(int);\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY _Tp __declval(long);\n_LIBCUDACXX_SUPPRESS_DEPRECATED_POP\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY decltype(_CUDA_VSTD::__declval<_Tp>(0)) declval() noexcept;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___UTILITY_DECLVAL_H\n", "../__utility/exchange.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___UTILITY_EXCHANGE_H\n#define _LIBCUDACXX___UTILITY_EXCHANGE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/is_nothrow_assignable.h\"\n#include \"../__type_traits/is_nothrow_move_constructible.h\"\n#include \"../__utility/forward.h\"\n#include \"../__utility/move.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate<class _T1, class _T2 = _T1>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n_T1 exchange(_T1& __obj, _T2&& __new_value)\n    noexcept(is_nothrow_move_constructible<_T1>::value && is_nothrow_assignable<_T1&, _T2>::value)\n{\n    _T1 __old_value = _CUDA_VSTD::move(__obj);\n    __obj = _CUDA_VSTD::forward<_T2>(__new_value);\n    return __old_value;\n}\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___UTILITY_EXCHANGE_H\n", "../__utility/forward.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___UTILITY_FORWARD_H\n#define _LIBCUDACXX___UTILITY_FORWARD_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/is_reference.h\"\n#include \"../__type_traits/remove_reference.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp>\n_LIBCUDACXX_NODISCARD_EXT inline _LIBCUDACXX_INLINE_VISIBILITY constexpr _Tp&&\nforward(__libcpp_remove_reference_t<_Tp>& __t) noexcept {\n  return static_cast<_Tp&&>(__t);\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_NODISCARD_EXT inline _LIBCUDACXX_INLINE_VISIBILITY constexpr _Tp&&\nforward(__libcpp_remove_reference_t<_Tp>&& __t) noexcept {\n  static_assert(!is_lvalue_reference<_Tp>::value, \"cannot forward an rvalue as an lvalue\");\n  return static_cast<_Tp&&>(__t);\n}\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___UTILITY_FORWARD_H\n", "../__utility/integer_sequence.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___UTILITY_INTEGER_SEQUENCE_H\n#define _LIBCUDACXX___UTILITY_INTEGER_SEQUENCE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/is_integral.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <size_t...> struct __tuple_indices {};\n\ntemplate <class _IdxType, _IdxType... _Values>\nstruct __integer_sequence {\n  template <template <class _OIdxType, _OIdxType...> class _ToIndexSeq, class _ToIndexType>\n  using __convert = _ToIndexSeq<_ToIndexType, _Values...>;\n\n  template <size_t _Sp>\n  using __to_tuple_indices = __tuple_indices<(_Values + _Sp)...>;\n};\n\n#if !__has_builtin(__make_integer_seq) || defined(_LIBCUDACXX_TESTING_FALLBACK_MAKE_INTEGER_SEQUENCE)\n\nnamespace __detail {\n\ntemplate<typename _Tp, size_t ..._Extra> struct __repeat;\ntemplate<typename _Tp, _Tp ..._Np, size_t ..._Extra> struct __repeat<__integer_sequence<_Tp, _Np...>, _Extra...> {\n  typedef _LIBCUDACXX_NODEBUG_TYPE __integer_sequence<_Tp,\n                           _Np...,\n                           sizeof...(_Np) + _Np...,\n                           2 * sizeof...(_Np) + _Np...,\n                           3 * sizeof...(_Np) + _Np...,\n                           4 * sizeof...(_Np) + _Np...,\n                           5 * sizeof...(_Np) + _Np...,\n                           6 * sizeof...(_Np) + _Np...,\n                           7 * sizeof...(_Np) + _Np...,\n                           _Extra...> type;\n};\n\ntemplate<size_t _Np> struct __parity;\ntemplate<size_t _Np> struct __make : __parity<_Np % 8>::template __pmake<_Np> {};\n\ntemplate<> struct __make<0> { typedef __integer_sequence<size_t> type; };\ntemplate<> struct __make<1> { typedef __integer_sequence<size_t, 0> type; };\ntemplate<> struct __make<2> { typedef __integer_sequence<size_t, 0, 1> type; };\ntemplate<> struct __make<3> { typedef __integer_sequence<size_t, 0, 1, 2> type; };\ntemplate<> struct __make<4> { typedef __integer_sequence<size_t, 0, 1, 2, 3> type; };\ntemplate<> struct __make<5> { typedef __integer_sequence<size_t, 0, 1, 2, 3, 4> type; };\ntemplate<> struct __make<6> { typedef __integer_sequence<size_t, 0, 1, 2, 3, 4, 5> type; };\ntemplate<> struct __make<7> { typedef __integer_sequence<size_t, 0, 1, 2, 3, 4, 5, 6> type; };\n\ntemplate<> struct __parity<0> { template<size_t _Np> struct __pmake : __repeat<typename __make<_Np / 8>::type> {}; };\ntemplate<> struct __parity<1> { template<size_t _Np> struct __pmake : __repeat<typename __make<_Np / 8>::type, _Np - 1> {}; };\ntemplate<> struct __parity<2> { template<size_t _Np> struct __pmake : __repeat<typename __make<_Np / 8>::type, _Np - 2, _Np - 1> {}; };\ntemplate<> struct __parity<3> { template<size_t _Np> struct __pmake : __repeat<typename __make<_Np / 8>::type, _Np - 3, _Np - 2, _Np - 1> {}; };\ntemplate<> struct __parity<4> { template<size_t _Np> struct __pmake : __repeat<typename __make<_Np / 8>::type, _Np - 4, _Np - 3, _Np - 2, _Np - 1> {}; };\ntemplate<> struct __parity<5> { template<size_t _Np> struct __pmake : __repeat<typename __make<_Np / 8>::type, _Np - 5, _Np - 4, _Np - 3, _Np - 2, _Np - 1> {}; };\ntemplate<> struct __parity<6> { template<size_t _Np> struct __pmake : __repeat<typename __make<_Np / 8>::type, _Np - 6, _Np - 5, _Np - 4, _Np - 3, _Np - 2, _Np - 1> {}; };\ntemplate<> struct __parity<7> { template<size_t _Np> struct __pmake : __repeat<typename __make<_Np / 8>::type, _Np - 7, _Np - 6, _Np - 5, _Np - 4, _Np - 3, _Np - 2, _Np - 1> {}; };\n\n} // namespace detail\n\n#endif  // !__has_builtin(__make_integer_seq) || defined(_LIBCUDACXX_TESTING_FALLBACK_MAKE_INTEGER_SEQUENCE)\n\n#if __has_builtin(__make_integer_seq)\ntemplate <size_t _Ep, size_t _Sp>\nusing __make_indices_imp =\n    typename __make_integer_seq<__integer_sequence, size_t, _Ep - _Sp>::template\n    __to_tuple_indices<_Sp>;\n#else\ntemplate <size_t _Ep, size_t _Sp>\nusing __make_indices_imp =\n    typename __detail::__make<_Ep - _Sp>::type::template __to_tuple_indices<_Sp>;\n\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\n\ntemplate<class _Tp, _Tp... _Ip>\nstruct _LIBCUDACXX_TEMPLATE_VIS integer_sequence\n{\n    typedef _Tp value_type;\n    static_assert( is_integral<_Tp>::value,\n                  \"std::integer_sequence can only be instantiated with an integral type\" );\n    static\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr\n    size_t\n    size() noexcept { return sizeof...(_Ip); }\n};\n\ntemplate<size_t... _Ip>\n    using index_sequence = integer_sequence<size_t, _Ip...>;\n\n#if __has_builtin(__make_integer_seq) && !defined(_LIBCUDACXX_TESTING_FALLBACK_MAKE_INTEGER_SEQUENCE)\n\ntemplate <class _Tp, _Tp _Ep>\nusing __make_integer_sequence _LIBCUDACXX_NODEBUG_TYPE = __make_integer_seq<integer_sequence, _Tp, _Ep>;\n\n#else\n\ntemplate<typename _Tp, _Tp _Np> using __make_integer_sequence_unchecked _LIBCUDACXX_NODEBUG_TYPE =\n  typename __detail::__make<_Np>::type::template __convert<integer_sequence, _Tp>;\n\ntemplate <class _Tp, _Tp _Ep>\nstruct __make_integer_sequence_checked\n{\n    static_assert(is_integral<_Tp>::value,\n                  \"std::make_integer_sequence can only be instantiated with an integral type\" );\n    static_assert(0 <= _Ep, \"std::make_integer_sequence must have a non-negative sequence length\");\n    // Workaround GCC bug by preventing bad installations when 0 <= _Ep\n    // https://gcc.gnu.org/bugzilla/show_bug.cgi?id=68929\n    typedef _LIBCUDACXX_NODEBUG_TYPE __make_integer_sequence_unchecked<_Tp, 0 <= _Ep ? _Ep : 0> type;\n};\n\ntemplate <class _Tp, _Tp _Ep>\nusing __make_integer_sequence _LIBCUDACXX_NODEBUG_TYPE = typename __make_integer_sequence_checked<_Tp, _Ep>::type;\n\n#endif\n\ntemplate<class _Tp, _Tp _Np>\n    using make_integer_sequence = __make_integer_sequence<_Tp, _Np>;\n\ntemplate<size_t _Np>\n    using make_index_sequence = make_integer_sequence<size_t, _Np>;\n\ntemplate<class... _Tp>\n    using index_sequence_for = make_index_sequence<sizeof...(_Tp)>;\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___UTILITY_INTEGER_SEQUENCE_H\n", "../__utility/move.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___UTILITY_MOVE_H\n#define _LIBCUDACXX___UTILITY_MOVE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/conditional.h\"\n#include \"../__type_traits/is_copy_constructible.h\"\n#include \"../__type_traits/is_nothrow_move_constructible.h\"\n#include \"../__type_traits/remove_reference.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp>\n_LIBCUDACXX_NODISCARD_EXT inline _LIBCUDACXX_INLINE_VISIBILITY constexpr __libcpp_remove_reference_t<_Tp>&&\nmove(_Tp&& __t) noexcept {\n  typedef _LIBCUDACXX_NODEBUG_TYPE __libcpp_remove_reference_t<_Tp> _Up;\n  return static_cast<_Up&&>(__t);\n}\n\ntemplate <class _Tp>\nusing __move_if_noexcept_result_t =\n    __conditional_t<!is_nothrow_move_constructible<_Tp>::value && is_copy_constructible<_Tp>::value, const _Tp&, _Tp&&>;\n\ntemplate <class _Tp>\n_LIBCUDACXX_NODISCARD_EXT inline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 __move_if_noexcept_result_t<_Tp>\nmove_if_noexcept(_Tp& __x) noexcept {\n  return _CUDA_VSTD::move(__x);\n}\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___UTILITY_MOVE_H\n", "../__utility/pair.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___UTILITY_PAIR_H\n#define _LIBCUDACXX___UTILITY_PAIR_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#ifndef _LIBCUDACXX_HAS_NO_SPACESHIP_OPERATOR\n#include \"../__compare/common_comparison_category.h\"\n#include \"../__compare/synth_three_way.h\"\n#endif // _LIBCUDACXX_HAS_NO_SPACESHIP_OPERATOR\n\n#include \"../__functional/unwrap_ref.h\"\n#include \"../__fwd/get.h\"\n#include \"../__fwd/tuple.h\"\n#include \"../__tuple_dir/sfinae_helpers.h\"\n#include \"../__tuple_dir/structured_bindings.h\"\n#include \"../__tuple_dir/tuple_element.h\"\n#include \"../__tuple_dir/tuple_indices.h\"\n#include \"../__tuple_dir/tuple_size.h\"\n#include \"../__type_traits/common_reference.h\"\n#include \"../__type_traits/conditional.h\"\n#include \"../__type_traits/decay.h\"\n#include \"../__type_traits/enable_if.h\"\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_assignable.h\"\n#include \"../__type_traits/is_constructible.h\"\n#include \"../__type_traits/is_convertible.h\"\n#include \"../__type_traits/is_copy_assignable.h\"\n#include \"../__type_traits/is_default_constructible.h\"\n#include \"../__type_traits/is_implicitly_default_constructible.h\"\n#include \"../__type_traits/is_move_assignable.h\"\n#include \"../__type_traits/is_nothrow_assignable.h\"\n#include \"../__type_traits/is_nothrow_constructible.h\"\n#include \"../__type_traits/is_nothrow_copy_assignable.h\"\n#include \"../__type_traits/is_nothrow_copy_constructible.h\"\n#include \"../__type_traits/is_nothrow_default_constructible.h\"\n#include \"../__type_traits/is_nothrow_move_assignable.h\"\n#include \"../__type_traits/is_nothrow_move_constructible.h\"\n#include \"../__type_traits/is_same.h\"\n#include \"../__type_traits/is_swappable.h\"\n#include \"../__type_traits/make_const_lvalue_ref.h\"\n#include \"../__utility/forward.h\"\n#include \"../__utility/move.h\"\n#include \"../__utility/piecewise_construct.h\"\n#include \"../cstddef\"\n\n// Provide compatability between `std::pair` and `cuda::std::pair`\n#if defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n#include <utility>\n#endif // defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_DEPRECATED_ABI_DISABLE_PAIR_TRIVIAL_COPY_CTOR)\ntemplate <class, class>\nstruct __non_trivially_copyable_base {\n  _LIBCUDACXX_INLINE_VISIBILITY constexpr\n  __non_trivially_copyable_base() noexcept {}\n  _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n  __non_trivially_copyable_base(__non_trivially_copyable_base const&) noexcept {}\n};\n#endif\n\ntemplate <class _T1, class _T2>\nstruct _LIBCUDACXX_TEMPLATE_VIS pair\n#if defined(_LIBCUDACXX_DEPRECATED_ABI_DISABLE_PAIR_TRIVIAL_COPY_CTOR)\n: private __non_trivially_copyable_base<_T1, _T2>\n#endif\n{\n    typedef _T1 first_type;\n    typedef _T2 second_type;\n\n    _T1 first;\n    _T2 second;\n\n    pair(pair const&) = default;\n    pair(pair&&) = default;\n\n    struct _CheckArgs {\n      struct __enable_implicit_default : public integral_constant<bool,\n                 __is_implicitly_default_constructible<_T1>::value\n              && __is_implicitly_default_constructible<_T2>::value>\n      {};\n\n      struct __enable_explicit_default : public integral_constant<bool,\n                 is_default_constructible<_T1>::value\n              && is_default_constructible<_T2>::value\n              && !__enable_implicit_default::value>\n      {};\n\n      template <class _U1, class _U2>\n      struct __is_pair_constructible : public integral_constant<bool,\n                 is_constructible<first_type, _U1>::value\n              && is_constructible<second_type, _U2>::value>\n      {};\n\n      template <class _U1, class _U2>\n      struct __is_implicit : public integral_constant<bool,\n                 is_convertible<_U1, first_type>::value\n              && is_convertible<_U2, second_type>::value>\n      {};\n\n      template <class _U1, class _U2>\n      struct __enable_explicit : public integral_constant<bool,\n                 __is_pair_constructible<_U1, _U2>::value\n             && !__is_implicit<_U1, _U2>::value>\n      {};\n\n      template <class _U1, class _U2>\n      struct __enable_implicit : public integral_constant<bool,\n                 __is_pair_constructible<_U1, _U2>::value\n             &&  __is_implicit<_U1, _U2>::value>\n      {};\n    };\n\n    template <bool _MaybeEnable>\n    using _CheckArgsDep _LIBCUDACXX_NODEBUG_TYPE = __conditional_t<\n      _MaybeEnable, _CheckArgs, __check_tuple_constructor_fail>;\n\n    struct _CheckTupleLikeConstructor {\n        template <class _Tuple>\n        struct __enable_implicit : public integral_constant<bool,\n                    __tuple_convertible<_Tuple, pair>::value>\n        {};\n        template <class _Tuple>\n        struct __enable_explicit : public integral_constant<bool,\n                    __tuple_constructible<_Tuple, pair>::value\n                && !__tuple_convertible<_Tuple, pair>::value>\n        {};\n        template <class _Tuple>\n        struct __enable_assign : public integral_constant<bool,\n                    __tuple_assignable<_Tuple, pair>::value>\n        {};\n    };\n\n    template <class _Tuple>\n    using _CheckTLC _LIBCUDACXX_NODEBUG_TYPE = __conditional_t<\n        __tuple_like_with_size<_Tuple, 2>::value\n            && !is_same<__decay_t<_Tuple>, pair>::value,\n        _CheckTupleLikeConstructor,\n        __check_tuple_constructor_fail\n    >;\n\n    template<bool _Dummy = true, __enable_if_t<\n            _CheckArgsDep<_Dummy>::__enable_explicit_default::value\n    >* = nullptr>\n    explicit _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    pair() noexcept(is_nothrow_default_constructible<first_type>::value &&\n                      is_nothrow_default_constructible<second_type>::value)\n        : first(), second() {}\n\n    template<bool _Dummy = true, __enable_if_t<\n            _CheckArgsDep<_Dummy>::__enable_implicit_default::value\n    >* = nullptr>\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    pair() noexcept(is_nothrow_default_constructible<first_type>::value &&\n                      is_nothrow_default_constructible<second_type>::value)\n        : first(), second() {}\n\n    template <bool _Dummy = true, __enable_if_t<\n             _CheckArgsDep<_Dummy>::template __enable_explicit<__make_const_lvalue_ref<_T1>, __make_const_lvalue_ref<_T2>>::value\n    >* = nullptr>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    explicit pair(_T1 const& __t1, _T2 const& __t2)\n        noexcept(is_nothrow_copy_constructible<first_type>::value &&\n                   is_nothrow_copy_constructible<second_type>::value)\n        : first(__t1), second(__t2) {}\n\n    template<bool _Dummy = true, __enable_if_t<\n            _CheckArgsDep<_Dummy>::template __enable_implicit<__make_const_lvalue_ref<_T1>, __make_const_lvalue_ref<_T2>>::value\n    >* = nullptr>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    pair(_T1 const& __t1, _T2 const& __t2)\n        noexcept(is_nothrow_copy_constructible<first_type>::value &&\n                   is_nothrow_copy_constructible<second_type>::value)\n        : first(__t1), second(__t2) {}\n\n    template <\n#if _LIBCUDACXX_STD_VER > 20 // http://wg21.link/P1951\n        class _U1 = _T1, class _U2 = _T2,\n#else\n        class _U1, class _U2,\n#endif\n        __enable_if_t<_CheckArgs::template __enable_explicit<_U1, _U2>::value>* = nullptr\n    >\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    explicit pair(_U1&& __u1, _U2&& __u2)\n        noexcept((is_nothrow_constructible<first_type, _U1>::value &&\n                    is_nothrow_constructible<second_type, _U2>::value))\n        : first(_CUDA_VSTD::forward<_U1>(__u1)), second(_CUDA_VSTD::forward<_U2>(__u2)) {}\n\n    template <\n#if _LIBCUDACXX_STD_VER > 20 // http://wg21.link/P1951\n        class _U1 = _T1, class _U2 = _T2,\n#else\n        class _U1, class _U2,\n#endif\n        __enable_if_t<_CheckArgs::template __enable_implicit<_U1, _U2>::value>* = nullptr\n    >\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    pair(_U1&& __u1, _U2&& __u2)\n        noexcept((is_nothrow_constructible<first_type, _U1>::value &&\n                    is_nothrow_constructible<second_type, _U2>::value))\n        : first(_CUDA_VSTD::forward<_U1>(__u1)), second(_CUDA_VSTD::forward<_U2>(__u2)) {}\n\n#if _LIBCUDACXX_STD_VER > 20\n    template<class _U1, class _U2, __enable_if_t<\n            _CheckArgs::template __is_pair_constructible<_U1&, _U2&>::value\n    >* = nullptr>\n    _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    explicit(!_CheckArgs::template __is_implicit<_U1&, _U2&>()) pair(pair<_U1, _U2>& __p)\n        noexcept((is_nothrow_constructible<first_type, _U1&>::value &&\n                  is_nothrow_constructible<second_type, _U2&>::value))\n        : first(__p.first), second(__p.second) {}\n\n#if defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n    template<class _U1, class _U2, __enable_if_t<\n            _CheckArgs::template __is_pair_constructible<_U1&, _U2&>::value\n    >* = nullptr>\n    _LIBCUDACXX_HOST _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    explicit(!_CheckArgs::template __is_implicit<_U1&, _U2&>()) pair(::std::pair<_U1, _U2>& __p)\n        noexcept((is_nothrow_constructible<first_type, _U1&>::value &&\n                  is_nothrow_constructible<second_type, _U2&>::value))\n        : first(__p.first), second(__p.second) {}\n#endif // defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n#endif // _LIBCUDACXX_STD_VER > 20\n\n    template<class _U1, class _U2, __enable_if_t<\n            _CheckArgs::template __enable_explicit<_U1 const&, _U2 const&>::value\n    >* = nullptr>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    explicit pair(pair<_U1, _U2> const& __p)\n        noexcept((is_nothrow_constructible<first_type, _U1 const&>::value &&\n                    is_nothrow_constructible<second_type, _U2 const&>::value))\n        : first(__p.first), second(__p.second) {}\n\n#if defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n    template<class _U1, class _U2, __enable_if_t<\n            _CheckArgs::template __enable_explicit<_U1 const&, _U2 const&>::value\n    >* = nullptr>\n    _LIBCUDACXX_HOST _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    explicit pair(::std::pair<_U1, _U2> const& __p)\n        noexcept((is_nothrow_constructible<first_type, _U1 const&>::value &&\n                    is_nothrow_constructible<second_type, _U2 const&>::value))\n        : first(__p.first), second(__p.second) {}\n#endif // defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n\n    template<class _U1, class _U2, __enable_if_t<\n            _CheckArgs::template __enable_implicit<_U1 const&, _U2 const&>::value\n    >* = nullptr>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    pair(pair<_U1, _U2> const& __p)\n        noexcept((is_nothrow_constructible<first_type, _U1 const&>::value &&\n                    is_nothrow_constructible<second_type, _U2 const&>::value))\n        : first(__p.first), second(__p.second) {}\n\n#if defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n    template<class _U1, class _U2, __enable_if_t<\n            _CheckArgs::template __enable_implicit<_U1 const&, _U2 const&>::value\n    >* = nullptr>\n    _LIBCUDACXX_HOST _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    pair(::std::pair<_U1, _U2> const& __p)\n        noexcept((is_nothrow_constructible<first_type, _U1 const&>::value &&\n                    is_nothrow_constructible<second_type, _U2 const&>::value))\n        : first(__p.first), second(__p.second) {}\n#endif // defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n\n    template<class _U1, class _U2, __enable_if_t<\n            _CheckArgs::template __enable_explicit<_U1, _U2>::value\n    >* = nullptr>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    explicit pair(pair<_U1, _U2>&&__p)\n        noexcept((is_nothrow_constructible<first_type, _U1&&>::value &&\n                    is_nothrow_constructible<second_type, _U2&&>::value))\n        : first(_CUDA_VSTD::forward<_U1>(__p.first)), second(_CUDA_VSTD::forward<_U2>(__p.second)) {}\n\n#if defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n    template<class _U1, class _U2, __enable_if_t<\n            _CheckArgs::template __enable_explicit<_U1, _U2>::value\n    >* = nullptr>\n    _LIBCUDACXX_HOST _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    explicit pair(::std::pair<_U1, _U2>&&__p)\n        noexcept((is_nothrow_constructible<first_type, _U1&&>::value &&\n                    is_nothrow_constructible<second_type, _U2&&>::value))\n        : first(_CUDA_VSTD::forward<_U1>(__p.first)), second(_CUDA_VSTD::forward<_U2>(__p.second)) {}\n#endif // defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n\n    template<class _U1, class _U2, __enable_if_t<\n            _CheckArgs::template __enable_implicit<_U1, _U2>::value\n    >* = nullptr>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    pair(pair<_U1, _U2>&& __p)\n        noexcept((is_nothrow_constructible<first_type, _U1&&>::value &&\n                    is_nothrow_constructible<second_type, _U2&&>::value))\n        : first(_CUDA_VSTD::forward<_U1>(__p.first)), second(_CUDA_VSTD::forward<_U2>(__p.second)) {}\n\n#if defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n    template<class _U1, class _U2, __enable_if_t<\n            _CheckArgs::template __enable_implicit<_U1, _U2>::value\n    >* = nullptr>\n    _LIBCUDACXX_HOST _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    pair(::std::pair<_U1, _U2>&& __p)\n        noexcept((is_nothrow_constructible<first_type, _U1&&>::value &&\n                    is_nothrow_constructible<second_type, _U2&&>::value))\n        : first(_CUDA_VSTD::forward<_U1>(__p.first)), second(_CUDA_VSTD::forward<_U2>(__p.second)) {}\n#endif // defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n\n#if _LIBCUDACXX_STD_VER > 20\n    template<class _U1, class _U2, __enable_if_t<\n            _CheckArgs::template __is_pair_constructible<const _U1&&, const _U2&&>::value\n    >* = nullptr>\n    _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    explicit(!_CheckArgs::template __is_implicit<const _U1&&, const _U2&&>::value)\n    pair(const pair<_U1, _U2>&& __p)\n        noexcept(is_nothrow_constructible<first_type, const _U1&&>::value &&\n                 is_nothrow_constructible<second_type, const _U2&&>::value)\n        : first(_CUDA_VSTD::move(__p.first)), second(_CUDA_VSTD::move(__p.second)) {}\n\n#if defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n    template<class _U1, class _U2, __enable_if_t<\n            _CheckArgs::template __is_pair_constructible<const _U1&&, const _U2&&>::value\n    >* = nullptr>\n    _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_HOST constexpr\n    explicit(!_CheckArgs::template __is_implicit<const _U1&&, const _U2&&>::value)\n    pair(const ::std::pair<_U1, _U2>&& __p)\n        noexcept(is_nothrow_constructible<first_type, const _U1&&>::value &&\n                 is_nothrow_constructible<second_type, const _U2&&>::value)\n        : first(_CUDA_VSTD::move(__p.first)), second(_CUDA_VSTD::move(__p.second)) {}\n#endif // defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n#endif // _LIBCUDACXX_STD_VER > 20\n\n    template<class _Tuple, __enable_if_t<\n            _CheckTLC<_Tuple>::template __enable_explicit<_Tuple>::value\n    >* = nullptr>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    explicit pair(_Tuple&& __p)\n        : first(_CUDA_VSTD::get<0>(_CUDA_VSTD::forward<_Tuple>(__p))),\n          second(_CUDA_VSTD::get<1>(_CUDA_VSTD::forward<_Tuple>(__p))) {}\n\n    template<class _Tuple, __enable_if_t<\n            _CheckTLC<_Tuple>::template __enable_implicit<_Tuple>::value\n    >* = nullptr>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    pair(_Tuple&& __p)\n        : first(_CUDA_VSTD::get<0>(_CUDA_VSTD::forward<_Tuple>(__p))),\n          second(_CUDA_VSTD::get<1>(_CUDA_VSTD::forward<_Tuple>(__p))) {}\n\n    template <class... _Args1, class... _Args2>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n    pair(piecewise_construct_t __pc,\n         tuple<_Args1...> __first_args, tuple<_Args2...> __second_args)\n        noexcept((is_nothrow_constructible<first_type, _Args1...>::value &&\n                    is_nothrow_constructible<second_type, _Args2...>::value))\n        : pair(__pc, __first_args, __second_args,\n                typename __make_tuple_indices<sizeof...(_Args1)>::type(),\n                typename __make_tuple_indices<sizeof...(_Args2) >::type()) {}\n\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n    pair& operator=(__conditional_t<\n                        is_copy_assignable<first_type>::value &&\n                        is_copy_assignable<second_type>::value,\n                    pair, __nat> const& __p)\n        noexcept(is_nothrow_copy_assignable<first_type>::value &&\n                   is_nothrow_copy_assignable<second_type>::value)\n    {\n        first = __p.first;\n        second = __p.second;\n        return *this;\n    }\n\n#if defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n    template<class _UT1 = _T1, __enable_if_t<is_copy_assignable<_UT1>::value &&\n                                             is_copy_assignable<_T2>::value, int> = 0>\n    _LIBCUDACXX_HOST _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n    pair& operator=(::std::pair<_T1, _T2> const& __p)\n        noexcept(is_nothrow_copy_assignable<first_type>::value &&\n                   is_nothrow_copy_assignable<second_type>::value)\n    {\n        first = __p.first;\n        second = __p.second;\n        return *this;\n    }\n#endif // defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n    pair& operator=(__conditional_t<\n                        is_move_assignable<first_type>::value &&\n                        is_move_assignable<second_type>::value,\n                    pair, __nat>&& __p)\n        noexcept(is_nothrow_move_assignable<first_type>::value &&\n                   is_nothrow_move_assignable<second_type>::value)\n    {\n        first = _CUDA_VSTD::forward<first_type>(__p.first);\n        second = _CUDA_VSTD::forward<second_type>(__p.second);\n        return *this;\n    }\n\n#if defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n    template<class _UT1 = _T1, __enable_if_t<is_move_assignable<_UT1>::value &&\n                                             is_move_assignable<_T2>::value, int> = 0>\n    _LIBCUDACXX_HOST _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n    pair& operator=(::std::pair<_T1, _T2>&& __p)\n        noexcept(is_nothrow_move_assignable<first_type>::value &&\n                   is_nothrow_move_assignable<second_type>::value)\n    {\n        first = _CUDA_VSTD::forward<first_type>(__p.first);\n        second = _CUDA_VSTD::forward<second_type>(__p.second);\n        return *this;\n    }\n#endif // defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n\n#if _LIBCUDACXX_STD_VER > 20\n    _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    const pair& operator=(pair const& __p) const\n      noexcept(is_nothrow_copy_assignable_v<const first_type> &&\n               is_nothrow_copy_assignable_v<const second_type>)\n      requires(is_copy_assignable_v<const first_type> &&\n               is_copy_assignable_v<const second_type>) {\n        first = __p.first;\n        second = __p.second;\n        return *this;\n    }\n\n#if defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n    _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_HOST constexpr\n    const pair& operator=(::std::pair<_T1, _T2> const& __p) const\n      noexcept(is_nothrow_copy_assignable_v<const first_type> &&\n               is_nothrow_copy_assignable_v<const second_type>)\n      requires(is_copy_assignable_v<const first_type> &&\n               is_copy_assignable_v<const second_type>) {\n        first = __p.first;\n        second = __p.second;\n        return *this;\n    }\n#endif // defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n\n    _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    const pair& operator=(pair&& __p) const\n      noexcept(is_nothrow_assignable_v<const first_type&, first_type> &&\n               is_nothrow_assignable_v<const second_type&, second_type>)\n      requires(is_assignable_v<const first_type&, first_type> &&\n               is_assignable_v<const second_type&, second_type>) {\n        first = _CUDA_VSTD::forward<first_type>(__p.first);\n        second = _CUDA_VSTD::forward<second_type>(__p.second);\n        return *this;\n    }\n\n#if defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n    _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_HOST constexpr\n    const pair& operator=(::std::pair<_T1, _T2>&& __p) const\n      noexcept(is_nothrow_assignable_v<const first_type&, first_type> &&\n               is_nothrow_assignable_v<const second_type&, second_type>)\n      requires(is_assignable_v<const first_type&, first_type> &&\n               is_assignable_v<const second_type&, second_type>) {\n        first = _CUDA_VSTD::forward<first_type>(__p.first);\n        second = _CUDA_VSTD::forward<second_type>(__p.second);\n        return *this;\n    }\n#endif // defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n\n    template<class _U1, class _U2>\n    _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    const pair& operator=(const pair<_U1, _U2>& __p) const\n      requires(is_assignable_v<const first_type&, const _U1&> &&\n               is_assignable_v<const second_type&, const _U2&>) {\n        first = __p.first;\n        second = __p.second;\n        return *this;\n    }\n\n#if defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n    template<class _U1, class _U2>\n    _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_HOST constexpr\n    const pair& operator=(const ::std::pair<_U1, _U2>& __p) const\n      requires(is_assignable_v<const first_type&, const _U1&> &&\n               is_assignable_v<const second_type&, const _U2&>) {\n        first = __p.first;\n        second = __p.second;\n        return *this;\n    }\n#endif // defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n\n    template<class _U1, class _U2>\n    _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    const pair& operator=(pair<_U1, _U2>&& __p) const\n      requires(is_assignable_v<const first_type&, _U1> &&\n               is_assignable_v<const second_type&, _U2>) {\n        first = _CUDA_VSTD::forward<_U1>(__p.first);\n        second = _CUDA_VSTD::forward<_U2>(__p.second);\n        return *this;\n    }\n\n#if defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n    template<class _U1, class _U2>\n    _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_HOST constexpr\n    const pair& operator=(::std::pair<_U1, _U2>&& __p) const\n      requires(is_assignable_v<const first_type&, _U1> &&\n               is_assignable_v<const second_type&, _U2>) {\n        first = _CUDA_VSTD::forward<_U1>(__p.first);\n        second = _CUDA_VSTD::forward<_U2>(__p.second);\n        return *this;\n    }\n#endif // defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n#endif // _LIBCUDACXX_STD_VER > 20\n\n    template <class _Tuple, __enable_if_t<\n            _CheckTLC<_Tuple>::template __enable_assign<_Tuple>::value\n     >* = nullptr>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n    pair& operator=(_Tuple&& __p) {\n        first = _CUDA_VSTD::get<0>(_CUDA_VSTD::forward<_Tuple>(__p));\n        second = _CUDA_VSTD::get<1>(_CUDA_VSTD::forward<_Tuple>(__p));\n        return *this;\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n    void swap(pair& __p) noexcept(__is_nothrow_swappable<first_type>::value &&\n                                    __is_nothrow_swappable<second_type>::value)\n    {\n        using _CUDA_VSTD::swap;\n        swap(first,  __p.first);\n        swap(second, __p.second);\n    }\n\n#if _LIBCUDACXX_STD_VER > 20\n    _LIBCUDACXX_HIDE_FROM_ABI constexpr\n    void swap(const pair& __p) const\n        noexcept(__is_nothrow_swappable<const first_type>::value &&\n                 __is_nothrow_swappable<const second_type>::value)\n    {\n        using _CUDA_VSTD::swap;\n        swap(first,  __p.first);\n        swap(second, __p.second);\n    }\n#endif // _LIBCUDACXX_STD_VER > 20\n\n#if defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n    _LIBCUDACXX_HOST _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    operator ::std::pair<_T1, _T2>() const { return { first, second }; }\n#endif // defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n\nprivate:\n\n    template <class... _Args1, class... _Args2, size_t... _I1, size_t... _I2>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n    pair(piecewise_construct_t,\n         tuple<_Args1...>& __first_args, tuple<_Args2...>& __second_args,\n         __tuple_indices<_I1...>, __tuple_indices<_I2...>);\n};\n\n#if _LIBCUDACXX_STD_VER > 14 && !defined(_LIBCUDACXX_HAS_NO_DEDUCTION_GUIDES)\ntemplate<class _T1, class _T2>\npair(_T1, _T2) -> pair<_T1, _T2>;\n#endif // _LIBCUDACXX_STD_VER > 14 && !defined(_LIBCUDACXX_HAS_NO_DEDUCTION_GUIDES)\n\n// [pairs.spec], specialized algorithms\n\ntemplate <class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nbool\noperator==(const pair<_T1,_T2>& __x, const pair<_T1,_T2>& __y)\n{\n    return __x.first == __y.first && __x.second == __y.second;\n}\n\n#ifndef _LIBCUDACXX_HAS_NO_SPACESHIP_OPERATOR\n\ntemplate <class _T1, class _T2>\n_LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY constexpr\ncommon_comparison_category_t<\n        __synth_three_way_result<_T1>,\n        __synth_three_way_result<_T2> >\noperator<=>(const pair<_T1,_T2>& __x, const pair<_T1,_T2>& __y)\n{\n    if (auto __c = _CUDA_VSTD::__synth_three_way(__x.first, __y.first); __c != 0) {\n      return __c;\n    }\n    return _CUDA_VSTD::__synth_three_way(__x.second, __y.second);\n}\n\n#else // _LIBCUDACXX_HAS_NO_SPACESHIP_OPERATOR\n\ntemplate <class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nbool\noperator!=(const pair<_T1,_T2>& __x, const pair<_T1,_T2>& __y)\n{\n    return !(__x == __y);\n}\n\ntemplate <class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nbool\noperator< (const pair<_T1,_T2>& __x, const pair<_T1,_T2>& __y)\n{\n    return __x.first < __y.first || (!(__y.first < __x.first) && __x.second < __y.second);\n}\n\ntemplate <class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nbool\noperator> (const pair<_T1,_T2>& __x, const pair<_T1,_T2>& __y)\n{\n    return __y < __x;\n}\n\ntemplate <class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nbool\noperator>=(const pair<_T1,_T2>& __x, const pair<_T1,_T2>& __y)\n{\n    return !(__x < __y);\n}\n\ntemplate <class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nbool\noperator<=(const pair<_T1,_T2>& __x, const pair<_T1,_T2>& __y)\n{\n    return !(__y < __x);\n}\n\n#endif // _LIBCUDACXX_HAS_NO_SPACESHIP_OPERATOR\n\n#if _LIBCUDACXX_STD_VER > 17\ntemplate <class _T1, class _T2, class _U1, class _U2, template<class> class _TQual, template<class> class _UQual>\n    requires requires { typename pair<common_reference_t<_TQual<_T1>, _UQual<_U1>>,\n                                      common_reference_t<_TQual<_T2>, _UQual<_U2>>>; }\nstruct basic_common_reference<pair<_T1, _T2>, pair<_U1, _U2>, _TQual, _UQual> {\n    using type = pair<common_reference_t<_TQual<_T1>, _UQual<_U1>>,\n                      common_reference_t<_TQual<_T2>, _UQual<_U2>>>;\n};\n\ntemplate <class _T1, class _T2, class _U1, class _U2>\n    requires requires { typename pair<common_type_t<_T1, _U1>, common_type_t<_T2, _U2>>; }\nstruct common_type<pair<_T1, _T2>, pair<_U1, _U2>> {\n    using type = pair<common_type_t<_T1, _U1>, common_type_t<_T2, _U2>>;\n};\n#endif // _LIBCUDACXX_STD_VER > 17\n\ntemplate <class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n__enable_if_t\n<\n    __is_swappable<_T1>::value &&\n    __is_swappable<_T2>::value,\n    void\n>\nswap(pair<_T1, _T2>& __x, pair<_T1, _T2>& __y)\n                     noexcept((__is_nothrow_swappable<_T1>::value &&\n                                 __is_nothrow_swappable<_T2>::value))\n{\n    __x.swap(__y);\n}\n\n#if _LIBCUDACXX_STD_VER > 20\ntemplate <class _T1, class _T2>\n  requires (__is_swappable<const _T1>::value &&\n            __is_swappable<const _T2>::value)\n_LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY constexpr\nvoid swap(const pair<_T1, _T2>& __x, const pair<_T1, _T2>& __y)\n    noexcept(noexcept(__x.swap(__y)))\n{\n    __x.swap(__y);\n}\n#endif // _LIBCUDACXX_STD_VER > 20\n\ntemplate <class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\npair<typename __unwrap_ref_decay<_T1>::type, typename __unwrap_ref_decay<_T2>::type>\nmake_pair(_T1&& __t1, _T2&& __t2)\n{\n    return pair<typename __unwrap_ref_decay<_T1>::type, typename __unwrap_ref_decay<_T2>::type>\n               (_CUDA_VSTD::forward<_T1>(__t1), _CUDA_VSTD::forward<_T2>(__t2));\n}\n\ntemplate <class _T1, class _T2>\n  struct _LIBCUDACXX_TEMPLATE_VIS tuple_size<pair<_T1, _T2> >\n    : public integral_constant<size_t, 2> {};\n\ntemplate <size_t _Ip, class _T1, class _T2>\nstruct _LIBCUDACXX_TEMPLATE_VIS tuple_element<_Ip, pair<_T1, _T2> >\n{\n    static_assert(_Ip < 2, \"Index out of bounds in std::tuple_element<std::pair<T1, T2>>\");\n};\n\ntemplate <class _T1, class _T2>\nstruct _LIBCUDACXX_TEMPLATE_VIS tuple_element<0, pair<_T1, _T2> >\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE _T1 type;\n};\n\ntemplate <class _T1, class _T2>\nstruct _LIBCUDACXX_TEMPLATE_VIS tuple_element<1, pair<_T1, _T2> >\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE _T2 type;\n};\n\ntemplate <size_t _Ip> struct __get_pair;\n\ntemplate <>\nstruct __get_pair<0>\n{\n    template <class _T1, class _T2>\n    static\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    _T1&\n    get(pair<_T1, _T2>& __p) noexcept {return __p.first;}\n\n    template <class _T1, class _T2>\n    static\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    const _T1&\n    get(const pair<_T1, _T2>& __p) noexcept {return __p.first;}\n\n    template <class _T1, class _T2>\n    static\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    _T1&&\n    get(pair<_T1, _T2>&& __p) noexcept {return _CUDA_VSTD::forward<_T1>(__p.first);}\n\n    template <class _T1, class _T2>\n    static\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    const _T1&&\n    get(const pair<_T1, _T2>&& __p) noexcept {return _CUDA_VSTD::forward<const _T1>(__p.first);}\n};\n\ntemplate <>\nstruct __get_pair<1>\n{\n    template <class _T1, class _T2>\n    static\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    _T2&\n    get(pair<_T1, _T2>& __p) noexcept {return __p.second;}\n\n    template <class _T1, class _T2>\n    static\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    const _T2&\n    get(const pair<_T1, _T2>& __p) noexcept {return __p.second;}\n\n    template <class _T1, class _T2>\n    static\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    _T2&&\n    get(pair<_T1, _T2>&& __p) noexcept {return _CUDA_VSTD::forward<_T2>(__p.second);}\n\n    template <class _T1, class _T2>\n    static\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    const _T2&&\n    get(const pair<_T1, _T2>&& __p) noexcept {return _CUDA_VSTD::forward<const _T2>(__p.second);}\n};\n\ntemplate <size_t _Ip, class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\ntypename tuple_element<_Ip, pair<_T1, _T2> >::type&\nget(pair<_T1, _T2>& __p) noexcept\n{\n    return __get_pair<_Ip>::get(__p);\n}\n\ntemplate <size_t _Ip, class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nconst typename tuple_element<_Ip, pair<_T1, _T2> >::type&\nget(const pair<_T1, _T2>& __p) noexcept\n{\n    return __get_pair<_Ip>::get(__p);\n}\n\ntemplate <size_t _Ip, class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\ntypename tuple_element<_Ip, pair<_T1, _T2> >::type&&\nget(pair<_T1, _T2>&& __p) noexcept\n{\n    return __get_pair<_Ip>::get(_CUDA_VSTD::move(__p));\n}\n\ntemplate <size_t _Ip, class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nconst typename tuple_element<_Ip, pair<_T1, _T2> >::type&&\nget(const pair<_T1, _T2>&& __p) noexcept\n{\n    return __get_pair<_Ip>::get(_CUDA_VSTD::move(__p));\n}\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr _T1 & get(pair<_T1, _T2>& __p) noexcept\n{\n    return __get_pair<0>::get(__p);\n}\n\ntemplate <class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr _T1 const & get(pair<_T1, _T2> const& __p) noexcept\n{\n    return __get_pair<0>::get(__p);\n}\n\ntemplate <class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr _T1 && get(pair<_T1, _T2>&& __p) noexcept\n{\n    return __get_pair<0>::get(_CUDA_VSTD::move(__p));\n}\n\ntemplate <class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr _T1 const && get(pair<_T1, _T2> const&& __p) noexcept\n{\n    return __get_pair<0>::get(_CUDA_VSTD::move(__p));\n}\n\ntemplate <class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr _T1 & get(pair<_T2, _T1>& __p) noexcept\n{\n    return __get_pair<1>::get(__p);\n}\n\ntemplate <class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr _T1 const & get(pair<_T2, _T1> const& __p) noexcept\n{\n    return __get_pair<1>::get(__p);\n}\n\ntemplate <class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr _T1 && get(pair<_T2, _T1>&& __p) noexcept\n{\n    return __get_pair<1>::get(_CUDA_VSTD::move(__p));\n}\n\ntemplate <class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr _T1 const && get(pair<_T2, _T1> const&& __p) noexcept\n{\n    return __get_pair<1>::get(_CUDA_VSTD::move(__p));\n}\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___UTILITY_PAIR_H\n", "../__utility/piecewise_construct.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___UTILITY_PIECEWISE_CONSTRUCT_H\n#define _LIBCUDACXX___UTILITY_PIECEWISE_CONSTRUCT_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\nstruct _LIBCUDACXX_TEMPLATE_VIS piecewise_construct_t { explicit piecewise_construct_t() = default; };\n#if defined(_LIBCUDACXX_BUILDING_LIBRARY)\nextern _LIBCUDACXX_EXPORTED_FROM_ABI const piecewise_construct_t piecewise_construct;// = piecewise_construct_t();\n#else\n/* _LIBCUDACXX_INLINE_VAR */ constexpr piecewise_construct_t piecewise_construct = piecewise_construct_t();\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___UTILITY_PIECEWISE_CONSTRUCT_H\n", "../__utility/swap.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___UTILITY_SWAP_H\n#define _LIBCUDACXX___UTILITY_SWAP_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/enable_if.h\"\n#include \"../__type_traits/is_move_assignable.h\"\n#include \"../__type_traits/is_move_constructible.h\"\n#include \"../__type_traits/is_nothrow_move_assignable.h\"\n#include \"../__type_traits/is_nothrow_move_constructible.h\"\n#include \"../__type_traits/is_swappable.h\"\n#include \"../__utility/move.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n__swap_result_t<_Tp>\nswap(_Tp& __x, _Tp& __y) noexcept(_LIBCUDACXX_TRAIT(is_nothrow_move_constructible, _Tp)\n                               && _LIBCUDACXX_TRAIT(is_nothrow_move_assignable, _Tp)) {\n  _Tp __t(_CUDA_VSTD::move(__x));\n  __x = _CUDA_VSTD::move(__y);\n  __y = _CUDA_VSTD::move(__t);\n}\n\ntemplate <class _Tp, size_t _Np>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n__enable_if_t<__is_swappable<_Tp>::value>\nswap(_Tp (&__a)[_Np], _Tp (&__b)[_Np]) noexcept(__is_nothrow_swappable<_Tp>::value) {\n  for (size_t __i = 0; __i != _Np; ++__i) {\n    swap(__a[__i], __b[__i]);\n  }\n}\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___UTILITY_SWAP_H\n", "../block/block_exchange.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_B096835643DEB4A4\n#define _JITIFY_INCLUDE_GUARD_B096835643DEB4A4\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.\n * \n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n * \n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * The cub::BlockExchange class provides [<em>collective</em>](index.html#sec0) methods for rearranging data partitioned across a CUDA thread block.\n */\n\n#include <cub/config.cuh>\n#include <cub/detail/uninitialized_copy.cuh>\n#include <cub/util_ptx.cuh>\n#include <cub/util_type.cuh>\n#include <cub/warp/warp_exchange.cuh>\n\nCUB_NAMESPACE_BEGIN\n\n/**\n * \\brief The BlockExchange class provides [<em>collective</em>](index.html#sec0) methods for rearranging data partitioned across a CUDA thread block. ![](transpose_logo.png)\n * \\ingroup BlockModule\n *\n * \\tparam T                    The data type to be exchanged.\n * \\tparam BLOCK_DIM_X          The thread block length in threads along the X dimension\n * \\tparam ITEMS_PER_THREAD     The number of items partitioned onto each thread.\n * \\tparam WARP_TIME_SLICING    <b>[optional]</b> When \\p true, only use enough shared memory for a single warp's worth of tile data, time-slicing the block-wide exchange over multiple synchronized rounds.  Yields a smaller memory footprint at the expense of decreased parallelism.  (Default: false)\n * \\tparam BLOCK_DIM_Y          <b>[optional]</b> The thread block length in threads along the Y dimension (default: 1)\n * \\tparam BLOCK_DIM_Z          <b>[optional]</b> The thread block length in threads along the Z dimension (default: 1)\n * \\tparam LEGACY_PTX_ARCH      <b>[optional]</b> Unused.\n *\n * \\par Overview\n * - It is commonplace for blocks of threads to rearrange data items between\n *   threads.  For example, the device-accessible memory subsystem prefers access patterns\n *   where data items are \"striped\" across threads (where consecutive threads access consecutive items),\n *   yet most block-wide operations prefer a \"blocked\" partitioning of items across threads\n *   (where consecutive items belong to a single thread).\n * - BlockExchange supports the following types of data exchanges:\n *   - Transposing between [<em>blocked</em>](index.html#sec5sec3) and [<em>striped</em>](index.html#sec5sec3) arrangements\n *   - Transposing between [<em>blocked</em>](index.html#sec5sec3) and [<em>warp-striped</em>](index.html#sec5sec3) arrangements\n *   - Scattering ranked items to a [<em>blocked arrangement</em>](index.html#sec5sec3)\n *   - Scattering ranked items to a [<em>striped arrangement</em>](index.html#sec5sec3)\n * - \\rowmajor\n *\n * \\par A Simple Example\n * \\blockcollective{BlockExchange}\n * \\par\n * The code snippet below illustrates the conversion from a \"blocked\" to a \"striped\" arrangement\n * of 512 integer items partitioned across 128 threads where each thread owns 4 items.\n * \\par\n * \\code\n * #include <cub/cub.cuh>   // or equivalently <cub/block/block_exchange.cuh>\n *\n * __global__ void ExampleKernel(int *d_data, ...)\n * {\n *     // Specialize BlockExchange for a 1D block of 128 threads owning 4 integer items each\n *     typedef cub::BlockExchange<int, 128, 4> BlockExchange;\n *\n *     // Allocate shared memory for BlockExchange\n *     __shared__ typename BlockExchange::TempStorage temp_storage;\n *\n *     // Load a tile of data striped across threads\n *     int thread_data[4];\n *     cub::LoadDirectStriped<128>(threadIdx.x, d_data, thread_data);\n *\n *     // Collectively exchange data into a blocked arrangement across threads\n *     BlockExchange(temp_storage).StripedToBlocked(thread_data);\n *\n * \\endcode\n * \\par\n * Suppose the set of striped input \\p thread_data across the block of threads is\n * <tt>{ [0,128,256,384], [1,129,257,385], ..., [127,255,383,511] }</tt>.\n * The corresponding output \\p thread_data in those threads will be\n * <tt>{ [0,1,2,3], [4,5,6,7], [8,9,10,11], ..., [508,509,510,511] }</tt>.\n *\n * \\par Performance Considerations\n * - Proper device-specific padding ensures zero bank conflicts for most types.\n *\n * \\par Re-using dynamically allocating shared memory\n * The following example under the examples/block folder illustrates usage of\n * dynamically shared memory with BlockReduce and how to re-purpose\n * the same memory region:\n * <a href=\"../../examples/block/example_block_reduce_dyn_smem.cu\">example_block_reduce_dyn_smem.cu</a>\n *\n * This example can be easily adapted to the storage required by BlockExchange.\n */\ntemplate <\n    typename    InputT,\n    int         BLOCK_DIM_X,\n    int         ITEMS_PER_THREAD,\n    bool        WARP_TIME_SLICING   = false,\n    int         BLOCK_DIM_Y         = 1,\n    int         BLOCK_DIM_Z         = 1,\n    int         LEGACY_PTX_ARCH     = 0>\nclass BlockExchange\n{\nprivate:\n\n    /******************************************************************************\n     * Constants\n     ******************************************************************************/\n\n    /// Constants\n    enum\n    {\n        /// The thread block size in threads\n        BLOCK_THREADS               = BLOCK_DIM_X * BLOCK_DIM_Y * BLOCK_DIM_Z,\n\n        LOG_WARP_THREADS            = CUB_LOG_WARP_THREADS(0),\n        WARP_THREADS                = 1 << LOG_WARP_THREADS,\n        WARPS                       = (BLOCK_THREADS + WARP_THREADS - 1) / WARP_THREADS,\n\n        LOG_SMEM_BANKS              = CUB_LOG_SMEM_BANKS(0),\n        SMEM_BANKS                  = 1 << LOG_SMEM_BANKS,\n\n        TILE_ITEMS                  = BLOCK_THREADS * ITEMS_PER_THREAD,\n\n        TIME_SLICES                 = (WARP_TIME_SLICING) ? WARPS : 1,\n\n        TIME_SLICED_THREADS         = (WARP_TIME_SLICING) ? CUB_MIN(BLOCK_THREADS, WARP_THREADS) : BLOCK_THREADS,\n        TIME_SLICED_ITEMS           = TIME_SLICED_THREADS * ITEMS_PER_THREAD,\n\n        WARP_TIME_SLICED_THREADS    = CUB_MIN(BLOCK_THREADS, WARP_THREADS),\n        WARP_TIME_SLICED_ITEMS      = WARP_TIME_SLICED_THREADS * ITEMS_PER_THREAD,\n\n        // Insert padding to avoid bank conflicts during raking when items per thread is a power of two and > 4 (otherwise we can typically use 128b loads)\n        INSERT_PADDING              = (ITEMS_PER_THREAD > 4) && (PowerOfTwo<ITEMS_PER_THREAD>::VALUE),\n        PADDING_ITEMS               = (INSERT_PADDING) ? (TIME_SLICED_ITEMS >> LOG_SMEM_BANKS) : 0,\n    };\n\n    /******************************************************************************\n     * Type definitions\n     ******************************************************************************/\n\n    /// Shared memory storage layout type\n    struct __align__(16) _TempStorage\n    {\n        InputT buff[TIME_SLICED_ITEMS + PADDING_ITEMS];\n    };\n\npublic:\n\n    /// \\smemstorage{BlockExchange}\n    struct TempStorage : Uninitialized<_TempStorage> {};\n\nprivate:\n\n\n    /******************************************************************************\n     * Thread fields\n     ******************************************************************************/\n\n    /// Shared storage reference\n    _TempStorage &temp_storage;\n\n    /// Linear thread-id\n    unsigned int linear_tid;\n    unsigned int lane_id;\n    unsigned int warp_id;\n    unsigned int warp_offset;\n\n\n    /******************************************************************************\n     * Utility methods\n     ******************************************************************************/\n\n    /// Internal storage allocator\n    __device__ __forceinline__ _TempStorage& PrivateStorage()\n    {\n        __shared__ _TempStorage private_storage;\n        return private_storage;\n    }\n\n\n    /**\n     * Transposes data items from <em>blocked</em> arrangement to <em>striped</em> arrangement.  Specialized for no timeslicing.\n     */\n    template <typename OutputT>\n    __device__ __forceinline__ void BlockedToStriped(\n        InputT          (&input_items)[ITEMS_PER_THREAD],      ///< [in] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.\n        OutputT         (&output_items)[ITEMS_PER_THREAD],     ///< [out] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.\n        Int2Type<false> /*time_slicing*/)\n    {\n        _Pragma(\"unroll\")\n        for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n        {\n            int item_offset = (linear_tid * ITEMS_PER_THREAD) + ITEM;\n            if (INSERT_PADDING) item_offset += item_offset >> LOG_SMEM_BANKS;\n            detail::uninitialized_copy(temp_storage.buff + item_offset,\n                                       input_items[ITEM]);\n        }\n\n        CTA_SYNC();\n\n        _Pragma(\"unroll\")\n        for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n        {\n            int item_offset = int(ITEM * BLOCK_THREADS) + linear_tid;\n            if (INSERT_PADDING) item_offset += item_offset >> LOG_SMEM_BANKS;\n            output_items[ITEM] = temp_storage.buff[item_offset];\n        }\n    }\n\n\n    /**\n     * Transposes data items from <em>blocked</em> arrangement to <em>striped</em> arrangement.  Specialized for warp-timeslicing.\n     */\n    template <typename OutputT>\n    __device__ __forceinline__ void BlockedToStriped(\n        InputT          (&input_items)[ITEMS_PER_THREAD],      ///< [in] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.\n        OutputT         (&output_items)[ITEMS_PER_THREAD],     ///< [out] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.\n        Int2Type<true>  /*time_slicing*/)\n    {\n        InputT temp_items[ITEMS_PER_THREAD];\n\n        _Pragma(\"unroll\")\n        for (int SLICE = 0; SLICE < TIME_SLICES; SLICE++)\n        {\n            const int SLICE_OFFSET  = SLICE * TIME_SLICED_ITEMS;\n            const int SLICE_OOB     = SLICE_OFFSET + TIME_SLICED_ITEMS;\n\n            CTA_SYNC();\n\n            if (warp_id == SLICE)\n            {\n                _Pragma(\"unroll\")\n                for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n                {\n                    int item_offset = (lane_id * ITEMS_PER_THREAD) + ITEM;\n                    if (INSERT_PADDING) item_offset += item_offset >> LOG_SMEM_BANKS;\n                    detail::uninitialized_copy(temp_storage.buff + item_offset,\n                                               input_items[ITEM]);\n                }\n            }\n\n            CTA_SYNC();\n\n            _Pragma(\"unroll\")\n            for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n            {\n                // Read a strip of items\n                const int STRIP_OFFSET  = ITEM * BLOCK_THREADS;\n                const int STRIP_OOB     = STRIP_OFFSET + BLOCK_THREADS;\n\n                if ((SLICE_OFFSET < STRIP_OOB) && (SLICE_OOB > STRIP_OFFSET))\n                {\n                    int item_offset = STRIP_OFFSET + linear_tid - SLICE_OFFSET;\n                    if ((item_offset >= 0) && (item_offset < TIME_SLICED_ITEMS))\n                    {\n                        if (INSERT_PADDING) item_offset += item_offset >> LOG_SMEM_BANKS;\n                        temp_items[ITEM] = temp_storage.buff[item_offset];\n                    }\n                }\n            }\n        }\n\n        // Copy\n        _Pragma(\"unroll\")\n        for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n        {\n            output_items[ITEM] = temp_items[ITEM];\n        }\n    }\n\n\n    /**\n     * Transposes data items from <em>blocked</em> arrangement to <em>warp-striped</em> arrangement. Specialized for no timeslicing\n     */\n    template <typename OutputT>\n    __device__ __forceinline__ void BlockedToWarpStriped(\n        InputT          (&input_items)[ITEMS_PER_THREAD],      ///< [in] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.\n        OutputT         (&output_items)[ITEMS_PER_THREAD],     ///< [out] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.\n        Int2Type<false> /*time_slicing*/)\n    {\n        _Pragma(\"unroll\")\n        for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n        {\n            int item_offset = warp_offset + ITEM + (lane_id * ITEMS_PER_THREAD);\n            if (INSERT_PADDING) item_offset += item_offset >> LOG_SMEM_BANKS;\n            detail::uninitialized_copy(temp_storage.buff + item_offset,\n                                       input_items[ITEM]);\n        }\n\n        WARP_SYNC(0xffffffff);\n\n        _Pragma(\"unroll\")\n        for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n        {\n            int item_offset = warp_offset + (ITEM * WARP_TIME_SLICED_THREADS) + lane_id;\n            if (INSERT_PADDING) item_offset += item_offset >> LOG_SMEM_BANKS;\n            output_items[ITEM] = temp_storage.buff[item_offset];\n        }\n    }\n\n    /**\n     * Transposes data items from <em>blocked</em> arrangement to <em>warp-striped</em> arrangement. Specialized for warp-timeslicing\n     */\n    template <typename OutputT>\n    __device__ __forceinline__ void BlockedToWarpStriped(\n        InputT          (&input_items)[ITEMS_PER_THREAD],      ///< [in] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.\n        OutputT         (&output_items)[ITEMS_PER_THREAD],     ///< [out] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.\n        Int2Type<true>  /*time_slicing*/)\n    {\n        if (warp_id == 0)\n        {\n            _Pragma(\"unroll\")\n            for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n            {\n                int item_offset = ITEM + (lane_id * ITEMS_PER_THREAD);\n                if (INSERT_PADDING) item_offset += item_offset >> LOG_SMEM_BANKS;\n                detail::uninitialized_copy(temp_storage.buff + item_offset,\n                                           input_items[ITEM]);\n            }\n\n            WARP_SYNC(0xffffffff);\n\n            _Pragma(\"unroll\")\n            for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n            {\n                int item_offset = (ITEM * WARP_TIME_SLICED_THREADS) + lane_id;\n                if (INSERT_PADDING) item_offset += item_offset >> LOG_SMEM_BANKS;\n                output_items[ITEM] = temp_storage.buff[item_offset];\n            }\n        }\n\n        _Pragma(\"unroll\")\n        for (unsigned int SLICE = 1; SLICE < TIME_SLICES; ++SLICE)\n        {\n            CTA_SYNC();\n\n            if (warp_id == SLICE)\n            {\n                _Pragma(\"unroll\")\n                for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n                {\n                    int item_offset = ITEM + (lane_id * ITEMS_PER_THREAD);\n                    if (INSERT_PADDING) item_offset += item_offset >> LOG_SMEM_BANKS;\n                    detail::uninitialized_copy(temp_storage.buff + item_offset,\n                                               input_items[ITEM]);\n                }\n\n                WARP_SYNC(0xffffffff);\n\n                _Pragma(\"unroll\")\n                for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n                {\n                    int item_offset = (ITEM * WARP_TIME_SLICED_THREADS) + lane_id;\n                    if (INSERT_PADDING) item_offset += item_offset >> LOG_SMEM_BANKS;\n                    output_items[ITEM] = temp_storage.buff[item_offset];\n                }\n            }\n        }\n    }\n\n\n    /**\n     * Transposes data items from <em>striped</em> arrangement to <em>blocked</em> arrangement.  Specialized for no timeslicing.\n     */\n    template <typename OutputT>\n    __device__ __forceinline__ void StripedToBlocked(\n        InputT          (&input_items)[ITEMS_PER_THREAD],      ///< [in] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.\n        OutputT         (&output_items)[ITEMS_PER_THREAD],     ///< [out] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.\n        Int2Type<false> /*time_slicing*/)\n    {\n        _Pragma(\"unroll\")\n        for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n        {\n            int item_offset = int(ITEM * BLOCK_THREADS) + linear_tid;\n            if (INSERT_PADDING) item_offset += item_offset >> LOG_SMEM_BANKS;\n            detail::uninitialized_copy(temp_storage.buff + item_offset,\n                                       input_items[ITEM]);\n        }\n\n        CTA_SYNC();\n\n        // No timeslicing\n        _Pragma(\"unroll\")\n        for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n        {\n            int item_offset = (linear_tid * ITEMS_PER_THREAD) + ITEM;\n            if (INSERT_PADDING) item_offset += item_offset >> LOG_SMEM_BANKS;\n            output_items[ITEM] = temp_storage.buff[item_offset];\n        }\n    }\n\n\n    /**\n     * Transposes data items from <em>striped</em> arrangement to <em>blocked</em> arrangement.  Specialized for warp-timeslicing.\n     */\n    template <typename OutputT>\n    __device__ __forceinline__ void StripedToBlocked(\n        InputT          (&input_items)[ITEMS_PER_THREAD],      ///< [in] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.\n        OutputT         (&output_items)[ITEMS_PER_THREAD],     ///< [out] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.\n        Int2Type<true>  /*time_slicing*/)\n    {\n        // Warp time-slicing\n        InputT temp_items[ITEMS_PER_THREAD];\n\n        _Pragma(\"unroll\")\n        for (int SLICE = 0; SLICE < TIME_SLICES; SLICE++)\n        {\n            const int SLICE_OFFSET  = SLICE * TIME_SLICED_ITEMS;\n            const int SLICE_OOB     = SLICE_OFFSET + TIME_SLICED_ITEMS;\n\n            CTA_SYNC();\n\n            _Pragma(\"unroll\")\n            for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n            {\n                // Write a strip of items\n                const int STRIP_OFFSET  = ITEM * BLOCK_THREADS;\n                const int STRIP_OOB     = STRIP_OFFSET + BLOCK_THREADS;\n\n                if ((SLICE_OFFSET < STRIP_OOB) && (SLICE_OOB > STRIP_OFFSET))\n                {\n                    int item_offset = STRIP_OFFSET + linear_tid - SLICE_OFFSET;\n                    if ((item_offset >= 0) && (item_offset < TIME_SLICED_ITEMS))\n                    {\n                        if (INSERT_PADDING) item_offset += item_offset >> LOG_SMEM_BANKS;\n                        detail::uninitialized_copy(temp_storage.buff +\n                                                     item_offset,\n                                                   input_items[ITEM]);\n                    }\n                }\n            }\n\n            CTA_SYNC();\n\n            if (warp_id == SLICE)\n            {\n                _Pragma(\"unroll\")\n                for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n                {\n                    int item_offset = (lane_id * ITEMS_PER_THREAD) + ITEM;\n                    if (INSERT_PADDING) item_offset += item_offset >> LOG_SMEM_BANKS;\n                    temp_items[ITEM] = temp_storage.buff[item_offset];\n                }\n            }\n        }\n\n        // Copy\n        _Pragma(\"unroll\")\n        for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n        {\n            output_items[ITEM] = temp_items[ITEM];\n        }\n    }\n\n\n    /**\n     * Transposes data items from <em>warp-striped</em> arrangement to <em>blocked</em> arrangement.  Specialized for no timeslicing\n     */\n    template <typename OutputT>\n    __device__ __forceinline__ void WarpStripedToBlocked(\n        InputT          (&input_items)[ITEMS_PER_THREAD],      ///< [in] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.\n        OutputT         (&output_items)[ITEMS_PER_THREAD],     ///< [out] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.\n        Int2Type<false> /*time_slicing*/)\n    {\n        _Pragma(\"unroll\")\n        for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n        {\n            int item_offset = warp_offset + (ITEM * WARP_TIME_SLICED_THREADS) + lane_id;\n            if (INSERT_PADDING) item_offset += item_offset >> LOG_SMEM_BANKS;\n            detail::uninitialized_copy(temp_storage.buff + item_offset,\n                                       input_items[ITEM]);\n        }\n\n        WARP_SYNC(0xffffffff);\n\n        _Pragma(\"unroll\")\n        for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n        {\n            int item_offset = warp_offset + ITEM + (lane_id * ITEMS_PER_THREAD);\n            if (INSERT_PADDING) item_offset += item_offset >> LOG_SMEM_BANKS;\n            detail::uninitialized_copy(output_items + ITEM,\n                                       temp_storage.buff[item_offset]);\n        }\n    }\n\n\n    /**\n     * Transposes data items from <em>warp-striped</em> arrangement to <em>blocked</em> arrangement.  Specialized for warp-timeslicing\n     */\n    template <typename OutputT>\n    __device__ __forceinline__ void WarpStripedToBlocked(\n        InputT          (&input_items)[ITEMS_PER_THREAD],      ///< [in] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.\n        OutputT         (&output_items)[ITEMS_PER_THREAD],     ///< [out] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.\n        Int2Type<true>  /*time_slicing*/)\n    {\n        _Pragma(\"unroll\")\n        for (unsigned int SLICE = 0; SLICE < TIME_SLICES; ++SLICE)\n        {\n            CTA_SYNC();\n\n            if (warp_id == SLICE)\n            {\n                _Pragma(\"unroll\")\n                for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n                {\n                    int item_offset = (ITEM * WARP_TIME_SLICED_THREADS) + lane_id;\n                    if (INSERT_PADDING) item_offset += item_offset >> LOG_SMEM_BANKS;\n                    detail::uninitialized_copy(temp_storage.buff + item_offset,\n                                               input_items[ITEM]);\n                }\n\n                WARP_SYNC(0xffffffff);\n\n                _Pragma(\"unroll\")\n                for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n                {\n                    int item_offset = ITEM + (lane_id * ITEMS_PER_THREAD);\n                    if (INSERT_PADDING) item_offset += item_offset >> LOG_SMEM_BANKS;\n                    output_items[ITEM] = temp_storage.buff[item_offset];\n                }\n            }\n        }\n    }\n\n\n    /**\n     * Exchanges data items annotated by rank into <em>blocked</em> arrangement.  Specialized for no timeslicing.\n     */\n    template <typename OutputT, typename OffsetT>\n    __device__ __forceinline__ void ScatterToBlocked(\n        InputT          (&input_items)[ITEMS_PER_THREAD],      ///< [in] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.\n        OutputT         (&output_items)[ITEMS_PER_THREAD],     ///< [out] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.\n        OffsetT         (&ranks)[ITEMS_PER_THREAD],            ///< [in] Corresponding scatter ranks\n        Int2Type<false> /*time_slicing*/)\n    {\n        _Pragma(\"unroll\")\n        for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n        {\n            int item_offset = ranks[ITEM];\n            if (INSERT_PADDING) item_offset = SHR_ADD(item_offset, LOG_SMEM_BANKS, item_offset);\n            detail::uninitialized_copy(temp_storage.buff + item_offset,\n                                       input_items[ITEM]);\n        }\n\n        CTA_SYNC();\n\n        _Pragma(\"unroll\")\n        for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n        {\n            int item_offset = (linear_tid * ITEMS_PER_THREAD) + ITEM;\n            if (INSERT_PADDING) item_offset = SHR_ADD(item_offset, LOG_SMEM_BANKS, item_offset);\n            output_items[ITEM] = temp_storage.buff[item_offset];\n        }\n    }\n\n    /**\n     * Exchanges data items annotated by rank into <em>blocked</em> arrangement.  Specialized for warp-timeslicing.\n     */\n    template <typename OutputT, typename OffsetT>\n    __device__ __forceinline__ void ScatterToBlocked(\n        InputT          (&input_items)[ITEMS_PER_THREAD],     ///< [in] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.\n        OutputT         (&output_items)[ITEMS_PER_THREAD],    ///< [out] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.\n        OffsetT         ranks[ITEMS_PER_THREAD],              ///< [in] Corresponding scatter ranks\n        Int2Type<true>  /*time_slicing*/)\n    {\n        InputT temp_items[ITEMS_PER_THREAD];\n\n        _Pragma(\"unroll\")\n        for (int SLICE = 0; SLICE < TIME_SLICES; SLICE++)\n        {\n            CTA_SYNC();\n\n            const int SLICE_OFFSET = TIME_SLICED_ITEMS * SLICE;\n\n            _Pragma(\"unroll\")\n            for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n            {\n                int item_offset = ranks[ITEM] - SLICE_OFFSET;\n                if ((item_offset >= 0) && (item_offset < WARP_TIME_SLICED_ITEMS))\n                {\n                    if (INSERT_PADDING) item_offset = SHR_ADD(item_offset, LOG_SMEM_BANKS, item_offset);\n                    detail::uninitialized_copy(temp_storage.buff + item_offset,\n                                               input_items[ITEM]);\n                }\n            }\n\n            CTA_SYNC();\n\n            if (warp_id == SLICE)\n            {\n                _Pragma(\"unroll\")\n                for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n                {\n                    int item_offset = (lane_id * ITEMS_PER_THREAD) + ITEM;\n                    if (INSERT_PADDING) item_offset = SHR_ADD(item_offset, LOG_SMEM_BANKS, item_offset);\n                    temp_items[ITEM] = temp_storage.buff[item_offset];\n                }\n            }\n        }\n\n        // Copy\n        _Pragma(\"unroll\")\n        for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n        {\n            output_items[ITEM] = temp_items[ITEM];\n        }\n    }\n\n\n    /**\n     * Exchanges data items annotated by rank into <em>striped</em> arrangement.  Specialized for no timeslicing.\n     */\n    template <typename OutputT, typename OffsetT>\n    __device__ __forceinline__ void ScatterToStriped(\n        InputT          (&input_items)[ITEMS_PER_THREAD],      ///< [in] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.\n        OutputT         (&output_items)[ITEMS_PER_THREAD],     ///< [out] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.\n        OffsetT         (&ranks)[ITEMS_PER_THREAD],            ///< [in] Corresponding scatter ranks\n        Int2Type<false> /*time_slicing*/)\n    {\n        _Pragma(\"unroll\")\n        for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n        {\n            int item_offset = ranks[ITEM];\n            if (INSERT_PADDING) item_offset = SHR_ADD(item_offset, LOG_SMEM_BANKS, item_offset);\n            detail::uninitialized_copy(temp_storage.buff + item_offset,\n                                       input_items[ITEM]);\n        }\n\n        CTA_SYNC();\n\n        _Pragma(\"unroll\")\n        for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n        {\n            int item_offset = int(ITEM * BLOCK_THREADS) + linear_tid;\n            if (INSERT_PADDING) item_offset = SHR_ADD(item_offset, LOG_SMEM_BANKS, item_offset);\n            output_items[ITEM] = temp_storage.buff[item_offset];\n        }\n    }\n\n\n    /**\n     * Exchanges data items annotated by rank into <em>striped</em> arrangement.  Specialized for warp-timeslicing.\n     */\n    template <typename OutputT, typename OffsetT>\n    __device__ __forceinline__ void ScatterToStriped(\n        InputT          (&input_items)[ITEMS_PER_THREAD],      ///< [in] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.\n        OutputT         (&output_items)[ITEMS_PER_THREAD],     ///< [out] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.\n        OffsetT         (&ranks)[ITEMS_PER_THREAD],            ///< [in] Corresponding scatter ranks\n        Int2Type<true> /*time_slicing*/)\n    {\n        InputT temp_items[ITEMS_PER_THREAD];\n\n        _Pragma(\"unroll\")\n        for (int SLICE = 0; SLICE < TIME_SLICES; SLICE++)\n        {\n            const int SLICE_OFFSET  = SLICE * TIME_SLICED_ITEMS;\n            const int SLICE_OOB     = SLICE_OFFSET + TIME_SLICED_ITEMS;\n\n            CTA_SYNC();\n\n            _Pragma(\"unroll\")\n            for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n            {\n                int item_offset = ranks[ITEM] - SLICE_OFFSET;\n                if ((item_offset >= 0) && (item_offset < WARP_TIME_SLICED_ITEMS))\n                {\n                    if (INSERT_PADDING) item_offset = SHR_ADD(item_offset, LOG_SMEM_BANKS, item_offset);\n                    detail::uninitialized_copy(temp_storage.buff + item_offset,\n                                               input_items[ITEM]);\n                }\n            }\n\n            CTA_SYNC();\n\n            _Pragma(\"unroll\")\n            for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n            {\n                // Read a strip of items\n                const int STRIP_OFFSET  = ITEM * BLOCK_THREADS;\n                const int STRIP_OOB     = STRIP_OFFSET + BLOCK_THREADS;\n\n                if ((SLICE_OFFSET < STRIP_OOB) && (SLICE_OOB > STRIP_OFFSET))\n                {\n                    int item_offset = STRIP_OFFSET + linear_tid - SLICE_OFFSET;\n                    if ((item_offset >= 0) && (item_offset < TIME_SLICED_ITEMS))\n                    {\n                        if (INSERT_PADDING) item_offset += item_offset >> LOG_SMEM_BANKS;\n                        temp_items[ITEM] = temp_storage.buff[item_offset];\n                    }\n                }\n            }\n        }\n\n        // Copy\n        _Pragma(\"unroll\")\n        for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n        {\n            output_items[ITEM] = temp_items[ITEM];\n        }\n    }\n\n\npublic:\n\n    /******************************************************************//**\n     * \\name Collective constructors\n     *********************************************************************/\n    //@{\n\n    /**\n     * \\brief Collective constructor using a private static allocation of shared memory as temporary storage.\n     */\n    __device__ __forceinline__ BlockExchange()\n    :\n        temp_storage(PrivateStorage()),\n        linear_tid(RowMajorTid(BLOCK_DIM_X, BLOCK_DIM_Y, BLOCK_DIM_Z)),\n        lane_id(LaneId()),\n        warp_id((WARPS == 1) ? 0 : linear_tid / WARP_THREADS),\n        warp_offset(warp_id * WARP_TIME_SLICED_ITEMS)\n    {}\n\n\n    /**\n     * \\brief Collective constructor using the specified memory allocation as temporary storage.\n     */\n    __device__ __forceinline__ BlockExchange(\n        TempStorage &temp_storage)             ///< [in] Reference to memory allocation having layout type TempStorage\n    :\n        temp_storage(temp_storage.Alias()),\n        linear_tid(RowMajorTid(BLOCK_DIM_X, BLOCK_DIM_Y, BLOCK_DIM_Z)),\n        lane_id(LaneId()),\n        warp_id((WARPS == 1) ? 0 : linear_tid / WARP_THREADS),\n        warp_offset(warp_id * WARP_TIME_SLICED_ITEMS)\n    {}\n\n\n    //@}  end member group\n    /******************************************************************//**\n     * \\name Structured exchanges\n     *********************************************************************/\n    //@{\n\n    /**\n     * \\brief Transposes data items from <em>striped</em> arrangement to <em>blocked</em> arrangement.\n     *\n     * \\par\n     * - \\smemreuse\n     *\n     * \\par Snippet\n     * The code snippet below illustrates the conversion from a \"striped\" to a \"blocked\" arrangement\n     * of 512 integer items partitioned across 128 threads where each thread owns 4 items.\n     * \\par\n     * \\code\n     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_exchange.cuh>\n     *\n     * __global__ void ExampleKernel(int *d_data, ...)\n     * {\n     *     // Specialize BlockExchange for a 1D block of 128 threads owning 4 integer items each\n     *     typedef cub::BlockExchange<int, 128, 4> BlockExchange;\n     *\n     *     // Allocate shared memory for BlockExchange\n     *     __shared__ typename BlockExchange::TempStorage temp_storage;\n     *\n     *     // Load a tile of ordered data into a striped arrangement across block threads\n     *     int thread_data[4];\n     *     cub::LoadDirectStriped<128>(threadIdx.x, d_data, thread_data);\n     *\n     *     // Collectively exchange data into a blocked arrangement across threads\n     *     BlockExchange(temp_storage).StripedToBlocked(thread_data, thread_data);\n     *\n     * \\endcode\n     * \\par\n     * Suppose the set of striped input \\p thread_data across the block of threads is\n     * <tt>{ [0,128,256,384], [1,129,257,385], ..., [127,255,383,511] }</tt> after loading from device-accessible memory.\n     * The corresponding output \\p thread_data in those threads will be\n     * <tt>{ [0,1,2,3], [4,5,6,7], [8,9,10,11], ..., [508,509,510,511] }</tt>.\n     *\n     */\n    template <typename OutputT>\n    __device__ __forceinline__ void StripedToBlocked(\n        InputT      (&input_items)[ITEMS_PER_THREAD],    ///< [in] Items to exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.\n        OutputT     (&output_items)[ITEMS_PER_THREAD])   ///< [out] Items from exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.\n    {\n        StripedToBlocked(input_items, output_items, Int2Type<WARP_TIME_SLICING>());\n    }\n\n\n    /**\n     * \\brief Transposes data items from <em>blocked</em> arrangement to <em>striped</em> arrangement.\n     *\n     * \\par\n     * - \\smemreuse\n     *\n     * \\par Snippet\n     * The code snippet below illustrates the conversion from a \"blocked\" to a \"striped\" arrangement\n     * of 512 integer items partitioned across 128 threads where each thread owns 4 items.\n     * \\par\n     * \\code\n     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_exchange.cuh>\n     *\n     * __global__ void ExampleKernel(int *d_data, ...)\n     * {\n     *     // Specialize BlockExchange for a 1D block of 128 threads owning 4 integer items each\n     *     typedef cub::BlockExchange<int, 128, 4> BlockExchange;\n     *\n     *     // Allocate shared memory for BlockExchange\n     *     __shared__ typename BlockExchange::TempStorage temp_storage;\n     *\n     *     // Obtain a segment of consecutive items that are blocked across threads\n     *     int thread_data[4];\n     *     ...\n     *\n     *     // Collectively exchange data into a striped arrangement across threads\n     *     BlockExchange(temp_storage).BlockedToStriped(thread_data, thread_data);\n     *\n     *     // Store data striped across block threads into an ordered tile\n     *     cub::StoreDirectStriped<STORE_DEFAULT, 128>(threadIdx.x, d_data, thread_data);\n     *\n     * \\endcode\n     * \\par\n     * Suppose the set of blocked input \\p thread_data across the block of threads is\n     * <tt>{ [0,1,2,3], [4,5,6,7], [8,9,10,11], ..., [508,509,510,511] }</tt>.\n     * The corresponding output \\p thread_data in those threads will be\n     * <tt>{ [0,128,256,384], [1,129,257,385], ..., [127,255,383,511] }</tt> in\n     * preparation for storing to device-accessible memory.\n     *\n     */\n    template <typename OutputT>\n    __device__ __forceinline__ void BlockedToStriped(\n        InputT      (&input_items)[ITEMS_PER_THREAD],    ///< [in] Items to exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.\n        OutputT     (&output_items)[ITEMS_PER_THREAD])   ///< [out] Items from exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.\n    {\n        BlockedToStriped(input_items, output_items, Int2Type<WARP_TIME_SLICING>());\n    }\n\n\n\n    /**\n     * \\brief Transposes data items from <em>warp-striped</em> arrangement to <em>blocked</em> arrangement.\n     *\n     * \\par\n     * - \\smemreuse\n     *\n     * \\par Snippet\n     * The code snippet below illustrates the conversion from a \"warp-striped\" to a \"blocked\" arrangement\n     * of 512 integer items partitioned across 128 threads where each thread owns 4 items.\n     * \\par\n     * \\code\n     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_exchange.cuh>\n     *\n     * __global__ void ExampleKernel(int *d_data, ...)\n     * {\n     *     // Specialize BlockExchange for a 1D block of 128 threads owning 4 integer items each\n     *     typedef cub::BlockExchange<int, 128, 4> BlockExchange;\n     *\n     *     // Allocate shared memory for BlockExchange\n     *     __shared__ typename BlockExchange::TempStorage temp_storage;\n     *\n     *     // Load a tile of ordered data into a warp-striped arrangement across warp threads\n     *     int thread_data[4];\n     *     cub::LoadSWarptriped<LOAD_DEFAULT>(threadIdx.x, d_data, thread_data);\n     *\n     *     // Collectively exchange data into a blocked arrangement across threads\n     *     BlockExchange(temp_storage).WarpStripedToBlocked(thread_data);\n     *\n     * \\endcode\n     * \\par\n     * Suppose the set of warp-striped input \\p thread_data across the block of threads is\n     * <tt>{ [0,32,64,96], [1,33,65,97], [2,34,66,98], ..., [415,447,479,511] }</tt>\n     * after loading from device-accessible memory.  (The first 128 items are striped across\n     * the first warp of 32 threads, the second 128 items are striped across the second warp, etc.)\n     * The corresponding output \\p thread_data in those threads will be\n     * <tt>{ [0,1,2,3], [4,5,6,7], [8,9,10,11], ..., [508,509,510,511] }</tt>.\n     *\n     */\n    template <typename OutputT>\n    __device__ __forceinline__ void WarpStripedToBlocked(\n        InputT      (&input_items)[ITEMS_PER_THREAD],    ///< [in] Items to exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.\n        OutputT     (&output_items)[ITEMS_PER_THREAD])   ///< [out] Items from exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.\n    {\n        WarpStripedToBlocked(input_items, output_items, Int2Type<WARP_TIME_SLICING>());\n    }\n\n\n\n    /**\n     * \\brief Transposes data items from <em>blocked</em> arrangement to <em>warp-striped</em> arrangement.\n     *\n     * \\par\n     * - \\smemreuse\n     *\n     * \\par Snippet\n     * The code snippet below illustrates the conversion from a \"blocked\" to a \"warp-striped\" arrangement\n     * of 512 integer items partitioned across 128 threads where each thread owns 4 items.\n     * \\par\n     * \\code\n     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_exchange.cuh>\n     *\n     * __global__ void ExampleKernel(int *d_data, ...)\n     * {\n     *     // Specialize BlockExchange for a 1D block of 128 threads owning 4 integer items each\n     *     typedef cub::BlockExchange<int, 128, 4> BlockExchange;\n     *\n     *     // Allocate shared memory for BlockExchange\n     *     __shared__ typename BlockExchange::TempStorage temp_storage;\n     *\n     *     // Obtain a segment of consecutive items that are blocked across threads\n     *     int thread_data[4];\n     *     ...\n     *\n     *     // Collectively exchange data into a warp-striped arrangement across threads\n     *     BlockExchange(temp_storage).BlockedToWarpStriped(thread_data, thread_data);\n     *\n     *     // Store data striped across warp threads into an ordered tile\n     *     cub::StoreDirectStriped<STORE_DEFAULT, 128>(threadIdx.x, d_data, thread_data);\n     *\n     * \\endcode\n     * \\par\n     * Suppose the set of blocked input \\p thread_data across the block of threads is\n     * <tt>{ [0,1,2,3], [4,5,6,7], [8,9,10,11], ..., [508,509,510,511] }</tt>.\n     * The corresponding output \\p thread_data in those threads will be\n     * <tt>{ [0,32,64,96], [1,33,65,97], [2,34,66,98], ..., [415,447,479,511] }</tt>\n     * in preparation for storing to device-accessible memory. (The first 128 items are striped across\n     * the first warp of 32 threads, the second 128 items are striped across the second warp, etc.)\n     *\n     */\n    template <typename OutputT>\n    __device__ __forceinline__ void BlockedToWarpStriped(\n        InputT      (&input_items)[ITEMS_PER_THREAD],    ///< [in] Items to exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.\n        OutputT     (&output_items)[ITEMS_PER_THREAD])   ///< [out] Items from exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.\n    {\n        BlockedToWarpStriped(input_items, output_items, Int2Type<WARP_TIME_SLICING>());\n    }\n\n\n\n    //@}  end member group\n    /******************************************************************//**\n     * \\name Scatter exchanges\n     *********************************************************************/\n    //@{\n\n\n    /**\n     * \\brief Exchanges data items annotated by rank into <em>blocked</em> arrangement.\n     *\n     * \\par\n     * - \\smemreuse\n     *\n     * \\tparam OffsetT                              <b>[inferred]</b> Signed integer type for local offsets\n     */\n    template <typename OutputT, typename OffsetT>\n    __device__ __forceinline__ void ScatterToBlocked(\n        InputT      (&input_items)[ITEMS_PER_THREAD],      ///< [in] Items to exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.\n        OutputT     (&output_items)[ITEMS_PER_THREAD],     ///< [out] Items from exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.\n        OffsetT     (&ranks)[ITEMS_PER_THREAD])            ///< [in] Corresponding scatter ranks\n    {\n        ScatterToBlocked(input_items, output_items, ranks, Int2Type<WARP_TIME_SLICING>());\n    }\n\n\n\n    /**\n     * \\brief Exchanges data items annotated by rank into <em>striped</em> arrangement.\n     *\n     * \\par\n     * - \\smemreuse\n     *\n     * \\tparam OffsetT                              <b>[inferred]</b> Signed integer type for local offsets\n     */\n    template <typename OutputT, typename OffsetT>\n    __device__ __forceinline__ void ScatterToStriped(\n        InputT      (&input_items)[ITEMS_PER_THREAD],      ///< [in] Items to exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.\n        OutputT     (&output_items)[ITEMS_PER_THREAD],     ///< [out] Items from exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.\n        OffsetT     (&ranks)[ITEMS_PER_THREAD])            ///< [in] Corresponding scatter ranks\n    {\n        ScatterToStriped(input_items, output_items, ranks, Int2Type<WARP_TIME_SLICING>());\n    }\n\n\n\n    /**\n     * \\brief Exchanges data items annotated by rank into <em>striped</em> arrangement.  Items with rank -1 are not exchanged.\n     *\n     * \\par\n     * - \\smemreuse\n     *\n     * \\tparam OffsetT                              <b>[inferred]</b> Signed integer type for local offsets\n     */\n    template <typename OutputT, typename OffsetT>\n    __device__ __forceinline__ void ScatterToStripedGuarded(\n        InputT      (&input_items)[ITEMS_PER_THREAD],      ///< [in] Items to exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.\n        OutputT     (&output_items)[ITEMS_PER_THREAD],     ///< [out] Items from exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.\n        OffsetT     (&ranks)[ITEMS_PER_THREAD])            ///< [in] Corresponding scatter ranks\n    {\n        _Pragma(\"unroll\")\n        for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n        {\n            int item_offset = ranks[ITEM];\n            if (INSERT_PADDING) item_offset = SHR_ADD(item_offset, LOG_SMEM_BANKS, item_offset);\n            if (ranks[ITEM] >= 0)\n                temp_storage.buff[item_offset] = input_items[ITEM];\n        }\n\n        CTA_SYNC();\n\n        _Pragma(\"unroll\")\n        for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n        {\n            int item_offset = int(ITEM * BLOCK_THREADS) + linear_tid;\n            if (INSERT_PADDING) item_offset = SHR_ADD(item_offset, LOG_SMEM_BANKS, item_offset);\n            output_items[ITEM] = temp_storage.buff[item_offset];\n        }\n    }\n\n\n\n\n    /**\n     * \\brief Exchanges valid data items annotated by rank into <em>striped</em> arrangement.\n     *\n     * \\par\n     * - \\smemreuse\n     *\n     * \\tparam OffsetT                              <b>[inferred]</b> Signed integer type for local offsets\n     * \\tparam ValidFlag                            <b>[inferred]</b> FlagT type denoting which items are valid\n     */\n    template <typename OutputT, typename OffsetT, typename ValidFlag>\n    __device__ __forceinline__ void ScatterToStripedFlagged(\n        InputT      (&input_items)[ITEMS_PER_THREAD],      ///< [in] Items to exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.\n        OutputT     (&output_items)[ITEMS_PER_THREAD],     ///< [out] Items from exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.\n        OffsetT     (&ranks)[ITEMS_PER_THREAD],            ///< [in] Corresponding scatter ranks\n        ValidFlag   (&is_valid)[ITEMS_PER_THREAD])         ///< [in] Corresponding flag denoting item validity\n    {\n        _Pragma(\"unroll\")\n        for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n        {\n            int item_offset = ranks[ITEM];\n            if (INSERT_PADDING) item_offset = SHR_ADD(item_offset, LOG_SMEM_BANKS, item_offset);\n            if (is_valid[ITEM])\n                temp_storage.buff[item_offset] = input_items[ITEM];\n        }\n\n        CTA_SYNC();\n\n        _Pragma(\"unroll\")\n        for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n        {\n            int item_offset = int(ITEM * BLOCK_THREADS) + linear_tid;\n            if (INSERT_PADDING) item_offset = SHR_ADD(item_offset, LOG_SMEM_BANKS, item_offset);\n            output_items[ITEM] = temp_storage.buff[item_offset];\n        }\n    }\n\n\n    //@}  end member group\n\n\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS    // Do not document\n\n\n    __device__ __forceinline__ void StripedToBlocked(\n        InputT      (&items)[ITEMS_PER_THREAD])   ///< [in-out] Items to exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.\n    {\n        StripedToBlocked(items, items);\n    }\n\n    __device__ __forceinline__ void BlockedToStriped(\n        InputT      (&items)[ITEMS_PER_THREAD])   ///< [in-out] Items to exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.\n    {\n        BlockedToStriped(items, items);\n    }\n\n    __device__ __forceinline__ void WarpStripedToBlocked(\n        InputT      (&items)[ITEMS_PER_THREAD])    ///< [in-out] Items to exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.\n    {\n        WarpStripedToBlocked(items, items);\n    }\n\n    __device__ __forceinline__ void BlockedToWarpStriped(\n        InputT      (&items)[ITEMS_PER_THREAD])    ///< [in-out] Items to exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.\n    {\n        BlockedToWarpStriped(items, items);\n    }\n\n    template <typename OffsetT>\n    __device__ __forceinline__ void ScatterToBlocked(\n        InputT      (&items)[ITEMS_PER_THREAD],    ///< [in-out] Items to exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.\n        OffsetT     (&ranks)[ITEMS_PER_THREAD])    ///< [in] Corresponding scatter ranks\n    {\n        ScatterToBlocked(items, items, ranks);\n    }\n\n    template <typename OffsetT>\n    __device__ __forceinline__ void ScatterToStriped(\n        InputT      (&items)[ITEMS_PER_THREAD],    ///< [in-out] Items to exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.\n        OffsetT     (&ranks)[ITEMS_PER_THREAD])    ///< [in] Corresponding scatter ranks\n    {\n        ScatterToStriped(items, items, ranks);\n    }\n\n    template <typename OffsetT>\n    __device__ __forceinline__ void ScatterToStripedGuarded(\n        InputT      (&items)[ITEMS_PER_THREAD],    ///< [in-out] Items to exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.\n        OffsetT     (&ranks)[ITEMS_PER_THREAD])    ///< [in] Corresponding scatter ranks\n    {\n        ScatterToStripedGuarded(items, items, ranks);\n    }\n\n    template <typename OffsetT, typename ValidFlag>\n    __device__ __forceinline__ void ScatterToStripedFlagged(\n        InputT      (&items)[ITEMS_PER_THREAD],        ///< [in-out] Items to exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.\n        OffsetT     (&ranks)[ITEMS_PER_THREAD],        ///< [in] Corresponding scatter ranks\n        ValidFlag   (&is_valid)[ITEMS_PER_THREAD])     ///< [in] Corresponding flag denoting item validity\n    {\n        ScatterToStriped(items, items, ranks, is_valid);\n    }\n\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\n\n};\n\n\nCUB_NAMESPACE_END\n\n#endif // _JITIFY_INCLUDE_GUARD_B096835643DEB4A4\n", "../config.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_1D666048A344D3BF\n#define _JITIFY_INCLUDE_GUARD_1D666048A344D3BF\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * Static configuration header for the CUB project.\n */\n\n#include \"util_arch.cuh\"\n#include \"util_compiler.cuh\"\n#include \"util_cpp_dialect.cuh\"\n#include \"util_deprecated.cuh\"\n#include \"util_macro.cuh\"\n#include \"util_namespace.cuh\"\n\n#endif // _JITIFY_INCLUDE_GUARD_1D666048A344D3BF\n", "../cstddef": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===--------------------------- cstddef ----------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX_CSTDDEF\n#define _LIBCUDACXX_CSTDDEF\n\n/*\n    cstddef synopsis\n\nMacros:\n\n    offsetof(type,member-designator)\n    NULL\n\nnamespace std\n{\n\nTypes:\n\n    ptrdiff_t\n    size_t\n    max_align_t\n    nullptr_t\n    byte // C++17\n\n}  // std\n\n*/\n\n#ifndef __cuda_std__\n#include <__config>\n#else\n#include \"__cuda/cstddef_prelude.h\"\n#endif //__cuda_std__\n\n#include \"__assert\" // all public C++ headers provide the assertion handler\n#include \"__type_traits/enable_if.h\"\n#include \"__type_traits/is_integral.h\"\n#include \"version\"\n\n#ifndef __cuda_std__\n#include <__pragma_push>\n#endif //__cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n#ifndef __cuda_std__\n// Don't include our own <stddef.h>; we don't want to declare ::nullptr_t.\n#ifdef _MSC_VER\n    #include <../ucrt/stddef.h>\n#else\n    #include_next <stddef.h>\n#endif\n#include <__nullptr>\n#endif //__cuda_std__\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\nusing ::ptrdiff_t;\nusing ::size_t;\n\n#if defined(__CLANG_MAX_ALIGN_T_DEFINED) || defined(_GCC_MAX_ALIGN_T) || \\\n    defined(__DEFINED_max_align_t) || defined(__NetBSD__)\n// Re-use the compiler's <stddef.h> max_align_t where possible.\nusing ::max_align_t;\n#else\ntypedef long double max_align_t;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 11\n#ifdef _LIBCUDACXX_BEGIN_NAMESPACE_STD_NOVERSION\n_LIBCUDACXX_BEGIN_NAMESPACE_STD_NOVERSION\n#else\nnamespace std  // purposefully not versioned\n{\n#endif //_LIBCUDACXX_BEGIN_NAMESPACE_STD_NOVERSION\nenum class byte : unsigned char {};\n\n_LIBCUDACXX_INLINE_VISIBILITY\nconstexpr byte  operator| (byte  __lhs, byte __rhs) noexcept\n{\n    return static_cast<byte>(\n      static_cast<unsigned char>(\n         static_cast<unsigned int>(__lhs) | static_cast<unsigned int>(__rhs)\n    ));\n}\n\n_LIBCUDACXX_INLINE_VISIBILITY\nconstexpr byte& operator|=(byte& __lhs, byte __rhs) noexcept\n{ return __lhs = __lhs | __rhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\nconstexpr byte  operator& (byte  __lhs, byte __rhs) noexcept\n{\n    return static_cast<byte>(\n      static_cast<unsigned char>(\n         static_cast<unsigned int>(__lhs) & static_cast<unsigned int>(__rhs)\n    ));\n}\n\n_LIBCUDACXX_INLINE_VISIBILITY\nconstexpr byte& operator&=(byte& __lhs, byte __rhs) noexcept\n{ return __lhs = __lhs & __rhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\nconstexpr byte  operator^ (byte  __lhs, byte __rhs) noexcept\n{\n    return static_cast<byte>(\n      static_cast<unsigned char>(\n         static_cast<unsigned int>(__lhs) ^ static_cast<unsigned int>(__rhs)\n    ));\n}\n\n_LIBCUDACXX_INLINE_VISIBILITY\nconstexpr byte& operator^=(byte& __lhs, byte __rhs) noexcept\n{ return __lhs = __lhs ^ __rhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\nconstexpr byte  operator~ (byte __b) noexcept\n{\n    return static_cast<byte>(\n      static_cast<unsigned char>(\n        ~static_cast<unsigned int>(__b)\n    ));\n}\n\ntemplate <class _Integer>\n  _LIBCUDACXX_INLINE_VISIBILITY\n  constexpr __enable_if_t<is_integral_v<_Integer>, byte> &\n  operator<<=(byte& __lhs, _Integer __shift) noexcept\n  { return __lhs = __lhs << __shift; }\n\ntemplate <class _Integer>\n  _LIBCUDACXX_INLINE_VISIBILITY\n  constexpr __enable_if_t<is_integral_v<_Integer>, byte>\n  operator<< (byte  __lhs, _Integer __shift) noexcept\n  { return static_cast<byte>(static_cast<unsigned char>(static_cast<unsigned int>(__lhs) << __shift)); }\n\ntemplate <class _Integer>\n  _LIBCUDACXX_INLINE_VISIBILITY\n  constexpr __enable_if_t<is_integral_v<_Integer>, byte> &\n  operator>>=(byte& __lhs, _Integer __shift) noexcept\n  { return __lhs = __lhs >> __shift; }\n\ntemplate <class _Integer>\n  _LIBCUDACXX_INLINE_VISIBILITY\n  constexpr __enable_if_t<is_integral_v<_Integer>, byte>\n  operator>> (byte  __lhs, _Integer __shift) noexcept\n  { return static_cast<byte>(static_cast<unsigned char>(static_cast<unsigned int>(__lhs) >> __shift)); }\n\ntemplate <class _Integer>\n  _LIBCUDACXX_INLINE_VISIBILITY\n  constexpr __enable_if_t<is_integral_v<_Integer>, _Integer>\n  to_integer(byte __b) noexcept { return static_cast<_Integer>(__b); }\n\n#ifdef _LIBCUDACXX_END_NAMESPACE_STD_NOVERSION\n_LIBCUDACXX_END_NAMESPACE_STD_NOVERSION\n#else\n}\n#endif //_LIBCUDACXX_END_NAMESPACE_STD_NOVERSION\n#endif // _LIBCUDACXX_STD_VER > 11\n\n#ifndef __cuda_std__\n#include <__pragma_pop>\n#endif //__cuda_std__\n\n#endif  // _LIBCUDACXX_CSTDDEF\n", "../cstdint": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===--------------------------- cstdint ----------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX_CSTDINT\n#define _LIBCUDACXX_CSTDINT\n\n/*\n    cstdint synopsis\n\nMacros:\n\n    INT8_MIN\n    INT16_MIN\n    INT32_MIN\n    INT64_MIN\n\n    INT8_MAX\n    INT16_MAX\n    INT32_MAX\n    INT64_MAX\n\n    UINT8_MAX\n    UINT16_MAX\n    UINT32_MAX\n    UINT64_MAX\n\n    INT_LEAST8_MIN\n    INT_LEAST16_MIN\n    INT_LEAST32_MIN\n    INT_LEAST64_MIN\n\n    INT_LEAST8_MAX\n    INT_LEAST16_MAX\n    INT_LEAST32_MAX\n    INT_LEAST64_MAX\n\n    UINT_LEAST8_MAX\n    UINT_LEAST16_MAX\n    UINT_LEAST32_MAX\n    UINT_LEAST64_MAX\n\n    INT_FAST8_MIN\n    INT_FAST16_MIN\n    INT_FAST32_MIN\n    INT_FAST64_MIN\n\n    INT_FAST8_MAX\n    INT_FAST16_MAX\n    INT_FAST32_MAX\n    INT_FAST64_MAX\n\n    UINT_FAST8_MAX\n    UINT_FAST16_MAX\n    UINT_FAST32_MAX\n    UINT_FAST64_MAX\n\n    INTPTR_MIN\n    INTPTR_MAX\n    UINTPTR_MAX\n\n    INTMAX_MIN\n    INTMAX_MAX\n\n    UINTMAX_MAX\n\n    PTRDIFF_MIN\n    PTRDIFF_MAX\n\n    SIG_ATOMIC_MIN\n    SIG_ATOMIC_MAX\n\n    SIZE_MAX\n\n    WCHAR_MIN\n    WCHAR_MAX\n\n    WINT_MIN\n    WINT_MAX\n\n    INT8_C(value)\n    INT16_C(value)\n    INT32_C(value)\n    INT64_C(value)\n\n    UINT8_C(value)\n    UINT16_C(value)\n    UINT32_C(value)\n    UINT64_C(value)\n\n    INTMAX_C(value)\n    UINTMAX_C(value)\n\nnamespace std\n{\n\nTypes:\n\n    int8_t\n    int16_t\n    int32_t\n    int64_t\n\n    uint8_t\n    uint16_t\n    uint32_t\n    uint64_t\n\n    int_least8_t\n    int_least16_t\n    int_least32_t\n    int_least64_t\n\n    uint_least8_t\n    uint_least16_t\n    uint_least32_t\n    uint_least64_t\n\n    int_fast8_t\n    int_fast16_t\n    int_fast32_t\n    int_fast64_t\n\n    uint_fast8_t\n    uint_fast16_t\n    uint_fast32_t\n    uint_fast64_t\n\n    intptr_t\n    uintptr_t\n\n    intmax_t\n    uintmax_t\n\n}  // std\n*/\n\n#ifndef __cuda_std__\n#include <__config>\n#else\n#include \"__cuda/cstdint_prelude.h\"\n#endif //__cuda_std__\n\n#ifndef _LIBCUDACXX_COMPILER_NVRTC\n#include <stdint.h>\n#endif // _LIBCUDACXX_COMPILER_NVRTC\n\n#include \"climits\"\n#include \"version\"\n\n#ifndef __cuda_std__\n#include <__pragma_push>\n#endif //__cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\nusing::int8_t;\nusing::int16_t;\nusing::int32_t;\nusing::int64_t;\n\nusing::uint8_t;\nusing::uint16_t;\nusing::uint32_t;\nusing::uint64_t;\n\nusing::int_least8_t;\nusing::int_least16_t;\nusing::int_least32_t;\nusing::int_least64_t;\n\nusing::uint_least8_t;\nusing::uint_least16_t;\nusing::uint_least32_t;\nusing::uint_least64_t;\n\nusing::int_fast8_t;\nusing::int_fast16_t;\nusing::int_fast32_t;\nusing::int_fast64_t;\n\nusing::uint_fast8_t;\nusing::uint_fast16_t;\nusing::uint_fast32_t;\nusing::uint_fast64_t;\n\nusing::intptr_t;\nusing::uintptr_t;\n\nusing::intmax_t;\nusing::uintmax_t;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#ifndef __cuda_std__\n#include <__pragma_pop>\n#endif //__cuda_std__\n\n#endif  // _LIBCUDACXX_CSTDINT\n", "../cstdlib": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===--------------------------- cstdlib ----------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX_CSTDLIB\n#define _LIBCUDACXX_CSTDLIB\n\n/*\n    cstdlib synopsis\n\nMacros:\n\n    EXIT_FAILURE\n    EXIT_SUCCESS\n    MB_CUR_MAX\n    NULL\n    RAND_MAX\n\nnamespace std\n{\n\nTypes:\n\n    size_t\n    div_t\n    ldiv_t\n    lldiv_t                                                               // C99\n\ndouble    atof (const char* nptr);\nint       atoi (const char* nptr);\nlong      atol (const char* nptr);\nlong long atoll(const char* nptr);                                        // C99\ndouble             strtod  (const char* restrict nptr, char** restrict endptr);\nfloat              strtof  (const char* restrict nptr, char** restrict endptr); // C99\nlong double        strtold (const char* restrict nptr, char** restrict endptr); // C99\nlong               strtol  (const char* restrict nptr, char** restrict endptr, int base);\nlong long          strtoll (const char* restrict nptr, char** restrict endptr, int base); // C99\nunsigned long      strtoul (const char* restrict nptr, char** restrict endptr, int base);\nunsigned long long strtoull(const char* restrict nptr, char** restrict endptr, int base); // C99\nint rand(void);\nvoid srand(unsigned int seed);\nvoid* calloc(size_t nmemb, size_t size);\nvoid free(void* ptr);\nvoid* malloc(size_t size);\nvoid* realloc(void* ptr, size_t size);\nvoid abort(void);\nint atexit(void (*func)(void));\nvoid exit(int status);\nvoid _Exit(int status);\nchar* getenv(const char* name);\nint system(const char* string);\nvoid* bsearch(const void* key, const void* base, size_t nmemb, size_t size,\n              int (*compar)(const void *, const void *));\nvoid qsort(void* base, size_t nmemb, size_t size,\n           int (*compar)(const void *, const void *));\nint         abs(      int j);\nlong        abs(     long j);\nlong long   abs(long long j);                                             // C++0X\nlong       labs(     long j);\nlong long llabs(long long j);                                             // C99\ndiv_t     div(      int numer,       int denom);\nldiv_t    div(     long numer,      long denom);\nlldiv_t   div(long long numer, long long denom);                          // C++0X\nldiv_t   ldiv(     long numer,      long denom);\nlldiv_t lldiv(long long numer, long long denom);                          // C99\nint mblen(const char* s, size_t n);\nint mbtowc(wchar_t* restrict pwc, const char* restrict s, size_t n);\nint wctomb(char* s, wchar_t wchar);\nsize_t mbstowcs(wchar_t* restrict pwcs, const char* restrict s, size_t n);\nsize_t wcstombs(char* restrict s, const wchar_t* restrict pwcs, size_t n);\nint at_quick_exit(void (*func)(void))                                     // C++11\nvoid quick_exit(int status);                                              // C++11\nvoid *aligned_alloc(size_t alignment, size_t size);                       // C11\n\n}  // std\n\n*/\n\n#ifndef __cuda_std__\n#include <__config>\n#include <stdlib.h>\n#endif //__cuda_std__\n\n#ifndef _LIBCUDACXX_COMPILER_NVRTC\n# include <cstdlib>\n#endif // _LIBCUDACXX_COMPILER_NVRTC\n\n#ifndef __cuda_std__\n#include <__pragma_push>\n#endif //__cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n#if defined(_LIBCUDACXX_CUDACC_BELOW_11_2)\n#ifdef __CUDA_ARCH__\n#  define _LIBCUDACXX_UNREACHABLE() __trap()\n#else // ^^^ __CUDA_ARCH__ ^^^ / vvv !__CUDA_ARCH__ vvv\n#  define _LIBCUDACXX_UNREACHABLE() __builtin_unreachable()\n#endif // !__CUDA_ARCH__\n#elif defined(_LIBCUDACXX_CUDACC_BELOW_11_3)\n#ifdef __CUDA_ARCH__\n#  define _LIBCUDACXX_UNREACHABLE() __builtin_assume(false)\n#else // ^^^ __CUDA_ARCH__ ^^^ / vvv !__CUDA_ARCH__ vvv\n#  define _LIBCUDACXX_UNREACHABLE() __builtin_unreachable()\n#endif // !__CUDA_ARCH__\n#elif defined(_LIBCUDACXX_COMPILER_MSVC)\n#  define _LIBCUDACXX_UNREACHABLE() __assume(false)\n#elif defined(_LIBCUDACXX_COMPILER_GCC) || __has_builtin(__builtin_unreachable)\n#  define _LIBCUDACXX_UNREACHABLE() __builtin_unreachable()\n#else // Other compilers\n#ifdef __CUDA_ARCH__\n#  define _LIBCUDACXX_UNREACHABLE() __trap()\n#else // ^^^ __CUDA_ARCH__ ^^^ / vvv !__CUDA_ARCH__ vvv\n#  define _LIBCUDACXX_UNREACHABLE() ::abort()\n#endif // !__CUDA_ARCH__\n#endif // Other compilers\n\n#ifdef _LIBCUDACXX_COMPILER_NVHPC\n#define _LIBCUDACXX_UNREACHABLE_AFTER_SWITCH()\n#else\n#define _LIBCUDACXX_UNREACHABLE_AFTER_SWITCH() _LIBCUDACXX_UNREACHABLE()\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if !defined(_LIBCUDACXX_COMPILER_NVRTC)\nusing ::size_t;\nusing ::div_t;\nusing ::ldiv_t;\n#ifndef _LIBCUDACXX_HAS_NO_LONG_LONG\nusing ::lldiv_t;\n#endif // _LIBCUDACXX_HAS_NO_LONG_LONG\nusing ::atof;\nusing ::atoi;\nusing ::atol;\n#ifndef _LIBCUDACXX_HAS_NO_LONG_LONG\nusing ::atoll;\n#endif // _LIBCUDACXX_HAS_NO_LONG_LONG\nusing ::strtod;\nusing ::strtof;\nusing ::strtold;\nusing ::strtol;\n#ifndef _LIBCUDACXX_HAS_NO_LONG_LONG\nusing ::strtoll;\n#endif // _LIBCUDACXX_HAS_NO_LONG_LONG\nusing ::strtoul;\n#ifndef _LIBCUDACXX_HAS_NO_LONG_LONG\nusing ::strtoull;\n#endif // _LIBCUDACXX_HAS_NO_LONG_LONG\nusing ::rand;\nusing ::srand;\nusing ::calloc;\nusing ::free;\nusing ::malloc;\nusing ::realloc;\nusing ::abort;\nusing ::atexit;\nusing ::exit;\nusing ::_Exit;\n#ifndef _LIBCUDACXX_WINDOWS_STORE_APP\nusing ::getenv;\nusing ::system;\n#endif\nusing ::bsearch;\nusing ::qsort;\nusing ::abs;\nusing ::labs;\n#ifndef _LIBCUDACXX_HAS_NO_LONG_LONG\nusing ::llabs;\n#endif // _LIBCUDACXX_HAS_NO_LONG_LONG\nusing ::div;\nusing ::ldiv;\n#ifndef _LIBCUDACXX_HAS_NO_LONG_LONG\nusing ::lldiv;\n#endif // _LIBCUDACXX_HAS_NO_LONG_LONG\nusing ::mblen;\nusing ::mbtowc;\nusing ::wctomb;\nusing ::mbstowcs;\nusing ::wcstombs;\n#if defined(_LIBCUDACXX_HAS_QUICK_EXIT)\nusing ::at_quick_exit;\nusing ::quick_exit;\n#endif\n#if _LIBCUDACXX_STD_VER > 14 && defined(_LIBCUDACXX_HAS_C11_FEATURES)\nusing ::aligned_alloc;\n#endif\n#endif // !defined(_LIBCUDACXX_COMPILER_NVRTC)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#ifndef __cuda_std__\n#include <__pragma_pop>\n#endif //__cuda_std__\n\n#endif  // _LIBCUDACXX_CSTDLIB\n", "../detail/type_traits.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_86A4C0B05335F23D\n#define _JITIFY_INCLUDE_GUARD_86A4C0B05335F23D\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * Wrappers and extensions around <type_traits> utilities.\n */\n\n#include <cub/util_cpp_dialect.cuh>\n#include <cub/util_namespace.cuh>\n\n#include <cuda/std/type_traits>\n\n\nCUB_NAMESPACE_BEGIN\nnamespace detail {\n\ntemplate <typename Invokable, typename... Args>\nusing invoke_result_t =\n#if CUB_CPP_DIALECT < 2017\n  typename ::cuda::std::result_of<Invokable(Args...)>::type;\n#else // 2017+\n  ::cuda::std::invoke_result_t<Invokable, Args...>;\n#endif\n\n/// The type of intermediate accumulator (according to P2322R6)\ntemplate <typename Invokable, typename InitT, typename InputT>\nusing accumulator_t = \n  typename ::cuda::std::decay<invoke_result_t<Invokable, InitT, InputT>>::type;\n\n} // namespace detail\nCUB_NAMESPACE_END\n\n#endif // _JITIFY_INCLUDE_GUARD_86A4C0B05335F23D\n", "../iterator/cache_modified_input_iterator.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_AFFCEF8B3A955144\n#define _JITIFY_INCLUDE_GUARD_AFFCEF8B3A955144\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * Random-access iterator types\n */\n\n#include <iterator>\n#include <iostream>\n\n#include \"../config.cuh\"\n#include \"../thread/thread_load.cuh\"\n#include \"../thread/thread_store.cuh\"\n\n#if (THRUST_VERSION >= 100700)\n    // This iterator is compatible with Thrust API 1.7 and newer\n    #include <thrust/iterator/iterator_facade.h>\n    #include <thrust/iterator/iterator_traits.h>\n#endif // THRUST_VERSION\n\n\nCUB_NAMESPACE_BEGIN\n\n\n\n/**\n * \\addtogroup UtilIterator\n * @{\n */\n\n\n/**\n * \\brief A random-access input wrapper for dereferencing array values using a PTX cache load modifier.\n *\n * \\par Overview\n * - CacheModifiedInputIterator is a random-access input iterator that wraps a native\n *   device pointer of type <tt>ValueType*</tt>. \\p ValueType references are\n *   made by reading \\p ValueType values through loads modified by \\p MODIFIER.\n * - Can be used to load any data type from memory using PTX cache load modifiers (e.g., \"LOAD_LDG\",\n *   \"LOAD_CG\", \"LOAD_CA\", \"LOAD_CS\", \"LOAD_CV\", etc.).\n * - Can be constructed, manipulated, and exchanged within and between host and device\n *   functions, but can only be dereferenced within device functions.\n * - Compatible with Thrust API v1.7 or newer.\n *\n * \\par Snippet\n * The code snippet below illustrates the use of \\p CacheModifiedInputIterator to\n * dereference a device array of double using the \"ldg\" PTX load modifier\n * (i.e., load values through texture cache).\n * \\par\n * \\code\n * #include <cub/cub.cuh>   // or equivalently <cub/iterator/cache_modified_input_iterator.cuh>\n *\n * // Declare, allocate, and initialize a device array\n * double *d_in;            // e.g., [8.0, 6.0, 7.0, 5.0, 3.0, 0.0, 9.0]\n *\n * // Create an iterator wrapper\n * cub::CacheModifiedInputIterator<cub::LOAD_LDG, double> itr(d_in);\n *\n * // Within device code:\n * printf(\"%f\\n\", itr[0]);  // 8.0\n * printf(\"%f\\n\", itr[1]);  // 6.0\n * printf(\"%f\\n\", itr[6]);  // 9.0\n *\n * \\endcode\n *\n * \\tparam CacheLoadModifier    The cub::CacheLoadModifier to use when accessing data\n * \\tparam ValueType            The value type of this iterator\n * \\tparam OffsetT              The difference type of this iterator (Default: \\p ptrdiff_t)\n */\ntemplate <\n    CacheLoadModifier   MODIFIER,\n    typename            ValueType,\n    typename            OffsetT = ptrdiff_t>\nclass CacheModifiedInputIterator\n{\npublic:\n\n    // Required iterator traits\n    typedef CacheModifiedInputIterator          self_type;              ///< My own type\n    typedef OffsetT                             difference_type;        ///< Type to express the result of subtracting one iterator from another\n    typedef ValueType                           value_type;             ///< The type of the element the iterator can point to\n    typedef ValueType*                          pointer;                ///< The type of a pointer to an element the iterator can point to\n    typedef ValueType                           reference;              ///< The type of a reference to an element the iterator can point to\n\n#if (THRUST_VERSION >= 100700)\n    // Use Thrust's iterator categories so we can use these iterators in Thrust 1.7 (or newer) methods\n    typedef typename THRUST_NS_QUALIFIER::detail::iterator_facade_category<\n        THRUST_NS_QUALIFIER::device_system_tag,\n        THRUST_NS_QUALIFIER::random_access_traversal_tag,\n        value_type,\n        reference\n      >::type iterator_category;                                        ///< The iterator category\n#else\n    typedef std::random_access_iterator_tag     iterator_category;      ///< The iterator category\n#endif  // THRUST_VERSION\n\n\npublic:\n\n    /// Wrapped native pointer\n    ValueType* ptr;\n\n    /// Constructor\n    template <typename QualifiedValueType>\n    __host__ __device__ __forceinline__ CacheModifiedInputIterator(\n        QualifiedValueType* ptr)     ///< Native pointer to wrap\n    :\n        ptr(const_cast<typename std::remove_cv<QualifiedValueType>::type *>(ptr))\n    {}\n\n    /// Postfix increment\n    __host__ __device__ __forceinline__ self_type operator++(int)\n    {\n        self_type retval = *this;\n        ptr++;\n        return retval;\n    }\n\n    /// Prefix increment\n    __host__ __device__ __forceinline__ self_type operator++()\n    {\n        ptr++;\n        return *this;\n    }\n\n    /// Indirection\n    __device__ __forceinline__ reference operator*() const\n    {\n        return ThreadLoad<MODIFIER>(ptr);\n    }\n\n    /// Addition\n    template <typename Distance>\n    __host__ __device__ __forceinline__ self_type operator+(Distance n) const\n    {\n        self_type retval(ptr + n);\n        return retval;\n    }\n\n    /// Addition assignment\n    template <typename Distance>\n    __host__ __device__ __forceinline__ self_type& operator+=(Distance n)\n    {\n        ptr += n;\n        return *this;\n    }\n\n    /// Subtraction\n    template <typename Distance>\n    __host__ __device__ __forceinline__ self_type operator-(Distance n) const\n    {\n        self_type retval(ptr - n);\n        return retval;\n    }\n\n    /// Subtraction assignment\n    template <typename Distance>\n    __host__ __device__ __forceinline__ self_type& operator-=(Distance n)\n    {\n        ptr -= n;\n        return *this;\n    }\n\n    /// Distance\n    __host__ __device__ __forceinline__ difference_type operator-(self_type other) const\n    {\n        return ptr - other.ptr;\n    }\n\n    /// Array subscript\n    template <typename Distance>\n    __device__ __forceinline__ reference operator[](Distance n) const\n    {\n        return ThreadLoad<MODIFIER>(ptr + n);\n    }\n\n    /// Structure dereference\n    __device__ __forceinline__ pointer operator->()\n    {\n        return &ThreadLoad<MODIFIER>(ptr);\n    }\n\n    /// Equal to\n    __host__ __device__ __forceinline__ bool operator==(const self_type& rhs) const\n    {\n        return (ptr == rhs.ptr);\n    }\n\n    /// Not equal to\n    __host__ __device__ __forceinline__ bool operator!=(const self_type& rhs) const\n    {\n        return (ptr != rhs.ptr);\n    }\n\n    /// ostream operator\n    friend std::ostream& operator<<(std::ostream& os, const self_type& /*itr*/)\n    {\n        return os;\n    }\n};\n\n\n\n/** @} */       // end group UtilIterator\n\nCUB_NAMESPACE_END\n\n#endif // _JITIFY_INCLUDE_GUARD_AFFCEF8B3A955144\n", "../limits": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===---------------------------- limits ----------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX_LIMITS\n#define _LIBCUDACXX_LIMITS\n\n/*\n    limits synopsis\n\nnamespace std\n{\n\ntemplate<class T>\nclass numeric_limits\n{\npublic:\n    static constexpr bool is_specialized = false;\n    static constexpr T min() noexcept;\n    static constexpr T max() noexcept;\n    static constexpr T lowest() noexcept;\n\n    static constexpr int  digits = 0;\n    static constexpr int  digits10 = 0;\n    static constexpr int  max_digits10 = 0;\n    static constexpr bool is_signed = false;\n    static constexpr bool is_integer = false;\n    static constexpr bool is_exact = false;\n    static constexpr int  radix = 0;\n    static constexpr T epsilon() noexcept;\n    static constexpr T round_error() noexcept;\n\n    static constexpr int  min_exponent = 0;\n    static constexpr int  min_exponent10 = 0;\n    static constexpr int  max_exponent = 0;\n    static constexpr int  max_exponent10 = 0;\n\n    static constexpr bool has_infinity = false;\n    static constexpr bool has_quiet_NaN = false;\n    static constexpr bool has_signaling_NaN = false;\n    static constexpr float_denorm_style has_denorm = denorm_absent;\n    static constexpr bool has_denorm_loss = false;\n    static constexpr T infinity() noexcept;\n    static constexpr T quiet_NaN() noexcept;\n    static constexpr T signaling_NaN() noexcept;\n    static constexpr T denorm_min() noexcept;\n\n    static constexpr bool is_iec559 = false;\n    static constexpr bool is_bounded = false;\n    static constexpr bool is_modulo = false;\n\n    static constexpr bool traps = false;\n    static constexpr bool tinyness_before = false;\n    static constexpr float_round_style round_style = round_toward_zero;\n};\n\nenum float_round_style\n{\n    round_indeterminate       = -1,\n    round_toward_zero         =  0,\n    round_to_nearest          =  1,\n    round_toward_infinity     =  2,\n    round_toward_neg_infinity =  3\n};\n\nenum float_denorm_style\n{\n    denorm_indeterminate = -1,\n    denorm_absent = 0,\n    denorm_present = 1\n};\n\ntemplate<> class numeric_limits<cv bool>;\n\ntemplate<> class numeric_limits<cv char>;\ntemplate<> class numeric_limits<cv signed char>;\ntemplate<> class numeric_limits<cv unsigned char>;\ntemplate<> class numeric_limits<cv wchar_t>;\ntemplate<> class numeric_limits<cv char8_t>; // C++20\ntemplate<> class numeric_limits<cv char16_t>;\ntemplate<> class numeric_limits<cv char32_t>;\n\ntemplate<> class numeric_limits<cv short>;\ntemplate<> class numeric_limits<cv int>;\ntemplate<> class numeric_limits<cv long>;\ntemplate<> class numeric_limits<cv long long>;\ntemplate<> class numeric_limits<cv unsigned short>;\ntemplate<> class numeric_limits<cv unsigned int>;\ntemplate<> class numeric_limits<cv unsigned long>;\ntemplate<> class numeric_limits<cv unsigned long long>;\n\ntemplate<> class numeric_limits<cv float>;\ntemplate<> class numeric_limits<cv double>;\ntemplate<> class numeric_limits<cv long double>;\n\n}  // std\n\n*/\n\n#ifndef __cuda_std__\n#include <__config>\n#else\n#ifdef _LIBCUDACXX_COMPILER_NVRTC\n#include \"climits\"\n#endif // _LIBCUDACXX_COMPILER_NVRTC\n#endif // __cuda_std__\n\n#include \"__assert\" // all public C++ headers provide the assertion handler\n#include \"type_traits\"\n#include \"version\"\n\n#ifndef __cuda_std__\n#include <__pragma_push>\n#endif //__cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n#if defined(_LIBCUDACXX_COMPILER_MSVC)\n#include \"support/win32/limits_msvc_win32.h\"\n#endif // _LIBCUDACXX_MSVCRT\n\n#if defined(_LIBCUDACXX_COMPILER_IBM)\n#include \"support/ibm/limits.h\"\n#endif // _LIBCUDACXX_COMPILER_IBM\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\nenum float_round_style\n{\n    round_indeterminate       = -1,\n    round_toward_zero         =  0,\n    round_to_nearest          =  1,\n    round_toward_infinity     =  2,\n    round_toward_neg_infinity =  3\n};\n\nenum float_denorm_style\n{\n    denorm_indeterminate = -1,\n    denorm_absent = 0,\n    denorm_present = 1\n};\n\ntemplate <class _Tp, bool = is_arithmetic<_Tp>::value>\nclass __libcpp_numeric_limits\n{\nprotected:\n    typedef _Tp type;\n\n    static constexpr bool is_specialized = false;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type min() noexcept {return type();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type max() noexcept {return type();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type lowest() noexcept {return type();}\n\n    static constexpr int  digits = 0;\n    static constexpr int  digits10 = 0;\n    static constexpr int  max_digits10 = 0;\n    static constexpr bool is_signed = false;\n    static constexpr bool is_integer = false;\n    static constexpr bool is_exact = false;\n    static constexpr int  radix = 0;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type epsilon() noexcept {return type();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type round_error() noexcept {return type();}\n\n    static constexpr int  min_exponent = 0;\n    static constexpr int  min_exponent10 = 0;\n    static constexpr int  max_exponent = 0;\n    static constexpr int  max_exponent10 = 0;\n\n    static constexpr bool has_infinity = false;\n    static constexpr bool has_quiet_NaN = false;\n    static constexpr bool has_signaling_NaN = false;\n    static constexpr float_denorm_style has_denorm = denorm_absent;\n    static constexpr bool has_denorm_loss = false;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type infinity() noexcept {return type();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type quiet_NaN() noexcept {return type();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type signaling_NaN() noexcept {return type();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type denorm_min() noexcept {return type();}\n\n    static constexpr bool is_iec559 = false;\n    static constexpr bool is_bounded = false;\n    static constexpr bool is_modulo = false;\n\n    static constexpr bool traps = false;\n    static constexpr bool tinyness_before = false;\n    static constexpr float_round_style round_style = round_toward_zero;\n};\n\ntemplate <class _Tp, int __digits, bool _IsSigned>\nstruct __libcpp_compute_min\n{\n    static constexpr _Tp value = static_cast<_Tp>(_Tp(1) << __digits);\n};\n\ntemplate <class _Tp, int __digits>\nstruct __libcpp_compute_min<_Tp, __digits, false>\n{\n    static constexpr _Tp value = _Tp(0);\n};\n\ntemplate <class _Tp>\nclass __libcpp_numeric_limits<_Tp, true>\n{\nprotected:\n    typedef _Tp type;\n\n    static constexpr bool is_specialized = true;\n\n    static constexpr bool is_signed = type(-1) < type(0);\n    static constexpr int  digits = static_cast<int>(sizeof(type) * __CHAR_BIT__ - is_signed);\n    static constexpr int  digits10 = digits * 3 / 10;\n    static constexpr int  max_digits10 = 0;\n    static constexpr type __min = __libcpp_compute_min<type, digits, is_signed>::value;\n    static constexpr type __max = is_signed ? type(type(~0) ^ __min) : type(~0);\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type min() noexcept {return __min;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type max() noexcept {return __max;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type lowest() noexcept {return min();}\n\n    static constexpr bool is_integer = true;\n    static constexpr bool is_exact = true;\n    static constexpr int  radix = 2;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type epsilon() noexcept {return type(0);}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type round_error() noexcept {return type(0);}\n\n    static constexpr int  min_exponent = 0;\n    static constexpr int  min_exponent10 = 0;\n    static constexpr int  max_exponent = 0;\n    static constexpr int  max_exponent10 = 0;\n\n    static constexpr bool has_infinity = false;\n    static constexpr bool has_quiet_NaN = false;\n    static constexpr bool has_signaling_NaN = false;\n    static constexpr float_denorm_style has_denorm = denorm_absent;\n    static constexpr bool has_denorm_loss = false;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type infinity() noexcept {return type(0);}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type quiet_NaN() noexcept {return type(0);}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type signaling_NaN() noexcept {return type(0);}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type denorm_min() noexcept {return type(0);}\n\n    static constexpr bool is_iec559 = false;\n    static constexpr bool is_bounded = true;\n    static constexpr bool is_modulo = !_CUDA_VSTD::is_signed<_Tp>::value;\n\n#if defined(__i386__) || defined(__x86_64__) || defined(__pnacl__) || \\\n    defined(__wasm__)\n    static constexpr bool traps = true;\n#else\n    static constexpr bool traps = false;\n#endif\n    static constexpr bool tinyness_before = false;\n    static constexpr float_round_style round_style = round_toward_zero;\n};\n\ntemplate <>\nclass __libcpp_numeric_limits<bool, true>\n{\nprotected:\n    typedef bool type;\n\n    static constexpr bool is_specialized = true;\n\n    static constexpr bool is_signed = false;\n    static constexpr int  digits = 1;\n    static constexpr int  digits10 = 0;\n    static constexpr int  max_digits10 = 0;\n    static constexpr type __min = false;\n    static constexpr type __max = true;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type min() noexcept {return __min;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type max() noexcept {return __max;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type lowest() noexcept {return min();}\n\n    static constexpr bool is_integer = true;\n    static constexpr bool is_exact = true;\n    static constexpr int  radix = 2;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type epsilon() noexcept {return type(0);}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type round_error() noexcept {return type(0);}\n\n    static constexpr int  min_exponent = 0;\n    static constexpr int  min_exponent10 = 0;\n    static constexpr int  max_exponent = 0;\n    static constexpr int  max_exponent10 = 0;\n\n    static constexpr bool has_infinity = false;\n    static constexpr bool has_quiet_NaN = false;\n    static constexpr bool has_signaling_NaN = false;\n    static constexpr float_denorm_style has_denorm = denorm_absent;\n    static constexpr bool has_denorm_loss = false;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type infinity() noexcept {return type(0);}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type quiet_NaN() noexcept {return type(0);}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type signaling_NaN() noexcept {return type(0);}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type denorm_min() noexcept {return type(0);}\n\n    static constexpr bool is_iec559 = false;\n    static constexpr bool is_bounded = true;\n    static constexpr bool is_modulo = false;\n\n    static constexpr bool traps = false;\n    static constexpr bool tinyness_before = false;\n    static constexpr float_round_style round_style = round_toward_zero;\n};\n\ntemplate <>\nclass __libcpp_numeric_limits<float, true>\n{\nprotected:\n    typedef float type;\n\n    static constexpr bool is_specialized = true;\n\n    static constexpr bool is_signed = true;\n    static constexpr int  digits = __FLT_MANT_DIG__;\n    static constexpr int  digits10 = __FLT_DIG__;\n    static constexpr int  max_digits10 = 2+(digits * 30103l)/100000l;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type min() noexcept {return __FLT_MIN__;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type max() noexcept {return __FLT_MAX__;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type lowest() noexcept {return -max();}\n\n    static constexpr bool is_integer = false;\n    static constexpr bool is_exact = false;\n    static constexpr int  radix = __FLT_RADIX__;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type epsilon() noexcept {return __FLT_EPSILON__;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type round_error() noexcept {return 0.5F;}\n\n    static constexpr int  min_exponent = __FLT_MIN_EXP__;\n    static constexpr int  min_exponent10 = __FLT_MIN_10_EXP__;\n    static constexpr int  max_exponent = __FLT_MAX_EXP__;\n    static constexpr int  max_exponent10 = __FLT_MAX_10_EXP__;\n\n    static constexpr bool has_infinity = true;\n    static constexpr bool has_quiet_NaN = true;\n    static constexpr bool has_signaling_NaN = true;\n    static constexpr float_denorm_style has_denorm = denorm_present;\n    static constexpr bool has_denorm_loss = false;\n#ifdef _LIBCUDACXX_COMPILER_NVRTC\n    _LIBCUDACXX_INLINE_VISIBILITY static type infinity() noexcept {return __builtin_huge_valf();}\n    _LIBCUDACXX_INLINE_VISIBILITY static type quiet_NaN() noexcept {return __builtin_nanf(\"\");}\n    _LIBCUDACXX_INLINE_VISIBILITY static type signaling_NaN() noexcept {return __builtin_nansf(\"\");}\n#else\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type infinity() noexcept {return __builtin_huge_valf();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type quiet_NaN() noexcept {return __builtin_nanf(\"\");}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type signaling_NaN() noexcept {return __builtin_nansf(\"\");}\n#endif\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type denorm_min() noexcept {return __FLT_DENORM_MIN__;}\n\n    static constexpr bool is_iec559 = true;\n    static constexpr bool is_bounded = true;\n    static constexpr bool is_modulo = false;\n\n    static constexpr bool traps = false;\n    static constexpr bool tinyness_before = false;\n    static constexpr float_round_style round_style = round_to_nearest;\n};\n\ntemplate <>\nclass __libcpp_numeric_limits<double, true>\n{\nprotected:\n    typedef double type;\n\n    static constexpr bool is_specialized = true;\n\n    static constexpr bool is_signed = true;\n    static constexpr int  digits = __DBL_MANT_DIG__;\n    static constexpr int  digits10 = __DBL_DIG__;\n    static constexpr int  max_digits10 = 2+(digits * 30103l)/100000l;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type min() noexcept {return __DBL_MIN__;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type max() noexcept {return __DBL_MAX__;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type lowest() noexcept {return -max();}\n\n    static constexpr bool is_integer = false;\n    static constexpr bool is_exact = false;\n    static constexpr int  radix = __FLT_RADIX__;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type epsilon() noexcept {return __DBL_EPSILON__;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type round_error() noexcept {return 0.5;}\n\n    static constexpr int  min_exponent = __DBL_MIN_EXP__;\n    static constexpr int  min_exponent10 = __DBL_MIN_10_EXP__;\n    static constexpr int  max_exponent = __DBL_MAX_EXP__;\n    static constexpr int  max_exponent10 = __DBL_MAX_10_EXP__;\n\n    static constexpr bool has_infinity = true;\n    static constexpr bool has_quiet_NaN = true;\n    static constexpr bool has_signaling_NaN = true;\n    static constexpr float_denorm_style has_denorm = denorm_present;\n    static constexpr bool has_denorm_loss = false;\n#ifdef _LIBCUDACXX_COMPILER_NVRTC\n    _LIBCUDACXX_INLINE_VISIBILITY static type infinity() noexcept {return __builtin_huge_val();}\n    _LIBCUDACXX_INLINE_VISIBILITY static type quiet_NaN() noexcept {return __builtin_nan(\"\");}\n    _LIBCUDACXX_INLINE_VISIBILITY static type signaling_NaN() noexcept {return __builtin_nans(\"\");}\n#else\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type infinity() noexcept {return __builtin_huge_val();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type quiet_NaN() noexcept {return __builtin_nan(\"\");}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type signaling_NaN() noexcept {return __builtin_nans(\"\");}\n#endif\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type denorm_min() noexcept {return __DBL_DENORM_MIN__;}\n\n    static constexpr bool is_iec559 = true;\n    static constexpr bool is_bounded = true;\n    static constexpr bool is_modulo = false;\n\n    static constexpr bool traps = false;\n    static constexpr bool tinyness_before = false;\n    static constexpr float_round_style round_style = round_to_nearest;\n};\n\ntemplate <>\nclass __libcpp_numeric_limits<long double, true>\n{\n#ifndef _LIBCUDACXX_HAS_NO_LONG_DOUBLE\nprotected:\n    typedef long double type;\n\n    static constexpr bool is_specialized = true;\n\n    static constexpr bool is_signed = true;\n    static constexpr int  digits = __LDBL_MANT_DIG__;\n    static constexpr int  digits10 = __LDBL_DIG__;\n    static constexpr int  max_digits10 = 2+(digits * 30103l)/100000l;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type min() noexcept {return __LDBL_MIN__;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type max() noexcept {return __LDBL_MAX__;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type lowest() noexcept {return -max();}\n\n    static constexpr bool is_integer = false;\n    static constexpr bool is_exact = false;\n    static constexpr int  radix = __FLT_RADIX__;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type epsilon() noexcept {return __LDBL_EPSILON__;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type round_error() noexcept {return 0.5L;}\n\n    static constexpr int  min_exponent = __LDBL_MIN_EXP__;\n    static constexpr int  min_exponent10 = __LDBL_MIN_10_EXP__;\n    static constexpr int  max_exponent = __LDBL_MAX_EXP__;\n    static constexpr int  max_exponent10 = __LDBL_MAX_10_EXP__;\n\n    static constexpr bool has_infinity = true;\n    static constexpr bool has_quiet_NaN = true;\n    static constexpr bool has_signaling_NaN = true;\n    static constexpr float_denorm_style has_denorm = denorm_present;\n    static constexpr bool has_denorm_loss = false;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type infinity() noexcept {return __builtin_huge_vall();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type quiet_NaN() noexcept {return __builtin_nanl(\"\");}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type signaling_NaN() noexcept {return __builtin_nansl(\"\");}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type denorm_min() noexcept {return __LDBL_DENORM_MIN__;}\n\n#if (defined(__ppc__) || defined(__ppc64__))\n    static constexpr bool is_iec559 = false;\n#else\n    static constexpr bool is_iec559 = true;\n#endif\n    static constexpr bool is_bounded = true;\n    static constexpr bool is_modulo = false;\n\n    static constexpr bool traps = false;\n    static constexpr bool tinyness_before = false;\n    static constexpr float_round_style round_style = round_to_nearest;\n#endif\n};\n\ntemplate <class _Tp>\nclass _LIBCUDACXX_TEMPLATE_VIS numeric_limits\n    : private __libcpp_numeric_limits<__remove_cv_t<_Tp>>\n{\n    typedef __libcpp_numeric_limits<__remove_cv_t<_Tp>> __base;\n    typedef typename __base::type type;\npublic:\n    static constexpr bool is_specialized = __base::is_specialized;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type min() noexcept {return __base::min();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type max() noexcept {return __base::max();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type lowest() noexcept {return __base::lowest();}\n\n    static constexpr int  digits = __base::digits;\n    static constexpr int  digits10 = __base::digits10;\n    static constexpr int  max_digits10 = __base::max_digits10;\n    static constexpr bool is_signed = __base::is_signed;\n    static constexpr bool is_integer = __base::is_integer;\n    static constexpr bool is_exact = __base::is_exact;\n    static constexpr int  radix = __base::radix;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type epsilon() noexcept {return __base::epsilon();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type round_error() noexcept {return __base::round_error();}\n\n    static constexpr int  min_exponent = __base::min_exponent;\n    static constexpr int  min_exponent10 = __base::min_exponent10;\n    static constexpr int  max_exponent = __base::max_exponent;\n    static constexpr int  max_exponent10 = __base::max_exponent10;\n\n    static constexpr bool has_infinity = __base::has_infinity;\n    static constexpr bool has_quiet_NaN = __base::has_quiet_NaN;\n    static constexpr bool has_signaling_NaN = __base::has_signaling_NaN;\n    static constexpr float_denorm_style has_denorm = __base::has_denorm;\n    static constexpr bool has_denorm_loss = __base::has_denorm_loss;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type infinity() noexcept {return __base::infinity();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type quiet_NaN() noexcept {return __base::quiet_NaN();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type signaling_NaN() noexcept {return __base::signaling_NaN();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type denorm_min() noexcept {return __base::denorm_min();}\n\n    static constexpr bool is_iec559 = __base::is_iec559;\n    static constexpr bool is_bounded = __base::is_bounded;\n    static constexpr bool is_modulo = __base::is_modulo;\n\n    static constexpr bool traps = __base::traps;\n    static constexpr bool tinyness_before = __base::tinyness_before;\n    static constexpr float_round_style round_style = __base::round_style;\n};\n\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::is_specialized;\ntemplate <class _Tp>\n    constexpr int numeric_limits<_Tp>::digits;\ntemplate <class _Tp>\n    constexpr int numeric_limits<_Tp>::digits10;\ntemplate <class _Tp>\n    constexpr int numeric_limits<_Tp>::max_digits10;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::is_signed;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::is_integer;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::is_exact;\ntemplate <class _Tp>\n    constexpr int numeric_limits<_Tp>::radix;\ntemplate <class _Tp>\n    constexpr int numeric_limits<_Tp>::min_exponent;\ntemplate <class _Tp>\n    constexpr int numeric_limits<_Tp>::min_exponent10;\ntemplate <class _Tp>\n    constexpr int numeric_limits<_Tp>::max_exponent;\ntemplate <class _Tp>\n    constexpr int numeric_limits<_Tp>::max_exponent10;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::has_infinity;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::has_quiet_NaN;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::has_signaling_NaN;\ntemplate <class _Tp>\n    constexpr float_denorm_style numeric_limits<_Tp>::has_denorm;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::has_denorm_loss;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::is_iec559;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::is_bounded;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::is_modulo;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::traps;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::tinyness_before;\ntemplate <class _Tp>\n    constexpr float_round_style numeric_limits<_Tp>::round_style;\n\ntemplate <class _Tp>\nclass _LIBCUDACXX_TEMPLATE_VIS numeric_limits<const _Tp>\n    : private numeric_limits<_Tp>\n{\n    typedef numeric_limits<_Tp> __base;\n    typedef _Tp type;\npublic:\n    static constexpr bool is_specialized = __base::is_specialized;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type min() noexcept {return __base::min();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type max() noexcept {return __base::max();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type lowest() noexcept {return __base::lowest();}\n\n    static constexpr int  digits = __base::digits;\n    static constexpr int  digits10 = __base::digits10;\n    static constexpr int  max_digits10 = __base::max_digits10;\n    static constexpr bool is_signed = __base::is_signed;\n    static constexpr bool is_integer = __base::is_integer;\n    static constexpr bool is_exact = __base::is_exact;\n    static constexpr int  radix = __base::radix;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type epsilon() noexcept {return __base::epsilon();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type round_error() noexcept {return __base::round_error();}\n\n    static constexpr int  min_exponent = __base::min_exponent;\n    static constexpr int  min_exponent10 = __base::min_exponent10;\n    static constexpr int  max_exponent = __base::max_exponent;\n    static constexpr int  max_exponent10 = __base::max_exponent10;\n\n    static constexpr bool has_infinity = __base::has_infinity;\n    static constexpr bool has_quiet_NaN = __base::has_quiet_NaN;\n    static constexpr bool has_signaling_NaN = __base::has_signaling_NaN;\n    static constexpr float_denorm_style has_denorm = __base::has_denorm;\n    static constexpr bool has_denorm_loss = __base::has_denorm_loss;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type infinity() noexcept {return __base::infinity();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type quiet_NaN() noexcept {return __base::quiet_NaN();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type signaling_NaN() noexcept {return __base::signaling_NaN();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type denorm_min() noexcept {return __base::denorm_min();}\n\n    static constexpr bool is_iec559 = __base::is_iec559;\n    static constexpr bool is_bounded = __base::is_bounded;\n    static constexpr bool is_modulo = __base::is_modulo;\n\n    static constexpr bool traps = __base::traps;\n    static constexpr bool tinyness_before = __base::tinyness_before;\n    static constexpr float_round_style round_style = __base::round_style;\n};\n\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::is_specialized;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const _Tp>::digits;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const _Tp>::digits10;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const _Tp>::max_digits10;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::is_signed;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::is_integer;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::is_exact;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const _Tp>::radix;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const _Tp>::min_exponent;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const _Tp>::min_exponent10;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const _Tp>::max_exponent;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const _Tp>::max_exponent10;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::has_infinity;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::has_quiet_NaN;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::has_signaling_NaN;\ntemplate <class _Tp>\n    constexpr float_denorm_style numeric_limits<const _Tp>::has_denorm;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::has_denorm_loss;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::is_iec559;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::is_bounded;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::is_modulo;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::traps;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::tinyness_before;\ntemplate <class _Tp>\n    constexpr float_round_style numeric_limits<const _Tp>::round_style;\n\ntemplate <class _Tp>\nclass _LIBCUDACXX_TEMPLATE_VIS numeric_limits<volatile _Tp>\n    : private numeric_limits<_Tp>\n{\n    typedef numeric_limits<_Tp> __base;\n    typedef _Tp type;\npublic:\n    static constexpr bool is_specialized = __base::is_specialized;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type min() noexcept {return __base::min();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type max() noexcept {return __base::max();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type lowest() noexcept {return __base::lowest();}\n\n    static constexpr int  digits = __base::digits;\n    static constexpr int  digits10 = __base::digits10;\n    static constexpr int  max_digits10 = __base::max_digits10;\n    static constexpr bool is_signed = __base::is_signed;\n    static constexpr bool is_integer = __base::is_integer;\n    static constexpr bool is_exact = __base::is_exact;\n    static constexpr int  radix = __base::radix;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type epsilon() noexcept {return __base::epsilon();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type round_error() noexcept {return __base::round_error();}\n\n    static constexpr int  min_exponent = __base::min_exponent;\n    static constexpr int  min_exponent10 = __base::min_exponent10;\n    static constexpr int  max_exponent = __base::max_exponent;\n    static constexpr int  max_exponent10 = __base::max_exponent10;\n\n    static constexpr bool has_infinity = __base::has_infinity;\n    static constexpr bool has_quiet_NaN = __base::has_quiet_NaN;\n    static constexpr bool has_signaling_NaN = __base::has_signaling_NaN;\n    static constexpr float_denorm_style has_denorm = __base::has_denorm;\n    static constexpr bool has_denorm_loss = __base::has_denorm_loss;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type infinity() noexcept {return __base::infinity();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type quiet_NaN() noexcept {return __base::quiet_NaN();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type signaling_NaN() noexcept {return __base::signaling_NaN();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type denorm_min() noexcept {return __base::denorm_min();}\n\n    static constexpr bool is_iec559 = __base::is_iec559;\n    static constexpr bool is_bounded = __base::is_bounded;\n    static constexpr bool is_modulo = __base::is_modulo;\n\n    static constexpr bool traps = __base::traps;\n    static constexpr bool tinyness_before = __base::tinyness_before;\n    static constexpr float_round_style round_style = __base::round_style;\n};\n\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::is_specialized;\ntemplate <class _Tp>\n    constexpr int numeric_limits<volatile _Tp>::digits;\ntemplate <class _Tp>\n    constexpr int numeric_limits<volatile _Tp>::digits10;\ntemplate <class _Tp>\n    constexpr int numeric_limits<volatile _Tp>::max_digits10;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::is_signed;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::is_integer;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::is_exact;\ntemplate <class _Tp>\n    constexpr int numeric_limits<volatile _Tp>::radix;\ntemplate <class _Tp>\n    constexpr int numeric_limits<volatile _Tp>::min_exponent;\ntemplate <class _Tp>\n    constexpr int numeric_limits<volatile _Tp>::min_exponent10;\ntemplate <class _Tp>\n    constexpr int numeric_limits<volatile _Tp>::max_exponent;\ntemplate <class _Tp>\n    constexpr int numeric_limits<volatile _Tp>::max_exponent10;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::has_infinity;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::has_quiet_NaN;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::has_signaling_NaN;\ntemplate <class _Tp>\n    constexpr float_denorm_style numeric_limits<volatile _Tp>::has_denorm;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::has_denorm_loss;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::is_iec559;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::is_bounded;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::is_modulo;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::traps;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::tinyness_before;\ntemplate <class _Tp>\n    constexpr float_round_style numeric_limits<volatile _Tp>::round_style;\n\ntemplate <class _Tp>\nclass _LIBCUDACXX_TEMPLATE_VIS numeric_limits<const volatile _Tp>\n    : private numeric_limits<_Tp>\n{\n    typedef numeric_limits<_Tp> __base;\n    typedef _Tp type;\npublic:\n    static constexpr bool is_specialized = __base::is_specialized;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type min() noexcept {return __base::min();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type max() noexcept {return __base::max();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type lowest() noexcept {return __base::lowest();}\n\n    static constexpr int  digits = __base::digits;\n    static constexpr int  digits10 = __base::digits10;\n    static constexpr int  max_digits10 = __base::max_digits10;\n    static constexpr bool is_signed = __base::is_signed;\n    static constexpr bool is_integer = __base::is_integer;\n    static constexpr bool is_exact = __base::is_exact;\n    static constexpr int  radix = __base::radix;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type epsilon() noexcept {return __base::epsilon();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type round_error() noexcept {return __base::round_error();}\n\n    static constexpr int  min_exponent = __base::min_exponent;\n    static constexpr int  min_exponent10 = __base::min_exponent10;\n    static constexpr int  max_exponent = __base::max_exponent;\n    static constexpr int  max_exponent10 = __base::max_exponent10;\n\n    static constexpr bool has_infinity = __base::has_infinity;\n    static constexpr bool has_quiet_NaN = __base::has_quiet_NaN;\n    static constexpr bool has_signaling_NaN = __base::has_signaling_NaN;\n    static constexpr float_denorm_style has_denorm = __base::has_denorm;\n    static constexpr bool has_denorm_loss = __base::has_denorm_loss;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type infinity() noexcept {return __base::infinity();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type quiet_NaN() noexcept {return __base::quiet_NaN();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type signaling_NaN() noexcept {return __base::signaling_NaN();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type denorm_min() noexcept {return __base::denorm_min();}\n\n    static constexpr bool is_iec559 = __base::is_iec559;\n    static constexpr bool is_bounded = __base::is_bounded;\n    static constexpr bool is_modulo = __base::is_modulo;\n\n    static constexpr bool traps = __base::traps;\n    static constexpr bool tinyness_before = __base::tinyness_before;\n    static constexpr float_round_style round_style = __base::round_style;\n};\n\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::is_specialized;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const volatile _Tp>::digits;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const volatile _Tp>::digits10;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const volatile _Tp>::max_digits10;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::is_signed;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::is_integer;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::is_exact;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const volatile _Tp>::radix;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const volatile _Tp>::min_exponent;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const volatile _Tp>::min_exponent10;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const volatile _Tp>::max_exponent;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const volatile _Tp>::max_exponent10;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::has_infinity;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::has_quiet_NaN;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::has_signaling_NaN;\ntemplate <class _Tp>\n    constexpr float_denorm_style numeric_limits<const volatile _Tp>::has_denorm;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::has_denorm_loss;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::is_iec559;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::is_bounded;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::is_modulo;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::traps;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::tinyness_before;\ntemplate <class _Tp>\n    constexpr float_round_style numeric_limits<const volatile _Tp>::round_style;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#ifndef __cuda_std__\n#include <__pragma_pop>\n#endif //__cuda_std__\n\n#endif  // _LIBCUDACXX_LIMITS\n", "../thread/thread_load.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_F6B33027D22A94A3\n#define _JITIFY_INCLUDE_GUARD_F6B33027D22A94A3\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * Thread utilities for reading memory using PTX cache modifiers.\n */\n\n#include <iterator>\n\n#include \"../config.cuh\"\n#include \"../util_ptx.cuh\"\n#include \"../util_type.cuh\"\n\nCUB_NAMESPACE_BEGIN\n\n/**\n * \\addtogroup UtilIo\n * @{\n */\n\n//-----------------------------------------------------------------------------\n// Tags and constants\n//-----------------------------------------------------------------------------\n\n/**\n * \\brief Enumeration of cache modifiers for memory load operations.\n */\nenum CacheLoadModifier\n{\n    LOAD_DEFAULT,       ///< Default (no modifier)\n    LOAD_CA,            ///< Cache at all levels\n    LOAD_CG,            ///< Cache at global level\n    LOAD_CS,            ///< Cache streaming (likely to be accessed once)\n    LOAD_CV,            ///< Cache as volatile (including cached system lines)\n    LOAD_LDG,           ///< Cache as texture\n    LOAD_VOLATILE,      ///< Volatile (any memory space)\n};\n\n\n/**\n * \\name Thread I/O (cache modified)\n * @{\n */\n\n/**\n * \\brief Thread utility for reading memory using cub::CacheLoadModifier cache modifiers.  Can be used to load any data type.\n *\n * \\par Example\n * \\code\n * #include <cub/cub.cuh>   // or equivalently <cub/thread/thread_load.cuh>\n *\n * // 32-bit load using cache-global modifier:\n * int *d_in;\n * int val = cub::ThreadLoad<cub::LOAD_CA>(d_in + threadIdx.x);\n *\n * // 16-bit load using default modifier\n * short *d_in;\n * short val = cub::ThreadLoad<cub::LOAD_DEFAULT>(d_in + threadIdx.x);\n *\n * // 256-bit load using cache-volatile modifier\n * double4 *d_in;\n * double4 val = cub::ThreadLoad<cub::LOAD_CV>(d_in + threadIdx.x);\n *\n * // 96-bit load using cache-streaming modifier\n * struct TestFoo { bool a; short b; };\n * TestFoo *d_struct;\n * TestFoo val = cub::ThreadLoad<cub::LOAD_CS>(d_in + threadIdx.x);\n * \\endcode\n *\n * \\tparam MODIFIER             <b>[inferred]</b> CacheLoadModifier enumeration\n * \\tparam InputIteratorT       <b>[inferred]</b> Input iterator type \\iterator\n */\ntemplate <CacheLoadModifier MODIFIER,\n          typename InputIteratorT>\n__device__ __forceinline__ cub::detail::value_t<InputIteratorT>\nThreadLoad(InputIteratorT itr);\n\n//@}  end member group\n\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS    // Do not document\n\n\n/// Helper structure for templated load iteration (inductive case)\ntemplate <int COUNT, int MAX>\nstruct IterateThreadLoad\n{\n    template <CacheLoadModifier MODIFIER, typename T>\n    static __device__ __forceinline__ void Load(T const *ptr, T *vals)\n    {\n        vals[COUNT] = ThreadLoad<MODIFIER>(ptr + COUNT);\n        IterateThreadLoad<COUNT + 1, MAX>::template Load<MODIFIER>(ptr, vals);\n    }\n\n    template <typename InputIteratorT, typename T>\n    static __device__ __forceinline__ void Dereference(InputIteratorT itr, T *vals)\n    {\n        vals[COUNT] = itr[COUNT];\n        IterateThreadLoad<COUNT + 1, MAX>::Dereference(itr, vals);\n    }\n};\n\n\n/// Helper structure for templated load iteration (termination case)\ntemplate <int MAX>\nstruct IterateThreadLoad<MAX, MAX>\n{\n    template <CacheLoadModifier MODIFIER, typename T>\n    static __device__ __forceinline__ void Load(T const * /*ptr*/, T * /*vals*/) {}\n\n    template <typename InputIteratorT, typename T>\n    static __device__ __forceinline__ void Dereference(InputIteratorT /*itr*/, T * /*vals*/) {}\n};\n\n\n/**\n * Define a uint4 (16B) ThreadLoad specialization for the given Cache load modifier\n */\n#define _CUB_LOAD_16(cub_modifier, ptx_modifier)                                             \\\n    template<>                                                                              \\\n    __device__ __forceinline__ uint4 ThreadLoad<cub_modifier, uint4 const *>(uint4 const *ptr)                   \\\n    {                                                                                       \\\n        uint4 retval;                                                                       \\\n        asm volatile (\"ld.\"#ptx_modifier\".v4.u32 {%0, %1, %2, %3}, [%4];\" :                 \\\n            \"=r\"(retval.x),                                                                 \\\n            \"=r\"(retval.y),                                                                 \\\n            \"=r\"(retval.z),                                                                 \\\n            \"=r\"(retval.w) :                                                                \\\n            _CUB_ASM_PTR_(ptr));                                                            \\\n        return retval;                                                                      \\\n    }                                                                                       \\\n    template<>                                                                              \\\n    __device__ __forceinline__ ulonglong2 ThreadLoad<cub_modifier, ulonglong2 const *>(ulonglong2 const *ptr)    \\\n    {                                                                                       \\\n        ulonglong2 retval;                                                                  \\\n        asm volatile (\"ld.\"#ptx_modifier\".v2.u64 {%0, %1}, [%2];\" :                         \\\n            \"=l\"(retval.x),                                                                 \\\n            \"=l\"(retval.y) :                                                                \\\n            _CUB_ASM_PTR_(ptr));                                                            \\\n        return retval;                                                                      \\\n    }\n\n/**\n * Define a uint2 (8B) ThreadLoad specialization for the given Cache load modifier\n */\n#define _CUB_LOAD_8(cub_modifier, ptx_modifier)                                              \\\n    template<>                                                                              \\\n    __device__ __forceinline__ ushort4 ThreadLoad<cub_modifier, ushort4 const *>(ushort4 const *ptr)             \\\n    {                                                                                       \\\n        ushort4 retval;                                                                     \\\n        asm volatile (\"ld.\"#ptx_modifier\".v4.u16 {%0, %1, %2, %3}, [%4];\" :                 \\\n            \"=h\"(retval.x),                                                                 \\\n            \"=h\"(retval.y),                                                                 \\\n            \"=h\"(retval.z),                                                                 \\\n            \"=h\"(retval.w) :                                                                \\\n            _CUB_ASM_PTR_(ptr));                                                            \\\n        return retval;                                                                      \\\n    }                                                                                       \\\n    template<>                                                                              \\\n    __device__ __forceinline__ uint2 ThreadLoad<cub_modifier, uint2 const *>(uint2 const *ptr)                   \\\n    {                                                                                       \\\n        uint2 retval;                                                                       \\\n        asm volatile (\"ld.\"#ptx_modifier\".v2.u32 {%0, %1}, [%2];\" :                         \\\n            \"=r\"(retval.x),                                                                 \\\n            \"=r\"(retval.y) :                                                                \\\n            _CUB_ASM_PTR_(ptr));                                                            \\\n        return retval;                                                                      \\\n    }                                                                                       \\\n    template<>                                                                              \\\n    __device__ __forceinline__ unsigned long long ThreadLoad<cub_modifier, unsigned long long const *>(unsigned long long const *ptr)    \\\n    {                                                                                       \\\n        unsigned long long retval;                                                          \\\n        asm volatile (\"ld.\"#ptx_modifier\".u64 %0, [%1];\" :                                  \\\n            \"=l\"(retval) :                                                                  \\\n            _CUB_ASM_PTR_(ptr));                                                            \\\n        return retval;                                                                      \\\n    }\n\n/**\n * Define a uint (4B) ThreadLoad specialization for the given Cache load modifier\n */\n#define _CUB_LOAD_4(cub_modifier, ptx_modifier)                                              \\\n    template<>                                                                              \\\n    __device__ __forceinline__ unsigned int ThreadLoad<cub_modifier, unsigned int const *>(unsigned int const *ptr)                      \\\n    {                                                                                       \\\n        unsigned int retval;                                                                \\\n        asm volatile (\"ld.\"#ptx_modifier\".u32 %0, [%1];\" :                                  \\\n            \"=r\"(retval) :                                                                  \\\n            _CUB_ASM_PTR_(ptr));                                                            \\\n        return retval;                                                                      \\\n    }\n\n\n/**\n * Define a unsigned short (2B) ThreadLoad specialization for the given Cache load modifier\n */\n#define _CUB_LOAD_2(cub_modifier, ptx_modifier)                                              \\\n    template<>                                                                              \\\n    __device__ __forceinline__ unsigned short ThreadLoad<cub_modifier, unsigned short const *>(unsigned short const *ptr)                \\\n    {                                                                                       \\\n        unsigned short retval;                                                              \\\n        asm volatile (\"ld.\"#ptx_modifier\".u16 %0, [%1];\" :                                  \\\n            \"=h\"(retval) :                                                                  \\\n            _CUB_ASM_PTR_(ptr));                                                            \\\n        return retval;                                                                      \\\n    }\n\n\n/**\n * Define an unsigned char (1B) ThreadLoad specialization for the given Cache load modifier\n */\n#define _CUB_LOAD_1(cub_modifier, ptx_modifier)                                              \\\n    template<>                                                                              \\\n    __device__ __forceinline__ unsigned char ThreadLoad<cub_modifier, unsigned char const *>(unsigned char const *ptr)                   \\\n    {                                                                                       \\\n        unsigned short retval;                                                              \\\n        asm volatile (                                                                      \\\n        \"{\"                                                                                 \\\n        \"   .reg .u8 datum;\"                                                                \\\n        \"    ld.\"#ptx_modifier\".u8 datum, [%1];\"                                            \\\n        \"    cvt.u16.u8 %0, datum;\"                                                         \\\n        \"}\" :                                                                               \\\n            \"=h\"(retval) :                                                                  \\\n            _CUB_ASM_PTR_(ptr));                                                            \\\n        return (unsigned char) retval;                                                      \\\n    }\n\n\n/**\n * Define powers-of-two ThreadLoad specializations for the given Cache load modifier\n */\n#define _CUB_LOAD_ALL(cub_modifier, ptx_modifier)                                            \\\n    _CUB_LOAD_16(cub_modifier, ptx_modifier)                                                 \\\n    _CUB_LOAD_8(cub_modifier, ptx_modifier)                                                  \\\n    _CUB_LOAD_4(cub_modifier, ptx_modifier)                                                  \\\n    _CUB_LOAD_2(cub_modifier, ptx_modifier)                                                  \\\n    _CUB_LOAD_1(cub_modifier, ptx_modifier)                                                  \\\n\n\n/**\n * Define powers-of-two ThreadLoad specializations for the various Cache load modifiers\n */\n_CUB_LOAD_ALL(LOAD_CA, ca)\n_CUB_LOAD_ALL(LOAD_CG, cg)\n_CUB_LOAD_ALL(LOAD_CS, cs)\n_CUB_LOAD_ALL(LOAD_CV, cv)\n_CUB_LOAD_ALL(LOAD_LDG, global.nc)\n\n\n// Macro cleanup\n#undef _CUB_LOAD_ALL\n#undef _CUB_LOAD_1\n#undef _CUB_LOAD_2\n#undef _CUB_LOAD_4\n#undef _CUB_LOAD_8\n#undef _CUB_LOAD_16\n\n\n\n/**\n * ThreadLoad definition for LOAD_DEFAULT modifier on iterator types\n */\ntemplate <typename InputIteratorT>\n__device__ __forceinline__ cub::detail::value_t<InputIteratorT>\nThreadLoad(InputIteratorT          itr,\n           Int2Type<LOAD_DEFAULT>  /*modifier*/,\n           Int2Type<false>         /*is_pointer*/)\n{\n    return *itr;\n}\n\n\n/**\n * ThreadLoad definition for LOAD_DEFAULT modifier on pointer types\n */\ntemplate <typename T>\n__device__ __forceinline__ T ThreadLoad(\n    T                       *ptr,\n    Int2Type<LOAD_DEFAULT>  /*modifier*/,\n    Int2Type<true>          /*is_pointer*/)\n{\n    return *ptr;\n}\n\n\n/**\n * ThreadLoad definition for LOAD_VOLATILE modifier on primitive pointer types\n */\ntemplate <typename T>\n__device__ __forceinline__ T ThreadLoadVolatilePointer(\n    T                       *ptr,\n    Int2Type<true>          /*is_primitive*/)\n{\n    T retval = *reinterpret_cast<volatile T*>(ptr);\n    return retval;\n}\n\n\n/**\n * ThreadLoad definition for LOAD_VOLATILE modifier on non-primitive pointer types\n */\ntemplate <typename T>\n__device__ __forceinline__ T ThreadLoadVolatilePointer(\n    T                       *ptr,\n    Int2Type<false>         /*is_primitive*/)\n{\n    typedef typename UnitWord<T>::VolatileWord VolatileWord;   // Word type for memcopying\n\n    const int VOLATILE_MULTIPLE = sizeof(T) / sizeof(VolatileWord);\n\n    T retval;\n    VolatileWord *words = reinterpret_cast<VolatileWord*>(&retval);\n    IterateThreadLoad<0, VOLATILE_MULTIPLE>::Dereference(\n        reinterpret_cast<volatile VolatileWord*>(ptr),\n        words);\n    return retval;\n}\n\n\n/**\n * ThreadLoad definition for LOAD_VOLATILE modifier on pointer types\n */\ntemplate <typename T>\n__device__ __forceinline__ T ThreadLoad(\n    T                       *ptr,\n    Int2Type<LOAD_VOLATILE> /*modifier*/,\n    Int2Type<true>          /*is_pointer*/)\n{\n    // Apply tags for partial-specialization\n    return ThreadLoadVolatilePointer(ptr, Int2Type<Traits<T>::PRIMITIVE>());\n}\n\n\n/**\n * ThreadLoad definition for generic modifiers on pointer types\n */\ntemplate <typename T, int MODIFIER>\n__device__ __forceinline__ T ThreadLoad(\n    T const                 *ptr,\n    Int2Type<MODIFIER>      /*modifier*/,\n    Int2Type<true>          /*is_pointer*/)\n{\n    typedef typename UnitWord<T>::DeviceWord DeviceWord;\n\n    const int DEVICE_MULTIPLE = sizeof(T) / sizeof(DeviceWord);\n\n    DeviceWord words[DEVICE_MULTIPLE];\n\n    IterateThreadLoad<0, DEVICE_MULTIPLE>::template Load<CacheLoadModifier(MODIFIER)>(\n        reinterpret_cast<DeviceWord*>(const_cast<T*>(ptr)),\n        words);\n\n    return *reinterpret_cast<T*>(words);\n}\n\n\n/**\n * ThreadLoad definition for generic modifiers\n */\ntemplate <\n    CacheLoadModifier MODIFIER,\n    typename InputIteratorT>\n__device__ __forceinline__ cub::detail::value_t<InputIteratorT>\nThreadLoad(InputIteratorT itr)\n{\n    // Apply tags for partial-specialization\n    return ThreadLoad(\n        itr,\n        Int2Type<MODIFIER>(),\n        Int2Type<std::is_pointer<InputIteratorT>::value>());\n}\n\n\n\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\n\n/** @} */       // end group UtilIo\n\n\nCUB_NAMESPACE_END\n\n#endif // _JITIFY_INCLUDE_GUARD_F6B33027D22A94A3\n", "../thread/thread_operators.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_94E67823631CB18F\n#define _JITIFY_INCLUDE_GUARD_94E67823631CB18F\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2022, NVIDIA CORPORATION.  All rights reserved.\n * \n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n * \n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" \n * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE \n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE \n * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * @file\n * Simple binary operator functor types\n */\n\n/******************************************************************************\n * Simple functor operators\n ******************************************************************************/\n\n#include <cub/config.cuh>\n#include <cub/util_cpp_dialect.cuh>\n#include <cub/util_type.cuh>\n\n#include <cuda/std/functional>\n#include <cuda/std/type_traits>\n#include <cuda/std/utility>\n\nCUB_NAMESPACE_BEGIN\n\n\n/**\n * @addtogroup UtilModule\n * @{\n */\n\n/// @brief Inequality functor (wraps equality functor)\ntemplate <typename EqualityOp>\nstruct InequalityWrapper\n{\n  /// Wrapped equality operator\n  EqualityOp op;\n\n  /// Constructor\n  __host__ __device__ __forceinline__ InequalityWrapper(EqualityOp op)\n      : op(op)\n  {}\n\n  /// Boolean inequality operator, returns `t != u`\n  template <typename T, typename U>\n  __host__ __device__ __forceinline__ bool operator()(T &&t, U &&u)\n  {\n    return !op(::cuda::std::forward<T>(t), ::cuda::std::forward<U>(u));\n  }\n};\n\n#if CUB_CPP_DIALECT > 2011\nusing Equality = ::cuda::std::equal_to<>;\nusing Inequality = ::cuda::std::not_equal_to<>;\nusing Sum = ::cuda::std::plus<>;\nusing Difference = ::cuda::std::minus<>;\nusing Division = ::cuda::std::divides<>;\n#else\n/// @brief Default equality functor\nstruct Equality\n{\n  /// Boolean equality operator, returns `t == u`\n  template <typename T, typename U>\n  __host__ __device__ __forceinline__ bool operator()(T &&t, U &&u) const\n  {\n    return ::cuda::std::forward<T>(t) == ::cuda::std::forward<U>(u);\n  }\n};\n\n/// @brief Default inequality functor\nstruct Inequality\n{\n  /// Boolean inequality operator, returns `t != u`\n  template <typename T, typename U>\n  __host__ __device__ __forceinline__ bool operator()(T &&t, U &&u) const\n  {\n    return ::cuda::std::forward<T>(t) != ::cuda::std::forward<U>(u);\n  }\n};\n\n/// @brief Default sum functor\nstruct Sum\n{\n  /// Binary sum operator, returns `t + u`\n  template <typename T, typename U>\n  __host__ __device__ __forceinline__ auto operator()(T &&t, U &&u) const\n    -> decltype(::cuda::std::forward<T>(t) + ::cuda::std::forward<U>(u))\n  {\n    return ::cuda::std::forward<T>(t) + ::cuda::std::forward<U>(u);\n  }\n};\n\n/// @brief Default difference functor\nstruct Difference\n{\n  /// Binary difference operator, returns `t - u`\n  template <typename T, typename U>\n  __host__ __device__ __forceinline__ auto operator()(T &&t, U &&u) const\n    -> decltype(::cuda::std::forward<T>(t) - ::cuda::std::forward<U>(u))\n  {\n    return ::cuda::std::forward<T>(t) - ::cuda::std::forward<U>(u);\n  }\n};\n\n/// @brief Default division functor\nstruct Division\n{\n  /// Binary division operator, returns `t / u`\n  template <typename T, typename U>\n  __host__ __device__ __forceinline__ auto operator()(T &&t, U &&u) const\n    -> decltype(::cuda::std::forward<T>(t) / ::cuda::std::forward<U>(u))\n  {\n    return ::cuda::std::forward<T>(t) / ::cuda::std::forward<U>(u);\n  }\n};\n#endif\n\n/// @brief Default max functor\nstruct Max\n{\n  /// Boolean max operator, returns `(t > u) ? t : u`\n  template <typename T, typename U>\n  __host__ __device__ __forceinline__\n    typename ::cuda::std::common_type<T, U>::type\n    operator()(T &&t, U &&u) const\n  {\n    return CUB_MAX(t, u);\n  }\n};\n\n/// @brief Arg max functor (keeps the value and offset of the first occurrence\n///        of the larger item)\nstruct ArgMax\n{\n  /// Boolean max operator, preferring the item having the smaller offset in\n  /// case of ties\n  template <typename T, typename OffsetT>\n  __host__ __device__ __forceinline__ KeyValuePair<OffsetT, T>\n  operator()(const KeyValuePair<OffsetT, T> &a,\n             const KeyValuePair<OffsetT, T> &b) const\n  {\n    // Mooch BUG (device reduce argmax gk110 3.2 million random fp32)\n    // return ((b.value > a.value) || \n    //         ((a.value == b.value) && (b.key < a.key))) \n    //      ? b : a;\n\n    if ((b.value > a.value) || ((a.value == b.value) && (b.key < a.key)))\n    {\n      return b;\n    }\n\n    return a;\n  }\n};\n\n/// @brief Default min functor\nstruct Min\n{\n  /// Boolean min operator, returns `(t < u) ? t : u`\n  template <typename T, typename U>\n  __host__ __device__ __forceinline__\n    typename ::cuda::std::common_type<T, U>::type\n    operator()(T &&t, U &&u) const\n  {\n    return CUB_MIN(t, u);\n  }\n};\n\n/// @brief Arg min functor (keeps the value and offset of the first occurrence\n///        of the smallest item)\nstruct ArgMin\n{\n  /// Boolean min operator, preferring the item having the smaller offset in\n  /// case of ties\n  template <typename T, typename OffsetT>\n  __host__ __device__ __forceinline__ KeyValuePair<OffsetT, T>\n  operator()(const KeyValuePair<OffsetT, T> &a,\n             const KeyValuePair<OffsetT, T> &b) const\n  {\n    // Mooch BUG (device reduce argmax gk110 3.2 million random fp32)\n    // return ((b.value < a.value) ||\n    //         ((a.value == b.value) && (b.key < a.key)))\n    //      ? b : a;\n\n    if ((b.value < a.value) || ((a.value == b.value) && (b.key < a.key)))\n    {\n      return b;\n    }\n\n    return a;\n  }\n};\n\nnamespace detail\n{\ntemplate <class OpT>\nstruct basic_binary_op_t\n{\n  static constexpr bool value = false;\n};\n\ntemplate <>\nstruct basic_binary_op_t<Sum>\n{\n  static constexpr bool value = true;\n};\n\ntemplate <>\nstruct basic_binary_op_t<Min>\n{\n  static constexpr bool value = true;\n};\n\ntemplate <>\nstruct basic_binary_op_t<Max>\n{\n  static constexpr bool value = true;\n};\n} // namespace detail\n\n/// @brief Default cast functor\ntemplate <typename B>\nstruct CastOp\n{\n  /// Cast operator, returns `(B) a`\n  template <typename A>\n  __host__ __device__ __forceinline__ B operator()(A &&a) const\n  {\n    return (B)a;\n  }\n};\n\n/// @brief Binary operator wrapper for switching non-commutative scan arguments\ntemplate <typename ScanOp>\nclass SwizzleScanOp\n{\nprivate:\n  /// Wrapped scan operator\n  ScanOp scan_op;\n\npublic:\n  /// Constructor\n  __host__ __device__ __forceinline__ SwizzleScanOp(ScanOp scan_op)\n      : scan_op(scan_op)\n  {}\n\n  /// Switch the scan arguments\n  template <typename T>\n  __host__ __device__ __forceinline__ T operator()(const T &a, const T &b)\n  {\n    T _a(a);\n    T _b(b);\n\n    return scan_op(_b, _a);\n  }\n};\n\n/**\n * @brief Reduce-by-segment functor.\n *\n * Given two cub::KeyValuePair inputs `a` and `b` and a binary associative \n * combining operator `f(const T &x, const T &y)`, an instance of this functor \n * returns a cub::KeyValuePair whose `key` field is `a.key + b.key`, and whose \n * `value` field is either `b.value` if `b.key` is non-zero, or \n * `f(a.value, b.value)` otherwise.\n *\n * ReduceBySegmentOp is an associative, non-commutative binary combining \n * operator for input sequences of cub::KeyValuePair pairings. Such sequences \n * are typically used to represent a segmented set of values to be reduced\n * and a corresponding set of {0,1}-valued integer \"head flags\" demarcating the\n * first value of each segment.\n *\n * @tparam ReductionOpT Binary reduction operator to apply to values\n */\ntemplate <typename ReductionOpT>\nstruct ReduceBySegmentOp\n{\n  /// Wrapped reduction operator\n  ReductionOpT op;\n\n  /// Constructor\n  __host__ __device__ __forceinline__ ReduceBySegmentOp() {}\n\n  /// Constructor\n  __host__ __device__ __forceinline__ ReduceBySegmentOp(ReductionOpT op)\n      : op(op)\n  {}\n\n  /**\n   * @brief Scan operator\n   *\n   * @tparam KeyValuePairT\n   *   KeyValuePair pairing of T (value) and OffsetT (head flag)\n   *\n   * @param[in] first\n   *   First partial reduction\n   *\n   * @param[in] second\n   *   Second partial reduction\n   */\n  template <typename KeyValuePairT>\n  __host__ __device__ __forceinline__ KeyValuePairT\n  operator()(const KeyValuePairT &first, const KeyValuePairT &second)\n  {\n    KeyValuePairT retval;\n    retval.key = first.key + second.key;\n#ifdef _NVHPC_CUDA // WAR bug on nvc++\n    if (second.key)\n    {\n      retval.value = second.value;\n    }\n    else\n    {\n      // If second.value isn't copied into a temporary here, nvc++ will\n      // crash while compiling the TestScanByKeyWithLargeTypes test in\n      // thrust/testing/scan_by_key.cu:\n      auto v2      = second.value;\n      retval.value = op(first.value, v2);\n    }\n#else // not nvc++:\n    // if (second.key) {\n    //   The second partial reduction spans a segment reset, so it's value\n    //   aggregate becomes the running aggregate\n    // else {\n    //   The second partial reduction does not span a reset, so accumulate both\n    //   into the running aggregate\n    // } \n    retval.value = (second.key) ? second.value : op(first.value, second.value);\n#endif\n    return retval;\n  }\n};\n\n/**\n * @tparam ReductionOpT Binary reduction operator to apply to values\n */\ntemplate <typename ReductionOpT>\nstruct ReduceByKeyOp\n{\n  /// Wrapped reduction operator\n  ReductionOpT op;\n\n  /// Constructor\n  __host__ __device__ __forceinline__ ReduceByKeyOp() {}\n\n  /// Constructor\n  __host__ __device__ __forceinline__ ReduceByKeyOp(ReductionOpT op)\n      : op(op)\n  {}\n\n  /**\n   * @brief Scan operator\n   *\n   * @param[in] first First partial reduction\n   * @param[in] second Second partial reduction\n   */\n  template <typename KeyValuePairT>\n  __host__ __device__ __forceinline__ KeyValuePairT\n  operator()(const KeyValuePairT &first, const KeyValuePairT &second)\n  {\n    KeyValuePairT retval = second;\n\n    if (first.key == second.key)\n    {\n      retval.value = op(first.value, retval.value);\n    }\n\n    return retval;\n  }\n};\n\ntemplate <typename BinaryOpT>\nstruct BinaryFlip\n{\n  BinaryOpT binary_op;\n\n  __device__ __host__ explicit BinaryFlip(BinaryOpT binary_op)\n      : binary_op(binary_op)\n  {}\n\n  template <typename T, typename U>\n  __device__ auto\n  operator()(T &&t, U &&u) -> decltype(binary_op(::cuda::std::forward<U>(u),\n                                                 ::cuda::std::forward<T>(t)))\n  {\n    return binary_op(::cuda::std::forward<U>(u), ::cuda::std::forward<T>(t));\n  }\n};\n\ntemplate <typename BinaryOpT>\n__device__ __host__ BinaryFlip<BinaryOpT> MakeBinaryFlip(BinaryOpT binary_op)\n{\n  return BinaryFlip<BinaryOpT>(binary_op);\n}\n\n/** @} */       // end group UtilModule\n\n\nCUB_NAMESPACE_END\n\n#endif // _JITIFY_INCLUDE_GUARD_94E67823631CB18F\n", "../thread/thread_store.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_C77D6E8CBB92E407\n#define _JITIFY_INCLUDE_GUARD_C77D6E8CBB92E407\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * Thread utilities for writing memory using PTX cache modifiers.\n */\n\n#include <cub/config.cuh>\n#include <cub/util_ptx.cuh>\n#include <cub/util_type.cuh>\n\nCUB_NAMESPACE_BEGIN\n\n/**\n * \\addtogroup UtilIo\n * @{\n */\n\n\n//-----------------------------------------------------------------------------\n// Tags and constants\n//-----------------------------------------------------------------------------\n\n/**\n * \\brief Enumeration of cache modifiers for memory store operations.\n */\nenum CacheStoreModifier\n{\n    STORE_DEFAULT,              ///< Default (no modifier)\n    STORE_WB,                   ///< Cache write-back all coherent levels\n    STORE_CG,                   ///< Cache at global level\n    STORE_CS,                   ///< Cache streaming (likely to be accessed once)\n    STORE_WT,                   ///< Cache write-through (to system memory)\n    STORE_VOLATILE,             ///< Volatile shared (any memory space)\n};\n\n\n/**\n * \\name Thread I/O (cache modified)\n * @{\n */\n\n/**\n * \\brief Thread utility for writing memory using cub::CacheStoreModifier cache modifiers.  Can be used to store any data type.\n *\n * \\par Example\n * \\code\n * #include <cub/cub.cuh>   // or equivalently <cub/thread/thread_store.cuh>\n *\n * // 32-bit store using cache-global modifier:\n * int *d_out;\n * int val;\n * cub::ThreadStore<cub::STORE_CG>(d_out + threadIdx.x, val);\n *\n * // 16-bit store using default modifier\n * short *d_out;\n * short val;\n * cub::ThreadStore<cub::STORE_DEFAULT>(d_out + threadIdx.x, val);\n *\n * // 256-bit store using write-through modifier\n * double4 *d_out;\n * double4 val;\n * cub::ThreadStore<cub::STORE_WT>(d_out + threadIdx.x, val);\n *\n * // 96-bit store using cache-streaming cache modifier\n * struct TestFoo { bool a; short b; };\n * TestFoo *d_struct;\n * TestFoo val;\n * cub::ThreadStore<cub::STORE_CS>(d_out + threadIdx.x, val);\n * \\endcode\n *\n * \\tparam MODIFIER             <b>[inferred]</b> CacheStoreModifier enumeration\n * \\tparam InputIteratorT       <b>[inferred]</b> Output iterator type \\iterator\n * \\tparam T                    <b>[inferred]</b> Data type of output value\n */\ntemplate <\n    CacheStoreModifier  MODIFIER,\n    typename            OutputIteratorT,\n    typename            T>\n__device__ __forceinline__ void ThreadStore(OutputIteratorT itr, T val);\n\n\n//@}  end member group\n\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS    // Do not document\n\n\n/// Helper structure for templated store iteration (inductive case)\ntemplate <int COUNT, int MAX>\nstruct IterateThreadStore\n{\n    template <CacheStoreModifier MODIFIER, typename T>\n    static __device__ __forceinline__ void Store(T *ptr, T *vals)\n    {\n        ThreadStore<MODIFIER>(ptr + COUNT, vals[COUNT]);\n        IterateThreadStore<COUNT + 1, MAX>::template Store<MODIFIER>(ptr, vals);\n    }\n\n    template <typename OutputIteratorT, typename T>\n    static __device__ __forceinline__ void Dereference(OutputIteratorT ptr, T *vals)\n    {\n        ptr[COUNT] = vals[COUNT];\n        IterateThreadStore<COUNT + 1, MAX>::Dereference(ptr, vals);\n    }\n\n};\n\n/// Helper structure for templated store iteration (termination case)\ntemplate <int MAX>\nstruct IterateThreadStore<MAX, MAX>\n{\n    template <CacheStoreModifier MODIFIER, typename T>\n    static __device__ __forceinline__ void Store(T * /*ptr*/, T * /*vals*/) {}\n\n    template <typename OutputIteratorT, typename T>\n    static __device__ __forceinline__ void Dereference(OutputIteratorT /*ptr*/, T * /*vals*/) {}\n};\n\n\n/**\n * Define a uint4 (16B) ThreadStore specialization for the given Cache load modifier\n */\n#define _CUB_STORE_16(cub_modifier, ptx_modifier)                                            \\\n    template<>                                                                              \\\n    __device__ __forceinline__ void ThreadStore<cub_modifier, uint4*, uint4>(uint4* ptr, uint4 val)                         \\\n    {                                                                                       \\\n        asm volatile (\"st.\"#ptx_modifier\".v4.u32 [%0], {%1, %2, %3, %4};\" : :               \\\n            _CUB_ASM_PTR_(ptr),                                                             \\\n            \"r\"(val.x),                                                                     \\\n            \"r\"(val.y),                                                                     \\\n            \"r\"(val.z),                                                                     \\\n            \"r\"(val.w));                                                                    \\\n    }                                                                                       \\\n    template<>                                                                              \\\n    __device__ __forceinline__ void ThreadStore<cub_modifier, ulonglong2*, ulonglong2>(ulonglong2* ptr, ulonglong2 val)     \\\n    {                                                                                       \\\n        asm volatile (\"st.\"#ptx_modifier\".v2.u64 [%0], {%1, %2};\" : :                       \\\n            _CUB_ASM_PTR_(ptr),                                                             \\\n            \"l\"(val.x),                                                                     \\\n            \"l\"(val.y));                                                                    \\\n    }\n\n\n/**\n * Define a uint2 (8B) ThreadStore specialization for the given Cache load modifier\n */\n#define _CUB_STORE_8(cub_modifier, ptx_modifier)                                             \\\n    template<>                                                                              \\\n    __device__ __forceinline__ void ThreadStore<cub_modifier, ushort4*, ushort4>(ushort4* ptr, ushort4 val)                 \\\n    {                                                                                       \\\n        asm volatile (\"st.\"#ptx_modifier\".v4.u16 [%0], {%1, %2, %3, %4};\" : :               \\\n            _CUB_ASM_PTR_(ptr),                                                             \\\n            \"h\"(val.x),                                                                     \\\n            \"h\"(val.y),                                                                     \\\n            \"h\"(val.z),                                                                     \\\n            \"h\"(val.w));                                                                    \\\n    }                                                                                       \\\n    template<>                                                                              \\\n    __device__ __forceinline__ void ThreadStore<cub_modifier, uint2*, uint2>(uint2* ptr, uint2 val)                         \\\n    {                                                                                       \\\n        asm volatile (\"st.\"#ptx_modifier\".v2.u32 [%0], {%1, %2};\" : :                       \\\n            _CUB_ASM_PTR_(ptr),                                                             \\\n            \"r\"(val.x),                                                                     \\\n            \"r\"(val.y));                                                                    \\\n    }                                                                                       \\\n    template<>                                                                              \\\n    __device__ __forceinline__ void ThreadStore<cub_modifier, unsigned long long*, unsigned long long>(unsigned long long* ptr, unsigned long long val)     \\\n    {                                                                                       \\\n        asm volatile (\"st.\"#ptx_modifier\".u64 [%0], %1;\" : :                                \\\n            _CUB_ASM_PTR_(ptr),                                                             \\\n            \"l\"(val));                                                                      \\\n    }\n\n/**\n * Define a unsigned int (4B) ThreadStore specialization for the given Cache load modifier\n */\n#define _CUB_STORE_4(cub_modifier, ptx_modifier)                                             \\\n    template<>                                                                              \\\n    __device__ __forceinline__ void ThreadStore<cub_modifier, unsigned int*, unsigned int>(unsigned int* ptr, unsigned int val)                             \\\n    {                                                                                       \\\n        asm volatile (\"st.\"#ptx_modifier\".u32 [%0], %1;\" : :                                \\\n            _CUB_ASM_PTR_(ptr),                                                             \\\n            \"r\"(val));                                                                      \\\n    }\n\n\n/**\n * Define a unsigned short (2B) ThreadStore specialization for the given Cache load modifier\n */\n#define _CUB_STORE_2(cub_modifier, ptx_modifier)                                             \\\n    template<>                                                                              \\\n    __device__ __forceinline__ void ThreadStore<cub_modifier, unsigned short*, unsigned short>(unsigned short* ptr, unsigned short val)                     \\\n    {                                                                                       \\\n        asm volatile (\"st.\"#ptx_modifier\".u16 [%0], %1;\" : :                                \\\n            _CUB_ASM_PTR_(ptr),                                                             \\\n            \"h\"(val));                                                                      \\\n    }\n\n\n/**\n * Define a unsigned char (1B) ThreadStore specialization for the given Cache load modifier\n */\n#define _CUB_STORE_1(cub_modifier, ptx_modifier)                                             \\\n    template<>                                                                              \\\n    __device__ __forceinline__ void ThreadStore<cub_modifier, unsigned char*, unsigned char>(unsigned char* ptr, unsigned char val)                         \\\n    {                                                                                       \\\n        asm volatile (                                                                      \\\n        \"{\"                                                                                 \\\n        \"   .reg .u8 datum;\"                                                                \\\n        \"   cvt.u8.u16 datum, %1;\"                                                          \\\n        \"   st.\"#ptx_modifier\".u8 [%0], datum;\"                                             \\\n        \"}\" : :                                                                             \\\n            _CUB_ASM_PTR_(ptr),                                                             \\\n            \"h\"((unsigned short) val));                                                               \\\n    }\n\n/**\n * Define powers-of-two ThreadStore specializations for the given Cache load modifier\n */\n#define _CUB_STORE_ALL(cub_modifier, ptx_modifier)                                           \\\n    _CUB_STORE_16(cub_modifier, ptx_modifier)                                                \\\n    _CUB_STORE_8(cub_modifier, ptx_modifier)                                                 \\\n    _CUB_STORE_4(cub_modifier, ptx_modifier)                                                 \\\n    _CUB_STORE_2(cub_modifier, ptx_modifier)                                                 \\\n    _CUB_STORE_1(cub_modifier, ptx_modifier)                                                 \\\n\n\n/**\n * Define ThreadStore specializations for the various Cache load modifiers\n */\n_CUB_STORE_ALL(STORE_WB, wb)\n_CUB_STORE_ALL(STORE_CG, cg)\n_CUB_STORE_ALL(STORE_CS, cs)\n_CUB_STORE_ALL(STORE_WT, wt)\n\n// Macro cleanup\n#undef _CUB_STORE_ALL\n#undef _CUB_STORE_1\n#undef _CUB_STORE_2\n#undef _CUB_STORE_4\n#undef _CUB_STORE_8\n#undef _CUB_STORE_16\n\n\n/**\n * ThreadStore definition for STORE_DEFAULT modifier on iterator types\n */\ntemplate <typename OutputIteratorT, typename T>\n__device__ __forceinline__ void ThreadStore(\n    OutputIteratorT             itr,\n    T                           val,\n    Int2Type<STORE_DEFAULT>     /*modifier*/,\n    Int2Type<false>             /*is_pointer*/)\n{\n    *itr = val;\n}\n\n\n/**\n * ThreadStore definition for STORE_DEFAULT modifier on pointer types\n */\ntemplate <typename T>\n__device__ __forceinline__ void ThreadStore(\n    T                           *ptr,\n    T                           val,\n    Int2Type<STORE_DEFAULT>     /*modifier*/,\n    Int2Type<true>              /*is_pointer*/)\n{\n    *ptr = val;\n}\n\n\n/**\n * ThreadStore definition for STORE_VOLATILE modifier on primitive pointer types\n */\ntemplate <typename T>\n__device__ __forceinline__ void ThreadStoreVolatilePtr(\n    T                           *ptr,\n    T                           val,\n    Int2Type<true>              /*is_primitive*/)\n{\n    *reinterpret_cast<volatile T*>(ptr) = val;\n}\n\n\n/**\n * ThreadStore definition for STORE_VOLATILE modifier on non-primitive pointer types\n */\ntemplate <typename T>\n__device__ __forceinline__ void ThreadStoreVolatilePtr(\n    T                           *ptr,\n    T                           val,\n    Int2Type<false>             /*is_primitive*/)\n{\n    // Create a temporary using shuffle-words, then store using volatile-words\n    typedef typename UnitWord<T>::VolatileWord  VolatileWord;\n    typedef typename UnitWord<T>::ShuffleWord   ShuffleWord;\n\n    const int VOLATILE_MULTIPLE = sizeof(T) / sizeof(VolatileWord);\n    const int SHUFFLE_MULTIPLE  = sizeof(T) / sizeof(ShuffleWord);\n\n    VolatileWord words[VOLATILE_MULTIPLE];\n\n    _Pragma(\"unroll\")\n    for (int i = 0; i < SHUFFLE_MULTIPLE; ++i)\n        reinterpret_cast<ShuffleWord*>(words)[i] = reinterpret_cast<ShuffleWord*>(&val)[i];\n\n    IterateThreadStore<0, VOLATILE_MULTIPLE>::template Dereference(\n        reinterpret_cast<volatile VolatileWord*>(ptr),\n        words);\n}\n\n\n/**\n * ThreadStore definition for STORE_VOLATILE modifier on pointer types\n */\ntemplate <typename T>\n__device__ __forceinline__ void ThreadStore(\n    T                           *ptr,\n    T                           val,\n    Int2Type<STORE_VOLATILE>    /*modifier*/,\n    Int2Type<true>              /*is_pointer*/)\n{\n    ThreadStoreVolatilePtr(ptr, val, Int2Type<Traits<T>::PRIMITIVE>());\n}\n\n\n/**\n * ThreadStore definition for generic modifiers on pointer types\n */\ntemplate <typename T, int MODIFIER>\n__device__ __forceinline__ void ThreadStore(\n    T                           *ptr,\n    T                           val,\n    Int2Type<MODIFIER>          /*modifier*/,\n    Int2Type<true>              /*is_pointer*/)\n{\n    // Create a temporary using shuffle-words, then store using device-words\n    typedef typename UnitWord<T>::DeviceWord    DeviceWord;\n    typedef typename UnitWord<T>::ShuffleWord   ShuffleWord;\n\n    const int DEVICE_MULTIPLE   = sizeof(T) / sizeof(DeviceWord);\n    const int SHUFFLE_MULTIPLE  = sizeof(T) / sizeof(ShuffleWord);\n\n    DeviceWord words[DEVICE_MULTIPLE];\n\n    _Pragma(\"unroll\")\n    for (int i = 0; i < SHUFFLE_MULTIPLE; ++i)\n        reinterpret_cast<ShuffleWord*>(words)[i] = reinterpret_cast<ShuffleWord*>(&val)[i];\n\n    IterateThreadStore<0, DEVICE_MULTIPLE>::template Store<CacheStoreModifier(MODIFIER)>(\n        reinterpret_cast<DeviceWord*>(ptr),\n        words);\n}\n\n\n/**\n * ThreadStore definition for generic modifiers\n */\ntemplate <CacheStoreModifier MODIFIER, typename OutputIteratorT, typename T>\n__device__ __forceinline__ void ThreadStore(OutputIteratorT itr, T val)\n{\n    ThreadStore(\n        itr,\n        val,\n        Int2Type<MODIFIER>(),\n        Int2Type<std::is_pointer<OutputIteratorT>::value>());\n}\n\n\n\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\n\n/** @} */       // end group UtilIo\n\n\nCUB_NAMESPACE_END\n\n#endif // _JITIFY_INCLUDE_GUARD_C77D6E8CBB92E407\n", "../tuple": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===--------------------------- tuple ------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX_TUPLE\n#define _LIBCUDACXX_TUPLE\n\n/*\n    tuple synopsis\n\nnamespace std\n{\n\ntemplate <class... T>\nclass tuple {\npublic:\n    explicit(see-below) constexpr tuple();\n    explicit(see-below) tuple(const T&...);  // constexpr in C++14\n    template <class... U>\n        explicit(see-below) tuple(U&&...);  // constexpr in C++14\n    tuple(const tuple&) = default;\n    tuple(tuple&&) = default;\n    template <class... U>\n        explicit(see-below) tuple(const tuple<U...>&);  // constexpr in C++14\n    template <class... U>\n        explicit(see-below) tuple(tuple<U...>&&);  // constexpr in C++14\n    template <class U1, class U2>\n        explicit(see-below) tuple(const pair<U1, U2>&); // iff sizeof...(T) == 2 // constexpr in C++14\n    template <class U1, class U2>\n        explicit(see-below) tuple(pair<U1, U2>&&); // iff sizeof...(T) == 2  // constexpr in C++14\n\n    // allocator-extended constructors\n    template <class Alloc>\n        tuple(allocator_arg_t, const Alloc& a);\n    template <class Alloc>\n        explicit(see-below) tuple(allocator_arg_t, const Alloc& a, const T&...);\n    template <class Alloc, class... U>\n        explicit(see-below) tuple(allocator_arg_t, const Alloc& a, U&&...);\n    template <class Alloc>\n        tuple(allocator_arg_t, const Alloc& a, const tuple&);\n    template <class Alloc>\n        tuple(allocator_arg_t, const Alloc& a, tuple&&);\n    template <class Alloc, class... U>\n        explicit(see-below) tuple(allocator_arg_t, const Alloc& a, const tuple<U...>&);\n    template <class Alloc, class... U>\n        explicit(see-below) tuple(allocator_arg_t, const Alloc& a, tuple<U...>&&);\n    template <class Alloc, class U1, class U2>\n        explicit(see-below) tuple(allocator_arg_t, const Alloc& a, const pair<U1, U2>&);\n    template <class Alloc, class U1, class U2>\n        explicit(see-below) tuple(allocator_arg_t, const Alloc& a, pair<U1, U2>&&);\n\n    tuple& operator=(const tuple&);\n    tuple&\n        operator=(tuple&&) noexcept(AND(is_nothrow_move_assignable<T>::value ...));\n    template <class... U>\n        tuple& operator=(const tuple<U...>&);\n    template <class... U>\n        tuple& operator=(tuple<U...>&&);\n    template <class U1, class U2>\n        tuple& operator=(const pair<U1, U2>&); // iff sizeof...(T) == 2\n    template <class U1, class U2>\n        tuple& operator=(pair<U1, U2>&&); // iff sizeof...(T) == 2\n\n    void swap(tuple&) noexcept(AND(swap(declval<T&>(), declval<T&>())...));\n};\n\ntemplate <class ...T>\ntuple(T...) -> tuple<T...>;                                         // since C++17\ntemplate <class T1, class T2>\ntuple(pair<T1, T2>) -> tuple<T1, T2>;                               // since C++17\ntemplate <class Alloc, class ...T>\ntuple(allocator_arg_t, Alloc, T...) -> tuple<T...>;                 // since C++17\ntemplate <class Alloc, class T1, class T2>\ntuple(allocator_arg_t, Alloc, pair<T1, T2>) -> tuple<T1, T2>;       // since C++17\ntemplate <class Alloc, class ...T>\ntuple(allocator_arg_t, Alloc, tuple<T...>) -> tuple<T...>;          // since C++17\n\ninline constexpr unspecified ignore;\n\ntemplate <class... T> tuple<V...>  make_tuple(T&&...); // constexpr in C++14\ntemplate <class... T> tuple<ATypes...> forward_as_tuple(T&&...) noexcept; // constexpr in C++14\ntemplate <class... T> tuple<T&...> tie(T&...) noexcept; // constexpr in C++14\ntemplate <class... Tuples> tuple<CTypes...> tuple_cat(Tuples&&... tpls); // constexpr in C++14\n\n// [tuple.apply], calling a function with a tuple of arguments:\ntemplate <class F, class Tuple>\n  constexpr decltype(auto) apply(F&& f, Tuple&& t); // C++17\ntemplate <class T, class Tuple>\n  constexpr T make_from_tuple(Tuple&& t); // C++17\n\n// 20.4.1.4, tuple helper classes:\ntemplate <class T> struct tuple_size; // undefined\ntemplate <class... T> struct tuple_size<tuple<T...>>;\ntemplate <class T>\n inline constexpr size_t tuple_size_v = tuple_size<T>::value; // C++17\ntemplate <size_t I, class T> struct tuple_element; // undefined\ntemplate <size_t I, class... T> struct tuple_element<I, tuple<T...>>;\ntemplate <size_t I, class T>\n  using tuple_element_t = typename tuple_element <I, T>::type; // C++14\n\n// 20.4.1.5, element access:\ntemplate <size_t I, class... T>\n    typename tuple_element<I, tuple<T...>>::type&\n    get(tuple<T...>&) noexcept; // constexpr in C++14\ntemplate <size_t I, class... T>\n    const typename tuple_element<I, tuple<T...>>::type&\n    get(const tuple<T...>&) noexcept; // constexpr in C++14\ntemplate <size_t I, class... T>\n    typename tuple_element<I, tuple<T...>>::type&&\n    get(tuple<T...>&&) noexcept; // constexpr in C++14\ntemplate <size_t I, class... T>\n    const typename tuple_element<I, tuple<T...>>::type&&\n    get(const tuple<T...>&&) noexcept; // constexpr in C++14\n\ntemplate <class T1, class... T>\n    constexpr T1& get(tuple<T...>&) noexcept;  // C++14\ntemplate <class T1, class... T>\n    constexpr const T1& get(const tuple<T...>&) noexcept;   // C++14\ntemplate <class T1, class... T>\n    constexpr T1&& get(tuple<T...>&&) noexcept;   // C++14\ntemplate <class T1, class... T>\n    constexpr const T1&& get(const tuple<T...>&&) noexcept;   // C++14\n\n// 20.4.1.6, relational operators:\ntemplate<class... T, class... U> bool operator==(const tuple<T...>&, const tuple<U...>&); // constexpr in C++14\ntemplate<class... T, class... U> bool operator<(const tuple<T...>&, const tuple<U...>&);  // constexpr in C++14\ntemplate<class... T, class... U> bool operator!=(const tuple<T...>&, const tuple<U...>&); // constexpr in C++14\ntemplate<class... T, class... U> bool operator>(const tuple<T...>&, const tuple<U...>&);  // constexpr in C++14\ntemplate<class... T, class... U> bool operator<=(const tuple<T...>&, const tuple<U...>&); // constexpr in C++14\ntemplate<class... T, class... U> bool operator>=(const tuple<T...>&, const tuple<U...>&); // constexpr in C++14\n\ntemplate <class... Types, class Alloc>\n  struct uses_allocator<tuple<Types...>, Alloc>;\n\ntemplate <class... Types>\n  void\n  swap(tuple<Types...>& x, tuple<Types...>& y) noexcept(noexcept(x.swap(y)));\n\n}  // std\n\n*/\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"__assert\" // all public C++ headers provide the assertion handler\n#include \"__functional_base\"\n#include \"__functional/unwrap_ref.h\"\n#include \"__fwd/array.h\"\n#include \"__type_traits/maybe_const.h\"\n#include \"__tuple_dir/apply_cv.h\"\n#include \"__tuple_dir/make_tuple_types.h\"\n#include \"__tuple_dir/sfinae_helpers.h\"\n#include \"__tuple_dir/structured_bindings.h\"\n#include \"__tuple_dir/tuple_element.h\"\n#include \"__tuple_dir/tuple_indices.h\"\n#include \"__tuple_dir/tuple_like.h\"\n#include \"__tuple_dir/tuple_size.h\"\n#include \"__tuple_dir/tuple_types.h\"\n#include \"__utility/forward.h\"\n#include \"__utility/integer_sequence.h\"\n#include \"__utility/move.h\"\n#include \"__utility/pair.h\"\n#include \"__utility/piecewise_construct.h\"\n#include \"__utility/swap.h\"\n#include \"climits\"\n#include \"cstddef\"\n#include \"cstdint\"\n#include \"utility\"\n#include \"type_traits\"\n\n// standard-mandated includes\n#include \"version\"\n\n// [tuple.syn]\n#ifndef _LIBCUDACXX_HAS_NO_SPACESHIP_OPERATOR\n#include \"compare\"\n#endif\n\n#ifndef __cuda_std__\n#include <__pragma_push>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n\n// __tuple_leaf\n\ntemplate <size_t _Ip, class _Hp,\n          bool=is_empty<_Hp>::value && !__libcpp_is_final<_Hp>::value\n         >\nclass __tuple_leaf;\n\ntemplate <size_t _Ip, class _Hp, bool _Ep>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nvoid swap(__tuple_leaf<_Ip, _Hp, _Ep>& __x, __tuple_leaf<_Ip, _Hp, _Ep>& __y)\n    noexcept(__is_nothrow_swappable<_Hp>::value)\n{\n    swap(__x.get(), __y.get());\n}\n\ntemplate <size_t _Ip, class _Hp, bool>\nclass __tuple_leaf\n{\n    _Hp __value_;\n\n    template <class _Tp>\n    _LIBCUDACXX_INLINE_VISIBILITY\n    static constexpr bool __can_bind_reference() {\n#if __has_keyword(__reference_binds_to_temporary)\n      return !__reference_binds_to_temporary(_Hp, _Tp);\n#else\n      return true;\n#endif\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY __tuple_leaf& operator=(const __tuple_leaf&);\npublic:\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr __tuple_leaf()\n             noexcept(is_nothrow_default_constructible<_Hp>::value) : __value_()\n       {static_assert(!is_reference<_Hp>::value,\n              \"Attempted to default construct a reference element in a tuple\");}\n\n    template <class _Alloc>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        __tuple_leaf(integral_constant<int, 0>, const _Alloc&)\n            : __value_()\n        {static_assert(!is_reference<_Hp>::value,\n              \"Attempted to default construct a reference element in a tuple\");}\n\n    template <class _Alloc>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        __tuple_leaf(integral_constant<int, 1>, const _Alloc& __a)\n            : __value_(allocator_arg_t(), __a)\n        {static_assert(!is_reference<_Hp>::value,\n              \"Attempted to default construct a reference element in a tuple\");}\n\n    template <class _Alloc>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        __tuple_leaf(integral_constant<int, 2>, const _Alloc& __a)\n            : __value_(__a)\n        {static_assert(!is_reference<_Hp>::value,\n              \"Attempted to default construct a reference element in a tuple\");}\n\n    template<class _Tp>\n    using __can_forward = _And<_IsNotSame<__remove_cvref_t<_Tp>, __tuple_leaf>,\n                               is_constructible<_Hp, _Tp>>;\n\n    template <class _Tp,\n              __enable_if_t<__can_forward<_Tp>::value, int> = 0>\n        _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n        explicit __tuple_leaf(_Tp&& __t) noexcept((is_nothrow_constructible<_Hp, _Tp>::value))\n            : __value_(_CUDA_VSTD::forward<_Tp>(__t))\n        {static_assert(__can_bind_reference<_Tp&&>(),\n       \"Attempted construction of reference element binds to a temporary whose lifetime has ended\");}\n\n    template <class _Tp, class _Alloc>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        explicit __tuple_leaf(integral_constant<int, 0>, const _Alloc&, _Tp&& __t)\n            : __value_(_CUDA_VSTD::forward<_Tp>(__t))\n        {static_assert(__can_bind_reference<_Tp&&>(),\n       \"Attempted construction of reference element binds to a temporary whose lifetime has ended\");}\n\n    template <class _Tp, class _Alloc>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        explicit __tuple_leaf(integral_constant<int, 1>, const _Alloc& __a, _Tp&& __t)\n            : __value_(allocator_arg_t(), __a, _CUDA_VSTD::forward<_Tp>(__t))\n        {static_assert(!is_reference<_Hp>::value,\n            \"Attempted to uses-allocator construct a reference element in a tuple\");}\n\n    template <class _Tp, class _Alloc>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        explicit __tuple_leaf(integral_constant<int, 2>, const _Alloc& __a, _Tp&& __t)\n            : __value_(_CUDA_VSTD::forward<_Tp>(__t), __a)\n        {static_assert(!is_reference<_Hp>::value,\n           \"Attempted to uses-allocator construct a reference element in a tuple\");}\n\n    __tuple_leaf(const __tuple_leaf& __t) = default;\n    __tuple_leaf(__tuple_leaf&& __t) = default;\n\n    template <class _Tp>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        __tuple_leaf&\n        operator=(_Tp&& __t) noexcept((is_nothrow_assignable<_Hp&, _Tp>::value))\n        {\n            __value_ = _CUDA_VSTD::forward<_Tp>(__t);\n            return *this;\n        }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    int swap(__tuple_leaf& __t) noexcept(__is_nothrow_swappable<__tuple_leaf>::value)\n    {\n        _CUDA_VSTD::swap(*this, __t);\n        return 0;\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11       _Hp& get()       noexcept {return __value_;}\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 const _Hp& get() const noexcept {return __value_;}\n};\n\ntemplate <size_t _Ip, class _Hp>\nclass __tuple_leaf<_Ip, _Hp, true>\n    : private _Hp\n{\n\n    _LIBCUDACXX_INLINE_VISIBILITY __tuple_leaf& operator=(const __tuple_leaf&);\npublic:\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr __tuple_leaf()\n             noexcept(is_nothrow_default_constructible<_Hp>::value) {}\n\n    template <class _Alloc>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        __tuple_leaf(integral_constant<int, 0>, const _Alloc&) {}\n\n    template <class _Alloc>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        __tuple_leaf(integral_constant<int, 1>, const _Alloc& __a)\n            : _Hp(allocator_arg_t(), __a) {}\n\n    template <class _Alloc>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        __tuple_leaf(integral_constant<int, 2>, const _Alloc& __a)\n            : _Hp(__a) {}\n\n    template<class _Tp>\n    using __can_forward = _And<_IsNotSame<__remove_cvref_t<_Tp>, __tuple_leaf>,\n                               is_constructible<_Hp, _Tp>>;\n\n    template <class _Tp,\n              __enable_if_t<__can_forward<_Tp>::value, int> = 0>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    explicit __tuple_leaf(_Tp&& __t) noexcept((is_nothrow_constructible<_Hp, _Tp>::value))\n        : _Hp(_CUDA_VSTD::forward<_Tp>(__t)) {}\n\n    template <class _Tp, class _Alloc>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        explicit __tuple_leaf(integral_constant<int, 0>, const _Alloc&, _Tp&& __t)\n            : _Hp(_CUDA_VSTD::forward<_Tp>(__t)) {}\n\n    template <class _Tp, class _Alloc>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        explicit __tuple_leaf(integral_constant<int, 1>, const _Alloc& __a, _Tp&& __t)\n            : _Hp(allocator_arg_t(), __a, _CUDA_VSTD::forward<_Tp>(__t)) {}\n\n    template <class _Tp, class _Alloc>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        explicit __tuple_leaf(integral_constant<int, 2>, const _Alloc& __a, _Tp&& __t)\n            : _Hp(_CUDA_VSTD::forward<_Tp>(__t), __a) {}\n\n    __tuple_leaf(__tuple_leaf const &) = default;\n    __tuple_leaf(__tuple_leaf &&) = default;\n\n    template <class _Tp>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        __tuple_leaf&\n        operator=(_Tp&& __t) noexcept((is_nothrow_assignable<_Hp&, _Tp>::value))\n        {\n            _Hp::operator=(_CUDA_VSTD::forward<_Tp>(__t));\n            return *this;\n        }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    int\n    swap(__tuple_leaf& __t) noexcept(__is_nothrow_swappable<__tuple_leaf>::value)\n    {\n        _CUDA_VSTD::swap(*this, __t);\n        return 0;\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11       _Hp& get()       noexcept {return static_cast<_Hp&>(*this);}\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 const _Hp& get() const noexcept {return static_cast<const _Hp&>(*this);}\n};\n\ntemplate <class ..._Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\nvoid __swallow(_Tp&&...) noexcept {}\n\ntemplate <class _Tp>\nstruct __all_default_constructible;\n\ntemplate <class ..._Tp>\nstruct __all_default_constructible<__tuple_types<_Tp...>>\n    : __all<is_default_constructible<_Tp>::value...>\n{ };\n\n// __tuple_impl\n\ntemplate<class _Indx, class ..._Tp> struct __tuple_impl;\n\ntemplate<size_t ..._Indx, class ..._Tp>\nstruct _LIBCUDACXX_DECLSPEC_EMPTY_BASES __tuple_impl<__tuple_indices<_Indx...>, _Tp...>\n    : public __tuple_leaf<_Indx, _Tp>...\n{\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr __tuple_impl()\n        noexcept(__all<is_nothrow_default_constructible<_Tp>::value...>::value) {}\n\n\n    // Handle non-allocator, full initialization\n    template <size_t ..._Uf, class ..._Tf,\n              size_t ..._Ul, class ..._Tl, class ..._Up,\n              _EnableIf<sizeof...(_Up) == sizeof...(_Tp), bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n        explicit\n        __tuple_impl(__tuple_indices<_Uf...>, __tuple_types<_Tf...>,\n                     __tuple_indices<_Ul...>, __tuple_types<_Tl...>,\n                     _Up&&... __u)\n                     noexcept((__all<is_nothrow_constructible<_Tf, _Up>::value...>::value)) :\n            __tuple_leaf<_Uf, _Tf>(_CUDA_VSTD::forward<_Up>(__u))...\n            {}\n\n    // Handle non-allocator, partial default initialization\n    template <size_t ..._Uf, class ..._Tf,\n              size_t ..._Ul, class ..._Tl, class ..._Up,\n              _EnableIf<(sizeof...(_Up) > 0) && (sizeof...(_Up) < sizeof...(_Tp)), bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n        explicit\n        __tuple_impl(__tuple_indices<_Uf...>, __tuple_types<_Tf...>,\n                     __tuple_indices<_Ul...>, __tuple_types<_Tl...>,\n                     _Up&&... __u)\n                     noexcept((__all<is_nothrow_constructible<_Tf, _Up>::value...>::value &&\n                                 __all<is_nothrow_default_constructible<_Tl>::value...>::value)) :\n            __tuple_leaf<_Uf, _Tf>(_CUDA_VSTD::forward<_Up>(__u))...,\n            __tuple_leaf<_Ul, _Tl>()...\n            {}\n\n    // Handle non-allocator, full default initialization\n    template <size_t ..._Uf, class ..._Tf,\n              size_t ..._Ul, class ..._Tl, class ..._Up,\n              _EnableIf<sizeof...(_Up) == 0, bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n        explicit\n        __tuple_impl(__tuple_indices<_Uf...>, __tuple_types<_Tf...>,\n                     __tuple_indices<_Ul...>, __tuple_types<_Tl...>,\n                     _Up&&...)\n                     noexcept((__all<is_nothrow_default_constructible<_Tl>::value...>::value)) :\n            __tuple_leaf<_Ul, _Tl>()...\n            {}\n\n\n    // Handle allocator aware, full initialization\n    template <class _Alloc, size_t ..._Uf, class ..._Tf,\n              size_t ..._Ul, class ..._Tl, class ..._Up,\n              _EnableIf<sizeof...(_Up) == sizeof...(_Tp), bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        explicit\n        __tuple_impl(allocator_arg_t, const _Alloc& __a,\n                     __tuple_indices<_Uf...>, __tuple_types<_Tf...>,\n                     __tuple_indices<_Ul...>, __tuple_types<_Tl...>,\n                     _Up&&... __u) :\n            __tuple_leaf<_Uf, _Tf>(__uses_alloc_ctor<_Tf, _Alloc, _Up>(), __a,\n                    _CUDA_VSTD::forward<_Up>(__u))...\n            {}\n\n    // Handle allocator aware, partial default initialization\n    template <class _Alloc, size_t ..._Uf, class ..._Tf,\n              size_t ..._Ul, class ..._Tl, class ..._Up,\n              _EnableIf<(sizeof...(_Up) > 0) && (sizeof...(_Up) < sizeof...(_Tp)), bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        explicit\n        __tuple_impl(allocator_arg_t, const _Alloc& __a,\n                     __tuple_indices<_Uf...>, __tuple_types<_Tf...>,\n                     __tuple_indices<_Ul...>, __tuple_types<_Tl...>,\n                     _Up&&... __u) :\n            __tuple_leaf<_Uf, _Tf>(__uses_alloc_ctor<_Tf, _Alloc, _Up>(), __a,\n                    _CUDA_VSTD::forward<_Up>(__u))...,\n            __tuple_leaf<_Ul, _Tl>(__uses_alloc_ctor<_Tl, _Alloc>(), __a)...\n            {}\n\n    // Handle allocator aware, full default initialization\n    template <class _Alloc, size_t ..._Uf, class ..._Tf,\n              size_t ..._Ul, class ..._Tl, class ..._Up,\n              _EnableIf<sizeof...(_Up) == 0, bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        explicit\n        __tuple_impl(allocator_arg_t, const _Alloc& __a,\n                     __tuple_indices<_Uf...>, __tuple_types<_Tf...>,\n                     __tuple_indices<_Ul...>, __tuple_types<_Tl...>,\n                     _Up&&...) :\n            __tuple_leaf<_Ul, _Tl>(__uses_alloc_ctor<_Tl, _Alloc>(), __a)...\n            {}\n\n    template <class _Tuple,\n              _EnableIf<\n                    __tuple_constructible<_Tuple, tuple<_Tp...> >::value,\n                    bool\n                > = false\n             >\n        _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n        __tuple_impl(_Tuple&& __t) noexcept((__all<is_nothrow_constructible<_Tp, typename tuple_element<_Indx,\n                                       typename __make_tuple_types<_Tuple>::type>::type>::value...>::value))\n            : __tuple_leaf<_Indx, _Tp>(_CUDA_VSTD::forward<typename tuple_element<_Indx,\n                                       typename __make_tuple_types<_Tuple>::type>::type>(_CUDA_VSTD::get<_Indx>(__t)))...\n            {}\n\n    template <class _Alloc, class _Tuple,\n              _EnableIf<\n                    __tuple_constructible<_Tuple, tuple<_Tp...> >::value,\n                    bool\n                > = false\n             >\n        _LIBCUDACXX_INLINE_VISIBILITY\n        __tuple_impl(allocator_arg_t, const _Alloc& __a, _Tuple&& __t)\n            : __tuple_leaf<_Indx, _Tp>(__uses_alloc_ctor<_Tp, _Alloc, typename tuple_element<_Indx,\n                                       typename __make_tuple_types<_Tuple>::type>::type>(), __a,\n                                       _CUDA_VSTD::forward<typename tuple_element<_Indx,\n                                       typename __make_tuple_types<_Tuple>::type>::type>(_CUDA_VSTD::get<_Indx>(__t)))...\n            {}\n\n    template <class _Tuple>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        typename enable_if\n        <\n            __tuple_assignable<_Tuple, tuple<_Tp...> >::value,\n            __tuple_impl&\n        >::type\n        operator=(_Tuple&& __t) noexcept((__all<is_nothrow_assignable<_Tp&, typename tuple_element<_Indx,\n                                       typename __make_tuple_types<_Tuple>::type>::type>::value...>::value))\n        {\n            __swallow(__tuple_leaf<_Indx, _Tp>::operator=(_CUDA_VSTD::forward<typename tuple_element<_Indx,\n                                       typename __make_tuple_types<_Tuple>::type>::type>(_CUDA_VSTD::get<_Indx>(__t)))...);\n            return *this;\n        }\n\n    __tuple_impl(const __tuple_impl&) = default;\n    __tuple_impl(__tuple_impl&&) = default;\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    __tuple_impl&\n    operator=(const __tuple_impl& __t) noexcept((__all<is_nothrow_copy_assignable<_Tp>::value...>::value))\n    {\n        __swallow(__tuple_leaf<_Indx, _Tp>::operator=(static_cast<const __tuple_leaf<_Indx, _Tp>&>(__t).get())...);\n        return *this;\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    __tuple_impl&\n    operator=(__tuple_impl&& __t) noexcept((__all<is_nothrow_move_assignable<_Tp>::value...>::value))\n    {\n        __swallow(__tuple_leaf<_Indx, _Tp>::operator=(_CUDA_VSTD::forward<_Tp>(static_cast<__tuple_leaf<_Indx, _Tp>&>(__t).get()))...);\n        return *this;\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    void swap(__tuple_impl& __t)\n        noexcept(__all<__is_nothrow_swappable<_Tp>::value...>::value)\n    {\n        __swallow(__tuple_leaf<_Indx, _Tp>::swap(static_cast<__tuple_leaf<_Indx, _Tp>&>(__t))...);\n    }\n};\n\n\n\ntemplate <class ..._Tp>\nclass _LIBCUDACXX_TEMPLATE_VIS tuple\n{\n    typedef __tuple_impl<typename __make_tuple_indices<sizeof...(_Tp)>::type, _Tp...> _BaseT;\n\n    _BaseT __base_;\n\n#if defined(_LIBCUDACXX_ENABLE_TUPLE_IMPLICIT_REDUCED_ARITY_EXTENSION)\n    static constexpr bool _EnableImplicitReducedArityExtension = true;\n#else\n    static constexpr bool _EnableImplicitReducedArityExtension = false;\n#endif\n\n    template <class ..._Args>\n    struct _PackExpandsToThisTuple : false_type {};\n\n    template <class _Arg>\n    struct _PackExpandsToThisTuple<_Arg>\n        : is_same<__remove_cvref_t<_Arg>, tuple> {};\n\n    template <bool _MaybeEnable, class _Dummy = void>\n    struct _CheckArgsConstructor : __check_tuple_constructor_fail {};\n\n    template <class _Dummy>\n    struct _CheckArgsConstructor<true, _Dummy>\n    {\n        template <int&...>\n        struct __enable_implicit_default {\n            static constexpr bool value =\n                __all<__is_implicitly_default_constructible<_Tp>::value...>::value;\n        };\n\n        template <int&...>\n        struct __enable_explicit_default {\n            static constexpr bool value =\n                __all<is_default_constructible<_Tp>::value ...>::value &&\n                !__enable_implicit_default<>::value;\n        };\n\n        template <class ..._Args>\n        struct __enable_explicit {\n            static constexpr bool value =\n                __tuple_constructible<\n                    tuple<_Args...>,\n                    typename __make_tuple_types<tuple,\n                             sizeof...(_Args) < sizeof...(_Tp) ?\n                                 sizeof...(_Args) :\n                                 sizeof...(_Tp)>::type\n                >::value &&\n                !__tuple_convertible<\n                    tuple<_Args...>,\n                    typename __make_tuple_types<tuple,\n                             sizeof...(_Args) < sizeof...(_Tp) ?\n                                 sizeof...(_Args) :\n                                 sizeof...(_Tp)>::type\n                >::value &&\n                __all_default_constructible<\n                    typename __make_tuple_types<tuple, sizeof...(_Tp),\n                             sizeof...(_Args) < sizeof...(_Tp) ?\n                                 sizeof...(_Args) :\n                                 sizeof...(_Tp)>::type\n                >::value;\n        };\n\n        template <class ..._Args>\n        struct __enable_implicit {\n            static constexpr bool value =\n               __tuple_constructible<\n                    tuple<_Args...>,\n                    typename __make_tuple_types<tuple,\n                             sizeof...(_Args) < sizeof...(_Tp) ?\n                                 sizeof...(_Args) :\n                                 sizeof...(_Tp)>::type\n                >::value &&\n                __tuple_convertible<\n                    tuple<_Args...>,\n                    typename __make_tuple_types<tuple,\n                             sizeof...(_Args) < sizeof...(_Tp) ?\n                                 sizeof...(_Args) :\n                                 sizeof...(_Tp)>::type\n                >::value &&\n                __all_default_constructible<\n                    typename __make_tuple_types<tuple, sizeof...(_Tp),\n                             sizeof...(_Args) < sizeof...(_Tp) ?\n                                 sizeof...(_Args) :\n                                 sizeof...(_Tp)>::type\n                >::value;\n        };\n    };\n\n    template <bool _MaybeEnable,\n              bool = sizeof...(_Tp) == 1,\n              class _Dummy = void>\n    struct _CheckTupleLikeConstructor : __check_tuple_constructor_fail {};\n\n    template <class _Dummy>\n    struct _CheckTupleLikeConstructor<true, false, _Dummy>\n    {\n        template <class _Tuple>\n        struct __enable_implicit {\n            static constexpr bool value =\n                   __tuple_constructible<_Tuple, tuple>::value\n                && __tuple_convertible<_Tuple, tuple>::value;\n        };\n\n        template <class _Tuple>\n        struct __enable_explicit {\n            static constexpr bool value =\n                   __tuple_constructible<_Tuple, tuple>::value\n               && !__tuple_convertible<_Tuple, tuple>::value;\n        };\n    };\n\n    template <class _Dummy>\n    struct _CheckTupleLikeConstructor<true, true, _Dummy>\n    {\n        template <class _Tuple>\n        struct _PreferTupleLikeConstructorImpl :\n            _Or<\n                // Don't attempt the two checks below if the tuple we are given\n                // has the same type as this tuple.\n                _IsSame<__remove_cvref_t<_Tuple>, tuple>,\n                _Lazy<_And,\n                    _Not<is_constructible<_Tp..., _Tuple>>,\n                    _Not<is_convertible<_Tuple, _Tp...>>\n                >\n            >\n        { };\n\n        // This trait is used to disable the tuple-like constructor when\n        // the UTypes... constructor should be selected instead.\n        // See LWG issue #2549.\n        template <class _Tuple>\n        using _PreferTupleLikeConstructor = _PreferTupleLikeConstructorImpl<_Tuple>;\n\n        template <class _Tuple>\n        struct __enable_implicit {\n            static constexpr bool value =\n                _And<\n                    __tuple_constructible<_Tuple, tuple>,\n                    __tuple_convertible<_Tuple, tuple>,\n                    _PreferTupleLikeConstructor<_Tuple>\n                >::value;\n        };\n\n        template <class _Tuple>\n        struct __enable_explicit {\n            static constexpr bool value =\n                _And<\n                    __tuple_constructible<_Tuple, tuple>,\n                    _PreferTupleLikeConstructor<_Tuple>,\n                    _Not<__tuple_convertible<_Tuple, tuple>>\n                >::value;\n        };\n    };\n\n    template <size_t _Jp, class ..._Up> friend\n        _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n        typename tuple_element<_Jp, tuple<_Up...> >::type& get(tuple<_Up...>&) noexcept;\n    template <size_t _Jp, class ..._Up> friend\n        _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n        const typename tuple_element<_Jp, tuple<_Up...> >::type& get(const tuple<_Up...>&) noexcept;\n    template <size_t _Jp, class ..._Up> friend\n        _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n        typename tuple_element<_Jp, tuple<_Up...> >::type&& get(tuple<_Up...>&&) noexcept;\n    template <size_t _Jp, class ..._Up> friend\n        _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n        const typename tuple_element<_Jp, tuple<_Up...> >::type&& get(const tuple<_Up...>&&) noexcept;\npublic:\n\n    template<bool _Dummy>\n    struct _EnableConstructor {\n        static constexpr bool __implicit_default =\n            _CheckArgsConstructor<_Dummy>::template __enable_implicit_default<>::value;\n        static constexpr bool __explicit_default =\n            _CheckArgsConstructor<_Dummy>::template __enable_explicit_default<>::value;\n\n        static constexpr bool __implicit_variadic =\n            _CheckArgsConstructor<_Dummy>::template __enable_implicit<_Tp const&...>::value;\n        static constexpr bool __explicit_variadic =\n            _CheckArgsConstructor<_Dummy>::template __enable_explicit<_Tp const&...>::value;\n    };\n\n    template <bool _Dummy = true, __enable_if_t<_EnableConstructor<_Dummy>::__implicit_default, bool> = false>\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    tuple() noexcept(__all<is_nothrow_default_constructible<_Tp>::value...>::value) {}\n\n    template <bool _Dummy = true, __enable_if_t<_EnableConstructor<_Dummy>::__explicit_default, bool> = false>\n    explicit _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    tuple() noexcept(__all<is_nothrow_default_constructible<_Tp>::value...>::value) {}\n\n    tuple(tuple const&) = default;\n    tuple(tuple&&) = default;\n\n    template <class _AllocArgT>\n    struct _EnableAllocConstructor {\n        static constexpr bool __implicit =\n            _CheckArgsConstructor<_IsSame<allocator_arg_t, _AllocArgT>::value >::template __enable_implicit_default<>::value;\n        static constexpr bool __explicit =\n            _CheckArgsConstructor<_IsSame<allocator_arg_t, _AllocArgT>::value >::template __enable_explicit_default<>::value;\n    };\n\n    template <class _AllocArgT, class _Alloc,\n              __enable_if_t<_EnableAllocConstructor<_AllocArgT>::__implicit, bool> = false>\n    _LIBCUDACXX_INLINE_VISIBILITY\n    tuple(_AllocArgT, _Alloc const& __a)\n      : __base_(allocator_arg_t(), __a,\n                    __tuple_indices<>(), __tuple_types<>(),\n                    typename __make_tuple_indices<sizeof...(_Tp), 0>::type(),\n                    __tuple_types<_Tp...>()) {}\n\n    template <class _AllocArgT, class _Alloc,\n              __enable_if_t<_EnableAllocConstructor<_AllocArgT>::__explicit, bool> = false>\n    explicit _LIBCUDACXX_INLINE_VISIBILITY\n    tuple(_AllocArgT, _Alloc const& __a)\n      : __base_(allocator_arg_t(), __a,\n                    __tuple_indices<>(), __tuple_types<>(),\n                    typename __make_tuple_indices<sizeof...(_Tp), 0>::type(),\n                    __tuple_types<_Tp...>()) {}\n\n    template <bool _Dummy = true, __enable_if_t<_EnableConstructor<_Dummy>::__implicit_variadic, bool> = false>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    tuple(const _Tp& ... __t) noexcept((__all<is_nothrow_copy_constructible<_Tp>::value...>::value))\n        : __base_(typename __make_tuple_indices<sizeof...(_Tp)>::type(),\n                typename __make_tuple_types<tuple, sizeof...(_Tp)>::type(),\n                typename __make_tuple_indices<0>::type(),\n                typename __make_tuple_types<tuple, 0>::type(),\n                __t...\n               ) {}\n\n    template <bool _Dummy = true, __enable_if_t<_EnableConstructor<_Dummy>::__explicit_variadic, bool> = false>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    explicit tuple(const _Tp& ... __t) noexcept((__all<is_nothrow_copy_constructible<_Tp>::value...>::value))\n        : __base_(typename __make_tuple_indices<sizeof...(_Tp)>::type(),\n                typename __make_tuple_types<tuple, sizeof...(_Tp)>::type(),\n                typename __make_tuple_indices<0>::type(),\n                typename __make_tuple_types<tuple, 0>::type(),\n                __t...\n               ) {}\n\n    template <class _Alloc, bool _Dummy = true, __enable_if_t<_EnableConstructor<_Dummy>::__implicit_variadic, bool> = false>\n      _LIBCUDACXX_INLINE_VISIBILITY\n      tuple(allocator_arg_t, const _Alloc& __a, const _Tp& ... __t)\n        : __base_(allocator_arg_t(), __a,\n                typename __make_tuple_indices<sizeof...(_Tp)>::type(),\n                typename __make_tuple_types<tuple, sizeof...(_Tp)>::type(),\n                typename __make_tuple_indices<0>::type(),\n                typename __make_tuple_types<tuple, 0>::type(),\n                __t...\n               ) {}\n\n    template <class _Alloc, bool _Dummy = true, __enable_if_t<_EnableConstructor<_Dummy>::__explicit_variadic, bool> = false>\n      _LIBCUDACXX_INLINE_VISIBILITY\n      explicit\n      tuple(allocator_arg_t, const _Alloc& __a, const _Tp& ... __t)\n        : __base_(allocator_arg_t(), __a,\n                typename __make_tuple_indices<sizeof...(_Tp)>::type(),\n                typename __make_tuple_types<tuple, sizeof...(_Tp)>::type(),\n                typename __make_tuple_indices<0>::type(),\n                typename __make_tuple_types<tuple, 0>::type(),\n                __t...\n               ) {}\n\n#if defined(_LIBCUDACXX_NO_TUPLE_NOEXCEPT)\n    template <class ..._Vp>\n    using __base_noexcept_constructible = false_type;\n#else\n    template <class ..._Vp>\n    using __base_noexcept_constructible = is_nothrow_constructible<\n                _BaseT,\n                _Vp...\n            >;\n#endif // defined(_LIBCUDACXX_NO_TUPLE_NOEXCEPT)\n\n    template <class ..._Up>\n    struct _EnableVariadicConvertingConstructor {\n        static constexpr bool _PackIsTuple = _PackExpandsToThisTuple<_Up...>::value;\n        static constexpr bool __exact_match = sizeof...(_Up) == sizeof...(_Tp) && !_PackIsTuple;\n        static constexpr bool __less_arity  = sizeof...(_Up) < sizeof...(_Tp) && !_PackIsTuple;\n\n        static constexpr bool __implicit =\n            _CheckArgsConstructor<__exact_match>::template __enable_implicit<_Up...>::value ||\n            _CheckArgsConstructor<__less_arity && _EnableImplicitReducedArityExtension >::template __enable_implicit<_Up...>::value;\n        static constexpr bool __explicit =\n            _CheckArgsConstructor<__exact_match>::template __enable_explicit<_Up...>::value ||\n            _CheckArgsConstructor<__less_arity && !_EnableImplicitReducedArityExtension>::template __enable_implicit<_Up...>::value;\n\n        static constexpr bool __implicit_alloc =\n            _CheckArgsConstructor<__exact_match>::template __enable_implicit<_Up...>::value;\n        static constexpr bool __explicit_alloc =\n            _CheckArgsConstructor<__exact_match>::template __enable_explicit<_Up...>::value;\n    };\n    template <class ..._Up,\n              __enable_if_t<_EnableVariadicConvertingConstructor<_Up...>::__implicit, bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n        tuple(_Up&&... __u)\n            noexcept((\n                __base_noexcept_constructible<\n                    typename __make_tuple_indices<sizeof...(_Up)>::type,\n                    typename __make_tuple_types<tuple, sizeof...(_Up)>::type,\n                    typename __make_tuple_indices<sizeof...(_Tp), sizeof...(_Up)>::type,\n                    typename __make_tuple_types<tuple, sizeof...(_Tp), sizeof...(_Up)>::type,\n                    _Up...\n                >::value\n            ))\n            : __base_(typename __make_tuple_indices<sizeof...(_Up)>::type(),\n                    typename __make_tuple_types<tuple, sizeof...(_Up)>::type(),\n                    typename __make_tuple_indices<sizeof...(_Tp), sizeof...(_Up)>::type(),\n                    typename __make_tuple_types<tuple, sizeof...(_Tp), sizeof...(_Up)>::type(),\n                    _CUDA_VSTD::forward<_Up>(__u)...) {}\n\n    template <class ..._Up,\n              __enable_if_t<_EnableVariadicConvertingConstructor<_Up...>::__explicit, bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n        explicit\n        tuple(_Up&&... __u)\n            noexcept((\n                __base_noexcept_constructible<\n                    typename __make_tuple_indices<sizeof...(_Up)>::type,\n                    typename __make_tuple_types<tuple, sizeof...(_Up)>::type,\n                    typename __make_tuple_indices<sizeof...(_Tp), sizeof...(_Up)>::type,\n                    typename __make_tuple_types<tuple, sizeof...(_Tp), sizeof...(_Up)>::type,\n                    _Up...\n                >::value\n            ))\n            : __base_(typename __make_tuple_indices<sizeof...(_Up)>::type(),\n                    typename __make_tuple_types<tuple, sizeof...(_Up)>::type(),\n                    typename __make_tuple_indices<sizeof...(_Tp), sizeof...(_Up)>::type(),\n                    typename __make_tuple_types<tuple, sizeof...(_Tp), sizeof...(_Up)>::type(),\n                    _CUDA_VSTD::forward<_Up>(__u)...) {}\n\n    template <class _Alloc, class ..._Up,\n              __enable_if_t<_EnableVariadicConvertingConstructor<_Up...>::__implicit_alloc, bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        tuple(allocator_arg_t, const _Alloc& __a, _Up&&... __u)\n            : __base_(allocator_arg_t(), __a,\n                    typename __make_tuple_indices<sizeof...(_Up)>::type(),\n                    typename __make_tuple_types<tuple, sizeof...(_Up)>::type(),\n                    typename __make_tuple_indices<sizeof...(_Tp), sizeof...(_Up)>::type(),\n                    typename __make_tuple_types<tuple, sizeof...(_Tp), sizeof...(_Up)>::type(),\n                    _CUDA_VSTD::forward<_Up>(__u)...) {}\n\n    template <class _Alloc, class ..._Up,\n              __enable_if_t<_EnableVariadicConvertingConstructor<_Up...>::__explicit_alloc, bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        explicit\n        tuple(allocator_arg_t, const _Alloc& __a, _Up&&... __u)\n            : __base_(allocator_arg_t(), __a,\n                    typename __make_tuple_indices<sizeof...(_Up)>::type(),\n                    typename __make_tuple_types<tuple, sizeof...(_Up)>::type(),\n                    typename __make_tuple_indices<sizeof...(_Tp), sizeof...(_Up)>::type(),\n                    typename __make_tuple_types<tuple, sizeof...(_Tp), sizeof...(_Up)>::type(),\n                    _CUDA_VSTD::forward<_Up>(__u)...) {}\n\n\n    template <class _Tuple, bool _DisableIfLValue>\n    struct _EnableTupleLikeConstructor {\n        static constexpr bool __implicit =\n            _CheckTupleLikeConstructor<__tuple_like_with_size<_Tuple, sizeof...(_Tp)>::value\n                             && !_PackExpandsToThisTuple<_Tuple>::value\n                             && (!is_lvalue_reference<_Tuple>::value || !_DisableIfLValue)\n                         >::template __enable_implicit<_Tuple>::value;\n\n        static constexpr bool __explicit =\n            _CheckTupleLikeConstructor<__tuple_like_with_size<_Tuple, sizeof...(_Tp)>::value\n                             && !_PackExpandsToThisTuple<_Tuple>::value\n                             && (!is_lvalue_reference<_Tuple>::value || !_DisableIfLValue)\n                         >::template __enable_explicit<_Tuple>::value;\n\n        static constexpr bool __implicit_alloc =\n            _CheckTupleLikeConstructor<__tuple_like_with_size<_Tuple, sizeof...(_Tp)>::value>::template __enable_implicit<_Tuple>::value;\n\n        static constexpr bool __explicit_alloc =\n            _CheckTupleLikeConstructor<__tuple_like_with_size<_Tuple, sizeof...(_Tp)>::value>::template __enable_explicit<_Tuple>::value;\n    };\n\n    template <class _Tuple, __enable_if_t<_EnableTupleLikeConstructor<_Tuple, true>::__implicit, bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n        tuple(_Tuple&& __t) noexcept((is_nothrow_constructible<_BaseT, _Tuple>::value))\n            : __base_(_CUDA_VSTD::forward<_Tuple>(__t)) {}\n\n    template <class _Tuple, __enable_if_t<_EnableTupleLikeConstructor<const _Tuple&, false>::__implicit, bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n        tuple(const _Tuple& __t) noexcept((is_nothrow_constructible<_BaseT, const _Tuple&>::value))\n            : __base_(__t) {}\n\n    template <class _Tuple, __enable_if_t<_EnableTupleLikeConstructor<_Tuple, true>::__explicit, bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n        explicit\n        tuple(_Tuple&& __t) noexcept((is_nothrow_constructible<_BaseT, _Tuple>::value))\n            : __base_(_CUDA_VSTD::forward<_Tuple>(__t)) {}\n\n    template <class _Tuple, __enable_if_t<_EnableTupleLikeConstructor<const _Tuple&, false>::__explicit, bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n        explicit\n        tuple(const _Tuple& __t) noexcept((is_nothrow_constructible<_BaseT, const _Tuple&>::value))\n            : __base_(__t) {}\n\n    template <class _Alloc, class _Tuple,\n              __enable_if_t<_EnableTupleLikeConstructor<_Tuple, true>::__implicit_alloc, bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        tuple(allocator_arg_t, const _Alloc& __a, _Tuple&& __t)\n            : __base_(allocator_arg_t(), __a, _CUDA_VSTD::forward<_Tuple>(__t)) {}\n\n    template <class _Alloc, class _Tuple,\n              __enable_if_t<_EnableTupleLikeConstructor<_Tuple, true>::__explicit_alloc, bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        explicit\n        tuple(allocator_arg_t, const _Alloc& __a, _Tuple&& __t)\n            : __base_(allocator_arg_t(), __a, _CUDA_VSTD::forward<_Tuple>(__t)) {}\n\n    using _CanCopyAssign = __all<is_copy_assignable<_Tp>::value...>;\n    using _CanMoveAssign = __all<is_move_assignable<_Tp>::value...>;\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    tuple& operator=(__conditional_t<_CanCopyAssign::value, tuple, __nat> const& __t)\n        noexcept((__all<is_nothrow_copy_assignable<_Tp>::value...>::value))\n    {\n        __base_.operator=(__t.__base_);\n        return *this;\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    tuple& operator=(__conditional_t<_CanMoveAssign::value, tuple, __nat>&& __t)\n        noexcept((__all<is_nothrow_move_assignable<_Tp>::value...>::value))\n    {\n        __base_.operator=(static_cast<_BaseT&&>(__t.__base_));\n        return *this;\n    }\n\n    template <class _Tuple,\n              __enable_if_t<__tuple_assignable<_Tuple, tuple>::value, bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        tuple&\n        operator=(_Tuple&& __t) noexcept((is_nothrow_assignable<_BaseT&, _Tuple>::value))\n        {\n            __base_.operator=(_CUDA_VSTD::forward<_Tuple>(__t));\n            return *this;\n        }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    void swap(tuple& __t) noexcept(__all<__is_nothrow_swappable<_Tp>::value...>::value)\n        {__base_.swap(__t.__base_);}\n};\n\ntemplate <>\nclass _LIBCUDACXX_TEMPLATE_VIS tuple<>\n{\npublic:\n    constexpr tuple() noexcept = default;\n    template <class _Alloc>\n    _LIBCUDACXX_INLINE_VISIBILITY\n        tuple(allocator_arg_t, const _Alloc&) noexcept {}\n    template <class _Alloc>\n    _LIBCUDACXX_INLINE_VISIBILITY\n        tuple(allocator_arg_t, const _Alloc&, const tuple&) noexcept {}\n    template <class _Up>\n    _LIBCUDACXX_INLINE_VISIBILITY\n        tuple(array<_Up, 0>) noexcept {}\n    template <class _Alloc, class _Up>\n    _LIBCUDACXX_INLINE_VISIBILITY\n        tuple(allocator_arg_t, const _Alloc&, array<_Up, 0>) noexcept {}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    void swap(tuple&) noexcept {}\n};\n\n#ifndef _LIBCUDACXX_HAS_NO_DEDUCTION_GUIDES\ntemplate <class ..._Tp>\ntuple(_Tp...) -> tuple<_Tp...>;\ntemplate <class _Tp1, class _Tp2>\ntuple(pair<_Tp1, _Tp2>) -> tuple<_Tp1, _Tp2>;\ntemplate <class _Alloc, class ..._Tp>\ntuple(allocator_arg_t, _Alloc, _Tp...) -> tuple<_Tp...>;\ntemplate <class _Alloc, class _Tp1, class _Tp2>\ntuple(allocator_arg_t, _Alloc, pair<_Tp1, _Tp2>) -> tuple<_Tp1, _Tp2>;\ntemplate <class _Alloc, class ..._Tp>\ntuple(allocator_arg_t, _Alloc, tuple<_Tp...>) -> tuple<_Tp...>;\n#endif\n\ntemplate <class ..._Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY\n__enable_if_t<_And<__is_swappable<_Tp>...>::value, void>\nswap(tuple<_Tp...>& __t, tuple<_Tp...>& __u) noexcept(__all<__is_nothrow_swappable<_Tp>::value...>::value) {\n    __t.swap(__u);\n}\n\n// get\ntemplate <size_t _Ip, class ..._Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\ntypename tuple_element<_Ip, tuple<_Tp...> >::type&\nget(tuple<_Tp...>& __t) noexcept\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE typename tuple_element<_Ip, tuple<_Tp...> >::type type;\n    return static_cast<__tuple_leaf<_Ip, type>&>(__t.__base_).get();\n}\n\ntemplate <size_t _Ip, class ..._Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nconst typename tuple_element<_Ip, tuple<_Tp...> >::type&\nget(const tuple<_Tp...>& __t) noexcept\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE typename tuple_element<_Ip, tuple<_Tp...> >::type type;\n    return static_cast<const __tuple_leaf<_Ip, type>&>(__t.__base_).get();\n}\n\ntemplate <size_t _Ip, class ..._Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\ntypename tuple_element<_Ip, tuple<_Tp...> >::type&&\nget(tuple<_Tp...>&& __t) noexcept\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE typename tuple_element<_Ip, tuple<_Tp...> >::type type;\n    return static_cast<type&&>(\n             static_cast<__tuple_leaf<_Ip, type>&&>(__t.__base_).get());\n}\n\ntemplate <size_t _Ip, class ..._Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nconst typename tuple_element<_Ip, tuple<_Tp...> >::type&&\nget(const tuple<_Tp...>&& __t) noexcept\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE typename tuple_element<_Ip, tuple<_Tp...> >::type type;\n    return static_cast<const type&&>(\n             static_cast<const __tuple_leaf<_Ip, type>&&>(__t.__base_).get());\n}\n\n#if _LIBCUDACXX_STD_VER > 11\n\nnamespace __find_detail {\n\nstatic constexpr size_t __not_found = ~size_t(0);\nstatic constexpr size_t __ambiguous = __not_found - 1;\n\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr size_t __find_idx_return(size_t __curr_i, size_t __res, bool __matches) {\n    return !__matches ? __res :\n        (__res == __not_found ? __curr_i : __ambiguous);\n}\n\ntemplate <size_t _Nx>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr size_t __find_idx(size_t __i, const bool (&__matches)[_Nx]) {\n  return __i == _Nx ? __not_found :\n      __find_idx_return(__i, __find_idx(__i + 1, __matches), __matches[__i]);\n}\n\ntemplate <class _T1, class ..._Args>\nstruct __find_exactly_one_checked {\n    static constexpr bool __matches[sizeof...(_Args)] = {is_same<_T1, _Args>::value...};\n    static constexpr size_t value = __find_detail::__find_idx(0, __matches);\n    static_assert(value != __not_found, \"type not found in type list\" );\n    static_assert(value != __ambiguous, \"type occurs more than once in type list\");\n};\n\ntemplate <class _T1>\nstruct __find_exactly_one_checked<_T1> {\n    static_assert(!is_same<_T1, _T1>::value, \"type not in empty type list\");\n};\n\n} // namespace __find_detail;\n\ntemplate <typename _T1, typename... _Args>\nstruct __find_exactly_one_t\n    : public __find_detail::__find_exactly_one_checked<_T1, _Args...> {\n};\n\ntemplate <class _T1, class... _Args>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr _T1& get(tuple<_Args...>& __tup) noexcept\n{\n    return _CUDA_VSTD::get<__find_exactly_one_t<_T1, _Args...>::value>(__tup);\n}\n\ntemplate <class _T1, class... _Args>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr _T1 const& get(tuple<_Args...> const& __tup) noexcept\n{\n    return _CUDA_VSTD::get<__find_exactly_one_t<_T1, _Args...>::value>(__tup);\n}\n\ntemplate <class _T1, class... _Args>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr _T1&& get(tuple<_Args...>&& __tup) noexcept\n{\n    return _CUDA_VSTD::get<__find_exactly_one_t<_T1, _Args...>::value>(_CUDA_VSTD::move(__tup));\n}\n\ntemplate <class _T1, class... _Args>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr _T1 const&& get(tuple<_Args...> const&& __tup) noexcept\n{\n    return _CUDA_VSTD::get<__find_exactly_one_t<_T1, _Args...>::value>(_CUDA_VSTD::move(__tup));\n}\n\n#endif\n\n// tie\n\ntemplate <class ..._Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\ntuple<_Tp&...>\ntie(_Tp&... __t) noexcept\n{\n    return tuple<_Tp&...>(__t...);\n}\n\ntemplate <class _Up>\nstruct __ignore_t\n{\n    template <class _Tp>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    const __ignore_t& operator=(_Tp&&) const {return *this;}\n};\n\nnamespace {\n#ifdef __CUDA_ARCH__\n    static\n#endif\n    _LIBCUDACXX_INLINE_VAR constexpr __ignore_t<unsigned char> ignore = __ignore_t<unsigned char>();\n}\n\ntemplate <class... _Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\ntuple<typename __unwrap_ref_decay<_Tp>::type...>\nmake_tuple(_Tp&&... __t)\n{\n    return tuple<typename __unwrap_ref_decay<_Tp>::type...>(_CUDA_VSTD::forward<_Tp>(__t)...);\n}\n\ntemplate <class... _Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\ntuple<_Tp&&...>\nforward_as_tuple(_Tp&&... __t) noexcept\n{\n    return tuple<_Tp&&...>(_CUDA_VSTD::forward<_Tp>(__t)...);\n}\n\ntemplate <size_t _Ip>\nstruct __tuple_equal\n{\n    template <class _Tp, class _Up>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    bool operator()(const _Tp& __x, const _Up& __y)\n    {\n        return __tuple_equal<_Ip - 1>()(__x, __y) && _CUDA_VSTD::get<_Ip-1>(__x) == _CUDA_VSTD::get<_Ip-1>(__y);\n    }\n};\n\ntemplate <>\nstruct __tuple_equal<0>\n{\n    template <class _Tp, class _Up>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    bool operator()(const _Tp&, const _Up&)\n    {\n        return true;\n    }\n};\n\ntemplate <class ..._Tp, class ..._Up>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nbool\noperator==(const tuple<_Tp...>& __x, const tuple<_Up...>& __y)\n{\n    static_assert (sizeof...(_Tp) == sizeof...(_Up), \"Can't compare tuples of different sizes\");\n    return __tuple_equal<sizeof...(_Tp)>()(__x, __y);\n}\n\ntemplate <class ..._Tp, class ..._Up>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nbool\noperator!=(const tuple<_Tp...>& __x, const tuple<_Up...>& __y)\n{\n    return !(__x == __y);\n}\n\ntemplate <size_t _Ip>\nstruct __tuple_less\n{\n    template <class _Tp, class _Up>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    bool operator()(const _Tp& __x, const _Up& __y)\n    {\n        const size_t __idx = tuple_size<_Tp>::value - _Ip;\n        if (_CUDA_VSTD::get<__idx>(__x) < _CUDA_VSTD::get<__idx>(__y))\n            return true;\n        if (_CUDA_VSTD::get<__idx>(__y) < _CUDA_VSTD::get<__idx>(__x))\n            return false;\n        return __tuple_less<_Ip-1>()(__x, __y);\n    }\n};\n\ntemplate <>\nstruct __tuple_less<0>\n{\n    template <class _Tp, class _Up>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    bool operator()(const _Tp&, const _Up&)\n    {\n        return false;\n    }\n};\n\ntemplate <class ..._Tp, class ..._Up>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nbool\noperator<(const tuple<_Tp...>& __x, const tuple<_Up...>& __y)\n{\n    static_assert (sizeof...(_Tp) == sizeof...(_Up), \"Can't compare tuples of different sizes\");\n    return __tuple_less<sizeof...(_Tp)>()(__x, __y);\n}\n\ntemplate <class ..._Tp, class ..._Up>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nbool\noperator>(const tuple<_Tp...>& __x, const tuple<_Up...>& __y)\n{\n    return __y < __x;\n}\n\ntemplate <class ..._Tp, class ..._Up>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nbool\noperator>=(const tuple<_Tp...>& __x, const tuple<_Up...>& __y)\n{\n    return !(__x < __y);\n}\n\ntemplate <class ..._Tp, class ..._Up>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nbool\noperator<=(const tuple<_Tp...>& __x, const tuple<_Up...>& __y)\n{\n    return !(__y < __x);\n}\n\n// tuple_cat\n\ntemplate <class _Tp, class _Up> struct __tuple_cat_type;\n\ntemplate <class ..._Ttypes, class ..._Utypes>\nstruct __tuple_cat_type<tuple<_Ttypes...>, __tuple_types<_Utypes...> >\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE tuple<_Ttypes..., _Utypes...> type;\n};\n\ntemplate <class _ResultTuple, bool _Is_Tuple0TupleLike, class ..._Tuples>\nstruct __tuple_cat_return_1\n{\n};\n\ntemplate <class ..._Types, class _Tuple0>\nstruct __tuple_cat_return_1<tuple<_Types...>, true, _Tuple0>\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE typename __tuple_cat_type<tuple<_Types...>,\n            typename __make_tuple_types<__remove_cvref_t<_Tuple0>>::type>::type\n                                                                           type;\n};\n\ntemplate <class ..._Types, class _Tuple0, class _Tuple1, class ..._Tuples>\nstruct __tuple_cat_return_1<tuple<_Types...>, true, _Tuple0, _Tuple1, _Tuples...>\n    : public __tuple_cat_return_1<\n                 typename __tuple_cat_type<\n                     tuple<_Types...>,\n                     typename __make_tuple_types<__remove_cvref_t<_Tuple0>>::type\n                 >::type,\n                 __tuple_like<typename remove_reference<_Tuple1>::type>::value,\n                 _Tuple1, _Tuples...>\n{\n};\n\ntemplate <class ..._Tuples> struct __tuple_cat_return;\n\ntemplate <class _Tuple0, class ..._Tuples>\nstruct __tuple_cat_return<_Tuple0, _Tuples...>\n    : public __tuple_cat_return_1<tuple<>,\n         __tuple_like<typename remove_reference<_Tuple0>::type>::value, _Tuple0,\n                                                                     _Tuples...>\n{\n};\n\ntemplate <>\nstruct __tuple_cat_return<>\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE tuple<> type;\n};\n\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\ntuple<>\ntuple_cat()\n{\n    return tuple<>();\n}\n\ntemplate <class _Rp, class _Indices, class _Tuple0, class ..._Tuples>\nstruct __tuple_cat_return_ref_imp;\n\ntemplate <class ..._Types, size_t ..._I0, class _Tuple0>\nstruct __tuple_cat_return_ref_imp<tuple<_Types...>, __tuple_indices<_I0...>, _Tuple0>\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE typename remove_reference<_Tuple0>::type _T0;\n    typedef tuple<_Types..., typename __apply_cv<_Tuple0,\n                          typename tuple_element<_I0, _T0>::type>::type&&...> type;\n};\n\ntemplate <class ..._Types, size_t ..._I0, class _Tuple0, class _Tuple1, class ..._Tuples>\nstruct __tuple_cat_return_ref_imp<tuple<_Types...>, __tuple_indices<_I0...>,\n                                  _Tuple0, _Tuple1, _Tuples...>\n    : public __tuple_cat_return_ref_imp<\n         tuple<_Types..., typename __apply_cv<_Tuple0,\n               typename tuple_element<_I0,\n                  typename remove_reference<_Tuple0>::type>::type>::type&&...>,\n         typename __make_tuple_indices<tuple_size<typename\n                                 remove_reference<_Tuple1>::type>::value>::type,\n         _Tuple1, _Tuples...>\n{\n};\n\ntemplate <class _Tuple0, class ..._Tuples>\nstruct __tuple_cat_return_ref\n    : public __tuple_cat_return_ref_imp<tuple<>,\n               typename __make_tuple_indices<\n                        tuple_size<typename remove_reference<_Tuple0>::type>::value\n               >::type, _Tuple0, _Tuples...>\n{\n};\n\ntemplate <class _Types, class _I0, class _J0>\nstruct __tuple_cat;\n\ntemplate <class ..._Types, size_t ..._I0, size_t ..._J0>\nstruct __tuple_cat<tuple<_Types...>, __tuple_indices<_I0...>, __tuple_indices<_J0...> >\n{\n    template <class _Tuple0>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    typename __tuple_cat_return_ref<tuple<_Types...>&&, _Tuple0&&>::type\n    operator()(tuple<_Types...> __t, _Tuple0&& __t0)\n    {\n        (void)__t;\n        return _CUDA_VSTD::forward_as_tuple(_CUDA_VSTD::forward<_Types>(_CUDA_VSTD::get<_I0>(__t))...,\n                                            _CUDA_VSTD::get<_J0>(_CUDA_VSTD::forward<_Tuple0>(__t0))...);\n    }\n\n    template <class _Tuple0, class _Tuple1, class ..._Tuples>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    typename __tuple_cat_return_ref<tuple<_Types...>&&, _Tuple0&&, _Tuple1&&, _Tuples&&...>::type\n    operator()(tuple<_Types...> __t, _Tuple0&& __t0, _Tuple1&& __t1, _Tuples&& ...__tpls)\n    {\n        (void)__t;\n        typedef _LIBCUDACXX_NODEBUG_TYPE typename remove_reference<_Tuple0>::type _T0;\n        typedef _LIBCUDACXX_NODEBUG_TYPE typename remove_reference<_Tuple1>::type _T1;\n        return __tuple_cat<\n           tuple<_Types..., typename __apply_cv<_Tuple0, typename tuple_element<_J0, _T0>::type>::type&&...>,\n           typename __make_tuple_indices<sizeof ...(_Types) + tuple_size<_T0>::value>::type,\n           typename __make_tuple_indices<tuple_size<_T1>::value>::type>()\n                           (_CUDA_VSTD::forward_as_tuple(\n                              _CUDA_VSTD::forward<_Types>(_CUDA_VSTD::get<_I0>(__t))...,\n                              _CUDA_VSTD::get<_J0>(_CUDA_VSTD::forward<_Tuple0>(__t0))...\n                            ),\n                            _CUDA_VSTD::forward<_Tuple1>(__t1),\n                            _CUDA_VSTD::forward<_Tuples>(__tpls)...);\n    }\n};\n\ntemplate <class _Tuple0, class... _Tuples>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\ntypename __tuple_cat_return<_Tuple0, _Tuples...>::type\ntuple_cat(_Tuple0&& __t0, _Tuples&&... __tpls)\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE typename remove_reference<_Tuple0>::type _T0;\n    return __tuple_cat<tuple<>, __tuple_indices<>,\n                  typename __make_tuple_indices<tuple_size<_T0>::value>::type>()\n                  (tuple<>(), _CUDA_VSTD::forward<_Tuple0>(__t0),\n                                            _CUDA_VSTD::forward<_Tuples>(__tpls)...);\n}\n\ntemplate <class ..._Tp, class _Alloc>\nstruct _LIBCUDACXX_TEMPLATE_VIS uses_allocator<tuple<_Tp...>, _Alloc>\n    : true_type {};\n\ntemplate <class _T1, class _T2>\ntemplate <class... _Args1, class... _Args2, size_t ..._I1, size_t ..._I2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\npair<_T1, _T2>::pair(piecewise_construct_t,\n                     tuple<_Args1...>& __first_args, tuple<_Args2...>& __second_args,\n                     __tuple_indices<_I1...>, __tuple_indices<_I2...>)\n    :  first(_CUDA_VSTD::forward<_Args1>(_CUDA_VSTD::get<_I1>( __first_args))...),\n      second(_CUDA_VSTD::forward<_Args2>(_CUDA_VSTD::get<_I2>(__second_args))...)\n{\n}\n\n#if _LIBCUDACXX_STD_VER > 14\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr size_t tuple_size_v = tuple_size<_Tp>::value;\n\n#define _LIBCUDACXX_NOEXCEPT_RETURN(...) noexcept(noexcept(__VA_ARGS__)) { return __VA_ARGS__; }\n\ntemplate <class _Fn, class _Tuple, size_t ..._Id>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr decltype(auto) __apply_tuple_impl(_Fn && __f, _Tuple && __t,\n                                            __tuple_indices<_Id...>)\n_LIBCUDACXX_NOEXCEPT_RETURN(\n    _CUDA_VSTD::__invoke(\n        _CUDA_VSTD::forward<_Fn>(__f),\n        _CUDA_VSTD::get<_Id>(_CUDA_VSTD::forward<_Tuple>(__t))...)\n)\n\ntemplate <class _Fn, class _Tuple>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr decltype(auto) apply(_Fn && __f, _Tuple && __t)\n_LIBCUDACXX_NOEXCEPT_RETURN(\n    _CUDA_VSTD::__apply_tuple_impl(\n        _CUDA_VSTD::forward<_Fn>(__f), _CUDA_VSTD::forward<_Tuple>(__t),\n        typename __make_tuple_indices<tuple_size_v<remove_reference_t<_Tuple>>>::type{})\n)\n\ntemplate <class _Tp, class _Tuple, size_t... _Idx>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr _Tp __make_from_tuple_impl(_Tuple&& __t, __tuple_indices<_Idx...>)\n_LIBCUDACXX_NOEXCEPT_RETURN(\n    _Tp(_CUDA_VSTD::get<_Idx>(_CUDA_VSTD::forward<_Tuple>(__t))...)\n)\n\ntemplate <class _Tp, class _Tuple>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr _Tp make_from_tuple(_Tuple&& __t)\n_LIBCUDACXX_NOEXCEPT_RETURN(\n    _CUDA_VSTD::__make_from_tuple_impl<_Tp>(_CUDA_VSTD::forward<_Tuple>(__t),\n        typename __make_tuple_indices<tuple_size_v<remove_reference_t<_Tuple>>>::type{})\n)\n\n#undef _LIBCUDACXX_NOEXCEPT_RETURN\n\n#endif // _LIBCUDACXX_STD_VER > 14\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#ifndef __cuda_std__\n#include <__pragma_pop>\n#endif //__cuda_std__\n\n#endif  // _LIBCUDACXX_TUPLE\n", "../util_ptx.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_8EAE810BB36A0FF0\n#define _JITIFY_INCLUDE_GUARD_8EAE810BB36A0FF0\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.\n * \n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n * \n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * PTX intrinsics\n */\n\n\n#include \"util_type.cuh\"\n#include \"util_arch.cuh\"\n#include \"util_namespace.cuh\"\n#include \"util_debug.cuh\"\n\n\nCUB_NAMESPACE_BEGIN\n\n\n/**\n * \\addtogroup UtilPtx\n * @{\n */\n\n\n/******************************************************************************\n * PTX helper macros\n ******************************************************************************/\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS    // Do not document\n\n/**\n * Register modifier for pointer-types (for inlining PTX assembly)\n */\n#if defined(_WIN64) || defined(__LP64__)\n    #define __CUB_LP64__ 1\n    // 64-bit register modifier for inlined asm\n    #define _CUB_ASM_PTR_ \"l\"\n    #define _CUB_ASM_PTR_SIZE_ \"u64\"\n#else\n    #define __CUB_LP64__ 0\n    // 32-bit register modifier for inlined asm\n    #define _CUB_ASM_PTR_ \"r\"\n    #define _CUB_ASM_PTR_SIZE_ \"u32\"\n#endif\n\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\n\n/******************************************************************************\n * Inlined PTX intrinsics\n ******************************************************************************/\n\nnamespace detail\n{\n/**\n * @brief Shifts @p val left by the amount specified by unsigned 32-bit value in @p num_bits. If @p\n * num_bits is larger than 32 bits, @p num_bits is clamped to 32.\n */\n__device__ __forceinline__ uint32_t LogicShiftLeft(uint32_t val, uint32_t num_bits)\n{\n  uint32_t ret{};\n  asm(\"shl.b32 %0, %1, %2;\" : \"=r\"(ret) : \"r\"(val), \"r\"(num_bits));\n  return ret;\n}\n\n/**\n * @brief Shifts @p val right by the amount specified by unsigned 32-bit value in @p num_bits. If @p\n * num_bits is larger than 32 bits, @p num_bits is clamped to 32.\n */\n__device__ __forceinline__ uint32_t LogicShiftRight(uint32_t val, uint32_t num_bits)\n{\n  uint32_t ret{};\n  asm(\"shr.b32 %0, %1, %2;\" : \"=r\"(ret) : \"r\"(val), \"r\"(num_bits));\n  return ret;\n}\n} // namespace detail\n\n/**\n * \\brief Shift-right then add.  Returns (\\p x >> \\p shift) + \\p addend.\n */\n__device__ __forceinline__ unsigned int SHR_ADD(\n    unsigned int x,\n    unsigned int shift,\n    unsigned int addend)\n{\n    unsigned int ret;\n    asm (\"vshr.u32.u32.u32.clamp.add %0, %1, %2, %3;\" :\n        \"=r\"(ret) : \"r\"(x), \"r\"(shift), \"r\"(addend));\n    return ret;\n}\n\n\n/**\n * \\brief Shift-left then add.  Returns (\\p x << \\p shift) + \\p addend.\n */\n__device__ __forceinline__ unsigned int SHL_ADD(\n    unsigned int x,\n    unsigned int shift,\n    unsigned int addend)\n{\n    unsigned int ret;\n    asm (\"vshl.u32.u32.u32.clamp.add %0, %1, %2, %3;\" :\n        \"=r\"(ret) : \"r\"(x), \"r\"(shift), \"r\"(addend));\n    return ret;\n}\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS    // Do not document\n\n/**\n * Bitfield-extract.\n */\ntemplate <typename UnsignedBits, int BYTE_LEN>\n__device__ __forceinline__ unsigned int BFE(\n    UnsignedBits            source,\n    unsigned int            bit_start,\n    unsigned int            num_bits,\n    Int2Type<BYTE_LEN>      /*byte_len*/)\n{\n    unsigned int bits;\n    asm (\"bfe.u32 %0, %1, %2, %3;\" : \"=r\"(bits) : \"r\"((unsigned int) source), \"r\"(bit_start), \"r\"(num_bits));\n    return bits;\n}\n\n\n/**\n * Bitfield-extract for 64-bit types.\n */\ntemplate <typename UnsignedBits>\n__device__ __forceinline__ unsigned int BFE(\n    UnsignedBits            source,\n    unsigned int            bit_start,\n    unsigned int            num_bits,\n    Int2Type<8>             /*byte_len*/)\n{\n    const unsigned long long MASK = (1ull << num_bits) - 1;\n    return (source >> bit_start) & MASK;\n}\n\n#if CUB_IS_INT128_ENABLED \n/**\n * Bitfield-extract for 128-bit types.\n */\ntemplate <typename UnsignedBits>\n__device__ __forceinline__ unsigned int BFE(\n    UnsignedBits            source,\n    unsigned int            bit_start,\n    unsigned int            num_bits,\n    Int2Type<16>            /*byte_len*/)\n{\n    const __uint128_t MASK = (__uint128_t{1} << num_bits) - 1;\n    return (source >> bit_start) & MASK;\n}\n#endif\n\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\n/**\n * \\brief Bitfield-extract.  Extracts \\p num_bits from \\p source starting at bit-offset \\p bit_start.  The input \\p source may be an 8b, 16b, 32b, or 64b unsigned integer type.\n */\ntemplate <typename UnsignedBits>\n__device__ __forceinline__ unsigned int BFE(\n    UnsignedBits source,\n    unsigned int bit_start,\n    unsigned int num_bits)\n{\n    return BFE(source, bit_start, num_bits, Int2Type<sizeof(UnsignedBits)>());\n}\n\n\n/**\n * \\brief Bitfield insert.  Inserts the \\p num_bits least significant bits of \\p y into \\p x at bit-offset \\p bit_start.\n */\n__device__ __forceinline__ void BFI(\n    unsigned int &ret,\n    unsigned int x,\n    unsigned int y,\n    unsigned int bit_start,\n    unsigned int num_bits)\n{\n    asm (\"bfi.b32 %0, %1, %2, %3, %4;\" :\n        \"=r\"(ret) : \"r\"(y), \"r\"(x), \"r\"(bit_start), \"r\"(num_bits));\n}\n\n\n/**\n * \\brief Three-operand add.  Returns \\p x + \\p y + \\p z.\n */\n__device__ __forceinline__ unsigned int IADD3(unsigned int x, unsigned int y, unsigned int z)\n{\n    asm (\"vadd.u32.u32.u32.add %0, %1, %2, %3;\" : \"=r\"(x) : \"r\"(x), \"r\"(y), \"r\"(z));\n    return x;\n}\n\n\n/**\n * \\brief Byte-permute. Pick four arbitrary bytes from two 32-bit registers, and reassemble them into a 32-bit destination register.  For SM2.0 or later.\n *\n * \\par\n * The bytes in the two source registers \\p a and \\p b are numbered from 0 to 7:\n * {\\p b, \\p a} = {{b7, b6, b5, b4}, {b3, b2, b1, b0}}. For each of the four bytes\n * {b3, b2, b1, b0} selected in the return value, a 4-bit selector is defined within\n * the four lower \"nibbles\" of \\p index: {\\p index } = {n7, n6, n5, n4, n3, n2, n1, n0}\n *\n * \\par Snippet\n * The code snippet below illustrates byte-permute.\n * \\par\n * \\code\n * #include <cub/cub.cuh>\n *\n * __global__ void ExampleKernel(...)\n * {\n *     int a        = 0x03020100;\n *     int b        = 0x07060504;\n *     int index    = 0x00007531;\n *\n *     int selected = PRMT(a, b, index);    // 0x07050301\n *\n * \\endcode\n *\n */\n__device__ __forceinline__ int PRMT(unsigned int a, unsigned int b, unsigned int index)\n{\n    int ret;\n    asm (\"prmt.b32 %0, %1, %2, %3;\" : \"=r\"(ret) : \"r\"(a), \"r\"(b), \"r\"(index));\n    return ret;\n}\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS    // Do not document\n\n/**\n * Sync-threads barrier.\n */\n__device__ __forceinline__ void BAR(int count)\n{\n    asm volatile(\"bar.sync 1, %0;\" : : \"r\"(count));\n}\n\n/**\n * CTA barrier\n */\n__device__  __forceinline__ void CTA_SYNC()\n{\n    __syncthreads();\n}\n\n\n/**\n * CTA barrier with predicate\n */\n__device__  __forceinline__ int CTA_SYNC_AND(int p)\n{\n    return __syncthreads_and(p);\n}\n\n\n/**\n * CTA barrier with predicate\n */\n__device__  __forceinline__ int CTA_SYNC_OR(int p)\n{\n    return __syncthreads_or(p);\n}\n\n\n/**\n * Warp barrier\n */\n__device__  __forceinline__ void WARP_SYNC(unsigned int member_mask)\n{\n    __syncwarp(member_mask);\n}\n\n\n/**\n * Warp any\n */\n__device__  __forceinline__ int WARP_ANY(int predicate, unsigned int member_mask)\n{\n    return __any_sync(member_mask, predicate);\n}\n\n\n/**\n * Warp any\n */\n__device__  __forceinline__ int WARP_ALL(int predicate, unsigned int member_mask)\n{\n    return __all_sync(member_mask, predicate);\n}\n\n\n/**\n * Warp ballot\n */\n__device__  __forceinline__ int WARP_BALLOT(int predicate, unsigned int member_mask)\n{\n    return __ballot_sync(member_mask, predicate);\n}\n\n\n/**\n * Warp synchronous shfl_up\n */\n__device__ __forceinline__ \nunsigned int SHFL_UP_SYNC(unsigned int word, int src_offset, int flags, unsigned int member_mask)\n{\n    asm volatile(\"shfl.sync.up.b32 %0, %1, %2, %3, %4;\"\n        : \"=r\"(word) : \"r\"(word), \"r\"(src_offset), \"r\"(flags), \"r\"(member_mask));\n    return word;\n}\n\n/**\n * Warp synchronous shfl_down\n */\n__device__ __forceinline__ \nunsigned int SHFL_DOWN_SYNC(unsigned int word, int src_offset, int flags, unsigned int member_mask)\n{\n    asm volatile(\"shfl.sync.down.b32 %0, %1, %2, %3, %4;\"\n        : \"=r\"(word) : \"r\"(word), \"r\"(src_offset), \"r\"(flags), \"r\"(member_mask));\n    return word;\n}\n\n/**\n * Warp synchronous shfl_idx\n */\n__device__ __forceinline__ \nunsigned int SHFL_IDX_SYNC(unsigned int word, int src_lane, int flags, unsigned int member_mask)\n{\n    asm volatile(\"shfl.sync.idx.b32 %0, %1, %2, %3, %4;\"\n        : \"=r\"(word) : \"r\"(word), \"r\"(src_lane), \"r\"(flags), \"r\"(member_mask));\n    return word;\n}\n\n/**\n * Warp synchronous shfl_idx\n */\n__device__ __forceinline__ \nunsigned int SHFL_IDX_SYNC(unsigned int word, int src_lane, unsigned int member_mask)\n{\n    return __shfl_sync(member_mask, word, src_lane);\n}\n\n/**\n * Floating point multiply. (Mantissa LSB rounds towards zero.)\n */\n__device__ __forceinline__ float FMUL_RZ(float a, float b)\n{\n    float d;\n    asm (\"mul.rz.f32 %0, %1, %2;\" : \"=f\"(d) : \"f\"(a), \"f\"(b));\n    return d;\n}\n\n\n/**\n * Floating point multiply-add. (Mantissa LSB rounds towards zero.)\n */\n__device__ __forceinline__ float FFMA_RZ(float a, float b, float c)\n{\n    float d;\n    asm (\"fma.rz.f32 %0, %1, %2, %3;\" : \"=f\"(d) : \"f\"(a), \"f\"(b), \"f\"(c));\n    return d;\n}\n\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\n/**\n * \\brief Terminates the calling thread\n */\n__device__ __forceinline__ void ThreadExit() {\n    asm volatile(\"exit;\");\n}    \n\n\n/**\n * \\brief  Abort execution and generate an interrupt to the host CPU\n */\n__device__ __forceinline__ void ThreadTrap() {\n    asm volatile(\"trap;\");\n}\n\n\n/**\n * \\brief Returns the row-major linear thread identifier for a multidimensional thread block\n */\n__device__ __forceinline__ int RowMajorTid(int block_dim_x, int block_dim_y, int block_dim_z)\n{\n    return ((block_dim_z == 1) ? 0 : (threadIdx.z * block_dim_x * block_dim_y)) +\n            ((block_dim_y == 1) ? 0 : (threadIdx.y * block_dim_x)) +\n            threadIdx.x;\n}\n\n\n/**\n * \\brief Returns the warp lane ID of the calling thread\n */\n__device__ __forceinline__ unsigned int LaneId()\n{\n    unsigned int ret;\n    asm (\"mov.u32 %0, %%laneid;\" : \"=r\"(ret) );\n    return ret;\n}\n\n\n/**\n * \\brief Returns the warp ID of the calling thread.  Warp ID is guaranteed to be unique among warps, but may not correspond to a zero-based ranking within the thread block.\n */\n__device__ __forceinline__ unsigned int WarpId()\n{\n    unsigned int ret;\n    asm (\"mov.u32 %0, %%warpid;\" : \"=r\"(ret) );\n    return ret;\n}\n\n/**\n * @brief Returns the warp mask for a warp of @p LOGICAL_WARP_THREADS threads\n *\n * @par\n * If the number of threads assigned to the virtual warp is not a power of two,\n * it's assumed that only one virtual warp exists.\n *\n * @tparam LOGICAL_WARP_THREADS <b>[optional]</b> The number of threads per\n *                              \"logical\" warp (may be less than the number of\n *                              hardware warp threads).\n * @param warp_id Id of virtual warp within architectural warp\n */\ntemplate <int LOGICAL_WARP_THREADS, int LEGACY_PTX_ARCH = 0>\n__host__ __device__ __forceinline__\nunsigned int WarpMask(unsigned int warp_id)\n{\n  constexpr bool is_pow_of_two = PowerOfTwo<LOGICAL_WARP_THREADS>::VALUE;\n  constexpr bool is_arch_warp  = LOGICAL_WARP_THREADS == CUB_WARP_THREADS(0);\n\n  unsigned int member_mask = 0xFFFFFFFFu >>\n                             (CUB_WARP_THREADS(0) - LOGICAL_WARP_THREADS);\n\n  if (is_pow_of_two && !is_arch_warp)\n  {\n    member_mask <<= warp_id * LOGICAL_WARP_THREADS;\n  }\n\n  return member_mask;\n}\n\n/**\n * \\brief Returns the warp lane mask of all lanes less than the calling thread\n */\n__device__ __forceinline__ unsigned int LaneMaskLt()\n{\n    unsigned int ret;\n    asm (\"mov.u32 %0, %%lanemask_lt;\" : \"=r\"(ret) );\n    return ret;\n}\n\n/**\n * \\brief Returns the warp lane mask of all lanes less than or equal to the calling thread\n */\n__device__ __forceinline__ unsigned int LaneMaskLe()\n{\n    unsigned int ret;\n    asm (\"mov.u32 %0, %%lanemask_le;\" : \"=r\"(ret) );\n    return ret;\n}\n\n/**\n * \\brief Returns the warp lane mask of all lanes greater than the calling thread\n */\n__device__ __forceinline__ unsigned int LaneMaskGt()\n{\n    unsigned int ret;\n    asm (\"mov.u32 %0, %%lanemask_gt;\" : \"=r\"(ret) );\n    return ret;\n}\n\n/**\n * \\brief Returns the warp lane mask of all lanes greater than or equal to the calling thread\n */\n__device__ __forceinline__ unsigned int LaneMaskGe()\n{\n    unsigned int ret;\n    asm (\"mov.u32 %0, %%lanemask_ge;\" : \"=r\"(ret) );\n    return ret;\n}\n\n/** @} */       // end group UtilPtx\n\n\n\n\n/**\n * \\brief Shuffle-up for any data type.  Each <em>warp-lane<sub>i</sub></em> obtains the value \\p input contributed by <em>warp-lane</em><sub><em>i</em>-<tt>src_offset</tt></sub>.  For thread lanes \\e i < src_offset, the thread's own \\p input is returned to the thread. ![](shfl_up_logo.png)\n * \\ingroup WarpModule\n *\n * \\tparam LOGICAL_WARP_THREADS     The number of threads per \"logical\" warp.  Must be a power-of-two <= 32.\n * \\tparam T                        <b>[inferred]</b> The input/output element type\n *\n * \\par\n * - Available only for SM3.0 or newer\n *\n * \\par Snippet\n * The code snippet below illustrates each thread obtaining a \\p double value from the\n * predecessor of its predecessor.\n * \\par\n * \\code\n * #include <cub/cub.cuh>   // or equivalently <cub/util_ptx.cuh>\n *\n * __global__ void ExampleKernel(...)\n * {\n *     // Obtain one input item per thread\n *     double thread_data = ...\n *\n *     // Obtain item from two ranks below\n *     double peer_data = ShuffleUp<32>(thread_data, 2, 0, 0xffffffff);\n *\n * \\endcode\n * \\par\n * Suppose the set of input \\p thread_data across the first warp of threads is <tt>{1.0, 2.0, 3.0, 4.0, 5.0, ..., 32.0}</tt>.\n * The corresponding output \\p peer_data will be <tt>{1.0, 2.0, 1.0, 2.0, 3.0, ..., 30.0}</tt>.\n *\n */\ntemplate <\n    int LOGICAL_WARP_THREADS,   ///< Number of threads per logical warp\n    typename T>\n__device__ __forceinline__ T ShuffleUp(\n    T               input,              ///< [in] The value to broadcast\n    int             src_offset,         ///< [in] The relative down-offset of the peer to read from\n    int             first_thread,       ///< [in] Index of first lane in logical warp (typically 0)\n    unsigned int    member_mask)        ///< [in] 32-bit mask of participating warp lanes\n{\n    /// The 5-bit SHFL mask for logically splitting warps into sub-segments starts 8-bits up\n    enum {\n        SHFL_C = (32 - LOGICAL_WARP_THREADS) << 8\n    };\n\n    typedef typename UnitWord<T>::ShuffleWord ShuffleWord;\n\n    const int       WORDS           = (sizeof(T) + sizeof(ShuffleWord) - 1) / sizeof(ShuffleWord);\n \n    T               output;\n    ShuffleWord     *output_alias   = reinterpret_cast<ShuffleWord *>(&output);\n    ShuffleWord     *input_alias    = reinterpret_cast<ShuffleWord *>(&input);\n\n    unsigned int shuffle_word;\n    shuffle_word = SHFL_UP_SYNC((unsigned int)input_alias[0], src_offset, first_thread | SHFL_C, member_mask);\n    output_alias[0] = shuffle_word;\n\n    _Pragma(\"unroll\")\n    for (int WORD = 1; WORD < WORDS; ++WORD)\n    {\n        shuffle_word       = SHFL_UP_SYNC((unsigned int)input_alias[WORD], src_offset, first_thread | SHFL_C, member_mask);\n        output_alias[WORD] = shuffle_word;\n    }\n\n    return output;\n}\n\n\n/**\n * \\brief Shuffle-down for any data type.  Each <em>warp-lane<sub>i</sub></em> obtains the value \\p input contributed by <em>warp-lane</em><sub><em>i</em>+<tt>src_offset</tt></sub>.  For thread lanes \\e i >= WARP_THREADS, the thread's own \\p input is returned to the thread.  ![](shfl_down_logo.png)\n * \\ingroup WarpModule\n *\n * \\tparam LOGICAL_WARP_THREADS     The number of threads per \"logical\" warp.  Must be a power-of-two <= 32.\n * \\tparam T                        <b>[inferred]</b> The input/output element type\n *\n * \\par\n * - Available only for SM3.0 or newer\n *\n * \\par Snippet\n * The code snippet below illustrates each thread obtaining a \\p double value from the\n * successor of its successor.\n * \\par\n * \\code\n * #include <cub/cub.cuh>   // or equivalently <cub/util_ptx.cuh>\n *\n * __global__ void ExampleKernel(...)\n * {\n *     // Obtain one input item per thread\n *     double thread_data = ...\n *\n *     // Obtain item from two ranks below\n *     double peer_data = ShuffleDown<32>(thread_data, 2, 31, 0xffffffff);\n *\n * \\endcode\n * \\par\n * Suppose the set of input \\p thread_data across the first warp of threads is <tt>{1.0, 2.0, 3.0, 4.0, 5.0, ..., 32.0}</tt>.\n * The corresponding output \\p peer_data will be <tt>{3.0, 4.0, 5.0, 6.0, 7.0, ..., 32.0}</tt>.\n *\n */\ntemplate <\n    int LOGICAL_WARP_THREADS,   ///< Number of threads per logical warp\n    typename T>\n__device__ __forceinline__ T ShuffleDown(\n    T               input,              ///< [in] The value to broadcast\n    int             src_offset,         ///< [in] The relative up-offset of the peer to read from\n    int             last_thread,        ///< [in] Index of last thread in logical warp (typically 31 for a 32-thread warp)\n    unsigned int    member_mask)        ///< [in] 32-bit mask of participating warp lanes\n{\n    /// The 5-bit SHFL mask for logically splitting warps into sub-segments starts 8-bits up\n    enum {\n        SHFL_C = (32 - LOGICAL_WARP_THREADS) << 8\n    };\n\n    typedef typename UnitWord<T>::ShuffleWord ShuffleWord;\n\n    const int       WORDS           = (sizeof(T) + sizeof(ShuffleWord) - 1) / sizeof(ShuffleWord);\n\n    T               output;\n    ShuffleWord     *output_alias   = reinterpret_cast<ShuffleWord *>(&output);\n    ShuffleWord     *input_alias    = reinterpret_cast<ShuffleWord *>(&input);\n\n    unsigned int shuffle_word;\n    shuffle_word    = SHFL_DOWN_SYNC((unsigned int)input_alias[0], src_offset, last_thread | SHFL_C, member_mask);\n    output_alias[0] = shuffle_word;\n\n    _Pragma(\"unroll\")\n    for (int WORD = 1; WORD < WORDS; ++WORD)\n    {\n        shuffle_word       = SHFL_DOWN_SYNC((unsigned int)input_alias[WORD], src_offset, last_thread | SHFL_C, member_mask);\n        output_alias[WORD] = shuffle_word;\n    }\n\n    return output;\n}\n\n\n/**\n * \\brief Shuffle-broadcast for any data type.  Each <em>warp-lane<sub>i</sub></em> obtains the value \\p input\n * contributed by <em>warp-lane</em><sub><tt>src_lane</tt></sub>.  For \\p src_lane < 0 or \\p src_lane >= WARP_THREADS,\n * then the thread's own \\p input is returned to the thread. ![](shfl_broadcast_logo.png)\n *\n * \\tparam LOGICAL_WARP_THREADS     The number of threads per \"logical\" warp.  Must be a power-of-two <= 32.\n * \\tparam T                        <b>[inferred]</b> The input/output element type\n *\n * \\ingroup WarpModule\n *\n * \\par\n * - Available only for SM3.0 or newer\n *\n * \\par Snippet\n * The code snippet below illustrates each thread obtaining a \\p double value from <em>warp-lane</em><sub>0</sub>.\n *\n * \\par\n * \\code\n * #include <cub/cub.cuh>   // or equivalently <cub/util_ptx.cuh>\n *\n * __global__ void ExampleKernel(...)\n * {\n *     // Obtain one input item per thread\n *     double thread_data = ...\n *\n *     // Obtain item from thread 0\n *     double peer_data = ShuffleIndex<32>(thread_data, 0, 0xffffffff);\n *\n * \\endcode\n * \\par\n * Suppose the set of input \\p thread_data across the first warp of threads is <tt>{1.0, 2.0, 3.0, 4.0, 5.0, ..., 32.0}</tt>.\n * The corresponding output \\p peer_data will be <tt>{1.0, 1.0, 1.0, 1.0, 1.0, ..., 1.0}</tt>.\n *\n */\ntemplate <\n    int LOGICAL_WARP_THREADS,   ///< Number of threads per logical warp\n    typename T>\n__device__ __forceinline__ T ShuffleIndex(\n    T               input,                  ///< [in] The value to broadcast\n    int             src_lane,               ///< [in] Which warp lane is to do the broadcasting\n    unsigned int    member_mask)            ///< [in] 32-bit mask of participating warp lanes\n{\n    /// The 5-bit SHFL mask for logically splitting warps into sub-segments starts 8-bits up\n    enum {\n        SHFL_C = ((32 - LOGICAL_WARP_THREADS) << 8) | (LOGICAL_WARP_THREADS - 1)\n    };\n\n    typedef typename UnitWord<T>::ShuffleWord ShuffleWord;\n\n    const int       WORDS           = (sizeof(T) + sizeof(ShuffleWord) - 1) / sizeof(ShuffleWord);\n\n    T               output;\n    ShuffleWord     *output_alias   = reinterpret_cast<ShuffleWord *>(&output);\n    ShuffleWord     *input_alias    = reinterpret_cast<ShuffleWord *>(&input);\n\n    unsigned int shuffle_word;\n    shuffle_word = SHFL_IDX_SYNC((unsigned int)input_alias[0],\n                                 src_lane,\n                                 SHFL_C,\n                                 member_mask);\n\n    output_alias[0] = shuffle_word;\n\n    _Pragma(\"unroll\")\n    for (int WORD = 1; WORD < WORDS; ++WORD)\n    {\n        shuffle_word = SHFL_IDX_SYNC((unsigned int)input_alias[WORD],\n                                     src_lane,\n                                     SHFL_C,\n                                     member_mask);\n\n        output_alias[WORD] = shuffle_word;\n    }\n\n    return output;\n}\n\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS    // Do not document\nnamespace detail \n{\n\n/** \n * Implementation detail for `MatchAny`. It provides specializations for full and partial warps. \n * For partial warps, inactive threads must be masked out. This is done in the partial warp \n * specialization below. \n * Usage:\n * ```\n * // returns a mask of threads with the same 4 least-significant bits of `label` \n * // in a warp with 16 active threads\n * warp_matcher_t<4, 16>::match_any(label); \n *\n * // returns a mask of threads with the same 4 least-significant bits of `label` \n * // in a warp with 32 active threads (no extra work is done)\n * warp_matcher_t<4, 32>::match_any(label); \n * ```\n */\ntemplate <int LABEL_BITS, int WARP_ACTIVE_THREADS>\nstruct warp_matcher_t \n{\n\n  static __device__ unsigned int match_any(unsigned int label)\n  {\n    return warp_matcher_t<LABEL_BITS, 32>::match_any(label) & ~(~0 << WARP_ACTIVE_THREADS);\n  }\n\n};\n\ntemplate <int LABEL_BITS>\nstruct warp_matcher_t<LABEL_BITS, CUB_PTX_WARP_THREADS> \n{\n\n  // match.any.sync.b32 is slower when matching a few bits\n  // using a ballot loop instead\n  static __device__ unsigned int match_any(unsigned int label)\n  {\n      unsigned int retval;\n\n      // Extract masks of common threads for each bit\n      _Pragma(\"unroll\")\n      for (int BIT = 0; BIT < LABEL_BITS; ++BIT)\n      {\n          unsigned int mask;\n          unsigned int current_bit = 1 << BIT;\n          asm (\"{\\n\"\n              \"    .reg .pred p;\\n\"\n              \"    and.b32 %0, %1, %2;\"\n              \"    setp.eq.u32 p, %0, %2;\\n\"\n              \"    vote.ballot.sync.b32 %0, p, 0xffffffff;\\n\"\n              \"    @!p not.b32 %0, %0;\\n\"\n              \"}\\n\" : \"=r\"(mask) : \"r\"(label), \"r\"(current_bit));\n\n          // Remove peers who differ\n          retval = (BIT == 0) ? mask : retval & mask;\n      }\n\n      return retval;\n  }\n\n};\n\n} // namespace detail\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\n/**\n * Compute a 32b mask of threads having the same least-significant\n * LABEL_BITS of \\p label as the calling thread.\n */\ntemplate <int LABEL_BITS, int WARP_ACTIVE_THREADS = CUB_PTX_WARP_THREADS>\ninline __device__ unsigned int MatchAny(unsigned int label)\n{\n  return detail::warp_matcher_t<LABEL_BITS, WARP_ACTIVE_THREADS>::match_any(label);\n}\n\nCUB_NAMESPACE_END\n\n#endif // _JITIFY_INCLUDE_GUARD_8EAE810BB36A0FF0\n", "../util_type.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_1375AABB974C3530\n#define _JITIFY_INCLUDE_GUARD_1375AABB974C3530\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.\n * \n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n * \n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * Common type manipulation (metaprogramming) utilities\n */\n\n#include <cfloat>\n#include <iostream>\n#include <iterator>\n#include <limits>\n\n//#include <cuda.h>\n\n#if !_NVHPC_CUDA\n    #include <cuda_fp16.h>\n#endif\n#if !_NVHPC_CUDA && !defined(CUB_DISABLE_BF16_SUPPORT)\n    #include <cuda_bf16.h>\n#endif\n\n#include <cub/detail/uninitialized_copy.cuh>\n#include <cub/util_arch.cuh>\n#include <cub/util_compiler.cuh>\n#include <cub/util_deprecated.cuh>\n#include <cub/util_macro.cuh>\n#include <cub/util_namespace.cuh>\n\n#include <cuda/std/type_traits>\n\nCUB_NAMESPACE_BEGIN\n\n#ifndef CUB_IS_INT128_ENABLED\n#if defined(__CUDACC_RTC__)\n#if defined(__CUDACC_RTC_INT128__)\n#define CUB_IS_INT128_ENABLED 1\n#endif // !defined(__CUDACC_RTC_INT128__)\n#else  // !defined(__CUDACC_RTC__)\n#if CUDA_VERSION >= 11050\n#if (CUB_HOST_COMPILER == CUB_HOST_COMPILER_GCC) || \\\n    (CUB_HOST_COMPILER == CUB_HOST_COMPILER_CLANG) || \\\n    defined(__ICC) || defined(_NVHPC_CUDA)\n#define CUB_IS_INT128_ENABLED 1\n#endif // GCC || CLANG || ICC || NVHPC\n#endif // CTK >= 11.5\n#endif // !defined(__CUDACC_RTC__)\n#endif // !defined(CUB_IS_INT128_ENABLED)\n\n/**\n * \\addtogroup UtilModule\n * @{\n */\n\n\n\n/******************************************************************************\n * Conditional types\n ******************************************************************************/\n\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS // Do not document\nnamespace detail\n{\n\n\ntemplate <bool Test, class T1, class T2>\nusing conditional_t = typename std::conditional<Test, T1, T2>::type;\n\n\ntemplate <typename Iterator>\nusing value_t = typename std::iterator_traits<Iterator>::value_type;\n\ntemplate <typename It,\n          typename FallbackT,\n          bool = ::cuda::std::is_same<\n            typename ::cuda::std::remove_cv<typename ::cuda::std::remove_pointer<It>::type>::type,\n            void>::value>\nstruct non_void_value_impl\n{\n  using type = FallbackT;\n};\n\ntemplate <typename It, typename FallbackT>\nstruct non_void_value_impl<It, FallbackT, false>\n{\n  using type = typename ::cuda::std::conditional<\n    ::cuda::std::is_same<typename std::iterator_traits<It>::value_type, void>::value,\n    FallbackT,\n    typename std::iterator_traits<It>::value_type>::type;\n};\n\n/**\n * The output value type\n * type = (if IteratorT's value type is void) ?\n * ... then the FallbackT,\n * ... else the IteratorT's value type\n */\ntemplate <typename It, typename FallbackT>\nusing non_void_value_t = typename non_void_value_impl<It, FallbackT>::type;\n} // namespace detail\n\n\n/**\n * \\brief Type selection (<tt>IF ? ThenType : ElseType</tt>)\n *\n * \\deprecated [Since 1.16.0] The cub::If APIs are deprecated.\n *             Use cub::detail::conditional_t instead.\n */\ntemplate <bool IF, typename ThenType, typename ElseType>\nstruct CUB_DEPRECATED If\n{\n  using Type = cub::detail::conditional_t<IF, ThenType, ElseType>;\n};\n\n\n/******************************************************************************\n * Type equality\n ******************************************************************************/\n\n/**\n * \\brief Type equality test\n *\n * \\deprecated [Since 1.16.0] The cub::Equals APIs are deprecated.\n *             Use std::is_same instead.\n */\ntemplate <typename A, typename B>\nstruct CUB_DEPRECATED Equals\n{\n  static constexpr int VALUE = std::is_same<A, B>::value ? 1 : 0;\n  static constexpr int NEGATE = VALUE ? 0 : 1;\n};\n\n\n/******************************************************************************\n * Static math\n ******************************************************************************/\n\n/**\n * \\brief Statically determine log2(N), rounded up.\n *\n * For example:\n *     Log2<8>::VALUE   // 3\n *     Log2<3>::VALUE   // 2\n */\ntemplate <int N, int CURRENT_VAL = N, int COUNT = 0>\nstruct Log2\n{\n    /// Static logarithm value\n    enum { VALUE = Log2<N, (CURRENT_VAL >> 1), COUNT + 1>::VALUE };         // Inductive case\n};\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS    // Do not document\n\ntemplate <int N, int COUNT>\nstruct Log2<N, 0, COUNT>\n{\n    enum {VALUE = (1 << (COUNT - 1) < N) ?                                  // Base case\n        COUNT :\n        COUNT - 1 };\n};\n\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\n/**\n * \\brief Statically determine if N is a power-of-two\n */\ntemplate <int N>\nstruct PowerOfTwo\n{\n    enum { VALUE = ((N & (N - 1)) == 0) };\n};\n\n\n\n/******************************************************************************\n * Pointer vs. iterator detection\n ******************************************************************************/\n\n/**\n * \\brief Pointer vs. iterator\n *\n * \\deprecated [Since 1.16.0] The cub::IsPointer APIs are deprecated.\n *             Use std::is_pointer instead.\n */\ntemplate <typename Tp>\nstruct CUB_DEPRECATED IsPointer\n{\n  static constexpr int VALUE = std::is_pointer<Tp>::value;\n};\n\n\n/******************************************************************************\n * Qualifier detection\n ******************************************************************************/\n\n/**\n * \\brief Volatile modifier test\n *\n * \\deprecated [Since 1.16.0] The cub::IsVolatile APIs are deprecated.\n *             Use std::is_volatile instead.\n */\ntemplate <typename Tp>\nstruct CUB_DEPRECATED IsVolatile\n{\n  static constexpr int VALUE = std::is_volatile<Tp>::value;\n};\n\n/******************************************************************************\n * Qualifier removal\n ******************************************************************************/\n\n/**\n * \\brief Removes \\p const and \\p volatile qualifiers from type \\p Tp.\n *\n * \\deprecated [Since 1.16.0] The cub::RemoveQualifiers APIs are deprecated.\n *             Use std::remove_cv instead.\n *\n * For example:\n *     <tt>typename RemoveQualifiers<volatile int>::Type         // int;</tt>\n */\ntemplate <typename Tp, typename Up = Tp>\nstruct CUB_DEPRECATED RemoveQualifiers\n{\n  using Type = typename std::remove_cv<Tp>::type;\n};\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\n\n/******************************************************************************\n * Marker types\n ******************************************************************************/\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS    // Do not document\n\n/**\n * \\brief A simple \"NULL\" marker type\n */\nstruct NullType\n{\n    using value_type = NullType;\n\n    template <typename T>\n    __host__ __device__ __forceinline__ NullType& operator =(const T&) { return *this; }\n\n    __host__ __device__ __forceinline__ bool operator ==(const NullType&) { return true; }\n\n    __host__ __device__ __forceinline__ bool operator !=(const NullType&) { return false; }\n};\n\n\n/**\n * \\brief Allows for the treatment of an integral constant as a type at compile-time (e.g., to achieve static call dispatch based on constant integral values)\n */\ntemplate <int A>\nstruct Int2Type\n{\n    enum {VALUE = A};\n};\n\n/**\n * \\brief Allows algorithms that take a value as input to take a future value that is not computed yet at launch time.\n *\n * Note that it is user's responsibility to ensure that the result will be ready before use via external synchronization\n * or stream-ordering dependencies.\n *\n * \\code\n * int *d_intermediate_result;\n * allocator.DeviceAllocate((void **)&d_intermediate_result, sizeof(int));\n * compute_intermediate_result<<<blocks, threads>>>(\n *     d_intermediate_result,  // output\n *     arg1,                   // input\n *     arg2);                  // input\n * cub::FutureValue<int> init_value(d_intermediate_result);\n * cub::DeviceScan::ExclusiveScan(\n *     d_temp_storage,\n *     temp_storage_bytes,\n *     d_in,\n *     d_out,\n *     cub::Sum(),\n *     init_value,\n *     num_items);\n * allocator.DeviceFree(d_intermediate_result);\n * \\endcode\n */\ntemplate <typename T, typename IterT = T*>\nstruct FutureValue\n{\n    using value_type = T;\n    using iterator_type = IterT;\n    explicit __host__ __device__ __forceinline__ FutureValue(IterT iter):m_iter(iter) {}\n    __host__ __device__ __forceinline__ operator T() {\n        return *m_iter;\n    }\n\nprivate:\n    IterT m_iter;\n};\n\nnamespace detail {\n\n/**\n * \\brief Allows algorithms to instantiate a single kernel to support both immediate value and future value.\n */\ntemplate <typename T, typename IterT = T*>\nstruct InputValue\n{\n    using value_type = T;\n    using iterator_type = IterT;\n    __host__ __device__ __forceinline__ operator T() {\n        if (m_is_future) {\n            return m_future_value;\n        }\n        return m_immediate_value;\n    }\n    explicit __host__ __device__ __forceinline__ InputValue(T immediate_value): m_is_future(false), m_immediate_value(immediate_value) {}\n    explicit __host__ __device__ __forceinline__ InputValue(FutureValue<T, IterT> future_value): m_is_future(true), m_future_value(future_value) {}\n    __host__ __device__ __forceinline__ InputValue(const InputValue &other): m_is_future(other.m_is_future) {\n        if (m_is_future) {\n            m_future_value = other.m_future_value;\n        } else {\n          detail::uninitialized_copy(&m_immediate_value,\n                                     other.m_immediate_value);\n        }\n    }\n\nprivate:\n    bool m_is_future;\n    union\n    {\n        FutureValue<T, IterT> m_future_value;\n        T m_immediate_value;\n    };\n};\n\n} // namespace detail\n\n\n/******************************************************************************\n * Size and alignment\n ******************************************************************************/\n\n/// Structure alignment\ntemplate <typename T>\nstruct AlignBytes\n{\n    struct Pad\n    {\n        T       val;\n        char    byte;\n    };\n\n    enum\n    {\n        /// The \"true CUDA\" alignment of T in bytes\n        ALIGN_BYTES = sizeof(Pad) - sizeof(T)\n    };\n\n    /// The \"truly aligned\" type\n    typedef T Type;\n};\n\n// Specializations where host C++ compilers (e.g., 32-bit Windows) may disagree\n// with device C++ compilers (EDG) on types passed as template parameters through\n// kernel functions\n\n#define __CUB_ALIGN_BYTES(t, b)         \\\n    template <> struct AlignBytes<t>    \\\n    { enum { ALIGN_BYTES = b }; typedef __align__(b) t Type; };\n\n__CUB_ALIGN_BYTES(short4, 8)\n__CUB_ALIGN_BYTES(ushort4, 8)\n__CUB_ALIGN_BYTES(int2, 8)\n__CUB_ALIGN_BYTES(uint2, 8)\n__CUB_ALIGN_BYTES(long long, 8)\n__CUB_ALIGN_BYTES(unsigned long long, 8)\n__CUB_ALIGN_BYTES(float2, 8)\n__CUB_ALIGN_BYTES(double, 8)\n#ifdef _WIN32\n    __CUB_ALIGN_BYTES(long2, 8)\n    __CUB_ALIGN_BYTES(ulong2, 8)\n#else\n    __CUB_ALIGN_BYTES(long2, 16)\n    __CUB_ALIGN_BYTES(ulong2, 16)\n#endif\n__CUB_ALIGN_BYTES(int4, 16)\n__CUB_ALIGN_BYTES(uint4, 16)\n__CUB_ALIGN_BYTES(float4, 16)\n__CUB_ALIGN_BYTES(long4, 16)\n__CUB_ALIGN_BYTES(ulong4, 16)\n__CUB_ALIGN_BYTES(longlong2, 16)\n__CUB_ALIGN_BYTES(ulonglong2, 16)\n__CUB_ALIGN_BYTES(double2, 16)\n__CUB_ALIGN_BYTES(longlong4, 16)\n__CUB_ALIGN_BYTES(ulonglong4, 16)\n__CUB_ALIGN_BYTES(double4, 16)\n\n// clang-format off\ntemplate <typename T> struct AlignBytes<volatile T> : AlignBytes<T> {};\ntemplate <typename T> struct AlignBytes<const T> : AlignBytes<T> {};\ntemplate <typename T> struct AlignBytes<const volatile T> : AlignBytes<T> {};\n// clang-format on\n\n/// Unit-words of data movement\ntemplate <typename T>\nstruct UnitWord\n{\n    enum {\n        ALIGN_BYTES = AlignBytes<T>::ALIGN_BYTES\n    };\n\n    template <typename Unit>\n    struct IsMultiple\n    {\n        enum {\n            UNIT_ALIGN_BYTES    = AlignBytes<Unit>::ALIGN_BYTES,\n            IS_MULTIPLE         = (sizeof(T) % sizeof(Unit) == 0) && (int(ALIGN_BYTES) % int(UNIT_ALIGN_BYTES) == 0)\n        };\n    };\n\n    /// Biggest shuffle word that T is a whole multiple of and is not larger than\n    /// the alignment of T\n    using ShuffleWord = cub::detail::conditional_t<\n      IsMultiple<int>::IS_MULTIPLE,\n      unsigned int,\n      cub::detail::conditional_t<IsMultiple<short>::IS_MULTIPLE,\n                                 unsigned short,\n                                 unsigned char>>;\n\n    /// Biggest volatile word that T is a whole multiple of and is not larger than\n    /// the alignment of T\n    using VolatileWord =\n      cub::detail::conditional_t<IsMultiple<long long>::IS_MULTIPLE,\n                                 unsigned long long,\n                                 ShuffleWord>;\n\n    /// Biggest memory-access word that T is a whole multiple of and is not larger\n    /// than the alignment of T\n    using DeviceWord =\n      cub::detail::conditional_t<IsMultiple<longlong2>::IS_MULTIPLE,\n                                 ulonglong2,\n                                 VolatileWord>;\n\n    /// Biggest texture reference word that T is a whole multiple of and is not\n    /// larger than the alignment of T\n    using TextureWord = cub::detail::conditional_t<\n      IsMultiple<int4>::IS_MULTIPLE,\n      uint4,\n      cub::detail::conditional_t<IsMultiple<int2>::IS_MULTIPLE, uint2, ShuffleWord>>;\n};\n\n// float2 specialization workaround (for SM10-SM13)\ntemplate <>\nstruct UnitWord <float2>\n{\n    typedef int         ShuffleWord;\n    typedef unsigned long long   VolatileWord;\n    typedef unsigned long long   DeviceWord;\n    typedef float2      TextureWord;\n};\n\n// float4 specialization workaround (for SM10-SM13)\ntemplate <>\nstruct UnitWord <float4>\n{\n    typedef int         ShuffleWord;\n    typedef unsigned long long  VolatileWord;\n    typedef ulonglong2          DeviceWord;\n    typedef float4              TextureWord;\n};\n\n\n// char2 specialization workaround (for SM10-SM13)\ntemplate <>\nstruct UnitWord <char2>\n{\n    typedef unsigned short      ShuffleWord;\n    typedef unsigned short      VolatileWord;\n    typedef unsigned short      DeviceWord;\n    typedef unsigned short      TextureWord;\n};\n\n// clang-format off\ntemplate <typename T> struct UnitWord<volatile T> : UnitWord<T> {};\ntemplate <typename T> struct UnitWord<const T> : UnitWord<T> {};\ntemplate <typename T> struct UnitWord<const volatile T> : UnitWord<T> {};\n// clang-format on\n\n\n/******************************************************************************\n * Vector type inference utilities.\n ******************************************************************************/\n\n/**\n * \\brief Exposes a member typedef \\p Type that names the corresponding CUDA vector type if one exists.  Otherwise \\p Type refers to the CubVector structure itself, which will wrap the corresponding \\p x, \\p y, etc. vector fields.\n */\ntemplate <typename T, int vec_elements> struct CubVector;\n\n\nenum\n{\n    /// The maximum number of elements in CUDA vector types\n    MAX_VEC_ELEMENTS = 4,\n};\n\n\n/**\n * Generic vector-1 type\n */\ntemplate <typename T>\nstruct CubVector<T, 1>\n{\n    T x;\n\n    typedef T BaseType;\n    typedef CubVector<T, 1> Type;\n};\n\n/**\n * Generic vector-2 type\n */\ntemplate <typename T>\nstruct CubVector<T, 2>\n{\n    T x;\n    T y;\n\n    typedef T BaseType;\n    typedef CubVector<T, 2> Type;\n};\n\n/**\n * Generic vector-3 type\n */\ntemplate <typename T>\nstruct CubVector<T, 3>\n{\n    T x;\n    T y;\n    T z;\n\n    typedef T BaseType;\n    typedef CubVector<T, 3> Type;\n};\n\n/**\n * Generic vector-4 type\n */\ntemplate <typename T>\nstruct CubVector<T, 4>\n{\n    T x;\n    T y;\n    T z;\n    T w;\n\n    typedef T BaseType;\n    typedef CubVector<T, 4> Type;\n};\n\n\n/**\n * Macro for expanding partially-specialized built-in vector types\n */\n#define CUB_DEFINE_VECTOR_TYPE(base_type,short_type)                                                    \\\n                                                                                                        \\\n    template<> struct CubVector<base_type, 1> : short_type##1                                           \\\n    {                                                                                                   \\\n      typedef base_type       BaseType;                                                                 \\\n      typedef short_type##1   Type;                                                                     \\\n      __host__ __device__ __forceinline__ CubVector operator+(const CubVector &other) const {           \\\n          CubVector retval;                                                                             \\\n          retval.x = x + other.x;                                                                       \\\n          return retval;                                                                                \\\n      }                                                                                                 \\\n      __host__ __device__ __forceinline__ CubVector operator-(const CubVector &other) const {           \\\n          CubVector retval;                                                                             \\\n          retval.x = x - other.x;                                                                       \\\n          return retval;                                                                                \\\n      }                                                                                                 \\\n    };                                                                                                  \\\n                                                                                                        \\\n    template<> struct CubVector<base_type, 2> : short_type##2                                           \\\n    {                                                                                                   \\\n        typedef base_type       BaseType;                                                               \\\n        typedef short_type##2   Type;                                                                   \\\n        __host__ __device__ __forceinline__ CubVector operator+(const CubVector &other) const {         \\\n            CubVector retval;                                                                           \\\n            retval.x = x + other.x;                                                                     \\\n            retval.y = y + other.y;                                                                     \\\n            return retval;                                                                              \\\n        }                                                                                               \\\n        __host__ __device__ __forceinline__ CubVector operator-(const CubVector &other) const {         \\\n            CubVector retval;                                                                           \\\n            retval.x = x - other.x;                                                                     \\\n            retval.y = y - other.y;                                                                     \\\n            return retval;                                                                              \\\n        }                                                                                               \\\n    };                                                                                                  \\\n                                                                                                        \\\n    template<> struct CubVector<base_type, 3> : short_type##3                                           \\\n    {                                                                                                   \\\n        typedef base_type       BaseType;                                                               \\\n        typedef short_type##3   Type;                                                                   \\\n        __host__ __device__ __forceinline__ CubVector operator+(const CubVector &other) const {         \\\n            CubVector retval;                                                                           \\\n            retval.x = x + other.x;                                                                     \\\n            retval.y = y + other.y;                                                                     \\\n            retval.z = z + other.z;                                                                     \\\n            return retval;                                                                              \\\n        }                                                                                               \\\n        __host__ __device__ __forceinline__ CubVector operator-(const CubVector &other) const {         \\\n            CubVector retval;                                                                           \\\n            retval.x = x - other.x;                                                                     \\\n            retval.y = y - other.y;                                                                     \\\n            retval.z = z - other.z;                                                                     \\\n            return retval;                                                                              \\\n        }                                                                                               \\\n    };                                                                                                  \\\n                                                                                                        \\\n    template<> struct CubVector<base_type, 4> : short_type##4                                           \\\n    {                                                                                                   \\\n        typedef base_type       BaseType;                                                               \\\n        typedef short_type##4   Type;                                                                   \\\n        __host__ __device__ __forceinline__ CubVector operator+(const CubVector &other) const {         \\\n            CubVector retval;                                                                           \\\n            retval.x = x + other.x;                                                                     \\\n            retval.y = y + other.y;                                                                     \\\n            retval.z = z + other.z;                                                                     \\\n            retval.w = w + other.w;                                                                     \\\n            return retval;                                                                              \\\n        }                                                                                               \\\n        __host__ __device__ __forceinline__ CubVector operator-(const CubVector &other) const {         \\\n            CubVector retval;                                                                           \\\n            retval.x = x - other.x;                                                                     \\\n            retval.y = y - other.y;                                                                     \\\n            retval.z = z - other.z;                                                                     \\\n            retval.w = w - other.w;                                                                     \\\n            return retval;                                                                              \\\n        }                                                                                               \\\n    };\n\n\n\n// Expand CUDA vector types for built-in primitives\n// clang-format off\nCUB_DEFINE_VECTOR_TYPE(char,               char)\nCUB_DEFINE_VECTOR_TYPE(signed char,        char)\nCUB_DEFINE_VECTOR_TYPE(short,              short)\nCUB_DEFINE_VECTOR_TYPE(int,                int)\nCUB_DEFINE_VECTOR_TYPE(long,               long)\nCUB_DEFINE_VECTOR_TYPE(long long,          longlong)\nCUB_DEFINE_VECTOR_TYPE(unsigned char,      uchar)\nCUB_DEFINE_VECTOR_TYPE(unsigned short,     ushort)\nCUB_DEFINE_VECTOR_TYPE(unsigned int,       uint)\nCUB_DEFINE_VECTOR_TYPE(unsigned long,      ulong)\nCUB_DEFINE_VECTOR_TYPE(unsigned long long, ulonglong)\nCUB_DEFINE_VECTOR_TYPE(float,              float)\nCUB_DEFINE_VECTOR_TYPE(double,             double)\nCUB_DEFINE_VECTOR_TYPE(bool,               uchar)\n// clang-format on\n\n// Undefine macros\n#undef CUB_DEFINE_VECTOR_TYPE\n\n\n/******************************************************************************\n * Wrapper types\n ******************************************************************************/\n\n/**\n * \\brief A storage-backing wrapper that allows types with non-trivial constructors to be aliased in unions\n */\ntemplate <typename T>\nstruct Uninitialized\n{\n    /// Biggest memory-access word that T is a whole multiple of and is not larger than the alignment of T\n    typedef typename UnitWord<T>::DeviceWord DeviceWord;\n\n    static constexpr std::size_t DATA_SIZE = sizeof(T);\n    static constexpr std::size_t WORD_SIZE = sizeof(DeviceWord);\n    static constexpr std::size_t WORDS = DATA_SIZE / WORD_SIZE;\n\n    /// Backing storage\n    DeviceWord storage[WORDS];\n\n    /// Alias\n    __host__ __device__ __forceinline__ T& Alias()\n    {\n        return reinterpret_cast<T&>(*this);\n    }\n};\n\n\n/**\n * \\brief A key identifier paired with a corresponding value\n */\ntemplate <\n    typename    _Key,\n    typename    _Value\n#if defined(_WIN32) && !defined(_WIN64)\n    , bool KeyIsLT = (AlignBytes<_Key>::ALIGN_BYTES < AlignBytes<_Value>::ALIGN_BYTES)\n    , bool ValIsLT = (AlignBytes<_Value>::ALIGN_BYTES < AlignBytes<_Key>::ALIGN_BYTES)\n#endif // #if defined(_WIN32) && !defined(_WIN64)\n    >\nstruct KeyValuePair\n{\n    typedef _Key    Key;                ///< Key data type\n    typedef _Value  Value;              ///< Value data type\n\n    Key     key;                        ///< Item key\n    Value   value;                      ///< Item value\n\n    /// Constructor\n    __host__ __device__ __forceinline__\n    KeyValuePair() {}\n\n    /// Constructor\n    __host__ __device__ __forceinline__\n    KeyValuePair(Key const& key, Value const& value) : key(key), value(value) {}\n\n    /// Inequality operator\n    __host__ __device__ __forceinline__ bool operator !=(const KeyValuePair &b)\n    {\n        return (value != b.value) || (key != b.key);\n    }\n};\n\n#if defined(_WIN32) && !defined(_WIN64)\n\n/**\n * Win32 won't do 16B alignment.  This can present two problems for\n * should-be-16B-aligned (but actually 8B aligned) built-in and intrinsics members:\n * 1) If a smaller-aligned item were to be listed first, the host compiler places the\n *    should-be-16B item at too early an offset (and disagrees with device compiler)\n * 2) Or, if a smaller-aligned item lists second, the host compiler gets the size\n *    of the struct wrong (and disagrees with device compiler)\n *\n * So we put the larger-should-be-aligned item first, and explicitly pad the\n * end of the struct\n */\n\n/// Smaller key specialization\ntemplate <typename K, typename V>\nstruct KeyValuePair<K, V, true, false>\n{\n    typedef K Key;\n    typedef V Value;\n\n    typedef char Pad[AlignBytes<V>::ALIGN_BYTES - AlignBytes<K>::ALIGN_BYTES];\n\n    Value   value;  // Value has larger would-be alignment and goes first\n    Key     key;\n    Pad     pad;\n\n    /// Constructor\n    __host__ __device__ __forceinline__\n    KeyValuePair() {}\n\n    /// Constructor\n    __host__ __device__ __forceinline__\n    KeyValuePair(Key const& key, Value const& value) : key(key), value(value) {}\n\n    /// Inequality operator\n    __host__ __device__ __forceinline__ bool operator !=(const KeyValuePair &b)\n    {\n        return (value != b.value) || (key != b.key);\n    }\n};\n\n\n/// Smaller value specialization\ntemplate <typename K, typename V>\nstruct KeyValuePair<K, V, false, true>\n{\n    typedef K Key;\n    typedef V Value;\n\n    typedef char Pad[AlignBytes<K>::ALIGN_BYTES - AlignBytes<V>::ALIGN_BYTES];\n\n    Key     key;    // Key has larger would-be alignment and goes first\n    Value   value;\n    Pad     pad;\n\n    /// Constructor\n    __host__ __device__ __forceinline__\n    KeyValuePair() {}\n\n    /// Constructor\n    __host__ __device__ __forceinline__\n    KeyValuePair(Key const& key, Value const& value) : key(key), value(value) {}\n\n    /// Inequality operator\n    __host__ __device__ __forceinline__ bool operator !=(const KeyValuePair &b)\n    {\n        return (value != b.value) || (key != b.key);\n    }\n};\n\n#endif // #if defined(_WIN32) && !defined(_WIN64)\n\n\n/**\n * \\brief A wrapper for passing simple static arrays as kernel parameters\n */\ntemplate <typename T, int COUNT>\nstruct ArrayWrapper\n{\n\n    /// Statically-sized array of type \\p T\n    T array[COUNT];\n\n    /// Constructor\n    __host__ __device__ __forceinline__ ArrayWrapper() {}\n};\n\n\n/**\n * \\brief Double-buffer storage wrapper for multi-pass stream transformations that require more than one storage array for streaming intermediate results back and forth.\n *\n * Many multi-pass computations require a pair of \"ping-pong\" storage\n * buffers (e.g., one for reading from and the other for writing to, and then\n * vice-versa for the subsequent pass).  This structure wraps a set of device\n * buffers and a \"selector\" member to track which is \"current\".\n */\ntemplate <typename T>\nstruct DoubleBuffer\n{\n    /// Pair of device buffer pointers\n    T *d_buffers[2];\n\n    ///  Selector into \\p d_buffers (i.e., the active/valid buffer)\n    int selector;\n\n    /// \\brief Constructor\n    __host__ __device__ __forceinline__ DoubleBuffer()\n    {\n        selector = 0;\n        d_buffers[0] = NULL;\n        d_buffers[1] = NULL;\n    }\n\n    /// \\brief Constructor\n    __host__ __device__ __forceinline__ DoubleBuffer(\n        T *d_current,         ///< The currently valid buffer\n        T *d_alternate)       ///< Alternate storage buffer of the same size as \\p d_current\n    {\n        selector = 0;\n        d_buffers[0] = d_current;\n        d_buffers[1] = d_alternate;\n    }\n\n    /// \\brief Return pointer to the currently valid buffer\n    __host__ __device__ __forceinline__ T* Current() { return d_buffers[selector]; }\n\n    /// \\brief Return pointer to the currently invalid buffer\n    __host__ __device__ __forceinline__ T* Alternate() { return d_buffers[selector ^ 1]; }\n\n};\n\n\n\n/******************************************************************************\n * Typedef-detection\n ******************************************************************************/\n\n\n/**\n * \\brief Defines a structure \\p detector_name that is templated on type \\p T.  The \\p detector_name struct exposes a constant member \\p VALUE indicating whether or not parameter \\p T exposes a nested type \\p nested_type_name\n */\n#define CUB_DEFINE_DETECT_NESTED_TYPE(detector_name, nested_type_name)  \\\n    template <typename T>                                               \\\n    struct detector_name                                                \\\n    {                                                                   \\\n        template <typename C>                                           \\\n        static char& test(typename C::nested_type_name*);               \\\n        template <typename>                                             \\\n        static int& test(...);                                          \\\n        enum                                                            \\\n        {                                                               \\\n            VALUE = sizeof(test<T>(0)) < sizeof(int)                    \\\n        };                                                              \\\n    };\n\n\n\n/******************************************************************************\n * Simple enable-if (similar to Boost)\n ******************************************************************************/\n\n/**\n * \\brief Simple enable-if (similar to Boost)\n *\n * \\deprecated [Since 1.16.0] The cub::If APIs are deprecated.\n *             Use std::enable_if instead.\n */\ntemplate <bool Condition, class T = void>\nstruct CUB_DEPRECATED EnableIf\n{\n  using Type = typename std::enable_if<Condition, T>::type;\n};\n\n/******************************************************************************\n * Typedef-detection\n ******************************************************************************/\n\n/**\n * \\brief Determine whether or not BinaryOp's functor is of the form <tt>bool operator()(const T& a, const T&b)</tt> or <tt>bool operator()(const T& a, const T&b, unsigned int idx)</tt>\n */\ntemplate <typename T, typename BinaryOp>\nstruct BinaryOpHasIdxParam\n{\nprivate:\n/*\n    template <typename BinaryOpT, bool (BinaryOpT::*)(const T &a, const T &b, unsigned int idx) const>  struct SFINAE1 {};\n    template <typename BinaryOpT, bool (BinaryOpT::*)(const T &a, const T &b, unsigned int idx)>        struct SFINAE2 {};\n    template <typename BinaryOpT, bool (BinaryOpT::*)(T a, T b, unsigned int idx) const>                struct SFINAE3 {};\n    template <typename BinaryOpT, bool (BinaryOpT::*)(T a, T b, unsigned int idx)>                      struct SFINAE4 {};\n*/\n    template <typename BinaryOpT, bool (BinaryOpT::*)(const T &a, const T &b, int idx) const>           struct SFINAE5 {};\n    template <typename BinaryOpT, bool (BinaryOpT::*)(const T &a, const T &b, int idx)>                 struct SFINAE6 {};\n    template <typename BinaryOpT, bool (BinaryOpT::*)(T a, T b, int idx) const>                         struct SFINAE7 {};\n    template <typename BinaryOpT, bool (BinaryOpT::*)(T a, T b, int idx)>                               struct SFINAE8 {};\n/*\n    template <typename BinaryOpT> static char Test(SFINAE1<BinaryOpT, &BinaryOpT::operator()> *);\n    template <typename BinaryOpT> static char Test(SFINAE2<BinaryOpT, &BinaryOpT::operator()> *);\n    template <typename BinaryOpT> static char Test(SFINAE3<BinaryOpT, &BinaryOpT::operator()> *);\n    template <typename BinaryOpT> static char Test(SFINAE4<BinaryOpT, &BinaryOpT::operator()> *);\n*/\n    template <typename BinaryOpT> __host__ __device__ static char Test(SFINAE5<BinaryOpT, &BinaryOpT::operator()> *);\n    template <typename BinaryOpT> __host__ __device__ static char Test(SFINAE6<BinaryOpT, &BinaryOpT::operator()> *);\n    template <typename BinaryOpT> __host__ __device__ static char Test(SFINAE7<BinaryOpT, &BinaryOpT::operator()> *);\n    template <typename BinaryOpT> __host__ __device__ static char Test(SFINAE8<BinaryOpT, &BinaryOpT::operator()> *);\n\n    template <typename BinaryOpT> static int Test(...);\n\npublic:\n\n    /// Whether the functor BinaryOp has a third <tt>unsigned int</tt> index param\n    static const bool HAS_PARAM = sizeof(Test<BinaryOp>(NULL)) == sizeof(char);\n};\n\n\n\n\n/******************************************************************************\n * Simple type traits utilities.\n *\n * For example:\n *     Traits<int>::CATEGORY             // SIGNED_INTEGER\n *     Traits<NullType>::NULL_TYPE       // true\n *     Traits<uint4>::CATEGORY           // NOT_A_NUMBER\n *     Traits<uint4>::PRIMITIVE;         // false\n *\n ******************************************************************************/\n\n/**\n * \\brief Basic type traits categories\n */\nenum Category\n{\n    NOT_A_NUMBER,\n    SIGNED_INTEGER,\n    UNSIGNED_INTEGER,\n    FLOATING_POINT\n};\n\n\n/**\n * \\brief Basic type traits\n */\ntemplate <Category _CATEGORY, bool _PRIMITIVE, bool _NULL_TYPE, typename _UnsignedBits, typename T>\nstruct BaseTraits\n{\n    /// Category\n    static const Category CATEGORY      = _CATEGORY;\n    enum\n    {\n        PRIMITIVE       = _PRIMITIVE,\n        NULL_TYPE       = _NULL_TYPE,\n    };\n};\n\n\n/**\n * Basic type traits (unsigned primitive specialization)\n */\ntemplate <typename _UnsignedBits, typename T>\nstruct BaseTraits<UNSIGNED_INTEGER, true, false, _UnsignedBits, T>\n{\n    typedef _UnsignedBits       UnsignedBits;\n\n    static const Category       CATEGORY    = UNSIGNED_INTEGER;\n    static const UnsignedBits   LOWEST_KEY  = UnsignedBits(0);\n    static const UnsignedBits   MAX_KEY     = UnsignedBits(-1);\n\n    enum\n    {\n        PRIMITIVE       = true,\n        NULL_TYPE       = false,\n    };\n\n\n    static __host__ __device__ __forceinline__ UnsignedBits TwiddleIn(UnsignedBits key)\n    {\n        return key;\n    }\n\n    static __host__ __device__ __forceinline__ UnsignedBits TwiddleOut(UnsignedBits key)\n    {\n        return key;\n    }\n\n    static __host__ __device__ __forceinline__ T Max()\n    {\n        UnsignedBits retval_bits = MAX_KEY;\n        T retval;\n        memcpy(&retval, &retval_bits, sizeof(T));\n        return retval;\n    }\n\n    static __host__ __device__ __forceinline__ T Lowest()\n    {\n        UnsignedBits retval_bits = LOWEST_KEY;\n        T retval;\n        memcpy(&retval, &retval_bits, sizeof(T));\n        return retval;\n    }\n};\n\n\n/**\n * Basic type traits (signed primitive specialization)\n */\ntemplate <typename _UnsignedBits, typename T>\nstruct BaseTraits<SIGNED_INTEGER, true, false, _UnsignedBits, T>\n{\n    typedef _UnsignedBits       UnsignedBits;\n\n    static const Category       CATEGORY    = SIGNED_INTEGER;\n    static const UnsignedBits   HIGH_BIT    = UnsignedBits(1) << ((sizeof(UnsignedBits) * 8) - 1);\n    static const UnsignedBits   LOWEST_KEY  = HIGH_BIT;\n    static const UnsignedBits   MAX_KEY     = UnsignedBits(-1) ^ HIGH_BIT;\n\n    enum\n    {\n        PRIMITIVE       = true,\n        NULL_TYPE       = false,\n    };\n\n    static __host__ __device__ __forceinline__ UnsignedBits TwiddleIn(UnsignedBits key)\n    {\n        return key ^ HIGH_BIT;\n    };\n\n    static __host__ __device__ __forceinline__ UnsignedBits TwiddleOut(UnsignedBits key)\n    {\n        return key ^ HIGH_BIT;\n    };\n\n    static __host__ __device__ __forceinline__ T Max()\n    {\n        UnsignedBits retval = MAX_KEY;\n        return reinterpret_cast<T&>(retval);\n    }\n\n    static __host__ __device__ __forceinline__ T Lowest()\n    {\n        UnsignedBits retval = LOWEST_KEY;\n        return reinterpret_cast<T&>(retval);\n    }\n};\n\ntemplate <typename _T>\nstruct FpLimits;\n\ntemplate <>\nstruct FpLimits<float>\n{\n    static __host__ __device__ __forceinline__ float Max() {\n        return FLT_MAX;\n    }\n\n    static __host__ __device__ __forceinline__ float Lowest() {\n        return FLT_MAX * float(-1);\n    }\n};\n\ntemplate <>\nstruct FpLimits<double>\n{\n    static __host__ __device__ __forceinline__ double Max() {\n        return DBL_MAX;\n    }\n\n    static __host__ __device__ __forceinline__ double Lowest() {\n        return DBL_MAX  * double(-1);\n    }\n};\n\n#if !_NVHPC_CUDA\ntemplate <>\nstruct FpLimits<__half>\n{\n    static __host__ __device__ __forceinline__ __half Max() {\n        unsigned short max_word = 0x7BFF;\n        return reinterpret_cast<__half&>(max_word);\n    }\n\n    static __host__ __device__ __forceinline__ __half Lowest() {\n        unsigned short lowest_word = 0xFBFF;\n        return reinterpret_cast<__half&>(lowest_word);\n    }\n};\n#endif\n\n#if !_NVHPC_CUDA && !defined(CUB_DISABLE_BF16_SUPPORT)\ntemplate <>\nstruct FpLimits<__nv_bfloat16>\n{\n    static __host__ __device__ __forceinline__ __nv_bfloat16 Max() {\n        unsigned short max_word = 0x7F7F;\n        return reinterpret_cast<__nv_bfloat16&>(max_word);\n    }\n\n    static __host__ __device__ __forceinline__ __nv_bfloat16 Lowest() {\n        unsigned short lowest_word = 0xFF7F;\n        return reinterpret_cast<__nv_bfloat16&>(lowest_word);\n    }\n};\n#endif\n\n/**\n * Basic type traits (fp primitive specialization)\n */\ntemplate <typename _UnsignedBits, typename T>\nstruct BaseTraits<FLOATING_POINT, true, false, _UnsignedBits, T>\n{\n    typedef _UnsignedBits       UnsignedBits;\n\n    static const Category       CATEGORY    = FLOATING_POINT;\n    static const UnsignedBits   HIGH_BIT    = UnsignedBits(1) << ((sizeof(UnsignedBits) * 8) - 1);\n    static const UnsignedBits   LOWEST_KEY  = UnsignedBits(-1);\n    static const UnsignedBits   MAX_KEY     = UnsignedBits(-1) ^ HIGH_BIT;\n\n    enum\n    {\n        PRIMITIVE       = true,\n        NULL_TYPE       = false,\n    };\n\n    static __host__ __device__ __forceinline__ UnsignedBits TwiddleIn(UnsignedBits key)\n    {\n        UnsignedBits mask = (key & HIGH_BIT) ? UnsignedBits(-1) : HIGH_BIT;\n        return key ^ mask;\n    };\n\n    static __host__ __device__ __forceinline__ UnsignedBits TwiddleOut(UnsignedBits key)\n    {\n        UnsignedBits mask = (key & HIGH_BIT) ? HIGH_BIT : UnsignedBits(-1);\n        return key ^ mask;\n    };\n\n    static __host__ __device__ __forceinline__ T Max() {\n        return FpLimits<T>::Max();\n    }\n\n    static __host__ __device__ __forceinline__ T Lowest() {\n        return FpLimits<T>::Lowest();\n    }\n};\n\n\n/**\n * \\brief Numeric type traits\n */\n// clang-format off\ntemplate <typename T> struct NumericTraits :            BaseTraits<NOT_A_NUMBER, false, false, T, T> {};\n\ntemplate <> struct NumericTraits<NullType> :            BaseTraits<NOT_A_NUMBER, false, true, NullType, NullType> {};\n\ntemplate <> struct NumericTraits<char> :                BaseTraits<(std::numeric_limits<char>::is_signed) ? SIGNED_INTEGER : UNSIGNED_INTEGER, true, false, unsigned char, char> {};\ntemplate <> struct NumericTraits<signed char> :         BaseTraits<SIGNED_INTEGER, true, false, unsigned char, signed char> {};\ntemplate <> struct NumericTraits<short> :               BaseTraits<SIGNED_INTEGER, true, false, unsigned short, short> {};\ntemplate <> struct NumericTraits<int> :                 BaseTraits<SIGNED_INTEGER, true, false, unsigned int, int> {};\ntemplate <> struct NumericTraits<long> :                BaseTraits<SIGNED_INTEGER, true, false, unsigned long, long> {};\ntemplate <> struct NumericTraits<long long> :           BaseTraits<SIGNED_INTEGER, true, false, unsigned long long, long long> {};\n\ntemplate <> struct NumericTraits<unsigned char> :       BaseTraits<UNSIGNED_INTEGER, true, false, unsigned char, unsigned char> {};\ntemplate <> struct NumericTraits<unsigned short> :      BaseTraits<UNSIGNED_INTEGER, true, false, unsigned short, unsigned short> {};\ntemplate <> struct NumericTraits<unsigned int> :        BaseTraits<UNSIGNED_INTEGER, true, false, unsigned int, unsigned int> {};\ntemplate <> struct NumericTraits<unsigned long> :       BaseTraits<UNSIGNED_INTEGER, true, false, unsigned long, unsigned long> {};\ntemplate <> struct NumericTraits<unsigned long long> :  BaseTraits<UNSIGNED_INTEGER, true, false, unsigned long long, unsigned long long> {};\n\n\n#if CUB_IS_INT128_ENABLED \ntemplate <>\nstruct NumericTraits<__uint128_t>\n{\n  using T = __uint128_t;\n  using UnsignedBits = __uint128_t;\n\n  static constexpr Category       CATEGORY    = UNSIGNED_INTEGER;\n  static constexpr UnsignedBits   LOWEST_KEY  = UnsignedBits(0);\n  static constexpr UnsignedBits   MAX_KEY     = UnsignedBits(-1);\n\n  static constexpr bool PRIMITIVE = false;\n  static constexpr bool NULL_TYPE = false;\n\n  static __host__ __device__ __forceinline__ UnsignedBits TwiddleIn(UnsignedBits key)\n  {\n    return key;\n  }\n\n  static __host__ __device__ __forceinline__ UnsignedBits TwiddleOut(UnsignedBits key)\n  {\n    return key;\n  }\n\n  static __host__ __device__ __forceinline__ T Max()\n  {\n    return MAX_KEY;\n  }\n\n  static __host__ __device__ __forceinline__ T Lowest()\n  {\n    return LOWEST_KEY;\n  }\n};\n\ntemplate <>\nstruct NumericTraits<__int128_t>\n{\n  using T = __int128_t;\n  using UnsignedBits = __uint128_t;\n\n  static constexpr Category       CATEGORY    = SIGNED_INTEGER;\n  static constexpr UnsignedBits   HIGH_BIT    = UnsignedBits(1) << ((sizeof(UnsignedBits) * 8) - 1);\n  static constexpr UnsignedBits   LOWEST_KEY  = HIGH_BIT;\n  static constexpr UnsignedBits   MAX_KEY     = UnsignedBits(-1) ^ HIGH_BIT;\n\n  static constexpr bool PRIMITIVE = false;\n  static constexpr bool NULL_TYPE = false;\n\n  static __host__ __device__ __forceinline__ UnsignedBits TwiddleIn(UnsignedBits key)\n  {\n    return key ^ HIGH_BIT;\n  };\n\n  static __host__ __device__ __forceinline__ UnsignedBits TwiddleOut(UnsignedBits key)\n  {\n    return key ^ HIGH_BIT;\n  };\n\n  static __host__ __device__ __forceinline__ T Max()\n  {\n    UnsignedBits retval = MAX_KEY;\n    return reinterpret_cast<T&>(retval);\n  }\n\n  static __host__ __device__ __forceinline__ T Lowest()\n  {\n    UnsignedBits retval = LOWEST_KEY;\n    return reinterpret_cast<T&>(retval);\n  }\n};\n#endif\n\ntemplate <> struct NumericTraits<float> :               BaseTraits<FLOATING_POINT, true, false, unsigned int, float> {};\ntemplate <> struct NumericTraits<double> :              BaseTraits<FLOATING_POINT, true, false, unsigned long long, double> {};\n#if !_NVHPC_CUDA\n    template <> struct NumericTraits<__half> :          BaseTraits<FLOATING_POINT, true, false, unsigned short, __half> {};\n#endif\n#if !_NVHPC_CUDA && !defined(CUB_DISABLE_BF16_SUPPORT)\n    template <> struct NumericTraits<__nv_bfloat16> :   BaseTraits<FLOATING_POINT, true, false, unsigned short, __nv_bfloat16> {};\n#endif\n\ntemplate <> struct NumericTraits<bool> :                BaseTraits<UNSIGNED_INTEGER, true, false, typename UnitWord<bool>::VolatileWord, bool> {};\n// clang-format on\n\n/**\n * \\brief Type traits\n */\ntemplate <typename T>\nstruct Traits : NumericTraits<typename std::remove_cv<T>::type> {};\n\n\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\n\n/** @} */       // end group UtilModule\n\nCUB_NAMESPACE_END\n\n#endif // _JITIFY_INCLUDE_GUARD_1375AABB974C3530\n", "__assert": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___ASSERT\n#define _LIBCUDACXX___ASSERT\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"__verbose_abort\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n// This is for backwards compatibility with code that might have been enabling\n// assertions through the Debug mode previously.\n// TODO: In LLVM 16, make it an error to define _LIBCUDACXX_DEBUG\n#if defined(_LIBCUDACXX_DEBUG)\n# ifndef _LIBCUDACXX_ENABLE_ASSERTIONS\n#   define _LIBCUDACXX_ENABLE_ASSERTIONS 1\n# endif\n#endif\n\n// Automatically enable assertions when the debug mode is enabled.\n#if defined(_LIBCUDACXX_ENABLE_DEBUG_MODE)\n# ifndef _LIBCUDACXX_ENABLE_ASSERTIONS\n#   define _LIBCUDACXX_ENABLE_ASSERTIONS 1\n# endif\n#endif\n\n#ifndef _LIBCUDACXX_ENABLE_ASSERTIONS\n# define _LIBCUDACXX_ENABLE_ASSERTIONS _LIBCUDACXX_ENABLE_ASSERTIONS_DEFAULT\n#endif\n\n#if _LIBCUDACXX_ENABLE_ASSERTIONS != 0 && _LIBCUDACXX_ENABLE_ASSERTIONS != 1\n# error \"_LIBCUDACXX_ENABLE_ASSERTIONS must be set to 0 or 1\"\n#endif\n\n#if _LIBCUDACXX_ENABLE_ASSERTIONS\n# define _LIBCUDACXX_ASSERT(expression, message)                                \\\n    (_LIBCUDACXX_DIAGNOSTIC_PUSH                                                \\\n    _LIBCUDACXX_CLANG_DIAGNOSTIC_IGNORED(\"-Wassume\")                            \\\n    __builtin_expect(static_cast<bool>(expression), 1) ?                        \\\n      (void)0 :                                                                 \\\n      ::_CUDA_VSTD::__libcpp_verbose_abort(\"%s:%d: assertion %s failed: %s\", __FILE__, __LINE__, #expression, message)\n    _LIBCUDACXX_DIAGNOSTIC_POP)\n#elif 0 // !defined(_LIBCUDACXX_ASSERTIONS_DISABLE_ASSUME) && __has_builtin(__builtin_assume)\n# define _LIBCUDACXX_ASSERT(expression, message)                                \\\n    (_LIBCUDACXX_DIAGNOSTIC_PUSH                                                \\\n    _LIBCUDACXX_CLANG_DIAGNOSTIC_IGNORED(\"-Wassume\")                            \\\n    __builtin_assume(static_cast<bool>(expression))                             \\\n    _LIBCUDACXX_DIAGNOSTIC_POP)\n#else\n# define _LIBCUDACXX_ASSERT(expression, message) ((void)0)\n#endif\n\n#endif // _LIBCUDACXX___ASSERT\n", "__availability": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___AVAILABILITY\n#define _LIBCUDACXX___AVAILABILITY\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n// Libc++ is shipped by various vendors. In particular, it is used as a system\n// library on macOS, iOS and other Apple platforms. In order for users to be\n// able to compile a binary that is intended to be deployed to an older version\n// of a platform, Clang provides availability attributes [1]. These attributes\n// can be placed on declarations and are used to describe the life cycle of a\n// symbol in the library.\n//\n// The main goal is to ensure a compile-time error if a symbol that hasn't been\n// introduced in a previously released library is used in a program that targets\n// that previously released library. Normally, this would be a load-time error\n// when one tries to launch the program against the older library.\n//\n// For example, the filesystem library was introduced in the dylib in macOS 10.15.\n// If a user compiles on a macOS 10.15 host but targets macOS 10.13 with their\n// program, the compiler would normally not complain (because the required\n// declarations are in the headers), but the dynamic loader would fail to find\n// the symbols when actually trying to launch the program on macOS 10.13. To\n// turn this into a compile-time issue instead, declarations are annotated with\n// when they were introduced, and the compiler can produce a diagnostic if the\n// program references something that isn't available on the deployment target.\n//\n// This mechanism is general in nature, and any vendor can add their markup to\n// the library (see below). Whenever a new feature is added that requires support\n// in the shared library, a macro should be added below to mark this feature\n// as unavailable. When vendors decide to ship the feature as part of their\n// shared library, they can update the markup appropriately.\n//\n// Furthermore, many features in the standard library have corresponding\n// feature-test macros. When a feature is made unavailable on some deployment\n// target, a macro should be defined to signal that it is unavailable. That\n// macro can then be picked up when feature-test macros are generated (see\n// generate_feature_test_macro_components.py) to make sure that feature-test\n// macros don't announce a feature as being implemented if it has been marked\n// as unavailable.\n//\n// Note that this mechanism is disabled by default in the \"upstream\" libc++.\n// Availability annotations are only meaningful when shipping libc++ inside\n// a platform (i.e. as a system library), and so vendors that want them should\n// turn those annotations on at CMake configuration time.\n//\n// [1]: https://clang.llvm.org/docs/AttributeReference.html#availability\n\n\n// For backwards compatibility, allow users to define _LIBCUDACXX_DISABLE_AVAILABILITY\n// for a while.\n#if defined(_LIBCUDACXX_DISABLE_AVAILABILITY)\n#   if !defined(_LIBCUDACXX_HAS_NO_VENDOR_AVAILABILITY_ANNOTATIONS)\n#       define _LIBCUDACXX_HAS_NO_VENDOR_AVAILABILITY_ANNOTATIONS\n#   endif\n#endif\n\n// Availability markup is disabled when building the library, or when the compiler\n// doesn't support the proper attributes.\n#if defined(_LIBCUDACXX_BUILDING_LIBRARY) ||                                        \\\n    defined(_LIBCXXABI_BUILDING_LIBRARY) ||                                     \\\n    !__has_feature(attribute_availability_with_strict) ||                       \\\n    !__has_feature(attribute_availability_in_templates) ||                      \\\n    !__has_extension(pragma_clang_attribute_external_declaration)\n#   if !defined(_LIBCUDACXX_HAS_NO_VENDOR_AVAILABILITY_ANNOTATIONS)\n#       define _LIBCUDACXX_HAS_NO_VENDOR_AVAILABILITY_ANNOTATIONS\n#   endif\n#endif\n\n#if defined(_LIBCUDACXX_HAS_NO_VENDOR_AVAILABILITY_ANNOTATIONS)\n\n    // This controls the availability of std::shared_mutex and std::shared_timed_mutex,\n    // which were added to the dylib later.\n#   define _LIBCUDACXX_AVAILABILITY_SHARED_MUTEX\n// #   define _LIBCUDACXX_AVAILABILITY_DISABLE_FTM___cpp_lib_shared_mutex\n// #   define _LIBCUDACXX_AVAILABILITY_DISABLE_FTM___cpp_lib_shared_timed_mutex\n\n    // These macros control the availability of std::bad_optional_access and\n    // other exception types. These were put in the shared library to prevent\n    // code bloat from every user program defining the vtable for these exception\n    // types.\n    //\n    // Note that when exceptions are disabled, the methods that normally throw\n    // these exceptions can be used even on older deployment targets, but those\n    // methods will abort instead of throwing.\n#   define _LIBCUDACXX_AVAILABILITY_BAD_OPTIONAL_ACCESS\n#   define _LIBCUDACXX_AVAILABILITY_BAD_VARIANT_ACCESS\n#   define _LIBCUDACXX_AVAILABILITY_BAD_ANY_CAST\n\n    // This controls the availability of std::uncaught_exceptions().\n#   define _LIBCUDACXX_AVAILABILITY_UNCAUGHT_EXCEPTIONS\n\n    // This controls the availability of the sized version of ::operator delete,\n    // ::operator delete[], and their align_val_t variants, which were all added\n    // in C++17, and hence not present in early dylibs.\n#   define _LIBCUDACXX_AVAILABILITY_SIZED_NEW_DELETE\n\n    // This controls the availability of the std::future_error exception.\n    //\n    // Note that when exceptions are disabled, the methods that normally throw\n    // std::future_error can be used even on older deployment targets, but those\n    // methods will abort instead of throwing.\n#   define _LIBCUDACXX_AVAILABILITY_FUTURE_ERROR\n\n    // This controls the availability of std::type_info's vtable.\n    // I can't imagine how using std::type_info can work at all if\n    // this isn't supported.\n#   define _LIBCUDACXX_AVAILABILITY_TYPEINFO_VTABLE\n\n    // This controls the availability of std::locale::category members\n    // (e.g. std::locale::collate), which are defined in the dylib.\n#   define _LIBCUDACXX_AVAILABILITY_LOCALE_CATEGORY\n\n    // This controls the availability of atomic operations on std::shared_ptr\n    // (e.g. `std::atomic_store(std::shared_ptr)`), which require a shared\n    // lock table located in the dylib.\n#   define _LIBCUDACXX_AVAILABILITY_ATOMIC_SHARED_PTR\n\n    // These macros control the availability of all parts of <filesystem> that\n    // depend on something in the dylib.\n#   define _LIBCUDACXX_AVAILABILITY_FILESYSTEM\n#   define _LIBCUDACXX_AVAILABILITY_FILESYSTEM_PUSH\n#   define _LIBCUDACXX_AVAILABILITY_FILESYSTEM_POP\n// #   define _LIBCUDACXX_AVAILABILITY_DISABLE_FTM___cpp_lib_filesystem\n\n    // This controls the availability of floating-point std::to_chars functions.\n    // These overloads were added later than the integer overloads.\n#   define _LIBCUDACXX_AVAILABILITY_TO_CHARS_FLOATING_POINT\n\n    // This controls the availability of the C++20 synchronization library,\n    // which requires shared library support for various operations\n    // (see libcxx/src/atomic.cpp). This includes <barier>, <latch>,\n    // <semaphore>, and notification functions on std::atomic.\n#   define _LIBCUDACXX_AVAILABILITY_SYNC\n// #   define _LIBCUDACXX_AVAILABILITY_DISABLE_FTM___cpp_lib_atomic_wait\n// #   define _LIBCUDACXX_AVAILABILITY_DISABLE_FTM___cpp_lib_barrier\n// #   define _LIBCUDACXX_AVAILABILITY_DISABLE_FTM___cpp_lib_latch\n// #   define _LIBCUDACXX_AVAILABILITY_DISABLE_FTM___cpp_lib_semaphore\n\n    // This controls the availability of the C++20 format library.\n    // The library is in development and not ABI stable yet. P2216 is\n    // retroactively accepted in C++20. This paper contains ABI breaking\n    // changes.\n#   define _LIBCUDACXX_AVAILABILITY_FORMAT\n// #   define _LIBCUDACXX_AVAILABILITY_DISABLE_FTM___cpp_lib_format\n\n    // This controls whether the default verbose termination function is\n    // provided by the library.\n    //\n    // Note that when users provide their own custom function, it doesn't\n    // matter whether the dylib provides a default function, and the\n    // availability markup can actually give a false positive diagnostic\n    // (it will think that no function is provided, when in reality the\n    // user has provided their own).\n    //\n    // Users can pass -D_LIBCUDACXX_AVAILABILITY_CUSTOM_VERBOSE_ABORT_PROVIDED\n    // to the compiler to tell the library not to define its own verbose abort.\n    // Note that defining this macro but failing to define a custom function\n    // will lead to a load-time error on back-deployment targets, so it should\n    // be avoided.\n// #   define _LIBCUDACXX_HAS_NO_VERBOSE_ABORT_IN_LIBRARY\n\n#elif defined(__APPLE__)\n\n#   define _LIBCUDACXX_AVAILABILITY_SHARED_MUTEX                                    \\\n        __attribute__((availability(macos,strict,introduced=10.12)))            \\\n        __attribute__((availability(ios,strict,introduced=10.0)))               \\\n        __attribute__((availability(tvos,strict,introduced=10.0)))              \\\n        __attribute__((availability(watchos,strict,introduced=3.0)))\n#   if (defined(__ENVIRONMENT_MAC_OS_X_VERSION_MIN_REQUIRED__) && __ENVIRONMENT_MAC_OS_X_VERSION_MIN_REQUIRED__ < 101200) ||    \\\n        (defined(__ENVIRONMENT_IPHONE_OS_VERSION_MIN_REQUIRED__) && __ENVIRONMENT_IPHONE_OS_VERSION_MIN_REQUIRED__ < 100000) || \\\n        (defined(__ENVIRONMENT_TV_OS_VERSION_MIN_REQUIRED__) && __ENVIRONMENT_TV_OS_VERSION_MIN_REQUIRED__ < 100000) ||         \\\n        (defined(__ENVIRONMENT_WATCH_OS_VERSION_MIN_REQUIRED__) && __ENVIRONMENT_WATCH_OS_VERSION_MIN_REQUIRED__ < 30000)\n#       define _LIBCUDACXX_AVAILABILITY_DISABLE_FTM___cpp_lib_shared_mutex\n#       define _LIBCUDACXX_AVAILABILITY_DISABLE_FTM___cpp_lib_shared_timed_mutex\n#   endif\n\n        // Note: bad_optional_access & friends were not introduced in the matching\n        // macOS and iOS versions, so the version mismatch between macOS and others\n        // is intended.\n#   define _LIBCUDACXX_AVAILABILITY_BAD_OPTIONAL_ACCESS                             \\\n        __attribute__((availability(macos,strict,introduced=10.13)))            \\\n        __attribute__((availability(ios,strict,introduced=12.0)))               \\\n        __attribute__((availability(tvos,strict,introduced=12.0)))              \\\n        __attribute__((availability(watchos,strict,introduced=5.0)))\n#   define _LIBCUDACXX_AVAILABILITY_BAD_VARIANT_ACCESS                              \\\n        _LIBCUDACXX_AVAILABILITY_BAD_OPTIONAL_ACCESS\n#   define _LIBCUDACXX_AVAILABILITY_BAD_ANY_CAST                                    \\\n        _LIBCUDACXX_AVAILABILITY_BAD_OPTIONAL_ACCESS\n\n#   define _LIBCUDACXX_AVAILABILITY_UNCAUGHT_EXCEPTIONS                             \\\n        __attribute__((availability(macos,strict,introduced=10.12)))            \\\n        __attribute__((availability(ios,strict,introduced=10.0)))               \\\n        __attribute__((availability(tvos,strict,introduced=10.0)))              \\\n        __attribute__((availability(watchos,strict,introduced=3.0)))\n\n#   define _LIBCUDACXX_AVAILABILITY_SIZED_NEW_DELETE                                \\\n        __attribute__((availability(macos,strict,introduced=10.12)))            \\\n        __attribute__((availability(ios,strict,introduced=10.0)))               \\\n        __attribute__((availability(tvos,strict,introduced=10.0)))              \\\n        __attribute__((availability(watchos,strict,introduced=3.0)))\n\n#   define _LIBCUDACXX_AVAILABILITY_FUTURE_ERROR                                    \\\n        __attribute__((availability(ios,strict,introduced=6.0)))\n\n#   define _LIBCUDACXX_AVAILABILITY_TYPEINFO_VTABLE                                 \\\n        __attribute__((availability(macos,strict,introduced=10.9)))             \\\n        __attribute__((availability(ios,strict,introduced=7.0)))\n\n#   define _LIBCUDACXX_AVAILABILITY_LOCALE_CATEGORY                                 \\\n        __attribute__((availability(macos,strict,introduced=10.9)))             \\\n        __attribute__((availability(ios,strict,introduced=7.0)))\n\n#   define _LIBCUDACXX_AVAILABILITY_ATOMIC_SHARED_PTR                               \\\n        __attribute__((availability(macos,strict,introduced=10.9)))             \\\n        __attribute__((availability(ios,strict,introduced=7.0)))\n\n#   define _LIBCUDACXX_AVAILABILITY_FILESYSTEM                                      \\\n        __attribute__((availability(macos,strict,introduced=10.15)))            \\\n        __attribute__((availability(ios,strict,introduced=13.0)))               \\\n        __attribute__((availability(tvos,strict,introduced=13.0)))              \\\n        __attribute__((availability(watchos,strict,introduced=6.0)))\n#   define _LIBCUDACXX_AVAILABILITY_FILESYSTEM_PUSH                                 \\\n        _Pragma(\"clang attribute push(__attribute__((availability(macos,strict,introduced=10.15))), apply_to=any(function,record))\") \\\n        _Pragma(\"clang attribute push(__attribute__((availability(ios,strict,introduced=13.0))), apply_to=any(function,record))\")    \\\n        _Pragma(\"clang attribute push(__attribute__((availability(tvos,strict,introduced=13.0))), apply_to=any(function,record))\")   \\\n        _Pragma(\"clang attribute push(__attribute__((availability(watchos,strict,introduced=6.0))), apply_to=any(function,record))\")\n#   define _LIBCUDACXX_AVAILABILITY_FILESYSTEM_POP                                  \\\n        _Pragma(\"clang attribute pop\")                                          \\\n        _Pragma(\"clang attribute pop\")                                          \\\n        _Pragma(\"clang attribute pop\")                                          \\\n        _Pragma(\"clang attribute pop\")\n#   if (defined(__ENVIRONMENT_MAC_OS_X_VERSION_MIN_REQUIRED__) && __ENVIRONMENT_MAC_OS_X_VERSION_MIN_REQUIRED__ < 101500) ||    \\\n        (defined(__ENVIRONMENT_IPHONE_OS_VERSION_MIN_REQUIRED__) && __ENVIRONMENT_IPHONE_OS_VERSION_MIN_REQUIRED__ < 130000) || \\\n        (defined(__ENVIRONMENT_TV_OS_VERSION_MIN_REQUIRED__) && __ENVIRONMENT_TV_OS_VERSION_MIN_REQUIRED__ < 130000) ||         \\\n        (defined(__ENVIRONMENT_WATCH_OS_VERSION_MIN_REQUIRED__) && __ENVIRONMENT_WATCH_OS_VERSION_MIN_REQUIRED__ < 60000)\n#       define _LIBCUDACXX_AVAILABILITY_DISABLE_FTM___cpp_lib_filesystem\n#   endif\n\n#   define _LIBCUDACXX_AVAILABILITY_TO_CHARS_FLOATING_POINT                         \\\n        __attribute__((unavailable))\n\n#   define _LIBCUDACXX_AVAILABILITY_SYNC                                            \\\n        __attribute__((availability(macos,strict,introduced=11.0)))             \\\n        __attribute__((availability(ios,strict,introduced=14.0)))               \\\n        __attribute__((availability(tvos,strict,introduced=14.0)))              \\\n        __attribute__((availability(watchos,strict,introduced=7.0)))\n#   if (defined(__ENVIRONMENT_MAC_OS_X_VERSION_MIN_REQUIRED__) && __ENVIRONMENT_MAC_OS_X_VERSION_MIN_REQUIRED__ < 110000) ||    \\\n        (defined(__ENVIRONMENT_IPHONE_OS_VERSION_MIN_REQUIRED__) && __ENVIRONMENT_IPHONE_OS_VERSION_MIN_REQUIRED__ < 140000) || \\\n        (defined(__ENVIRONMENT_TV_OS_VERSION_MIN_REQUIRED__) && __ENVIRONMENT_TV_OS_VERSION_MIN_REQUIRED__ < 140000) ||         \\\n        (defined(__ENVIRONMENT_WATCH_OS_VERSION_MIN_REQUIRED__) && __ENVIRONMENT_WATCH_OS_VERSION_MIN_REQUIRED__ < 70000)\n#       define _LIBCUDACXX_AVAILABILITY_DISABLE_FTM___cpp_lib_atomic_wait\n#       define _LIBCUDACXX_AVAILABILITY_DISABLE_FTM___cpp_lib_barrier\n#       define _LIBCUDACXX_AVAILABILITY_DISABLE_FTM___cpp_lib_latch\n#       define _LIBCUDACXX_AVAILABILITY_DISABLE_FTM___cpp_lib_semaphore\n#   endif\n\n#   define _LIBCUDACXX_AVAILABILITY_FORMAT                                          \\\n        __attribute__((unavailable))\n#   define _LIBCUDACXX_AVAILABILITY_DISABLE_FTM___cpp_lib_format\n\n#   define _LIBCUDACXX_HAS_NO_VERBOSE_ABORT_IN_LIBRARY\n\n#else\n\n// ...New vendors can add availability markup here...\n\n#   error \"It looks like you're trying to enable vendor availability markup, but you haven't defined the corresponding macros yet!\"\n\n#endif\n\n// Define availability attributes that depend on _LIBCUDACXX_NO_EXCEPTIONS.\n// Those are defined in terms of the availability attributes above, and\n// should not be vendor-specific.\n#if defined(_LIBCUDACXX_NO_EXCEPTIONS)\n#   define _LIBCUDACXX_AVAILABILITY_FUTURE\n#   define _LIBCUDACXX_AVAILABILITY_THROW_BAD_ANY_CAST\n#   define _LIBCUDACXX_AVAILABILITY_THROW_BAD_OPTIONAL_ACCESS\n#   define _LIBCUDACXX_AVAILABILITY_THROW_BAD_VARIANT_ACCESS\n#else\n#   define _LIBCUDACXX_AVAILABILITY_FUTURE                    _LIBCUDACXX_AVAILABILITY_FUTURE_ERROR\n#   define _LIBCUDACXX_AVAILABILITY_THROW_BAD_ANY_CAST        _LIBCUDACXX_AVAILABILITY_BAD_ANY_CAST\n#   define _LIBCUDACXX_AVAILABILITY_THROW_BAD_OPTIONAL_ACCESS _LIBCUDACXX_AVAILABILITY_BAD_OPTIONAL_ACCESS\n#   define _LIBCUDACXX_AVAILABILITY_THROW_BAD_VARIANT_ACCESS  _LIBCUDACXX_AVAILABILITY_BAD_VARIANT_ACCESS\n#endif\n\n#endif // _LIBCUDACXX___AVAILABILITY\n", "__concepts/_One_of.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_ONE_OF_H\n#define _LIBCUDACXX___CONCEPTS_ONE_OF_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__type_traits/disjunction.h\"\n#include \"../__type_traits/is_same.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 14\ntemplate <class _Ty, class... _Others>\n_LIBCUDACXX_CONCEPT _One_of = (is_same_v<_Ty, _Others> || ...);\n#elif _LIBCUDACXX_STD_VER > 11\ntemplate <class _Ty, class... _Others>\n_LIBCUDACXX_CONCEPT _One_of = _LIBCUDACXX_TRAIT(disjunction, is_same<_Ty, _Others>...);\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_ONE_OF_H\n", "__concepts/__concept_macros.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Copyright (c) Facebook, Inc. and its affiliates.\n// Copyright (c) 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n//\n// Part of libcu++, the C++ Standard Library for your entire system,\n// under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _CUDA___CONCEPTS\n#define _CUDA___CONCEPTS\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#if _LIBCUDACXX_STD_VER > 11\n\n#define _LIBCUDACXX_PP_CAT_(_Xp, ...) _Xp##__VA_ARGS__\n#define _LIBCUDACXX_PP_CAT(_Xp, ...) _LIBCUDACXX_PP_CAT_(_Xp, __VA_ARGS__)\n\n#define _LIBCUDACXX_PP_CAT2_(_Xp, ...) _Xp##__VA_ARGS__\n#define _LIBCUDACXX_PP_CAT2(_Xp, ...) _LIBCUDACXX_PP_CAT2_(_Xp, __VA_ARGS__)\n\n#define _LIBCUDACXX_PP_CAT3_(_Xp, ...) _Xp##__VA_ARGS__\n#define _LIBCUDACXX_PP_CAT3(_Xp, ...) _LIBCUDACXX_PP_CAT3_(_Xp, __VA_ARGS__)\n\n#define _LIBCUDACXX_PP_CAT4_(_Xp, ...) _Xp##__VA_ARGS__\n#define _LIBCUDACXX_PP_CAT4(_Xp, ...) _LIBCUDACXX_PP_CAT4_(_Xp, __VA_ARGS__)\n\n#define _LIBCUDACXX_PP_EVAL_(_Xp, _ARGS) _Xp _ARGS\n#define _LIBCUDACXX_PP_EVAL(_Xp, ...) _LIBCUDACXX_PP_EVAL_(_Xp, (__VA_ARGS__))\n\n#define _LIBCUDACXX_PP_EVAL2_(_Xp, _ARGS) _Xp _ARGS\n#define _LIBCUDACXX_PP_EVAL2(_Xp, ...) _LIBCUDACXX_PP_EVAL2_(_Xp, (__VA_ARGS__))\n\n#define _LIBCUDACXX_PP_EXPAND(...) __VA_ARGS__\n#define _LIBCUDACXX_PP_EAT(...)\n\n#define _LIBCUDACXX_PP_CHECK(...)                                              \\\n  _LIBCUDACXX_PP_EXPAND(_LIBCUDACXX_PP_CHECK_N(__VA_ARGS__, 0, ))\n#define _LIBCUDACXX_PP_CHECK_N(_Xp, _Num, ...) _Num\n#define _LIBCUDACXX_PP_PROBE(_Xp) _Xp, 1,\n#define _LIBCUDACXX_PP_PROBE_N(_Xp, _Num) _Xp, _Num,\n\n#define _LIBCUDACXX_PP_IS_PAREN(_Xp)                                           \\\n  _LIBCUDACXX_PP_CHECK(_LIBCUDACXX_PP_IS_PAREN_PROBE _Xp)\n#define _LIBCUDACXX_PP_IS_PAREN_PROBE(...) _LIBCUDACXX_PP_PROBE(~)\n\n// The final _LIBCUDACXX_PP_EXPAND here is to avoid\n// https://stackoverflow.com/questions/5134523/msvc-doesnt-expand-va-args-correctly\n#define _LIBCUDACXX_PP_COUNT(...)                                              \\\n  _LIBCUDACXX_PP_EXPAND(_LIBCUDACXX_PP_COUNT_(                                 \\\n      __VA_ARGS__, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, \\\n      35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18,  \\\n      17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, ))            \\\n  /**/\n#define _LIBCUDACXX_PP_COUNT_(                                                 \\\n    _01, _02, _03, _04, _05, _06, _07, _08, _09, _10, _11, _12, _13, _14, _15, \\\n    _16, _17, _18, _19, _20, _21, _22, _23, _24, _25, _26, _27, _28, _29, _30, \\\n    _31, _32, _33, _34, _35, _36, _37, _38, _39, _40, _41, _42, _43, _44, _45, \\\n    _46, _47, _48, _49, _50, _Np, ...)                                         \\\n  _Np /**/\n\n#define _LIBCUDACXX_PP_IIF(_BIT) _LIBCUDACXX_PP_CAT_(_LIBCUDACXX_PP_IIF_, _BIT)\n#define _LIBCUDACXX_PP_IIF_0(_TRUE, ...) __VA_ARGS__\n#define _LIBCUDACXX_PP_IIF_1(_TRUE, ...) _TRUE\n\n#define _LIBCUDACXX_PP_LPAREN (\n\n#define _LIBCUDACXX_PP_NOT(_BIT) _LIBCUDACXX_PP_CAT_(_LIBCUDACXX_PP_NOT_, _BIT)\n#define _LIBCUDACXX_PP_NOT_0 1\n#define _LIBCUDACXX_PP_NOT_1 0\n\n#define _LIBCUDACXX_PP_EMPTY()\n#define _LIBCUDACXX_PP_COMMA() ,\n#define _LIBCUDACXX_PP_LBRACE() {\n#define _LIBCUDACXX_PP_RBRACE() }\n#define _LIBCUDACXX_PP_COMMA_IIF(_Xp)                                          \\\n  _LIBCUDACXX_PP_IIF(_Xp)(_LIBCUDACXX_PP_EMPTY, _LIBCUDACXX_PP_COMMA)() /**/\n\n#define _LIBCUDACXX_PP_FOR_EACH(_Mp, ...)                                      \\\n  _LIBCUDACXX_PP_FOR_EACH_N(_LIBCUDACXX_PP_COUNT(__VA_ARGS__), _Mp, __VA_ARGS__)\n#define _LIBCUDACXX_PP_FOR_EACH_N(_Np, _Mp, ...)                               \\\n  _LIBCUDACXX_PP_CAT2(_LIBCUDACXX_PP_FOR_EACH_, _Np)(_Mp, __VA_ARGS__)\n#define _LIBCUDACXX_PP_FOR_EACH_1(_Mp, _1) _Mp(_1)\n#define _LIBCUDACXX_PP_FOR_EACH_2(_Mp, _1, _2) _Mp(_1) _Mp(_2)\n#define _LIBCUDACXX_PP_FOR_EACH_3(_Mp, _1, _2, _3) _Mp(_1) _Mp(_2) _Mp(_3)\n#define _LIBCUDACXX_PP_FOR_EACH_4(_Mp, _1, _2, _3, _4)                         \\\n  _Mp(_1) _Mp(_2) _Mp(_3) _Mp(_4)\n#define _LIBCUDACXX_PP_FOR_EACH_5(_Mp, _1, _2, _3, _4, _5)                     \\\n  _Mp(_1) _Mp(_2) _Mp(_3) _Mp(_4) _Mp(_5)\n#define _LIBCUDACXX_PP_FOR_EACH_6(_Mp, _1, _2, _3, _4, _5, _6)                 \\\n  _Mp(_1) _Mp(_2) _Mp(_3) _Mp(_4) _Mp(_5) _Mp(_6)\n#define _LIBCUDACXX_PP_FOR_EACH_7(_Mp, _1, _2, _3, _4, _5, _6, _7)             \\\n  _Mp(_1) _Mp(_2) _Mp(_3) _Mp(_4) _Mp(_5) _Mp(_6) _Mp(_7)\n#define _LIBCUDACXX_PP_FOR_EACH_8(_Mp, _1, _2, _3, _4, _5, _6, _7, _8)         \\\n  _Mp(_1) _Mp(_2) _Mp(_3) _Mp(_4) _Mp(_5) _Mp(_6) _Mp(_7) _Mp(_8)\n\n#define _LIBCUDACXX_PP_PROBE_EMPTY_PROBE__LIBCUDACXX_PP_PROBE_EMPTY            \\\n  _LIBCUDACXX_PP_PROBE(~)\n\n#define _LIBCUDACXX_PP_PROBE_EMPTY()\n#define _LIBCUDACXX_PP_IS_NOT_EMPTY(...)                                       \\\n  _LIBCUDACXX_PP_EVAL(                                                         \\\n      _LIBCUDACXX_PP_CHECK,                                                    \\\n      _LIBCUDACXX_PP_CAT(_LIBCUDACXX_PP_PROBE_EMPTY_PROBE_,                    \\\n                         _LIBCUDACXX_PP_PROBE_EMPTY __VA_ARGS__()))            \\\n  /**/\n\n#define _LIBCUDACXX_PP_TAIL(_, ...) __VA_ARGS__\n\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_M0(_REQ)                             \\\n  _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_SELECT_(_REQ)(_REQ)\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_M1(_REQ) _LIBCUDACXX_PP_EXPAND _REQ\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_(...)                                \\\n  { _LIBCUDACXX_PP_FOR_EACH(_LIBCUDACXX_CONCEPT_FRAGMENT_REQS_M, __VA_ARGS__) }\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_SELECT_(_REQ)                        \\\n  _LIBCUDACXX_PP_CAT3(                                                         \\\n      _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_SELECT_,                               \\\n      _LIBCUDACXX_PP_EVAL(                                                     \\\n          _LIBCUDACXX_PP_CHECK,                                                \\\n          _LIBCUDACXX_PP_CAT3(_LIBCUDACXX_CONCEPT_FRAGMENT_REQS_SELECT_PROBE_, \\\n                              _REQ)))                                          \\\n  /**/\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_SELECT_PROBE_requires                \\\n  _LIBCUDACXX_PP_PROBE_N(~, 1)\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_SELECT_PROBE_noexcept                \\\n  _LIBCUDACXX_PP_PROBE_N(~, 2)\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_SELECT_PROBE_typename                \\\n  _LIBCUDACXX_PP_PROBE_N(~, 3)\n\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_SELECT_0 _LIBCUDACXX_PP_EXPAND\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_SELECT_1                             \\\n  _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_REQUIRES_OR_NOEXCEPT\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_SELECT_2                             \\\n  _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_REQUIRES_OR_NOEXCEPT\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_SELECT_3                             \\\n  _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_REQUIRES_OR_NOEXCEPT\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_REQUIRES_OR_NOEXCEPT(_REQ)           \\\n  _LIBCUDACXX_PP_CAT4(_LIBCUDACXX_CONCEPT_FRAGMENT_REQS_REQUIRES_, _REQ)\n#define _LIBCUDACXX_PP_EAT_TYPENAME_PROBE_typename _LIBCUDACXX_PP_PROBE(~)\n#define _LIBCUDACXX_PP_EAT_TYPENAME_SELECT_(_Xp, ...)                          \\\n  _LIBCUDACXX_PP_CAT3(                                                         \\\n      _LIBCUDACXX_PP_EAT_TYPENAME_SELECT_,                                     \\\n      _LIBCUDACXX_PP_EVAL(                                                     \\\n          _LIBCUDACXX_PP_CHECK,                                                \\\n          _LIBCUDACXX_PP_CAT3(_LIBCUDACXX_PP_EAT_TYPENAME_PROBE_, _Xp)))\n#define _LIBCUDACXX_PP_EAT_TYPENAME_(...)                                      \\\n  _LIBCUDACXX_PP_EVAL2(_LIBCUDACXX_PP_EAT_TYPENAME_SELECT_, __VA_ARGS__, )     \\\n  (__VA_ARGS__)\n#define _LIBCUDACXX_PP_EAT_TYPENAME_SELECT_0(...) __VA_ARGS__\n#define _LIBCUDACXX_PP_EAT_TYPENAME_SELECT_1(...)                              \\\n  _LIBCUDACXX_PP_CAT3(_LIBCUDACXX_PP_EAT_TYPENAME_, __VA_ARGS__)\n#define _LIBCUDACXX_PP_EAT_TYPENAME_typename\n\n#if (defined(__cpp_concepts) && _LIBCUDACXX_STD_VER >= 20) ||                  \\\n    defined(_LIBCUDACXX_DOXYGEN_INVOKED)\n\n#define _LIBCUDACXX_CONCEPT concept\n\n#define _LIBCUDACXX_CONCEPT_FRAGMENT(_NAME, ...)                               \\\n  concept _NAME =                                                              \\\n      _LIBCUDACXX_PP_CAT(_LIBCUDACXX_CONCEPT_FRAGMENT_REQS_, __VA_ARGS__)\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_requires(...)                        \\\n  requires(__VA_ARGS__) _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_M(_REQ)                              \\\n  _LIBCUDACXX_PP_CAT2(_LIBCUDACXX_CONCEPT_FRAGMENT_REQS_M,                     \\\n                      _LIBCUDACXX_PP_IS_PAREN(_REQ))                           \\\n  (_REQ);\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_REQUIRES_requires(...)               \\\n  requires __VA_ARGS__\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_REQUIRES_typename(...)               \\\n  typename _LIBCUDACXX_PP_EAT_TYPENAME_(__VA_ARGS__)\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_REQUIRES_noexcept(...)               \\\n  { __VA_ARGS__ }                                                              \\\n  noexcept\n\n#define _LIBCUDACXX_FRAGMENT(_NAME, ...) _NAME<__VA_ARGS__>\n\n#else\n\n#define _LIBCUDACXX_CONCEPT _LIBCUDACXX_INLINE_VAR constexpr bool\n\n#define _LIBCUDACXX_CONCEPT_FRAGMENT(_NAME, ...)                               \\\n  _LIBCUDACXX_INLINE_VISIBILITY auto _NAME##_LIBCUDACXX_CONCEPT_FRAGMENT_impl_ \\\n          _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_##__VA_ARGS__ > {}                 \\\n  template <typename... _As>                                                   \\\n  _LIBCUDACXX_INLINE_VISIBILITY char _NAME##_LIBCUDACXX_CONCEPT_FRAGMENT_(     \\\n      _Concept::_Tag<_As...> *,                                                \\\n      decltype(&_NAME##_LIBCUDACXX_CONCEPT_FRAGMENT_impl_<_As...>));           \\\n  _LIBCUDACXX_INLINE_VISIBILITY char(                                          \\\n      &_NAME##_LIBCUDACXX_CONCEPT_FRAGMENT_(...))[2] /**/\n#if defined(_MSC_VER) && !defined(__clang__)\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_TRUE(...)                                 \\\n  _Concept::_Is_true<decltype(_LIBCUDACXX_PP_FOR_EACH(                         \\\n      _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_M, __VA_ARGS__) void())>()\n#else\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_TRUE(...)                                 \\\n  !(decltype(_LIBCUDACXX_PP_FOR_EACH(_LIBCUDACXX_CONCEPT_FRAGMENT_REQS_M,      \\\n                                     __VA_ARGS__) void(),                      \\\n             false){})\n#endif\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_requires(...)                        \\\n  (__VA_ARGS__)->_Concept::_Enable_if_t < _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_2_\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_2_(...)                              \\\n  _LIBCUDACXX_CONCEPT_FRAGMENT_TRUE(__VA_ARGS__)\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_M(_REQ)                              \\\n  _LIBCUDACXX_PP_CAT2(_LIBCUDACXX_CONCEPT_FRAGMENT_REQS_M,                     \\\n                      _LIBCUDACXX_PP_IS_PAREN(_REQ))                           \\\n  (_REQ),\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_REQUIRES_requires(...)               \\\n  _Concept::_Requires<__VA_ARGS__>\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_REQUIRES_typename(...)               \\\n  static_cast<_Concept::_Tag<__VA_ARGS__> *>(nullptr)\n#if defined(_LIBCUDACXX_COMPILER_GCC)\n// GCC can't mangle noexcept expressions, so just check that the\n// expression is well-formed.\n// https://gcc.gnu.org/bugzilla/show_bug.cgi?id=70790\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_REQUIRES_noexcept(...) __VA_ARGS__\n#else\n#define _LIBCUDACXX_CONCEPT_FRAGMENT_REQS_REQUIRES_noexcept(...)               \\\n  _Concept::_Requires<noexcept(__VA_ARGS__)>\n#endif\n\n#define _LIBCUDACXX_FRAGMENT(_NAME, ...)                                       \\\n  (1u == sizeof(_NAME##_LIBCUDACXX_CONCEPT_FRAGMENT_(                          \\\n             static_cast<_Concept::_Tag<__VA_ARGS__> *>(nullptr), nullptr)))\n\n#endif\n\n////////////////////////////////////////////////////////////////////////////////\n// _LIBCUDACXX_TEMPLATE\n// Usage:\n//   _LIBCUDACXX_TEMPLATE(typename A, typename _Bp)\n//     (requires Concept1<A> _LIBCUDACXX_AND Concept2<_Bp>)\n//   void foo(A a, _Bp b)\n//   {}\n#if (defined(__cpp_concepts) && _LIBCUDACXX_STD_VER >= 20)\n#define _LIBCUDACXX_TEMPLATE(...)                                              \\\n  template <__VA_ARGS__> _LIBCUDACXX_PP_EXPAND /**/\n#define _LIBCUDACXX_AND &&                     /**/\n#define _LIBCUDACXX_TRAILING_REQUIRES(...)                                     \\\n  -> __VA_ARGS__ _LIBCUDACXX_PP_EXPAND\n#else\n#define _LIBCUDACXX_TEMPLATE(...)                                              \\\n  template <__VA_ARGS__ _LIBCUDACXX_TEMPLATE_SFINAE_AUX_ /**/\n#define _LIBCUDACXX_AND                                                        \\\n  &&_LIBCUDACXX_true_, int > = 0, _Concept::_Enable_if_t < /**/\n#define _LIBCUDACXX_TRAILING_REQUIRES(...)                                     \\\n  -> _Concept::_Requires_t<__VA_ARGS__ _LIBCUDACXX_TRAILING_REQUIRES_AUX_\n#endif\n\n#define _LIBCUDACXX_TEMPLATE_SFINAE(...)                                       \\\n  template <__VA_ARGS__ _LIBCUDACXX_TEMPLATE_SFINAE_AUX_ /**/\n#define _LIBCUDACXX_TEMPLATE_SFINAE_AUX_(...)                                  \\\n  , bool _LIBCUDACXX_true_ = true,                                             \\\n         _Concept::_Enable_if_t <                                              \\\n                 _LIBCUDACXX_PP_CAT(_LIBCUDACXX_TEMPLATE_SFINAE_AUX_3_,        \\\n                                    __VA_ARGS__) &&                            \\\n             _LIBCUDACXX_true_,                                                \\\n         int > = 0 > /**/\n#define _LIBCUDACXX_TRAILING_REQUIRES_AUX_(...)                                \\\n  , _LIBCUDACXX_PP_CAT(_LIBCUDACXX_TEMPLATE_SFINAE_AUX_3_, __VA_ARGS__)> /**/\n#define _LIBCUDACXX_TEMPLATE_SFINAE_AUX_3_requires\n\nnamespace _Concept {\ntemplate <bool> struct _Select {};\n\ntemplate <> struct _Select<true> { template <class _Tp> using type = _Tp; };\n\ntemplate <bool _Bp, class _Tp = void>\nusing _Enable_if_t = typename _Select<_Bp>::template type<_Tp>;\n\ntemplate <class _Tp, bool _Bp>\nusing _Requires_t = typename _Select<_Bp>::template type<_Tp>;\n\ntemplate <typename...> struct _Tag;\ntemplate <class>\n_LIBCUDACXX_INLINE_VISIBILITY inline constexpr bool _Is_true() {\n  return true;\n}\n\n#if defined(_LIBCUDACXX_COMPILER_CLANG) || defined(_LIBCUDACXX_COMPILER_MSVC)\ntemplate <bool _Bp>\n_LIBCUDACXX_INLINE_VISIBILITY _Concept::_Enable_if_t<_Bp> _Requires() {}\n#else\ntemplate <bool _Bp, _Concept::_Enable_if_t<_Bp, int> = 0>\n_LIBCUDACXX_INLINE_VAR constexpr int _Requires = 0;\n#endif\n} // namespace _Concept\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n#endif //_CUDA___CONCEPTS\n", "__concepts/arithmetic.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_ARITHMETIC_H\n#define _LIBCUDACXX___CONCEPTS_ARITHMETIC_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__type_traits/is_arithmetic.h\"\n#include \"../__type_traits/is_floating_point.h\"\n#include \"../__type_traits/is_integral.h\"\n#include \"../__type_traits/is_signed_integer.h\"\n#include \"../__type_traits/is_signed.h\"\n#include \"../__type_traits/is_unsigned_integer.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 11\n\n// [concepts.arithmetic], arithmetic concepts\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT integral = _LIBCUDACXX_TRAIT(is_integral, _Tp);\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT signed_integral = integral<_Tp> && _LIBCUDACXX_TRAIT(is_signed, _Tp);\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT unsigned_integral = integral<_Tp> && !signed_integral<_Tp>;\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT floating_point = _LIBCUDACXX_TRAIT(is_floating_point, _Tp);\n\n// Concept helpers for the internal type traits for the fundamental types.\ntemplate <class _Tp>\n_LIBCUDACXX_CONCEPT __libcpp_unsigned_integer = __libcpp_is_unsigned_integer<_Tp>::value;\ntemplate <class _Tp>\n_LIBCUDACXX_CONCEPT __libcpp_signed_integer = __libcpp_is_signed_integer<_Tp>::value;\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_ARITHMETIC_H\n", "__concepts/assignable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_ASSIGNABLE_H\n#define _LIBCUDACXX___CONCEPTS_ASSIGNABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__concepts/common_reference_with.h\"\n#include \"../__concepts/same_as.h\"\n#include \"../__type_traits/is_reference.h\"\n#include \"../__type_traits/make_const_lvalue_ref.h\"\n#include \"../__utility/forward.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\n\n// [concept.assignable]\n\ntemplate<class _Lhs, class _Rhs>\nconcept assignable_from =\n  is_lvalue_reference_v<_Lhs> &&\n  common_reference_with<__make_const_lvalue_ref<_Lhs>, __make_const_lvalue_ref<_Rhs>> &&\n  requires (_Lhs __lhs, _Rhs&& __rhs) {\n    { __lhs = _CUDA_VSTD::forward<_Rhs>(__rhs) } -> same_as<_Lhs>;\n  };\n\n#elif _LIBCUDACXX_STD_VER > 11\n\ntemplate<class _Lhs, class _Rhs>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __assignable_from_,\n  requires(_Lhs __lhs, _Rhs&& __rhs)(\n    requires(_LIBCUDACXX_TRAIT(is_lvalue_reference, _Lhs)),\n    requires(common_reference_with<__make_const_lvalue_ref<_Lhs>, __make_const_lvalue_ref<_Rhs>>),\n    requires(same_as<_Lhs, decltype(__lhs = _CUDA_VSTD::forward<_Rhs>(__rhs))>)\n  ));\n\ntemplate<class _Lhs, class _Rhs>\n_LIBCUDACXX_CONCEPT assignable_from = _LIBCUDACXX_FRAGMENT(__assignable_from_, _Lhs, _Rhs);\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_ASSIGNABLE_H\n", "__concepts/boolean_testable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_BOOLEAN_TESTABLE_H\n#define _LIBCUDACXX___CONCEPTS_BOOLEAN_TESTABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__concepts/convertible_to.h\"\n#include \"../__utility/forward.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\n\n// [concepts.booleantestable]\n\ntemplate<class _Tp>\nconcept __boolean_testable_impl = convertible_to<_Tp, bool>;\n\ntemplate<class _Tp>\nconcept __boolean_testable = __boolean_testable_impl<_Tp> && requires(_Tp&& __t) {\n  { !_CUDA_VSTD::forward<_Tp>(__t) } -> __boolean_testable_impl;\n};\n\n#elif _LIBCUDACXX_STD_VER > 11\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT __boolean_testable_impl = convertible_to<_Tp, bool>;\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __boolean_testable_,\n  requires(_Tp&& __t)(\n    requires(__boolean_testable_impl<_Tp>),\n    requires(__boolean_testable_impl<decltype(!_CUDA_VSTD::forward<_Tp>(__t))>)\n  ));\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT __boolean_testable = _LIBCUDACXX_FRAGMENT(__boolean_testable_, _Tp);\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_BOOLEAN_TESTABLE_H\n", "__concepts/class_or_enum.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_CLASS_OR_ENUM_H\n#define _LIBCUDACXX___CONCEPTS_CLASS_OR_ENUM_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__type_traits/is_class.h\"\n#include \"../__type_traits/is_enum.h\"\n#include \"../__type_traits/is_union.h\"\n#include \"../__type_traits/remove_cvref.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 11\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT __class_or_enum = _LIBCUDACXX_TRAIT(is_class, _Tp) || _LIBCUDACXX_TRAIT(is_union, _Tp) || _LIBCUDACXX_TRAIT(is_enum, _Tp);\n\n// Work around Clang bug https://llvm.org/PR52970\n// TODO: remove this workaround once libc++ no longer has to support Clang 13 (it was fixed in Clang 14).\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT __workaround_52970 = _LIBCUDACXX_TRAIT(is_class, remove_cvref_t<_Tp>) || _LIBCUDACXX_TRAIT(is_union, remove_cvref_t<_Tp>);\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_CLASS_OR_ENUM_H\n", "__concepts/common_reference_with.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_COMMON_REFERENCE_WITH_H\n#define _LIBCUDACXX___CONCEPTS_COMMON_REFERENCE_WITH_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__concepts/convertible_to.h\"\n#include \"../__concepts/same_as.h\"\n#include \"../__type_traits/common_reference.h\"\n#include \"../__type_traits/copy_cv.h\"\n#include \"../__type_traits/copy_cvref.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\n\n// [concept.commonref]\n\ntemplate<class _Tp, class _Up>\nconcept common_reference_with =\n  same_as<common_reference_t<_Tp, _Up>, common_reference_t<_Up, _Tp>> &&\n  convertible_to<_Tp, common_reference_t<_Tp, _Up>> &&\n  convertible_to<_Up, common_reference_t<_Tp, _Up>>;\n\n#elif _LIBCUDACXX_STD_VER > 11\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __common_reference_exists_,\n  requires()(\n    typename(common_reference_t<_Tp, _Up>),\n    typename(common_reference_t<_Up, _Tp>)\n  ));\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT _Common_reference_exists = _LIBCUDACXX_FRAGMENT(__common_reference_exists_, _Tp, _Up);\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __common_reference_with_,\n  requires()(\n    requires(_Common_reference_exists<_Tp, _Up>),\n    requires(same_as<common_reference_t<_Tp, _Up>, common_reference_t<_Up, _Tp>>),\n    requires(convertible_to<_Tp, common_reference_t<_Tp, _Up>>),\n    requires(convertible_to<_Up, common_reference_t<_Tp, _Up>>)\n  ));\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT common_reference_with = _LIBCUDACXX_FRAGMENT(__common_reference_with_, _Tp, _Up);\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_COMMON_REFERENCE_WITH_H\n", "__concepts/common_with.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_COMMON_WITH_H\n#define _LIBCUDACXX___CONCEPTS_COMMON_WITH_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__concepts/common_reference_with.h\"\n#include \"../__concepts/same_as.h\"\n#include \"../__type_traits/add_lvalue_reference.h\"\n#include \"../__type_traits/common_type.h\"\n#include \"../__utility/declval.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\n\n// [concept.common]\n\ntemplate<class _Tp, class _Up>\nconcept common_with =\n  same_as<common_type_t<_Tp, _Up>, common_type_t<_Up, _Tp>> &&\n  requires {\n    static_cast<common_type_t<_Tp, _Up>>(_CUDA_VSTD::declval<_Tp>());\n    static_cast<common_type_t<_Tp, _Up>>(_CUDA_VSTD::declval<_Up>());\n  } &&\n  common_reference_with<\n    add_lvalue_reference_t<const _Tp>,\n    add_lvalue_reference_t<const _Up>> &&\n  common_reference_with<\n    add_lvalue_reference_t<common_type_t<_Tp, _Up>>,\n    common_reference_t<\n      add_lvalue_reference_t<const _Tp>,\n      add_lvalue_reference_t<const _Up>>>;\n\n#elif _LIBCUDACXX_STD_VER > 11\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __common_type_exists_,\n  requires()(\n    typename(common_type_t<_Tp, _Up>),\n    typename(common_type_t<_Up, _Tp>)\n  ));\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT _Common_type_exists = _LIBCUDACXX_FRAGMENT(__common_type_exists_, _Tp, _Up);\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __common_type_constructible_,\n  requires()(\n    requires(_Common_type_exists<_Tp, _Up>),\n    static_cast<common_type_t<_Tp, _Up>>(_CUDA_VSTD::declval<_Tp>()),\n    static_cast<common_type_t<_Tp, _Up>>(_CUDA_VSTD::declval<_Up>())\n  ));\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT _Common_type_constructible = _LIBCUDACXX_FRAGMENT(__common_type_constructible_, _Tp, _Up);\n\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __common_with_,\n  requires()(\n    requires(_Common_type_constructible<_Tp, _Up>),\n    requires(same_as<common_type_t<_Tp, _Up>, common_type_t<_Up, _Tp>>),\n    requires(common_reference_with<\n              add_lvalue_reference_t<const _Tp>,\n              add_lvalue_reference_t<const _Up>>),\n    requires(common_reference_with<\n              add_lvalue_reference_t<common_type_t<_Tp, _Up>>,\n              common_reference_t<\n                add_lvalue_reference_t<const _Tp>,\n                add_lvalue_reference_t<const _Up>>>)));\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT common_with = _LIBCUDACXX_FRAGMENT(__common_with_, _Tp, _Up);\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_COMMON_WITH_H\n", "__concepts/constructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_CONSTRUCTIBLE_H\n#define _LIBCUDACXX___CONCEPTS_CONSTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__concepts/convertible_to.h\"\n#include \"../__concepts/destructible.h\"\n#include \"../__type_traits/add_lvalue_reference.h\"\n#include \"../__type_traits/is_constructible.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\n\n// [concept.constructible]\ntemplate<class _Tp, class... _Args>\nconcept constructible_from =\n    destructible<_Tp> && is_constructible_v<_Tp, _Args...>;\n\n// [concept.default.init]\ntemplate<class _Tp>\nconcept __default_initializable = requires { ::new _Tp; };\n\ntemplate<class _Tp>\nconcept default_initializable = constructible_from<_Tp> &&\n    requires { _Tp{}; } && __default_initializable<_Tp>;\n\n// [concept.moveconstructible]\ntemplate<class _Tp>\nconcept move_constructible =\n  constructible_from<_Tp, _Tp> && convertible_to<_Tp, _Tp>;\n\n// [concept.copyconstructible]\ntemplate<class _Tp>\nconcept copy_constructible =\n  move_constructible<_Tp> &&\n  constructible_from<_Tp, _Tp&> && convertible_to<_Tp&, _Tp> &&\n  constructible_from<_Tp, const _Tp&> && convertible_to<const _Tp&, _Tp> &&\n  constructible_from<_Tp, const _Tp> && convertible_to<const _Tp, _Tp>;\n\n#elif _LIBCUDACXX_STD_VER > 11\n\ntemplate<class _Tp, class... _Args>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __constructible_from_,\n  requires()(\n    requires(destructible<_Tp>),\n    requires(_LIBCUDACXX_TRAIT(is_constructible, _Tp, _Args...))\n  ));\n\ntemplate<class _Tp, class... _Args>\n_LIBCUDACXX_CONCEPT constructible_from = _LIBCUDACXX_FRAGMENT(__constructible_from_, _Tp, _Args...);\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __default_initializable_,\n  requires()(\n    (::new _Tp)\n  ));\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT __default_initializable = _LIBCUDACXX_FRAGMENT(__default_initializable_, _Tp);\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  _Default_initializable_,\n  requires(_Tp = _Tp{}) (\n    requires(constructible_from<_Tp>),\n    requires(__default_initializable<_Tp>)\n  ));\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT default_initializable = _LIBCUDACXX_FRAGMENT(_Default_initializable_, _Tp);\n\n// [concept.moveconstructible]\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __move_constructible_,\n  requires()(\n    requires(constructible_from<_Tp, _Tp>),\n    requires(convertible_to<_Tp, _Tp>)\n  ));\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT move_constructible = _LIBCUDACXX_FRAGMENT(__move_constructible_, _Tp);\n\n// [concept.copyconstructible]\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __copy_constructible_,\n  requires()(\n    requires(move_constructible<_Tp>),\n    requires(constructible_from<_Tp, add_lvalue_reference_t<_Tp>> && convertible_to<add_lvalue_reference_t<_Tp>, _Tp>),\n    requires(constructible_from<_Tp, const add_lvalue_reference_t<_Tp>> && convertible_to<const add_lvalue_reference_t<_Tp>, _Tp>),\n    requires(constructible_from<_Tp, const _Tp> && convertible_to<const _Tp, _Tp>)\n  ));\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT copy_constructible =  _LIBCUDACXX_FRAGMENT(__copy_constructible_, _Tp);\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_CONSTRUCTIBLE_H\n", "__concepts/convertible_to.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_CONVERTIBLE_TO_H\n#define _LIBCUDACXX___CONCEPTS_CONVERTIBLE_TO_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__type_traits/is_convertible.h\"\n#include \"../__utility/declval.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\n\n// [concept.convertible]\n\ntemplate<class _From, class _To>\nconcept convertible_to =\n  is_convertible_v<_From, _To> &&\n  requires {\n    static_cast<_To>(_CUDA_VSTD::declval<_From>());\n  };\n\n#elif _LIBCUDACXX_STD_VER > 11\n\n#if defined(_LIBCUDACXX_COMPILER_MSVC)\n_LIBCUDACXX_NV_DIAG_SUPPRESS(1211) // nonstandard cast to array type ignored\n#endif // _LIBCUDACXX_COMPILER_MSVC\n\n// We cannot put this conversion check with the other constraint, as types with deleted operator will break here\ntemplate<class _From, class _To>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __test_conversion_,\n  requires()(\n    static_cast<_To>(_CUDA_VSTD::declval<_From>())\n  ));\n\ntemplate<class _From, class _To>\n_LIBCUDACXX_CONCEPT __test_conversion = _LIBCUDACXX_FRAGMENT(__test_conversion_, _From, _To);\n\ntemplate<class _From, class _To>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __convertible_to_,\n  requires()(\n    requires(_LIBCUDACXX_TRAIT(is_convertible, _From, _To)),\n    requires(__test_conversion<_From, _To>)\n  ));\n\ntemplate<class _From, class _To>\n_LIBCUDACXX_CONCEPT convertible_to = _LIBCUDACXX_FRAGMENT(__convertible_to_, _From, _To);\n\n#if defined(_LIBCUDACXX_COMPILER_MSVC)\n_LIBCUDACXX_NV_DIAG_DEFAULT(1211) // nonstandard cast to array type ignored\n#endif // _LIBCUDACXX_COMPILER_MSVC\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_CONVERTIBLE_TO_H\n", "__concepts/copyable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_COPYABLE_H\n#define _LIBCUDACXX___CONCEPTS_COPYABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__concepts/assignable.h\"\n#include \"../__concepts/constructible.h\"\n#include \"../__concepts/movable.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\n\n// [concepts.object]\n\ntemplate<class _Tp>\nconcept copyable =\n  copy_constructible<_Tp> &&\n  movable<_Tp> &&\n  assignable_from<_Tp&, _Tp&> &&\n  assignable_from<_Tp&, const _Tp&> &&\n  assignable_from<_Tp&, const _Tp>;\n\n#elif _LIBCUDACXX_STD_VER > 11\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __copyable_,\n  requires()(\n    requires(copy_constructible<_Tp>),\n    requires(movable<_Tp>),\n    requires(assignable_from<_Tp&, _Tp&>),\n    requires(assignable_from<_Tp&, const _Tp&>),\n    requires(assignable_from<_Tp&, const _Tp>)\n  ));\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT copyable = _LIBCUDACXX_FRAGMENT(__copyable_,_Tp);\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_COPYABLE_H\n", "__concepts/derived_from.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_DERIVED_FROM_H\n#define _LIBCUDACXX___CONCEPTS_DERIVED_FROM_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__type_traits/add_pointer.h\"\n#include \"../__type_traits/is_base_of.h\"\n#include \"../__type_traits/is_convertible.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\n\n// [concept.derived]\n\ntemplate<class _Dp, class _Bp>\nconcept derived_from =\n  is_base_of_v<_Bp, _Dp> &&\n  is_convertible_v<const volatile _Dp*, const volatile _Bp*>;\n\n#elif _LIBCUDACXX_STD_VER > 11\n\ntemplate<class _Dp, class _Bp>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __derived_from_,\n  requires()(\n    requires(_LIBCUDACXX_TRAIT(is_base_of, _Bp, _Dp)),\n    requires(_LIBCUDACXX_TRAIT(is_convertible, add_pointer_t<const volatile _Dp>, add_pointer_t<const volatile _Bp>))\n  ));\n\ntemplate<class _Dp, class _Bp>\n_LIBCUDACXX_CONCEPT derived_from = _LIBCUDACXX_FRAGMENT(__derived_from_, _Dp, _Bp);\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_DERIVED_FROM_H\n", "__concepts/destructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_DESTRUCTIBLE_H\n#define _LIBCUDACXX___CONCEPTS_DESTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__type_traits/enable_if.h\"\n#include \"../__type_traits/is_destructible.h\"\n#include \"../__type_traits/is_object.h\"\n#include \"../__type_traits/is_nothrow_destructible.h\"\n#include \"../__type_traits/void_t.h\"\n#include \"../__utility/declval.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 11\n\n#if defined(_LIBCUDACXX_COMPILER_MSVC)\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT destructible = __is_nothrow_destructible(_Tp);\n\n#else // ^^^ _LIBCUDACXX_COMPILER_MSVC ^^^ / vvv !_LIBCUDACXX_COMPILER_MSVC vvv\n\ntemplate<class _Tp, class = void, class = void>\n_LIBCUDACXX_INLINE_VAR constexpr bool __destructible_impl = false;\n\ntemplate<class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool __destructible_impl<_Tp,\n                                   __enable_if_t<_LIBCUDACXX_TRAIT(is_object, _Tp)>,\n#if defined(_LIBCUDACXX_COMPILER_GCC)\n                                   __enable_if_t<_LIBCUDACXX_TRAIT(is_destructible, _Tp)>>\n#else // ^^^ defined(_LIBCUDACXX_COMPILER_GCC) ^^^ / vvv !_LIBCUDACXX_COMPILER_GCC vvv\n                                   __void_t<decltype(_CUDA_VSTD::declval<_Tp>().~_Tp())>>\n#endif // !_LIBCUDACXX_COMPILER_GCC\n                                   = noexcept(_CUDA_VSTD::declval<_Tp>().~_Tp());\n\ntemplate<class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool __destructible = __destructible_impl<_Tp>;\n\ntemplate<class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool __destructible<_Tp&> = true;\n\ntemplate<class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool __destructible<_Tp&&> = true;\n\ntemplate<class _Tp, size_t _Nm>\n_LIBCUDACXX_INLINE_VAR constexpr bool __destructible<_Tp[_Nm]> = __destructible<_Tp>;\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT destructible = __destructible<_Tp>;\n\n#endif // !_LIBCUDACXX_COMPILER_MSVC\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_DESTRUCTIBLE_H\n", "__concepts/different_from.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_DIFFERENT_FROM_H\n#define _LIBCUDACXX___CONCEPTS_DIFFERENT_FROM_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__concepts/same_as.h\"\n#include \"../__type_traits/remove_cvref.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 11\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT __different_from = !same_as<remove_cvref_t<_Tp>, remove_cvref_t<_Up>>;\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_DIFFERENT_FROM_H\n", "__concepts/equality_comparable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_EQUALITY_COMPARABLE_H\n#define _LIBCUDACXX___CONCEPTS_EQUALITY_COMPARABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__concepts/boolean_testable.h\"\n#include \"../__concepts/common_reference_with.h\"\n#include \"../__type_traits/common_reference.h\"\n#include \"../__type_traits/make_const_lvalue_ref.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\n\n// [concept.equalitycomparable]\n\ntemplate<class _Tp, class _Up>\nconcept __weakly_equality_comparable_with =\n  requires(__make_const_lvalue_ref<_Tp> __t, __make_const_lvalue_ref<_Up> __u) {\n    { __t == __u } -> __boolean_testable;\n    { __t != __u } -> __boolean_testable;\n    { __u == __t } -> __boolean_testable;\n    { __u != __t } -> __boolean_testable;\n  };\n\ntemplate<class _Tp>\nconcept equality_comparable = __weakly_equality_comparable_with<_Tp, _Tp>;\n\ntemplate<class _Tp, class _Up>\nconcept equality_comparable_with =\n  equality_comparable<_Tp> && equality_comparable<_Up> &&\n  common_reference_with<__make_const_lvalue_ref<_Tp>, __make_const_lvalue_ref<_Up>> &&\n  equality_comparable<\n    common_reference_t<\n      __make_const_lvalue_ref<_Tp>,\n      __make_const_lvalue_ref<_Up>>> &&\n  __weakly_equality_comparable_with<_Tp, _Up>;\n\n#elif _LIBCUDACXX_STD_VER > 11\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __with_lvalue_reference_,\n  requires()(\n    typename(__make_const_lvalue_ref<_Tp>)\n  ));\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT _With_lvalue_reference = _LIBCUDACXX_FRAGMENT(__with_lvalue_reference_, _Tp);\n\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __weakly_equality_comparable_with_,\n  requires(__make_const_lvalue_ref<_Tp> __t, __make_const_lvalue_ref<_Up> __u) //\n  (requires(_With_lvalue_reference<_Tp>),\n   requires(_With_lvalue_reference<_Up>),\n   requires(__boolean_testable<decltype(__t == __u)>),\n   requires(__boolean_testable<decltype(__t != __u)>),\n   requires(__boolean_testable<decltype(__u == __t)>),\n   requires(__boolean_testable<decltype(__u != __t)>)));\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT __weakly_equality_comparable_with =\n  _LIBCUDACXX_FRAGMENT(__weakly_equality_comparable_with_, _Tp, _Up);\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT equality_comparable = __weakly_equality_comparable_with<_Tp, _Tp>;\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __equality_comparable_with_,\n  requires()(\n    requires(equality_comparable<_Tp>),\n    requires(equality_comparable<_Up>),\n    requires(common_reference_with<__make_const_lvalue_ref<_Tp>, __make_const_lvalue_ref<_Up>>),\n    requires(equality_comparable<\n    common_reference_t<\n      __make_const_lvalue_ref<_Tp>,\n      __make_const_lvalue_ref<_Up>>>),\n    requires(__weakly_equality_comparable_with<_Tp, _Up>)));\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT equality_comparable_with = _LIBCUDACXX_FRAGMENT(__equality_comparable_with_, _Tp, _Up);\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_EQUALITY_COMPARABLE_H\n", "__concepts/invocable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_INVOCABLE_H\n#define _LIBCUDACXX___CONCEPTS_INVOCABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__functional/invoke.h\"\n#include \"../__utility/forward.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\n\n// [concept.invocable]\n\ntemplate<class _Fn, class... _Args>\nconcept invocable = requires(_Fn&& __fn, _Args&&... __args) {\n  _CUDA_VSTD::__invoke(_CUDA_VSTD::forward<_Fn>(__fn), _CUDA_VSTD::forward<_Args>(__args)...); // not required to be equality preserving\n};\n\n// [concept.regular.invocable]\n\ntemplate<class _Fn, class... _Args>\nconcept regular_invocable = invocable<_Fn, _Args...>;\n\ntemplate <class _Fun, class... _Args>\nconcept __invoke_constructible = requires(_Fun&& __fun, _Args&&... __args) {\n    static_cast<remove_cvref_t<invoke_result_t<_Fun, _Args...>>>(\n        _CUDA_VSTD::invoke(_CUDA_VSTD::forward<_Fun>(__fun), _CUDA_VSTD::forward<_Args>(__args)...));\n};\n\n#elif _LIBCUDACXX_STD_VER > 11\n\ntemplate<class _Fn, class... _Args>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  _Invocable_,\n  requires(_Fn&& __fn, _Args&&... __args)(\n    (_CUDA_VSTD::invoke(_CUDA_VSTD::forward<_Fn>(__fn), _CUDA_VSTD::forward<_Args>(__args)...))\n  ));\n\ntemplate<class _Fn, class... _Args>\n_LIBCUDACXX_CONCEPT invocable = _LIBCUDACXX_FRAGMENT(_Invocable_, _Fn, _Args...);\n\ntemplate<class _Fn, class... _Args>\n_LIBCUDACXX_CONCEPT regular_invocable = invocable<_Fn, _Args...>;\n\ntemplate <class _Fun, class... _Args>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __invoke_constructible_,\n  requires(_Fun&& __fun, _Args&&... __args)(\n    (static_cast<__remove_cvref_t<invoke_result_t<_Fun, _Args...>>>(\n        _CUDA_VSTD::invoke(_CUDA_VSTD::forward<_Fun>(__fun), _CUDA_VSTD::forward<_Args>(__args)...)))\n  ));\ntemplate <class _Fun, class... _Args>\n_LIBCUDACXX_CONCEPT __invoke_constructible = _LIBCUDACXX_FRAGMENT(__invoke_constructible_, _Fun, _Args...);\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_INVOCABLE_H\n", "__concepts/movable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_MOVABLE_H\n#define _LIBCUDACXX___CONCEPTS_MOVABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__concepts/assignable.h\"\n#include \"../__concepts/constructible.h\"\n#include \"../__concepts/swappable.h\"\n#include \"../__type_traits/is_object.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\n\ntemplate<class _Tp>\nconcept movable =\n  is_object_v<_Tp> &&\n  move_constructible<_Tp>&&\n  assignable_from<_Tp&, _Tp> &&\n  swappable<_Tp>;\n\n#elif _LIBCUDACXX_STD_VER > 11\n\n// [concepts.object]\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  _Movable_,\n  requires()\n  (requires(is_object_v<_Tp>),\n   requires(move_constructible<_Tp>),\n   requires(assignable_from<_Tp&, _Tp>),\n   requires(swappable<_Tp>)));\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT movable = _LIBCUDACXX_FRAGMENT(_Movable_, _Tp);\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_MOVABLE_H\n", "__concepts/predicate.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_PREDICATE_H\n#define _LIBCUDACXX___CONCEPTS_PREDICATE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__concepts/boolean_testable.h\"\n#include \"../__concepts/invocable.h\"\n#include \"../__functional/invoke.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\n\ntemplate<class _Fn, class... _Args>\nconcept predicate =\n  regular_invocable<_Fn, _Args...> && __boolean_testable<invoke_result_t<_Fn, _Args...>>;\n\n#elif _LIBCUDACXX_STD_VER > 11\n\n// [concept.predicate]\ntemplate<class _Fn, class... _Args>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  _Predicate_,\n  requires()\n  (requires(regular_invocable<_Fn, _Args...>),\n   requires(__boolean_testable<invoke_result_t<_Fn, _Args...>>)));\n\ntemplate<class _Fn, class... _Args>\n_LIBCUDACXX_CONCEPT predicate = _LIBCUDACXX_FRAGMENT(_Predicate_, _Fn, _Args...);\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_PREDICATE_H\n", "__concepts/regular.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_REGULAR_H\n#define _LIBCUDACXX___CONCEPTS_REGULAR_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__concepts/equality_comparable.h\"\n#include \"../__concepts/semiregular.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\n\n// [concept.object]\n\ntemplate<class _Tp>\nconcept regular = semiregular<_Tp> && equality_comparable<_Tp>;\n\n#elif _LIBCUDACXX_STD_VER > 11\n\n// [concept.object]\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __regular_,\n  requires()(\n    requires(semiregular<_Tp>),\n    requires(equality_comparable<_Tp>)\n  ));\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT regular = _LIBCUDACXX_FRAGMENT(__regular_, _Tp);\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_REGULAR_H\n", "__concepts/relation.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_RELATION_H\n#define _LIBCUDACXX___CONCEPTS_RELATION_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__concepts/predicate.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\n\n// [concept.relation]\n\ntemplate<class _Rp, class _Tp, class _Up>\nconcept relation =\n  predicate<_Rp, _Tp, _Tp> && predicate<_Rp, _Up, _Up> &&\n  predicate<_Rp, _Tp, _Up> && predicate<_Rp, _Up, _Tp>;\n\n// [concept.equiv]\n\ntemplate<class _Rp, class _Tp, class _Up>\nconcept equivalence_relation = relation<_Rp, _Tp, _Up>;\n\n// [concept.strictweakorder]\n\ntemplate<class _Rp, class _Tp, class _Up>\nconcept strict_weak_order = relation<_Rp, _Tp, _Up>;\n\n#elif _LIBCUDACXX_STD_VER > 11\n\ntemplate<class _Rp, class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __relation_,\n  requires()(\n    requires(predicate<_Rp, _Tp, _Tp>),\n    requires(predicate<_Rp, _Up, _Up>),\n    requires(predicate<_Rp, _Tp, _Up>),\n    requires(predicate<_Rp, _Up, _Tp>)\n  ));\n\ntemplate<class _Rp, class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT relation = _LIBCUDACXX_FRAGMENT(__relation_, _Rp, _Tp, _Up);\n\n// [concept.equiv]\n\ntemplate<class _Rp, class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT equivalence_relation = relation<_Rp, _Tp, _Up>;\n\n// [concept.strictweakorder]\n\ntemplate<class _Rp, class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT strict_weak_order = relation<_Rp, _Tp, _Up>;\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_RELATION_H\n", "__concepts/same_as.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_SAME_AS_H\n#define _LIBCUDACXX___CONCEPTS_SAME_AS_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__type_traits/is_same.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 11\n\n// [concept.same]\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT __same_as_impl = _IsSame<_Tp, _Up>::value;\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT same_as = __same_as_impl<_Tp, _Up> && __same_as_impl<_Up, _Tp>;\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_SAME_AS_H\n", "__concepts/semiregular.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_SEMIREGULAR_H\n#define _LIBCUDACXX___CONCEPTS_SEMIREGULAR_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__concepts/constructible.h\"\n#include \"../__concepts/copyable.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\n\n// [concept.object]\n\ntemplate<class _Tp>\nconcept semiregular = copyable<_Tp> && default_initializable<_Tp>;\n\n#elif _LIBCUDACXX_STD_VER > 11\n\n// [concept.object]\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __semiregular_,\n  requires()(\n    requires(copyable<_Tp>),\n    requires(default_initializable<_Tp>)\n  ));\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT semiregular = _LIBCUDACXX_FRAGMENT(__semiregular_, _Tp);\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_SEMIREGULAR_H\n", "__concepts/swappable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_SWAPPABLE_H\n#define _LIBCUDACXX___CONCEPTS_SWAPPABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__concepts/assignable.h\"\n#include \"../__concepts/class_or_enum.h\"\n#include \"../__concepts/common_reference_with.h\"\n#include \"../__concepts/constructible.h\"\n#include \"../__type_traits/extent.h\"\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_nothrow_move_assignable.h\"\n#include \"../__type_traits/is_nothrow_move_constructible.h\"\n#include \"../__type_traits/remove_cvref.h\"\n#include \"../__type_traits/type_identity.h\"\n#include \"../__type_traits/void_t.h\"\n#include \"../__utility/declval.h\"\n#include \"../__utility/exchange.h\"\n#include \"../__utility/forward.h\"\n#include \"../__utility/move.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n#if defined(_LIBCUDACXX_COMPILER_MSVC)\n_LIBCUDACXX_NV_DIAG_SUPPRESS(461) // nonstandard cast to array type ignored\n#endif // _LIBCUDACXX_COMPILER_MSVC\n\n#if _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_BEGIN_NAMESPACE_RANGES\n\n// [concept.swappable]\n\n_LIBCUDACXX_BEGIN_NAMESPACE_CPO(__swap)\n\n  template<class _Tp>\n  void swap(_Tp&, _Tp&) = delete;\n\n#if _LIBCUDACXX_STD_VER > 17\n  template<class _Tp, class _Up>\n  concept __unqualified_swappable_with =\n    (__class_or_enum<remove_cvref_t<_Tp>> || __class_or_enum<remove_cvref_t<_Up>>) &&\n    requires(_Tp&& __t, _Up&& __u) {\n      swap(_CUDA_VSTD::forward<_Tp>(__t), _CUDA_VSTD::forward<_Up>(__u));\n    };\n\n  template<class _Tp>\n  concept __exchangeable =\n    !__unqualified_swappable_with<_Tp&, _Tp&> &&\n    move_constructible<_Tp> &&\n    assignable_from<_Tp&, _Tp>;\n\n#else // ^^^ CXX20 ^^^ / vvv CXX17 vvv\n\n  template<class _Tp, class _Up>\n  _LIBCUDACXX_CONCEPT_FRAGMENT(\n    __unqualified_swappable_with_,\n    requires(_Tp&& __t, _Up&& __u)(\n      (swap(_CUDA_VSTD::forward<_Tp>(__t), _CUDA_VSTD::forward<_Up>(__u)))\n    ));\n\n  template<class _Tp, class _Up>\n  _LIBCUDACXX_CONCEPT __unqualified_swappable_with = _LIBCUDACXX_FRAGMENT(__unqualified_swappable_with_, _Tp, _Up);\n\n  template<class _Tp>\n  _LIBCUDACXX_CONCEPT_FRAGMENT(\n    __exchangeable_,\n    requires()(\n      requires(!__unqualified_swappable_with<_Tp&, _Tp&>),\n      requires(move_constructible<_Tp>),\n      requires(assignable_from<_Tp&, _Tp>)\n    ));\n\n  template<class _Tp>\n  _LIBCUDACXX_CONCEPT __exchangeable = _LIBCUDACXX_FRAGMENT(__exchangeable_, _Tp);\n#endif // _LIBCUDACXX_STD_VER < 20\n\n\n#if _LIBCUDACXX_STD_VER > 17 && !defined(_LIBCUDACXX_COMPILER_NVHPC) // nvbug4051640\n  struct __fn;\n\n_LIBCUDACXX_NV_DIAG_SUPPRESS(2642)\n  template<class _Tp, class _Up, size_t _Size>\n  concept __swappable_arrays =\n    !__unqualified_swappable_with<_Tp(&)[_Size], _Up(&)[_Size]> &&\n    extent_v<_Tp> == extent_v<_Up> &&\n    requires(_Tp(& __t)[_Size], _Up(& __u)[_Size], const __fn& __swap) {\n      __swap(__t[0], __u[0]);\n    };\n_LIBCUDACXX_NV_DIAG_DEFAULT(2642)\n\n#else\n  template<class _Tp, class _Up, size_t _Size, class = void>\n  _LIBCUDACXX_INLINE_VAR constexpr bool __swappable_arrays = false;\n#endif // _LIBCUDACXX_STD_VER < 20 || defined(_LIBCUDACXX_COMPILER_NVHPC)\n\n\n  template<class _Tp, class _Up, class = void>\n  _LIBCUDACXX_INLINE_VAR constexpr bool __noexcept_swappable_arrays = false;\n\n  struct __fn {\n    // 2.1   `S` is `(void)swap(E1, E2)`* if `E1` or `E2` has class or enumeration type and...\n    // *The name `swap` is used here unqualified.\n    _LIBCUDACXX_TEMPLATE(class _Tp, class _Up)\n      (requires __unqualified_swappable_with<_Tp, _Up>)\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr void operator()(_Tp&& __t, _Up&& __u) const\n      noexcept(noexcept(swap(_CUDA_VSTD::forward<_Tp>(__t), _CUDA_VSTD::forward<_Up>(__u))))\n    {\n      swap(_CUDA_VSTD::forward<_Tp>(__t), _CUDA_VSTD::forward<_Up>(__u));\n    }\n\n    // 2.2   Otherwise, if `E1` and `E2` are lvalues of array types with equal extent and...\n    _LIBCUDACXX_TEMPLATE(class _Tp, class _Up, size_t _Size)\n      (requires __swappable_arrays<_Tp, _Up, _Size>)\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr void operator()(_Tp(& __t)[_Size], _Up(& __u)[_Size]) const\n      noexcept(__noexcept_swappable_arrays<_Tp, _Up>)\n    {\n      // TODO(cjdb): replace with `_CUDA_VRANGES::swap_ranges`.\n      for (size_t __i = 0; __i < _Size; ++__i) {\n        (*this)(__t[__i], __u[__i]);\n      }\n    }\n\n    // 2.3   Otherwise, if `E1` and `E2` are lvalues of the same type `T` that models...\n    _LIBCUDACXX_TEMPLATE(class _Tp)\n      (requires __exchangeable<_Tp>)\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr void operator()(_Tp& __x, _Tp& __y) const\n      noexcept(_LIBCUDACXX_TRAIT(is_nothrow_move_constructible, _Tp) && _LIBCUDACXX_TRAIT(is_nothrow_move_assignable, _Tp))\n    {\n      __y = _CUDA_VSTD::exchange(__x, _CUDA_VSTD::move(__y));\n    }\n  };\n\n#if _LIBCUDACXX_STD_VER < 20 || defined(_LIBCUDACXX_COMPILER_NVHPC)\n  template<class _Tp, class _Up, class _Size>\n  _LIBCUDACXX_CONCEPT_FRAGMENT(\n    __swappable_arrays_,\n    requires(_Tp(& __t)[_Size::value], _Up(& __u)[_Size::value], const __fn& __swap)(\n      requires(!__unqualified_swappable_with<_Tp(&)[_Size::value], _Up(&)[_Size::value]>),\n      requires(_LIBCUDACXX_TRAIT(extent, _Tp) == _LIBCUDACXX_TRAIT(extent, _Up)),\n      (__swap(__t[0], __u[0]))\n    ));\n\n  template<class _Tp, class _Up, size_t _Size>\n  _LIBCUDACXX_INLINE_VAR constexpr bool __swappable_arrays<_Tp, _Up, _Size, void_t<type_identity_t<_Tp>>> =\n    _LIBCUDACXX_FRAGMENT(__swappable_arrays_, _Tp, _Up, _CUDA_VSTD::integral_constant<size_t, _Size>);\n#endif // _LIBCUDACXX_STD_VER < 20 || defined(_LIBCUDACXX_COMPILER_NVHPC)\n\n  template<class _Tp, class _Up>\n  _LIBCUDACXX_INLINE_VAR constexpr bool __noexcept_swappable_arrays<_Tp, _Up, void_t<type_identity_t<_Tp>>> =\n    noexcept(__swap::__fn{}(_CUDA_VSTD::declval<_Tp&>(), _CUDA_VSTD::declval<_Up&>()));\n\n_LIBCUDACXX_END_NAMESPACE_CPO\n\ninline namespace __cpo {\n  _LIBCUDACXX_CPO_ACCESSIBILITY auto swap = __swap::__fn{};\n} // namespace __cpo\n_LIBCUDACXX_END_NAMESPACE_RANGES\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\ntemplate<class _Tp>\nconcept swappable = requires(_Tp& __a, _Tp& __b) { _CUDA_VRANGES::swap(__a, __b); };\n\ntemplate<class _Tp, class _Up>\nconcept swappable_with =\n  common_reference_with<_Tp, _Up> &&\n  requires(_Tp&& __t, _Up&& __u) {\n    _CUDA_VRANGES::swap(_CUDA_VSTD::forward<_Tp>(__t), _CUDA_VSTD::forward<_Tp>(__t));\n    _CUDA_VRANGES::swap(_CUDA_VSTD::forward<_Up>(__u), _CUDA_VSTD::forward<_Up>(__u));\n    _CUDA_VRANGES::swap(_CUDA_VSTD::forward<_Tp>(__t), _CUDA_VSTD::forward<_Up>(__u));\n    _CUDA_VRANGES::swap(_CUDA_VSTD::forward<_Up>(__u), _CUDA_VSTD::forward<_Tp>(__t));\n  };\n#else\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __swappable_,\n  requires(_Tp& __a, _Tp& __b)(\n    (_CUDA_VRANGES::swap(__a, __b))\n  ));\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT swappable = _LIBCUDACXX_FRAGMENT(__swappable_, _Tp);\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __swappable_with_,\n  requires(_Tp&& __t, _Up&& __u)(\n    requires(common_reference_with<_Tp, _Up>),\n    (_CUDA_VRANGES::swap(_CUDA_VSTD::forward<_Tp>(__t), _CUDA_VSTD::forward<_Tp>(__t))),\n    (_CUDA_VRANGES::swap(_CUDA_VSTD::forward<_Up>(__u), _CUDA_VSTD::forward<_Up>(__u))),\n    (_CUDA_VRANGES::swap(_CUDA_VSTD::forward<_Tp>(__t), _CUDA_VSTD::forward<_Up>(__u))),\n    (_CUDA_VRANGES::swap(_CUDA_VSTD::forward<_Up>(__u), _CUDA_VSTD::forward<_Tp>(__t)))\n  ));\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT swappable_with = _LIBCUDACXX_FRAGMENT(__swappable_with_, _Tp, _Up);\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n#if defined(_LIBCUDACXX_COMPILER_MSVC)\n_LIBCUDACXX_NV_DIAG_DEFAULT(461) // nonstandard cast to array type ignored\n#endif // _LIBCUDACXX_COMPILER_MSVC\n\n\n#endif // _LIBCUDACXX___CONCEPTS_SWAPPABLE_H\n", "__concepts/totally_ordered.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CONCEPTS_TOTALLY_ORDERED_H\n#define _LIBCUDACXX___CONCEPTS_TOTALLY_ORDERED_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__concepts/boolean_testable.h\"\n#include \"../__concepts/equality_comparable.h\"\n#include \"../__type_traits/common_reference.h\"\n#include \"../__type_traits/make_const_lvalue_ref.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\n\n// [concept.totallyordered]\n\ntemplate<class _Tp, class _Up>\nconcept __partially_ordered_with =\n  requires(__make_const_lvalue_ref<_Tp> __t, __make_const_lvalue_ref<_Up> __u) {\n    { __t <  __u } -> __boolean_testable;\n    { __t >  __u } -> __boolean_testable;\n    { __t <= __u } -> __boolean_testable;\n    { __t >= __u } -> __boolean_testable;\n    { __u <  __t } -> __boolean_testable;\n    { __u >  __t } -> __boolean_testable;\n    { __u <= __t } -> __boolean_testable;\n    { __u >= __t } -> __boolean_testable;\n  };\n\ntemplate<class _Tp>\nconcept totally_ordered = equality_comparable<_Tp> && __partially_ordered_with<_Tp, _Tp>;\n\ntemplate<class _Tp, class _Up>\nconcept totally_ordered_with =\n  totally_ordered<_Tp> && totally_ordered<_Up> &&\n  equality_comparable_with<_Tp, _Up> &&\n  totally_ordered<\n    common_reference_t<\n      __make_const_lvalue_ref<_Tp>,\n      __make_const_lvalue_ref<_Up>>> &&\n  __partially_ordered_with<_Tp, _Up>;\n\n#elif _LIBCUDACXX_STD_VER > 11\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __partially_ordered_with_,\n  requires(__make_const_lvalue_ref<_Tp> __t, __make_const_lvalue_ref<_Up> __u) //\n  (requires(__boolean_testable<decltype(__t <  __u)>),\n   requires(__boolean_testable<decltype(__t >  __u)>),\n   requires(__boolean_testable<decltype(__t <= __u)>),\n   requires(__boolean_testable<decltype(__t >= __u)>),\n   requires(__boolean_testable<decltype(__u <  __t)>),\n   requires(__boolean_testable<decltype(__u >  __t)>),\n   requires(__boolean_testable<decltype(__u <= __t)>),\n   requires(__boolean_testable<decltype(__u >= __t)>)));\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT __partially_ordered_with = _LIBCUDACXX_FRAGMENT(__partially_ordered_with_, _Tp, _Up);\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __totally_ordered_,\n  requires()(\n    requires(equality_comparable<_Tp>),\n    requires(__partially_ordered_with<_Tp, _Tp>)\n  ));\n\ntemplate<class _Tp>\n_LIBCUDACXX_CONCEPT totally_ordered = _LIBCUDACXX_FRAGMENT(__totally_ordered_, _Tp);\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT_FRAGMENT(\n  __totally_ordered_with_,\n  requires()(\n    requires(totally_ordered<_Tp>),\n    requires(totally_ordered<_Up>),\n    requires(equality_comparable_with<_Tp, _Up>),\n    requires(totally_ordered<\n    common_reference_t<\n      __make_const_lvalue_ref<_Tp>,\n      __make_const_lvalue_ref<_Up>>>),\n    requires(__partially_ordered_with<_Tp, _Up>)));\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_CONCEPT totally_ordered_with = _LIBCUDACXX_FRAGMENT(__totally_ordered_with_, _Tp, _Up);;\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CONCEPTS_TOTALLY_ORDERED_H\n", "__cuda/atomic.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CUDA_ATOMIC_H\n#define _LIBCUDACXX___CUDA_ATOMIC_H\n\n#ifndef __cuda_std__\n#error \"<__cuda/atomic> should only be included in from <cuda/std/atomic>\"\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_CUDA\n\nusing std::__detail::thread_scope;\nusing std::__detail::thread_scope_system;\nusing std::__detail::thread_scope_device;\nusing std::__detail::thread_scope_block;\nusing std::__detail::thread_scope_thread;\n\nnamespace __detail {\nusing std::__detail::__thread_scope_block_tag;\nusing std::__detail::__thread_scope_device_tag;\nusing std::__detail::__thread_scope_system_tag;\n}\n\nusing memory_order = std::memory_order;\n\nconstexpr memory_order memory_order_relaxed = std::memory_order_relaxed;\nconstexpr memory_order memory_order_consume = std::memory_order_consume;\nconstexpr memory_order memory_order_acquire = std::memory_order_acquire;\nconstexpr memory_order memory_order_release = std::memory_order_release;\nconstexpr memory_order memory_order_acq_rel = std::memory_order_acq_rel;\nconstexpr memory_order memory_order_seq_cst = std::memory_order_seq_cst;\n\n// atomic<T>\n\ntemplate <class _Tp, thread_scope _Sco = thread_scope::thread_scope_system>\nstruct atomic\n    : public std::__atomic_base<_Tp, _Sco>\n{\n    typedef std::__atomic_base<_Tp, _Sco> __base;\n\n    constexpr atomic() noexcept = default;\n    _LIBCUDACXX_HOST_DEVICE\n    constexpr atomic(_Tp __d) noexcept : __base(__d) {}\n\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp operator=(_Tp __d) volatile noexcept\n        {__base::store(__d); return __d;}\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp operator=(_Tp __d) noexcept\n        {__base::store(__d); return __d;}\n\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp fetch_max(const _Tp & __op, memory_order __m = memory_order_seq_cst) volatile noexcept\n    {\n        return std::__detail::__cxx_atomic_fetch_max(&this->__a_, __op, __m);\n    }\n\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp fetch_min(const _Tp & __op, memory_order __m = memory_order_seq_cst) volatile noexcept\n    {\n        return std::__detail::__cxx_atomic_fetch_min(&this->__a_, __op, __m);\n    }\n};\n\n// atomic<T*>\n\ntemplate <class _Tp, thread_scope _Sco>\nstruct atomic<_Tp*, _Sco>\n    : public std::__atomic_base<_Tp*, _Sco>\n{\n    typedef std::__atomic_base<_Tp*, _Sco> __base;\n\n    constexpr atomic() noexcept = default;\n    _LIBCUDACXX_HOST_DEVICE\n    constexpr atomic(_Tp* __d) noexcept : __base(__d) {}\n\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* operator=(_Tp* __d) volatile noexcept\n        {__base::store(__d); return __d;}\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* operator=(_Tp* __d) noexcept\n        {__base::store(__d); return __d;}\n\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* fetch_add(ptrdiff_t __op, memory_order __m = memory_order_seq_cst)\n                                                                        volatile noexcept\n        {return __cxx_atomic_fetch_add(&this->__a_, __op, __m);}\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* fetch_add(ptrdiff_t __op, memory_order __m = memory_order_seq_cst) noexcept\n        {return __cxx_atomic_fetch_add(&this->__a_, __op, __m);}\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* fetch_sub(ptrdiff_t __op, memory_order __m = memory_order_seq_cst)\n                                                                        volatile noexcept\n        {return __cxx_atomic_fetch_sub(&this->__a_, __op, __m);}\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* fetch_sub(ptrdiff_t __op, memory_order __m = memory_order_seq_cst) noexcept\n        {return __cxx_atomic_fetch_sub(&this->__a_, __op, __m);}\n\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* operator++(int) volatile noexcept            {return fetch_add(1);}\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* operator++(int) noexcept                     {return fetch_add(1);}\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* operator--(int) volatile noexcept            {return fetch_sub(1);}\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* operator--(int) noexcept                     {return fetch_sub(1);}\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* operator++() volatile noexcept               {return fetch_add(1) + 1;}\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* operator++() noexcept                        {return fetch_add(1) + 1;}\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* operator--() volatile noexcept               {return fetch_sub(1) - 1;}\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* operator--() noexcept                        {return fetch_sub(1) - 1;}\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* operator+=(ptrdiff_t __op) volatile noexcept {return fetch_add(__op) + __op;}\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* operator+=(ptrdiff_t __op) noexcept          {return fetch_add(__op) + __op;}\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* operator-=(ptrdiff_t __op) volatile noexcept {return fetch_sub(__op) - __op;}\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* operator-=(ptrdiff_t __op) noexcept          {return fetch_sub(__op) - __op;}\n};\n\n// atomic_ref<T>\n\ntemplate <class _Tp, thread_scope _Sco = thread_scope::thread_scope_system>\nstruct atomic_ref\n    : public std::__atomic_base_ref<_Tp, _Sco>\n{\n    typedef std::__atomic_base_ref<_Tp, _Sco> __base;\n\n    _LIBCUDACXX_HOST_DEVICE\n    constexpr atomic_ref(_Tp& __d) noexcept : __base(__d) {}\n\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp operator=(_Tp __d) const volatile noexcept\n        {__base::store(__d); return __d;}\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp operator=(_Tp __d) const noexcept\n        {__base::store(__d); return __d;}\n\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp fetch_max(const _Tp & __op, memory_order __m = memory_order_seq_cst) const volatile noexcept\n    {\n        return std::__detail::__cxx_atomic_fetch_max(&this->__a_, __op, __m);\n    }\n\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp fetch_min(const _Tp & __op, memory_order __m = memory_order_seq_cst) const volatile noexcept\n    {\n        return std::__detail::__cxx_atomic_fetch_min(&this->__a_, __op, __m);\n    }\n};\n\n// atomic_ref<T*>\n\ntemplate <class _Tp, thread_scope _Sco>\nstruct atomic_ref<_Tp*, _Sco>\n    : public std::__atomic_base_ref<_Tp*, _Sco>\n{\n    typedef std::__atomic_base_ref<_Tp*, _Sco> __base;\n\n    _LIBCUDACXX_HOST_DEVICE\n    constexpr atomic_ref(_Tp*& __d) noexcept : __base(__d) {}\n\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* operator=(_Tp* __d) const volatile noexcept\n        {__base::store(__d); return __d;}\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* operator=(_Tp* __d) const noexcept\n        {__base::store(__d); return __d;}\n\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* fetch_add(ptrdiff_t __op,\n                   memory_order __m = memory_order_seq_cst) const volatile noexcept\n        {return __cxx_atomic_fetch_add(&this->__a_, __op, __m);}\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* fetch_add(ptrdiff_t __op,\n                   memory_order __m = memory_order_seq_cst) const noexcept\n        {return __cxx_atomic_fetch_add(&this->__a_, __op, __m);}\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* fetch_sub(ptrdiff_t __op,\n                   memory_order __m = memory_order_seq_cst) const volatile noexcept\n        {return __cxx_atomic_fetch_sub(&this->__a_, __op, __m);}\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* fetch_sub(ptrdiff_t __op,\n                   memory_order __m = memory_order_seq_cst) const noexcept\n        {return __cxx_atomic_fetch_sub(&this->__a_, __op, __m);}\n\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* operator++(int) const volatile noexcept            {return fetch_add(1);}\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* operator++(int) const noexcept                     {return fetch_add(1);}\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* operator--(int) const volatile noexcept            {return fetch_sub(1);}\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* operator--(int) const noexcept                     {return fetch_sub(1);}\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* operator++() const volatile noexcept               {return fetch_add(1) + 1;}\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* operator++() const noexcept                        {return fetch_add(1) + 1;}\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* operator--() const volatile noexcept               {return fetch_sub(1) - 1;}\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* operator--() const noexcept                        {return fetch_sub(1) - 1;}\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* operator+=(ptrdiff_t __op) const volatile noexcept {return fetch_add(__op) + __op;}\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* operator+=(ptrdiff_t __op) const noexcept          {return fetch_add(__op) + __op;}\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* operator-=(ptrdiff_t __op) const volatile noexcept {return fetch_sub(__op) - __op;}\n    _LIBCUDACXX_HOST_DEVICE\n    _Tp* operator-=(ptrdiff_t __op) const noexcept          {return fetch_sub(__op) - __op;}\n};\n\ninline _LIBCUDACXX_HOST_DEVICE void atomic_thread_fence(memory_order __m, thread_scope _Scope = thread_scope::thread_scope_system) {\n    NV_DISPATCH_TARGET(\n        NV_IS_DEVICE, (\n            switch(_Scope) {\n            case thread_scope::thread_scope_system:\n                std::__detail::__atomic_thread_fence_cuda((int)__m, __detail::__thread_scope_system_tag());\n                break;\n            case thread_scope::thread_scope_device:\n                std::__detail::__atomic_thread_fence_cuda((int)__m, __detail::__thread_scope_device_tag());\n                break;\n            case thread_scope::thread_scope_block:\n                std::__detail::__atomic_thread_fence_cuda((int)__m, __detail::__thread_scope_block_tag());\n                break;\n            // Atomics scoped to themselves do not require fencing\n            case thread_scope::thread_scope_thread:\n                break;\n            }\n        ),\n        NV_IS_HOST, (\n            (void) _Scope;\n            std::atomic_thread_fence(__m);\n        )\n    )\n}\n\ninline _LIBCUDACXX_HOST_DEVICE void atomic_signal_fence(memory_order __m) {\n    std::atomic_signal_fence(__m);\n}\n\n_LIBCUDACXX_END_NAMESPACE_CUDA\n\n#endif // _LIBCUDACXX___CUDA_ATOMIC_H\n", "__cuda/atomic_prelude.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CUDA_ATOMIC_PRELUDE_H\n#define _LIBCUDACXX___CUDA_ATOMIC_PRELUDE_H\n\n#ifndef __cuda_std__\n#error \"<__cuda/atomic_prelude> should only be included in from <cuda/std/atomic>\"\n#endif // __cuda_std__\n\n#ifndef _LIBCUDACXX_COMPILER_NVRTC\n    #include \"../cassert\" // TRANSITION: Fix transitive includes\n    #include <atomic>\n    static_assert(ATOMIC_BOOL_LOCK_FREE == 2, \"\");\n    static_assert(ATOMIC_CHAR_LOCK_FREE == 2, \"\");\n    static_assert(ATOMIC_CHAR16_T_LOCK_FREE == 2, \"\");\n    static_assert(ATOMIC_CHAR32_T_LOCK_FREE == 2, \"\");\n    static_assert(ATOMIC_WCHAR_T_LOCK_FREE == 2, \"\");\n    static_assert(ATOMIC_SHORT_LOCK_FREE == 2, \"\");\n    static_assert(ATOMIC_INT_LOCK_FREE == 2, \"\");\n    static_assert(ATOMIC_LONG_LOCK_FREE == 2, \"\");\n    static_assert(ATOMIC_LLONG_LOCK_FREE == 2, \"\");\n    static_assert(ATOMIC_POINTER_LOCK_FREE == 2, \"\");\n    #undef ATOMIC_BOOL_LOCK_FREE\n    #undef ATOMIC_BOOL_LOCK_FREE\n    #undef ATOMIC_CHAR_LOCK_FREE\n    #undef ATOMIC_CHAR16_T_LOCK_FREE\n    #undef ATOMIC_CHAR32_T_LOCK_FREE\n    #undef ATOMIC_WCHAR_T_LOCK_FREE\n    #undef ATOMIC_SHORT_LOCK_FREE\n    #undef ATOMIC_INT_LOCK_FREE\n    #undef ATOMIC_LONG_LOCK_FREE\n    #undef ATOMIC_LLONG_LOCK_FREE\n    #undef ATOMIC_POINTER_LOCK_FREE\n    #undef ATOMIC_FLAG_INIT\n    #undef ATOMIC_VAR_INIT\n#endif // _LIBCUDACXX_COMPILER_NVRTC\n\n// pre-define lock free query for heterogeneous compatibility\n#ifndef _LIBCUDACXX_ATOMIC_IS_LOCK_FREE\n#define _LIBCUDACXX_ATOMIC_IS_LOCK_FREE(__x) (__x <= 8)\n#endif\n\n#ifndef _LIBCUDACXX_COMPILER_NVRTC\n#include <thread>\n#include <errno.h>\n#endif // _LIBCUDACXX_COMPILER_NVRTC\n\n#endif // _LIBCUDACXX___CUDA_ATOMIC_PRELUDE_H\n", "__cuda/barrier.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CUDA_BARRIER_H\n#define _LIBCUDACXX___CUDA_BARRIER_H\n\n#ifndef __cuda_std__\n#error \"<__cuda/barrier> should only be included in from <cuda/std/barrier>\"\n#endif // __cuda_std__\n\n#if defined(__CUDA_MINIMUM_ARCH__) && __CUDA_MINIMUM_ARCH__ < 700\n#  error \"CUDA synchronization primitives are only supported for sm_70 and up.\"\n#endif\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n#if defined(_LIBCUDACXX_COMPILER_NVRTC)\n#define _LIBCUDACXX_OFFSET_IS_ZERO(type, member) !(&(((type *)0)->member))\n#else\n#define _LIBCUDACXX_OFFSET_IS_ZERO(type, member) !offsetof(type, member)\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_CUDA\n\n// foward declaration required for memcpy_async, pipeline \"sync\" defined here\ntemplate<thread_scope _Scope>\nclass pipeline;\n\ntemplate<_CUDA_VSTD::size_t _Alignment>\nstruct aligned_size_t {\n    static constexpr _CUDA_VSTD::size_t align = _Alignment;\n    _CUDA_VSTD::size_t value;\n    _LIBCUDACXX_INLINE_VISIBILITY\n    explicit aligned_size_t(size_t __s) : value(__s) { }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    operator size_t() const { return value; }\n};\n\n// Type only used for logging purpose\nenum async_contract_fulfillment\n{\n    none,\n    async\n};\n\ntemplate<thread_scope _Sco, class _CompletionF = _CUDA_VSTD::__empty_completion>\nclass barrier : public _CUDA_VSTD::__barrier_base<_CompletionF, _Sco> {\npublic:\n    barrier() = default;\n\n    barrier(const barrier &) = delete;\n    barrier & operator=(const barrier &) = delete;\n\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    barrier(_CUDA_VSTD::ptrdiff_t __expected, _CompletionF __completion = _CompletionF())\n        : _CUDA_VSTD::__barrier_base<_CompletionF, _Sco>(__expected, __completion) {\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    friend void init(barrier * __b, _CUDA_VSTD::ptrdiff_t __expected) {\n#if (_LIBCUDACXX_DEBUG_LEVEL >= 2)\n        _LIBCUDACXX_DEBUG_ASSERT(__expected >= 0);\n#endif\n\n        new (__b) barrier(__expected);\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    friend void init(barrier * __b, _CUDA_VSTD::ptrdiff_t __expected, _CompletionF __completion) {\n#if (_LIBCUDACXX_DEBUG_LEVEL >= 2)\n        _LIBCUDACXX_DEBUG_ASSERT(__expected >= 0);\n#endif\n        new (__b) barrier(__expected, __completion);\n    }\n};\n\nstruct __block_scope_barrier_base {};\n\n_LIBCUDACXX_END_NAMESPACE_CUDA\n\n_LIBCUDACXX_BEGIN_NAMESPACE_CUDA_DEVICE\n\n_LIBCUDACXX_DEVICE\ninline _CUDA_VSTD::uint64_t * barrier_native_handle(barrier<thread_scope_block> & b);\n\n_LIBCUDACXX_END_NAMESPACE_CUDA_DEVICE\n\n_LIBCUDACXX_BEGIN_NAMESPACE_CUDA\n\ntemplate<>\nclass barrier<thread_scope_block, _CUDA_VSTD::__empty_completion> : public __block_scope_barrier_base {\n    using __barrier_base = _CUDA_VSTD::__barrier_base<_CUDA_VSTD::__empty_completion, (int)thread_scope_block>;\n    __barrier_base __barrier;\n\n    _LIBCUDACXX_DEVICE\n    friend inline _CUDA_VSTD::uint64_t * device::_LIBCUDACXX_ABI_NAMESPACE::barrier_native_handle(barrier<thread_scope_block> & b);\n\ntemplate<typename _Barrier>\nfriend class _CUDA_VSTD::__barrier_poll_tester_phase;\ntemplate<typename _Barrier>\nfriend class _CUDA_VSTD::__barrier_poll_tester_parity;\n\npublic:\n    using arrival_token = typename __barrier_base::arrival_token;\n    barrier() = default;\n\n    barrier(const barrier &) = delete;\n    barrier & operator=(const barrier &) = delete;\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    barrier(_CUDA_VSTD::ptrdiff_t __expected, _CUDA_VSTD::__empty_completion __completion = _CUDA_VSTD::__empty_completion()) {\n        static_assert(_LIBCUDACXX_OFFSET_IS_ZERO(barrier<thread_scope_block>, __barrier), \"fatal error: bad barrier layout\");\n        init(this, __expected, __completion);\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    ~barrier() {\n        NV_DISPATCH_TARGET(\n            NV_PROVIDES_SM_90, (\n                if (__isShared(&__barrier)) {\n                    asm volatile (\"mbarrier.inval.shared.b64 [%0];\"\n                        :: \"r\"(static_cast<_CUDA_VSTD::uint32_t>(__cvta_generic_to_shared(&__barrier)))\n                        : \"memory\");\n                }\n                else if (__isClusterShared(&__barrier)) {\n                    __trap();\n                }\n            ), NV_PROVIDES_SM_80, (\n                if (__isShared(&__barrier)) {\n                    asm volatile (\"mbarrier.inval.shared.b64 [%0];\"\n                        :: \"r\"(static_cast<_CUDA_VSTD::uint32_t>(__cvta_generic_to_shared(&__barrier)))\n                        : \"memory\");\n                }\n            )\n        )\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    friend void init(barrier * __b, _CUDA_VSTD::ptrdiff_t __expected, _CUDA_VSTD::__empty_completion __completion = _CUDA_VSTD::__empty_completion()) {\n        NV_DISPATCH_TARGET(\n            NV_PROVIDES_SM_90, (\n                if (__isShared(&__b->__barrier)) {\n                    asm volatile (\"mbarrier.init.shared.b64 [%0], %1;\"\n                        :: \"r\"(static_cast<_CUDA_VSTD::uint32_t>(__cvta_generic_to_shared(&__b->__barrier))),\n                            \"r\"(static_cast<_CUDA_VSTD::uint32_t>(__expected))\n                        : \"memory\");\n                }\n                else if (__isClusterShared(&__b->__barrier))\n                {\n                    __trap();\n                }\n                else\n                {\n                    new (&__b->__barrier) __barrier_base(__expected);\n                }\n            ),\n            NV_PROVIDES_SM_80, (\n                if (__isShared(&__b->__barrier)) {\n                    asm volatile (\"mbarrier.init.shared.b64 [%0], %1;\"\n                        :: \"r\"(static_cast<_CUDA_VSTD::uint32_t>(__cvta_generic_to_shared(&__b->__barrier))),\n                            \"r\"(static_cast<_CUDA_VSTD::uint32_t>(__expected))\n                        : \"memory\");\n                }\n                else\n                {\n                    new (&__b->__barrier) __barrier_base(__expected);\n                }\n            ), NV_ANY_TARGET, (\n                new (&__b->__barrier) __barrier_base(__expected);\n            )\n        )\n    }\n\n    _LIBCUDACXX_NODISCARD_ATTRIBUTE _LIBCUDACXX_INLINE_VISIBILITY\n    arrival_token arrive(_CUDA_VSTD::ptrdiff_t __update = 1) {\n#if (_LIBCUDACXX_DEBUG_LEVEL >= 2)\n        _LIBCUDACXX_DEBUG_ASSERT(__update >= 0);\n        _LIBCUDACXX_DEBUG_ASSERT(__expected_unit >=0);\n#endif\n        arrival_token __token = {};\n        NV_DISPATCH_TARGET(\n            NV_PROVIDES_SM_90, (\n                if (!__isClusterShared(&__barrier)) {\n                    return __barrier.arrive(__update);\n                }\n                else if (!__isShared(&__barrier)) {\n                    __trap();\n                }\n\n                asm volatile (\"mbarrier.arrive.shared.b64 %0, [%1], %2;\"\n                    : \"=l\"(__token)\n                    : \"r\"(static_cast<_CUDA_VSTD::uint32_t>(__cvta_generic_to_shared(&__barrier))),\n                    \"r\"(static_cast<_CUDA_VSTD::uint32_t>(__update))\n                    : \"memory\");\n            ), NV_PROVIDES_SM_80, (\n                if (!__isShared(&__barrier)) {\n                    return __barrier.arrive(__update);\n                }\n\n                // Need 2 instructions, can't finish barrier with arrive > 1\n                if (__update > 1) {\n                    asm volatile (\"mbarrier.arrive.noComplete.shared.b64 %0, [%1], %2;\"\n                        : \"=l\"(__token)\n                        : \"r\"(static_cast<_CUDA_VSTD::uint32_t>(__cvta_generic_to_shared(&__barrier))),\n                            \"r\"(static_cast<_CUDA_VSTD::uint32_t>(__update - 1))\n                        : \"memory\");\n                }\n                asm volatile (\"mbarrier.arrive.shared.b64 %0, [%1];\"\n                    : \"=l\"(__token)\n                    : \"r\"(static_cast<_CUDA_VSTD::uint32_t>(__cvta_generic_to_shared(&__barrier)))\n                    : \"memory\");\n            ), NV_IS_DEVICE, (\n                if (!__isShared(&__barrier)) {\n                    return __barrier.arrive(__update);\n                }\n\n                unsigned int __mask = __activemask();\n                unsigned int __activeA = __match_any_sync(__mask, __update);\n                unsigned int __activeB = __match_any_sync(__mask, reinterpret_cast<_CUDA_VSTD::uintptr_t>(&__barrier));\n                unsigned int __active = __activeA & __activeB;\n                int __inc = __popc(__active) * __update;\n\n                unsigned __laneid;\n                asm (\"mov.u32 %0, %laneid;\" : \"=r\"(__laneid));\n                int __leader = __ffs(__active) - 1;\n                // All threads in mask synchronize here, establishing cummulativity to the __leader:\n                __syncwarp(__mask);\n                if(__leader == __laneid)\n                {\n                    __token = __barrier.arrive(__inc);\n                }\n                __token = __shfl_sync(__active, __token, __leader);\n            ), NV_IS_HOST, (\n                __token = __barrier.arrive(__update);\n            )\n        )\n        return __token;\n    }\n\nprivate:\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline bool __test_wait_sm_80(arrival_token __token) const {\n        int32_t __ready = 0;\n        NV_DISPATCH_TARGET(\n            NV_PROVIDES_SM_80, (\n                asm volatile (\"{\\n\\t\"\n                            \".reg .pred p;\\n\\t\"\n                            \"mbarrier.test_wait.shared.b64 p, [%1], %2;\\n\\t\"\n                            \"selp.b32 %0, 1, 0, p;\\n\\t\"\n                            \"}\"\n                        : \"=r\"(__ready)\n                        : \"r\"(static_cast<_CUDA_VSTD::uint32_t>(__cvta_generic_to_shared(&__barrier))),\n                          \"l\"(__token)\n                        : \"memory\");\n            )\n        )\n        return __ready;\n    }\n\n    // Document de drop > uint32_t for __nanosec on public for APIs\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool __try_wait(arrival_token __token) const {\n        NV_DISPATCH_TARGET(\n            NV_PROVIDES_SM_90, (\n                int32_t __ready = 0;\n                if (!__isClusterShared(&__barrier)) {\n                    return _CUDA_VSTD::__call_try_wait(__barrier, _CUDA_VSTD::move(__token));\n                }\n                else if (!__isShared(&__barrier)) {\n                    __trap();\n                }\n                asm volatile (\"{\\n\\t\"\n                        \".reg .pred p;\\n\\t\"\n                        \"mbarrier.try_wait.shared.b64 p, [%1], %2;\\n\\t\"\n                        \"selp.b32 %0, 1, 0, p;\\n\\t\"\n                        \"}\"\n                    : \"=r\"(__ready)\n                    : \"r\"(static_cast<_CUDA_VSTD::uint32_t>(__cvta_generic_to_shared(&__barrier))),\n                    \"l\"(__token)\n                    : \"memory\");\n                return __ready;\n            ), NV_PROVIDES_SM_80, (\n                if (!__isShared(&__barrier)) {\n                    return _CUDA_VSTD::__call_try_wait(__barrier, _CUDA_VSTD::move(__token));\n                }\n                return __test_wait_sm_80(__token);\n            ), NV_ANY_TARGET, (\n                    return _CUDA_VSTD::__call_try_wait(__barrier, _CUDA_VSTD::move(__token));\n            )\n        )\n    }\n\n    // Document de drop > uint32_t for __nanosec on public for APIs\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool __try_wait(arrival_token __token, _CUDA_VSTD::chrono::nanoseconds __nanosec) const {\n        if (__nanosec.count() < 1) {\n            return __try_wait(_CUDA_VSTD::move(__token));\n        }\n\n        NV_DISPATCH_TARGET(\n            NV_PROVIDES_SM_90, (\n                int32_t __ready = 0;\n                if (!__isClusterShared(&__barrier)) {\n                    return _CUDA_VSTD::__libcpp_thread_poll_with_backoff(\n                        _CUDA_VSTD::__barrier_poll_tester_phase<barrier>(this, _CUDA_VSTD::move(__token)),\n                        __nanosec);\n                }\n                else if (!__isShared(&__barrier)) {\n                    __trap();\n                }\n\n                _CUDA_VSTD::chrono::high_resolution_clock::time_point const __start = _CUDA_VSTD::chrono::high_resolution_clock::now();\n                _CUDA_VSTD::chrono::nanoseconds __elapsed;\n                do {\n                    const _CUDA_VSTD::uint32_t __wait_nsec = static_cast<_CUDA_VSTD::uint32_t>((__nanosec - __elapsed).count());\n                    asm volatile (\"{\\n\\t\"\n                            \".reg .pred p;\\n\\t\"\n                            \"mbarrier.try_wait.shared.b64 p, [%1], %2, %3;\\n\\t\"\n                            \"selp.b32 %0, 1, 0, p;\\n\\t\"\n                            \"}\"\n                            : \"=r\"(__ready)\n                            : \"r\"(static_cast<_CUDA_VSTD::uint32_t>(__cvta_generic_to_shared(&__barrier))),\n                            \"l\"(__token)\n                            \"r\"(__wait_nsec)\n                            : \"memory\");\n                    __elapsed = _CUDA_VSTD::chrono::high_resolution_clock::now() - __start;\n                } while (!__ready && (__nanosec > __elapsed));\n                return __ready;\n            ), NV_PROVIDES_SM_80, (\n                bool __ready = 0;\n                if (!__isShared(&__barrier)) {\n                    return _CUDA_VSTD::__libcpp_thread_poll_with_backoff(\n                        _CUDA_VSTD::__barrier_poll_tester_phase<barrier>(this, _CUDA_VSTD::move(__token)),\n                        __nanosec);\n                }\n\n                _CUDA_VSTD::chrono::high_resolution_clock::time_point const __start = _CUDA_VSTD::chrono::high_resolution_clock::now();\n                do {\n                    __ready = __test_wait_sm_80(__token);\n                } while (!__ready &&\n                        __nanosec > (_CUDA_VSTD::chrono::high_resolution_clock::now() - __start));\n                return __ready;\n            ), NV_ANY_TARGET, (\n                return _CUDA_VSTD::__libcpp_thread_poll_with_backoff(\n                        _CUDA_VSTD::__barrier_poll_tester_phase<barrier>(this, _CUDA_VSTD::move(__token)),\n                        _CUDA_VSTD::chrono::nanoseconds(__nanosec));\n            )\n        )\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline bool __test_wait_parity_sm_80(bool __phase_parity) const {\n        uint16_t __ready = 0;\n        NV_DISPATCH_TARGET(\n            NV_PROVIDES_SM_80, (\n                asm volatile (\"{\"\n                    \".reg .pred %p;\"\n                    \"mbarrier.test_wait.parity.shared.b64 %p, [%1], %2;\"\n                    \"selp.u16 %0, 1, 0, %p;\"\n                    \"}\"\n                    : \"=h\"(__ready)\n                    : \"r\"(static_cast<uint32_t>(__cvta_generic_to_shared(&__barrier))),\n                        \"r\"(static_cast<uint32_t>(__phase_parity))\n                    : \"memory\");\n            )\n        )\n        return __ready;\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool __try_wait_parity(bool __phase_parity)  const {\n        NV_DISPATCH_TARGET(\n            NV_PROVIDES_SM_90, (\n                if (!__isClusterShared(&__barrier)) {\n                    return _CUDA_VSTD::__call_try_wait_parity(__barrier, __phase_parity);\n                }\n                else if (!__isShared(&__barrier)) {\n                    __trap();\n                }\n                int32_t __ready = 0;\n\n                asm volatile (\"{\\n\\t\"\n                        \".reg .pred p;\\n\\t\"\n                        \"mbarrier.try_wait.parity.shared.b64 p, [%1], %2;\\n\\t\"\n                        \"selp.b32 %0, 1, 0, p;\\n\\t\"\n                        \"}\"\n                        : \"=r\"(__ready)\n                        : \"r\"(static_cast<_CUDA_VSTD::uint32_t>(__cvta_generic_to_shared(&__barrier))),\n                          \"r\"(static_cast<_CUDA_VSTD::uint32_t>(__phase_parity))\n                        :);\n\n                return __ready;\n            ), NV_PROVIDES_SM_80, (\n                if (!__isShared(&__barrier)) {\n                    return _CUDA_VSTD::__call_try_wait_parity(__barrier, __phase_parity);\n                }\n\n                return __test_wait_parity_sm_80(__phase_parity);\n            ), NV_ANY_TARGET, (\n                return _CUDA_VSTD::__call_try_wait_parity(__barrier, __phase_parity);\n            )\n        )\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool __try_wait_parity(bool __phase_parity, _CUDA_VSTD::chrono::nanoseconds __nanosec) const {\n        if (__nanosec.count() < 1) {\n            return __try_wait_parity(__phase_parity);\n        }\n\n        NV_DISPATCH_TARGET(\n            NV_PROVIDES_SM_90, (\n                int32_t __ready = 0;\n                if (!__isClusterShared(&__barrier)) {\n                    return _CUDA_VSTD::__libcpp_thread_poll_with_backoff(\n                            _CUDA_VSTD::__barrier_poll_tester_parity<barrier>(this, __phase_parity),\n                            __nanosec);\n                }\n                else if (!__isShared(&__barrier)) {\n                    __trap();\n                }\n\n                _CUDA_VSTD::chrono::high_resolution_clock::time_point const __start = _CUDA_VSTD::chrono::high_resolution_clock::now();\n                _CUDA_VSTD::chrono::nanoseconds __elapsed;\n                do {\n                    const _CUDA_VSTD::uint32_t __wait_nsec = static_cast<_CUDA_VSTD::uint32_t>((__nanosec - __elapsed).count());\n                    asm volatile (\"{\\n\\t\"\n                            \".reg .pred p;\\n\\t\"\n                            \"mbarrier.try_wait.parity.shared.b64 p, [%1], %2, %3;\\n\\t\"\n                            \"selp.b32 %0, 1, 0, p;\\n\\t\"\n                            \"}\"\n                            : \"=r\"(__ready)\n                            : \"r\"(static_cast<_CUDA_VSTD::uint32_t>(__cvta_generic_to_shared(&__barrier))),\n                              \"r\"(static_cast<_CUDA_VSTD::uint32_t>(__phase_parity)),\n                              \"r\"(__wait_nsec)\n                            : \"memory\");\n                    __elapsed = _CUDA_VSTD::chrono::high_resolution_clock::now() - __start;\n                } while (!__ready && (__nanosec > __elapsed));\n\n                return __ready;\n            ), NV_PROVIDES_SM_80, (\n                bool __ready = 0;\n                if (!__isShared(&__barrier)) {\n                    return _CUDA_VSTD::__libcpp_thread_poll_with_backoff(\n                        _CUDA_VSTD::__barrier_poll_tester_parity<barrier>(this, __phase_parity),\n                        __nanosec);\n                }\n\n                _CUDA_VSTD::chrono::high_resolution_clock::time_point const __start = _CUDA_VSTD::chrono::high_resolution_clock::now();\n                do {\n                    __ready = __test_wait_parity_sm_80(__phase_parity);\n                } while (!__ready &&\n                          __nanosec > (_CUDA_VSTD::chrono::high_resolution_clock::now() - __start));\n\n                return __ready;\n            ), NV_ANY_TARGET, (\n                return _CUDA_VSTD::__libcpp_thread_poll_with_backoff(\n                        _CUDA_VSTD::__barrier_poll_tester_parity<barrier>(this, __phase_parity),\n                        __nanosec);\n            )\n        )\n    }\n\npublic:\n    _LIBCUDACXX_INLINE_VISIBILITY\n    void wait(arrival_token && __phase) const {\n        _CUDA_VSTD::__libcpp_thread_poll_with_backoff(_CUDA_VSTD::__barrier_poll_tester_phase<barrier>(this, _CUDA_VSTD::move(__phase)));\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    void wait_parity(bool __phase_parity) const {\n        _CUDA_VSTD::__libcpp_thread_poll_with_backoff(_CUDA_VSTD::__barrier_poll_tester_parity<barrier>(this, __phase_parity));\n    }\n\n    inline _LIBCUDACXX_INLINE_VISIBILITY\n    void arrive_and_wait() {\n        wait(arrive());\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    void arrive_and_drop() {\n        NV_DISPATCH_TARGET(\n            NV_PROVIDES_SM_90, (\n                if (!__isClusterShared(&__barrier)) {\n                    return __barrier.arrive_and_drop();\n                }\n                else if (!__isShared(&__barrier)) {\n                    __trap();\n                }\n\n                asm volatile (\"mbarrier.arrive_drop.shared.b64 _, [%0];\"\n                    :: \"r\"(static_cast<_CUDA_VSTD::uint32_t>(__cvta_generic_to_shared(&__barrier)))\n                    : \"memory\");\n            ), NV_PROVIDES_SM_80, (\n                // Fallback to slowpath on device\n                if (!__isShared(&__barrier)) {\n                    __barrier.arrive_and_drop();\n                    return;\n                }\n\n                asm volatile (\"mbarrier.arrive_drop.shared.b64 _, [%0];\"\n                    :: \"r\"(static_cast<_CUDA_VSTD::uint32_t>(__cvta_generic_to_shared(&__barrier)))\n                    : \"memory\");\n            ), NV_ANY_TARGET, (\n                // Fallback to slowpath on device\n                __barrier.arrive_and_drop();\n            )\n        )\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    static constexpr ptrdiff_t max() noexcept {\n        return (1 << 20) - 1;\n    }\n\n    template<class _Rep, class _Period>\n    _LIBCUDACXX_NODISCARD_ATTRIBUTE _LIBCUDACXX_INLINE_VISIBILITY\n    bool try_wait_for(arrival_token && __token, const _CUDA_VSTD::chrono::duration<_Rep, _Period>& __dur) {\n        auto __nanosec = _CUDA_VSTD::chrono::duration_cast<_CUDA_VSTD::chrono::nanoseconds>(__dur);\n\n        return __try_wait(_CUDA_VSTD::move(__token), __nanosec);\n    }\n\n    template<class _Clock, class _Duration>\n    _LIBCUDACXX_NODISCARD_ATTRIBUTE _LIBCUDACXX_INLINE_VISIBILITY\n    bool try_wait_until(arrival_token && __token, const _CUDA_VSTD::chrono::time_point<_Clock, _Duration>& __time) {\n        return try_wait_for(_CUDA_VSTD::move(__token), (__time - _Clock::now()));\n    }\n\n    template<class _Rep, class _Period>\n    _LIBCUDACXX_NODISCARD_ATTRIBUTE _LIBCUDACXX_INLINE_VISIBILITY\n    bool try_wait_parity_for(bool __phase_parity, const _CUDA_VSTD::chrono::duration<_Rep, _Period>& __dur) {\n        auto __nanosec = _CUDA_VSTD::chrono::duration_cast<_CUDA_VSTD::chrono::nanoseconds>(__dur);\n\n        return __try_wait_parity(__phase_parity, __nanosec);\n    }\n\n    template<class _Clock, class _Duration>\n    _LIBCUDACXX_NODISCARD_ATTRIBUTE _LIBCUDACXX_INLINE_VISIBILITY\n    bool try_wait_parity_until(bool __phase_parity, const _CUDA_VSTD::chrono::time_point<_Clock, _Duration>& __time) {\n        return try_wait_parity_for(__phase_parity, (__time - _Clock::now()));\n    }\n};\n\n_LIBCUDACXX_END_NAMESPACE_CUDA\n\n_LIBCUDACXX_BEGIN_NAMESPACE_CUDA_DEVICE\n\n_LIBCUDACXX_DEVICE\ninline _CUDA_VSTD::uint64_t * barrier_native_handle(barrier<thread_scope_block> & b) {\n    return reinterpret_cast<_CUDA_VSTD::uint64_t *>(&b.__barrier);\n}\n\n_LIBCUDACXX_END_NAMESPACE_CUDA_DEVICE\n\n_LIBCUDACXX_BEGIN_NAMESPACE_CUDA\n\ntemplate<>\nclass barrier<thread_scope_thread, _CUDA_VSTD::__empty_completion> : private barrier<thread_scope_block> {\n    using __base = barrier<thread_scope_block>;\n\npublic:\n    using __base::__base;\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    friend void init(barrier * __b, _CUDA_VSTD::ptrdiff_t __expected, _CUDA_VSTD::__empty_completion __completion = _CUDA_VSTD::__empty_completion()) {\n        init(static_cast<__base *>(__b), __expected, __completion);\n    }\n\n    using __base::arrive;\n    using __base::wait;\n    using __base::arrive_and_wait;\n    using __base::arrive_and_drop;\n    using __base::max;\n};\n\ntemplate <typename ... _Ty>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr bool __unused(_Ty...) {return true;}\n\ntemplate <typename _Ty>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr bool __unused(_Ty&) {return true;}\n\ntemplate<_CUDA_VSTD::size_t _Alignment>\n_LIBCUDACXX_INLINE_VISIBILITY\ninline void __strided_memcpy(char * __destination, char const * __source, _CUDA_VSTD::size_t __total_size, _CUDA_VSTD::size_t __rank, _CUDA_VSTD::size_t __stride = 1) {\n    if (__stride == 1) {\n        memcpy(__destination, __source, __total_size);\n    }\n    else {\n        for (_CUDA_VSTD::size_t __offset = __rank * _Alignment; __offset < __total_size; __offset += __stride * _Alignment) {\n            memcpy(__destination + __offset, __source + __offset, _Alignment);\n        }\n    }\n}\n\ntemplate<_CUDA_VSTD::size_t _Alignment, bool _Large = (_Alignment > 16)>\nstruct __memcpy_async_impl {\n    _LIBCUDACXX_DEVICE static inline async_contract_fulfillment __copy(char * __destination, char const * __source, _CUDA_VSTD::size_t __total_size, _CUDA_VSTD::size_t __rank, _CUDA_VSTD::size_t __stride) {\n        __strided_memcpy<_Alignment>(__destination, __source, __total_size, __rank, __stride);\n        return async_contract_fulfillment::none;\n    }\n};\n\ntemplate<>\nstruct __memcpy_async_impl<4, false> {\n    _LIBCUDACXX_DEVICE static inline async_contract_fulfillment __copy(char * __destination, char const * __source, _CUDA_VSTD::size_t __total_size, _CUDA_VSTD::size_t __rank, _CUDA_VSTD::size_t __stride) {\n        // Guard from host visibility when compiling in host only contexts\n        NV_IF_TARGET(\n            NV_IS_DEVICE, (\n                for (_CUDA_VSTD::size_t __offset = __rank * 4; __offset < __total_size; __offset += __stride * 4) {\n                    asm volatile (\"cp.async.ca.shared.global [%0], [%1], 4, 4;\"\n                        :: \"r\"(static_cast<_CUDA_VSTD::uint32_t>(__cvta_generic_to_shared(__destination + __offset))),\n                            \"l\"(__source + __offset)\n                        : \"memory\");\n                }\n            )\n        )\n        return async_contract_fulfillment::async;\n    }\n};\n\ntemplate<>\nstruct __memcpy_async_impl<8, false> {\n    _LIBCUDACXX_DEVICE static inline async_contract_fulfillment __copy(char * __destination, char const * __source, _CUDA_VSTD::size_t __total_size, _CUDA_VSTD::size_t __rank, _CUDA_VSTD::size_t __stride) {\n        // Guard from host visibility when compiling in host only contexts\n        NV_IF_TARGET(\n            NV_IS_DEVICE, (\n                for (_CUDA_VSTD::size_t __offset = __rank * 8; __offset < __total_size; __offset += __stride * 8) {\n                    asm volatile (\"cp.async.ca.shared.global [%0], [%1], 8, 8;\"\n                        :: \"r\"(static_cast<_CUDA_VSTD::uint32_t>(__cvta_generic_to_shared(__destination + __offset))),\n                            \"l\"(__source + __offset)\n                        : \"memory\");\n                }\n            )\n        )\n        return async_contract_fulfillment::async;\n    }\n};\n\ntemplate<>\nstruct __memcpy_async_impl<16, false> {\n    _LIBCUDACXX_DEVICE static inline async_contract_fulfillment __copy(char * __destination, char const * __source, _CUDA_VSTD::size_t __total_size, _CUDA_VSTD::size_t __rank, _CUDA_VSTD::size_t __stride) {\n        // Guard from host visibility when compiling in host only contexts\n        NV_IF_TARGET(\n            NV_IS_DEVICE, (\n                for (_CUDA_VSTD::size_t __offset = __rank * 16; __offset < __total_size; __offset += __stride * 16) {\n                    asm volatile (\"cp.async.cg.shared.global [%0], [%1], 16, 16;\"\n                        :: \"r\"(static_cast<_CUDA_VSTD::uint32_t>(__cvta_generic_to_shared(__destination + __offset))),\n                            \"l\"(__source + __offset)\n                        : \"memory\");\n                }\n            )\n        )\n        return async_contract_fulfillment::async;\n    }\n};\n\ntemplate<_CUDA_VSTD::size_t _Alignment>\nstruct __memcpy_async_impl<_Alignment, true> : public __memcpy_async_impl<16, false> { };\n\nstruct __memcpy_arrive_on_impl {\n    template<thread_scope _Sco, typename _CompF, bool _Is_mbarrier = (_Sco >= thread_scope_block) && _CUDA_VSTD::is_same<_CompF, _CUDA_VSTD::__empty_completion>::value>\n    _LIBCUDACXX_INLINE_VISIBILITY static inline void __arrive_on(barrier<_Sco, _CompF> & __barrier, async_contract_fulfillment __is_async) {\n          NV_DISPATCH_TARGET(\n              NV_PROVIDES_SM_90, (\n                  if (_Is_mbarrier && __isClusterShared(&__barrier) && !__isShared(&__barrier)) {\n                      __trap();\n                  }\n              )\n          )\n\n          NV_DISPATCH_TARGET(\n              NV_PROVIDES_SM_80, (\n                  if (__is_async == async_contract_fulfillment::async) {\n                      if (_Is_mbarrier && __isShared(&__barrier)) {\n                          asm volatile (\"cp.async.mbarrier.arrive.shared.b64 [%0];\"\n                              :: \"r\"(static_cast<_CUDA_VSTD::uint32_t>(__cvta_generic_to_shared(&__barrier)))\n                              : \"memory\");\n                      }\n                      else {\n                          asm volatile (\"cp.async.wait_all;\"\n                              ::: \"memory\");\n                      }\n                  }\n              )\n          )\n    }\n\n    template<thread_scope _Sco>\n    _LIBCUDACXX_INLINE_VISIBILITY static inline void __arrive_on(pipeline<_Sco> & __pipeline, async_contract_fulfillment __is_async) {\n        // pipeline does not sync on memcpy_async, defeat pipeline purpose otherwise\n        __unused(__pipeline);\n        __unused(__is_async);\n    }\n};\n\ntemplate<_CUDA_VSTD::size_t _Native_alignment, typename _Group, typename _Sync>\n_LIBCUDACXX_INLINE_VISIBILITY\nvoid inline __memcpy_async_sm_dispatch(\n        _Group const & __group, char * __destination, char const * __source,\n        _CUDA_VSTD::size_t __size, _Sync & __sync, async_contract_fulfillment & __is_async) {\n    // Broken out of __memcpy_async to avoid nesting dispatches\n    NV_DISPATCH_TARGET(\n        NV_PROVIDES_SM_80,\n            __is_async = __memcpy_async_impl<16>::__copy(__destination, __source, __size, __group.thread_rank(), __group.size());\n    )\n}\n\ntemplate<_CUDA_VSTD::size_t _Native_alignment, typename _Group, typename _Sync>\n_LIBCUDACXX_INLINE_VISIBILITY\nasync_contract_fulfillment inline __memcpy_async(\n        _Group const & __group, char * __destination, char const * __source,\n        _CUDA_VSTD::size_t __size, _Sync & __sync) {\n    async_contract_fulfillment __is_async = async_contract_fulfillment::none;\n\n    NV_DISPATCH_TARGET(\n        NV_PROVIDES_SM_80,\n\n        if (__isShared(__destination) && __isGlobal(__source)) {\n            if (_Native_alignment < 4) {\n                auto __source_address = reinterpret_cast<_CUDA_VSTD::uintptr_t>(__source);\n                auto __destination_address = reinterpret_cast<_CUDA_VSTD::uintptr_t>(__destination);\n\n                // Lowest bit set will tell us what the common alignment of the three values is.\n                auto _Alignment = __ffs(__source_address | __destination_address | __size);\n\n                switch (_Alignment) {\n                    default:\n                        __memcpy_async_sm_dispatch<_Native_alignment>(__group, __destination, __source, __size, __sync, __is_async);\n                    case 4: __is_async = __memcpy_async_impl<8>::__copy(__destination, __source, __size, __group.thread_rank(), __group.size()); break;\n                    case 3: __is_async = __memcpy_async_impl<4>::__copy(__destination, __source, __size, __group.thread_rank(), __group.size()); break;\n                    case 2: // fallthrough\n                    case 1: __is_async = __memcpy_async_impl<1>::__copy(__destination, __source, __size, __group.thread_rank(), __group.size()); break;\n                }\n            }\n            else {\n                __is_async = __memcpy_async_impl<_Native_alignment>::__copy(__destination, __source, __size, __group.thread_rank(), __group.size());\n            }\n        }\n        else\n        {\n            __strided_memcpy<_Native_alignment>(__destination, __source, __size, __group.thread_rank(), __group.size());\n        }\n\n        __memcpy_arrive_on_impl::__arrive_on(__sync, __is_async);\n        , NV_ANY_TARGET,\n            __strided_memcpy<_Native_alignment>(__destination, __source, __size, __group.thread_rank(), __group.size());\n    )\n\n    return __is_async;\n}\n\nstruct __single_thread_group {\n    _LIBCUDACXX_INLINE_VISIBILITY\n    void sync() const {}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr _CUDA_VSTD::size_t size() const { return 1; };\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr _CUDA_VSTD::size_t thread_rank() const { return 0; };\n};\n\ntemplate<typename _Group, class _Tp, thread_scope _Sco, typename _CompF>\n_LIBCUDACXX_INLINE_VISIBILITY\nasync_contract_fulfillment memcpy_async(_Group const & __group, _Tp * __destination, _Tp const * __source, _CUDA_VSTD::size_t __size, barrier<_Sco, _CompF> & __barrier) {\n    // When compiling with NVCC and GCC 4.8, certain user defined types that _are_ trivially copyable are\n    // incorrectly classified as not trivially copyable. Remove this assertion to allow for their usage with\n    // memcpy_async when compiling with GCC 4.8.\n    // FIXME: remove the #if once GCC 4.8 is no longer supported.\n#if !defined(_LIBCUDACXX_COMPILER_GCC) || _GNUC_VER > 408\n    static_assert(_CUDA_VSTD::is_trivially_copyable<_Tp>::value, \"memcpy_async requires a trivially copyable type\");\n#endif\n\n    return __memcpy_async<alignof(_Tp)>(__group, reinterpret_cast<char *>(__destination), reinterpret_cast<char const *>(__source), __size, __barrier);\n}\n\ntemplate<typename _Group, class _Tp, _CUDA_VSTD::size_t _Alignment, thread_scope _Sco, typename _CompF, _CUDA_VSTD::size_t _Larger_alignment = (alignof(_Tp) > _Alignment) ? alignof(_Tp) : _Alignment>\n_LIBCUDACXX_INLINE_VISIBILITY\nasync_contract_fulfillment memcpy_async(_Group const & __group, _Tp * __destination, _Tp const * __source, aligned_size_t<_Alignment> __size, barrier<_Sco, _CompF> & __barrier) {\n    // When compiling with NVCC and GCC 4.8, certain user defined types that _are_ trivially copyable are\n    // incorrectly classified as not trivially copyable. Remove this assertion to allow for their usage with\n    // memcpy_async when compiling with GCC 4.8.\n    // FIXME: remove the #if once GCC 4.8 is no longer supported.\n#if !defined(_LIBCUDACXX_COMPILER_GCC) || _GNUC_VER > 408\n    static_assert(_CUDA_VSTD::is_trivially_copyable<_Tp>::value, \"memcpy_async requires a trivially copyable type\");\n#endif\n\n    return __memcpy_async<_Larger_alignment>(__group, reinterpret_cast<char *>(__destination), reinterpret_cast<char const *>(__source), __size, __barrier);\n}\n\ntemplate<class _Tp, typename _Size, thread_scope _Sco, typename _CompF>\n_LIBCUDACXX_INLINE_VISIBILITY\nasync_contract_fulfillment memcpy_async(_Tp * __destination, _Tp const * __source, _Size __size, barrier<_Sco, _CompF> & __barrier) {\n    return memcpy_async(__single_thread_group{}, __destination, __source, __size, __barrier);\n}\n\ntemplate<typename _Group, thread_scope _Sco, typename _CompF>\n_LIBCUDACXX_INLINE_VISIBILITY\nasync_contract_fulfillment memcpy_async(_Group const & __group, void * __destination, void const * __source, _CUDA_VSTD::size_t __size, barrier<_Sco, _CompF> & __barrier) {\n    return __memcpy_async<1>(__group, reinterpret_cast<char *>(__destination), reinterpret_cast<char const *>(__source), __size, __barrier);\n}\n\ntemplate<typename _Group, _CUDA_VSTD::size_t _Alignment, thread_scope _Sco, typename _CompF>\n_LIBCUDACXX_INLINE_VISIBILITY\nasync_contract_fulfillment memcpy_async(_Group const & __group, void * __destination, void const * __source, aligned_size_t<_Alignment> __size, barrier<_Sco, _CompF> & __barrier) {\n    return __memcpy_async<_Alignment>(__group, reinterpret_cast<char *>(__destination), reinterpret_cast<char const *>(__source), __size, __barrier);\n}\n\ntemplate<typename _Size, thread_scope _Sco, typename _CompF>\n_LIBCUDACXX_INLINE_VISIBILITY\nasync_contract_fulfillment memcpy_async(void * __destination, void const * __source, _Size __size, barrier<_Sco, _CompF> & __barrier) {\n    return memcpy_async(__single_thread_group{}, __destination, __source, __size, __barrier);\n}\n\n_LIBCUDACXX_END_NAMESPACE_CUDA\n\n#endif // _LIBCUDACXX___CUDA_BARRIER_H\n", "__cuda/chrono.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CUDA_CHRONO_H\n#define _LIBCUDACXX___CUDA_CHRONO_H\n\n#ifndef __cuda_std__\n#error \"<__cuda/chrono> should only be included in from <cuda/std/chrono>\"\n#endif // __cuda_std__\n\n#include <nv/target>\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\nnamespace chrono {\n\ninline _LIBCUDACXX_INLINE_VISIBILITY\nsystem_clock::time_point system_clock::now() noexcept\n{\nNV_DISPATCH_TARGET(\nNV_IS_DEVICE, (\n    uint64_t __time;\n    asm volatile(\"mov.u64 %0, %%globaltimer;\":\"=l\"(__time)::);\n    return time_point(duration_cast<duration>(nanoseconds(__time)));\n),\nNV_IS_HOST, (\n    return time_point(duration_cast<duration>(nanoseconds(\n            ::std::chrono::duration_cast<::std::chrono::nanoseconds>(\n                ::std::chrono::system_clock::now().time_since_epoch()\n            ).count()\n           )));\n));\n}\n\ninline _LIBCUDACXX_INLINE_VISIBILITY\ntime_t system_clock::to_time_t(const system_clock::time_point& __t) noexcept\n{\n    return time_t(duration_cast<seconds>(__t.time_since_epoch()).count());\n}\n\ninline _LIBCUDACXX_INLINE_VISIBILITY\nsystem_clock::time_point system_clock::from_time_t(time_t __t) noexcept\n{\n    return time_point(seconds(__t));;\n}\n}\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CUDA_CHRONO_H\n", "__cuda/climits_prelude.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CUDA_CLIMITS_PRELUDE_H\n#define _LIBCUDACXX___CUDA_CLIMITS_PRELUDE_H\n\n#ifndef __cuda_std__\n#error \"<__cuda/climits_prelude> should only be included in from <cuda/std/climits>\"\n#endif // __cuda_std__\n\n#ifndef _LIBCUDACXX_COMPILER_NVRTC\n    #include <climits>\n    #include <limits.h>\n    #include <cstdint>\n#else // ^^^ !_LIBCUDACXX_COMPILER_NVRTC ^^^ / vvv _LIBCUDACXX_COMPILER_NVRTC vvv\n    #define CHAR_BIT 8\n\n    #define SCHAR_MIN (-128)\n    #define SCHAR_MAX 127\n    #define UCHAR_MAX 255\n    #define __CHAR_UNSIGNED__ ('\\xff' > 0) // CURSED\n    #if __CHAR_UNSIGNED__\n        #define CHAR_MIN 0\n        #define CHAR_MAX UCHAR_MAX\n    #else\n        #define CHAR_MIN SCHAR_MIN\n        #define CHAR_MAX SCHAR_MAX\n    #endif\n    #define SHRT_MIN (-SHRT_MAX - 1)\n    #define SHRT_MAX 0x7fff\n    #define USHRT_MAX 0xffff\n    #define INT_MIN (-INT_MAX - 1)\n    #define INT_MAX 0x7fffffff\n    #define UINT_MAX 0xffffffff\n    #define LONG_MIN (-LONG_MAX - 1)\n    #ifdef __LP64__\n        #define LONG_MAX LLONG_MAX\n        #define ULONG_MAX ULLONG_MAX\n    #else\n        #define LONG_MAX INT_MAX\n        #define ULONG_MAX UINT_MAX\n    #endif\n    #define LLONG_MIN (-LLONG_MAX - 1)\n    #define LLONG_MAX 0x7fffffffffffffffLL\n    #define ULLONG_MAX 0xffffffffffffffffUL\n\n    #define __FLT_RADIX__ 2\n    #define __FLT_MANT_DIG__ 24\n    #define __FLT_DIG__ 6\n    #define __FLT_MIN__ 1.17549435082228750796873653722224568e-38F\n    #define __FLT_MAX__ 3.40282346638528859811704183484516925e+38F\n    #define __FLT_EPSILON__ 1.19209289550781250000000000000000000e-7F\n    #define __FLT_MIN_EXP__ (-125)\n    #define __FLT_MIN_10_EXP__ (-37)\n    #define __FLT_MAX_EXP__ 128\n    #define __FLT_MAX_10_EXP__ 38\n    #define __FLT_DENORM_MIN__ 1.40129846432481707092372958328991613e-45F\n    #define __DBL_MANT_DIG__ 53\n    #define __DBL_DIG__ 15\n    #define __DBL_MIN__ 2.22507385850720138309023271733240406e-308\n    #define __DBL_MAX__ 1.79769313486231570814527423731704357e+308\n    #define __DBL_EPSILON__ 2.22044604925031308084726333618164062e-16\n    #define __DBL_MIN_EXP__ (-1021)\n    #define __DBL_MIN_10_EXP__ (-307)\n    #define __DBL_MAX_EXP__ 1024\n    #define __DBL_MAX_10_EXP__ 308\n    #define __DBL_DENORM_MIN__ 4.94065645841246544176568792868221372e-324\n\n    template<typename _To, typename _From>\n    static _LIBCUDACXX_DEVICE _LIBCUDACXX_FORCE_INLINE\n    _To __cowchild_cast(_From __from)\n    {\n        static_assert(sizeof(_From) == sizeof(_To), \"\");\n        union __cast { _From __from; _To __to; };\n        __cast __c;\n        __c.__from = __from;\n        return __c.__to;\n    }\n\n    #define __builtin_huge_valf() __cowchild_cast<float>(0x7f800000)\n    #define __builtin_nanf(__dummy) __cowchild_cast<float>(0x7fc00000)\n    #define __builtin_nansf(__dummy) __cowchild_cast<float>(0x7fa00000)\n    #define __builtin_huge_val() __cowchild_cast<double>(0x7ff0000000000000)\n    #define __builtin_nan(__dummy) __cowchild_cast<double>(0x7ff8000000000000)\n    #define __builtin_nans(__dummy) __cowchild_cast<double>(0x7ff4000000000000)\n#endif // _LIBCUDACXX_COMPILER_NVRTC\n\n#endif // _LIBCUDACXX___CUDA_CLIMITS_PRELUDE_H\n", "__cuda/cstddef_prelude.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CUDA_CSTDDEF_PRELUDE_H\n#define _LIBCUDACXX___CUDA_CSTDDEF_PRELUDE_H\n\n#ifndef __cuda_std__\n#error \"<__cuda/cstddef_prelude> should only be included in from <cuda/std/cstddef>\"\n#endif // __cuda_std__\n\n#ifndef _LIBCUDACXX_COMPILER_NVRTC\n#include <cstddef>\n#include <stddef.h>\n#else\n#define offsetof(type, member) (_CUDA_VSTD::size_t)((char*)&(((type *)0)->member) - (char*)0)\n#endif // _LIBCUDACXX_COMPILER_NVRTC\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntypedef decltype(nullptr) nullptr_t;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___CUDA_CSTDDEF_PRELUDE_H\n", "__cuda/cstdint_prelude.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___CUDA_CSTDINT_PRELUDE_H\n#define _LIBCUDACXX___CUDA_CSTDINT_PRELUDE_H\n\n#ifndef __cuda_std__\n#error \"<__cuda/cstdint_prelude> should only be included in from <cuda/std/cstdint>\"\n#endif // __cuda_std__\n\n#ifndef _LIBCUDACXX_COMPILER_NVRTC\n    #include <cstdint>\n#else // ^^^ !_LIBCUDACXX_COMPILER_NVRTC ^^^ / vvv _LIBCUDACXX_COMPILER_NVRTC vvv\n    typedef signed char int8_t;\n    typedef unsigned char uint8_t;\n    typedef signed short int16_t;\n    typedef unsigned short uint16_t;\n    typedef signed int int32_t;\n    typedef unsigned int uint32_t;\n    typedef signed long long int64_t;\n    typedef unsigned long long uint64_t;\n\n#define _LIBCUDACXX_ADDITIONAL_INTS(N) \\\n    typedef int##N##_t int_fast##N##_t; \\\n    typedef uint##N##_t uint_fast##N##_t; \\\n    typedef int##N##_t int_least##N##_t; \\\n    typedef uint##N##_t uint_least##N##_t\n\n    _LIBCUDACXX_ADDITIONAL_INTS(8);\n    _LIBCUDACXX_ADDITIONAL_INTS(16);\n    _LIBCUDACXX_ADDITIONAL_INTS(32);\n    _LIBCUDACXX_ADDITIONAL_INTS(64);\n#undef _LIBCUDACXX_ADDITIONAL_INTS\n\n    typedef int64_t intptr_t;\n    typedef uint64_t uintptr_t;\n    typedef int64_t intmax_t;\n    typedef uint64_t uintmax_t;\n\n    #define INT8_MIN SCHAR_MIN\n    #define INT16_MIN SHRT_MIN\n    #define INT32_MIN INT_MIN\n    #define INT64_MIN LLONG_MIN\n    #define INT8_MAX SCHAR_MAX\n    #define INT16_MAX SHRT_MAX\n    #define INT32_MAX INT_MAX\n    #define INT64_MAX LLONG_MAX\n    #define UINT8_MAX UCHAR_MAX\n    #define UINT16_MAX USHRT_MAX\n    #define UINT32_MAX UINT_MAX\n    #define UINT64_MAX ULLONG_MAX\n    #define INT_FAST8_MIN SCHAR_MIN\n    #define INT_FAST16_MIN SHRT_MIN\n    #define INT_FAST32_MIN INT_MIN\n    #define INT_FAST64_MIN LLONG_MIN\n    #define INT_FAST8_MAX SCHAR_MAX\n    #define INT_FAST16_MAX SHRT_MAX\n    #define INT_FAST32_MAX INT_MAX\n    #define INT_FAST64_MAX LLONG_MAX\n    #define UINT_FAST8_MAX UCHAR_MAX\n    #define UINT_FAST16_MAX USHRT_MAX\n    #define UINT_FAST32_MAX UINT_MAX\n    #define UINT_FAST64_MAX ULLONG_MAX\n\n    #define INT8_C(X) ((int_least8_t)(X))\n    #define INT16_C(X) ((int_least16_t)(X))\n    #define INT32_C(X) ((int_least32_t)(X))\n    #define INT64_C(X) ((int_least64_t)(X))\n    #define UINT8_C(X) ((uint_least8_t)(X))\n    #define UINT16_C(X) ((uint_least16_t)(X))\n    #define UINT32_C(X) ((uint_least32_t)(X))\n    #define UINT64_C(X) ((uint_least64_t)(X))\n    #define INTMAX_C(X) ((intmax_t)(X))\n    #define UINTMAX_C(X) ((uintmax_t)(X))\n#endif // _LIBCUDACXX_COMPILER_NVRTC\n\n#endif // _LIBCUDACXX___CUDA_CSTDINT_PRELUDE_H\n", "__debug": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___DEBUG\n#define _LIBCUDACXX___DEBUG\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"__assert\"\n#include \"__type_traits/is_constant_evaluated.h\"\n#include \"cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n#if defined(_LIBCUDACXX_ENABLE_DEBUG_MODE) && !defined(_LIBCUDACXX_DEBUG_RANDOMIZE_UNSPECIFIED_STABILITY)\n# define _LIBCUDACXX_DEBUG_RANDOMIZE_UNSPECIFIED_STABILITY\n#endif\n\n#if defined(_LIBCUDACXX_ENABLE_DEBUG_MODE) && !defined(_LIBCUDACXX_DEBUG_ITERATOR_BOUNDS_CHECKING)\n# define _LIBCUDACXX_DEBUG_ITERATOR_BOUNDS_CHECKING\n#endif\n\n#ifdef _LIBCUDACXX_ENABLE_DEBUG_MODE\n#   define _LIBCUDACXX_DEBUG_ASSERT(x, m) _LIBCUDACXX_ASSERT(::std::__libcpp_is_constant_evaluated() || (x), m)\n#else\n#   define _LIBCUDACXX_DEBUG_ASSERT(x, m) ((void)0)\n#endif\n\n#if defined(_LIBCUDACXX_ENABLE_DEBUG_MODE) || defined(_LIBCUDACXX_BUILDING_LIBRARY)\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\nstruct _LIBCUDACXX_TYPE_VIS __c_node;\n\nstruct _LIBCUDACXX_TYPE_VIS __i_node\n{\n    void* __i_;\n    __i_node* __next_;\n    __c_node* __c_;\n\n    __i_node(const __i_node&) = delete;\n    __i_node& operator=(const __i_node&) = delete;\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    __i_node(void* __i, __i_node* __next, __c_node* __c)\n        : __i_(__i), __next_(__next), __c_(__c) {}\n    ~__i_node();\n};\n\nstruct _LIBCUDACXX_TYPE_VIS __c_node\n{\n    void* __c_;\n    __c_node* __next_;\n    __i_node** beg_;\n    __i_node** end_;\n    __i_node** cap_;\n\n    __c_node(const __c_node&) = delete;\n    __c_node& operator=(const __c_node&) = delete;\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    explicit __c_node(void* __c, __c_node* __next)\n        : __c_(__c), __next_(__next), beg_(nullptr), end_(nullptr), cap_(nullptr) {}\n    virtual ~__c_node();\n\n    virtual bool __dereferenceable(const void*) const = 0;\n    virtual bool __decrementable(const void*) const = 0;\n    virtual bool __addable(const void*, ptrdiff_t) const = 0;\n    virtual bool __subscriptable(const void*, ptrdiff_t) const = 0;\n\n    void __add(__i_node* __i);\n    _LIBCUDACXX_HIDDEN void __remove(__i_node* __i);\n};\n\ntemplate <class _Cont>\nstruct _C_node\n    : public __c_node\n{\n    explicit _C_node(void* __c, __c_node* __n)\n        : __c_node(__c, __n) {}\n\n    bool __dereferenceable(const void*) const override;\n    bool __decrementable(const void*) const override;\n    bool __addable(const void*, ptrdiff_t) const override;\n    bool __subscriptable(const void*, ptrdiff_t) const override;\n};\n\ntemplate <class _Cont>\ninline bool\n_C_node<_Cont>::__dereferenceable(const void* __i) const\n{\n    typedef typename _Cont::const_iterator iterator;\n    const iterator* __j = static_cast<const iterator*>(__i);\n    _Cont* _Cp = static_cast<_Cont*>(__c_);\n    return _Cp->__dereferenceable(__j);\n}\n\ntemplate <class _Cont>\ninline bool\n_C_node<_Cont>::__decrementable(const void* __i) const\n{\n    typedef typename _Cont::const_iterator iterator;\n    const iterator* __j = static_cast<const iterator*>(__i);\n    _Cont* _Cp = static_cast<_Cont*>(__c_);\n    return _Cp->__decrementable(__j);\n}\n\ntemplate <class _Cont>\ninline bool\n_C_node<_Cont>::__addable(const void* __i, ptrdiff_t __n) const\n{\n    typedef typename _Cont::const_iterator iterator;\n    const iterator* __j = static_cast<const iterator*>(__i);\n    _Cont* _Cp = static_cast<_Cont*>(__c_);\n    return _Cp->__addable(__j, __n);\n}\n\ntemplate <class _Cont>\ninline bool\n_C_node<_Cont>::__subscriptable(const void* __i, ptrdiff_t __n) const\n{\n    typedef typename _Cont::const_iterator iterator;\n    const iterator* __j = static_cast<const iterator*>(__i);\n    _Cont* _Cp = static_cast<_Cont*>(__c_);\n    return _Cp->__subscriptable(__j, __n);\n}\n\nclass _LIBCUDACXX_TYPE_VIS __libcpp_db\n{\n    __c_node** __cbeg_;\n    __c_node** __cend_;\n    size_t   __csz_;\n    __i_node** __ibeg_;\n    __i_node** __iend_;\n    size_t   __isz_;\n\n    explicit __libcpp_db();\npublic:\n    __libcpp_db(const __libcpp_db&) = delete;\n    __libcpp_db& operator=(const __libcpp_db&) = delete;\n\n    ~__libcpp_db();\n\n    class __db_c_iterator;\n    class __db_c_const_iterator;\n    class __db_i_iterator;\n    class __db_i_const_iterator;\n\n    __db_c_const_iterator __c_end() const;\n    __db_i_const_iterator __i_end() const;\n\n    typedef __c_node*(_InsertConstruct)(void*, void*, __c_node*);\n\n    template <class _Cont>\n    _LIBCUDACXX_INLINE_VISIBILITY static __c_node* __create_C_node(void *__mem, void *__c, __c_node *__next) {\n        return ::new (__mem) _C_node<_Cont>(__c, __next);\n    }\n\n    template <class _Cont>\n    _LIBCUDACXX_INLINE_VISIBILITY\n    void __insert_c(_Cont* __c)\n    {\n        __insert_c(static_cast<void*>(__c), &__create_C_node<_Cont>);\n    }\n\n    void __insert_i(void* __i);\n    void __insert_c(void* __c, _InsertConstruct* __fn);\n    void __erase_c(void* __c);\n\n    void __insert_ic(void* __i, const void* __c);\n    void __iterator_copy(void* __i, const void* __i0);\n    void __erase_i(void* __i);\n\n    void* __find_c_from_i(void* __i) const;\n    void __invalidate_all(void* __c);\n    __c_node* __find_c_and_lock(void* __c) const;\n    __c_node* __find_c(void* __c) const;\n    void unlock() const;\n\n    void swap(void* __c1, void* __c2);\n\n\n    bool __dereferenceable(const void* __i) const;\n    bool __decrementable(const void* __i) const;\n    bool __addable(const void* __i, ptrdiff_t __n) const;\n    bool __subscriptable(const void* __i, ptrdiff_t __n) const;\n    bool __less_than_comparable(const void* __i, const void* __j) const;\nprivate:\n    _LIBCUDACXX_HIDDEN\n    __i_node* __insert_iterator(void* __i);\n    _LIBCUDACXX_HIDDEN\n    __i_node* __find_iterator(const void* __i) const;\n\n    friend _LIBCUDACXX_FUNC_VIS __libcpp_db* __get_db();\n};\n\n_LIBCUDACXX_FUNC_VIS __libcpp_db* __get_db();\n_LIBCUDACXX_FUNC_VIS const __libcpp_db* __get_const_db();\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // defined(_LIBCUDACXX_ENABLE_DEBUG_MODE) || defined(_LIBCUDACXX_BUILDING_LIBRARY)\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n_LIBCUDACXX_CONSTEXPR_AFTER_CXX11 inline void __debug_db_insert_c(_Tp* __c) {\n#ifdef _LIBCUDACXX_ENABLE_DEBUG_MODE\n    if (!__libcpp_is_constant_evaluated())\n        __get_db()->__insert_c(__c);\n#else\n    (void)(__c);\n#endif\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n_LIBCUDACXX_CONSTEXPR_AFTER_CXX11 inline void __debug_db_insert_i(_Tp* __i) {\n#ifdef _LIBCUDACXX_ENABLE_DEBUG_MODE\n    if (!__libcpp_is_constant_evaluated())\n        __get_db()->__insert_i(__i);\n#else\n    (void)(__i);\n#endif\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n_LIBCUDACXX_CONSTEXPR_AFTER_CXX11 inline void __debug_db_erase_c(_Tp* __c) {\n#ifdef _LIBCUDACXX_ENABLE_DEBUG_MODE\n    if (!__libcpp_is_constant_evaluated())\n        __get_db()->__erase_c(__c);\n#else\n    (void)(__c);\n#endif\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n_LIBCUDACXX_CONSTEXPR_AFTER_CXX11 inline void __debug_db_swap(_Tp* __lhs, _Tp* __rhs) {\n#ifdef _LIBCUDACXX_ENABLE_DEBUG_MODE\n    if (!__libcpp_is_constant_evaluated())\n        __get_db()->swap(__lhs, __rhs);\n#else\n    (void)(__lhs);\n    (void)(__rhs);\n#endif\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n_LIBCUDACXX_CONSTEXPR_AFTER_CXX11 inline void __debug_db_invalidate_all(_Tp* __c) {\n#ifdef _LIBCUDACXX_ENABLE_DEBUG_MODE\n    if (!__libcpp_is_constant_evaluated())\n        __get_db()->__invalidate_all(__c);\n#else\n    (void)(__c);\n#endif\n}\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___DEBUG\n", "__functional/binary_function.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_BINARY_FUNCTION_H\n#define _LIBCUDACXX___FUNCTIONAL_BINARY_FUNCTION_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER <= 14 || defined(_LIBCUDACXX_ENABLE_CXX17_REMOVED_UNARY_BINARY_FUNCTION)\n\ntemplate <class _Arg1, class _Arg2, class _Result>\nstruct _LIBCUDACXX_TEMPLATE_VIS _LIBCUDACXX_DEPRECATED_IN_CXX11 binary_function\n{\n    typedef _Arg1   first_argument_type;\n    typedef _Arg2   second_argument_type;\n    typedef _Result result_type;\n};\n\n#endif // _LIBCUDACXX_STD_VER <= 14 || defined(_LIBCUDACXX_ENABLE_CXX17_REMOVED_UNARY_BINARY_FUNCTION)\n\ntemplate <class _Arg1, class _Arg2, class _Result> struct __binary_function_keep_layout_base {\n#if _LIBCUDACXX_STD_VER <= 17 || defined(_LIBCUDACXX_ENABLE_CXX20_REMOVED_BINDER_TYPEDEFS)\n  using first_argument_type _LIBCUDACXX_DEPRECATED_IN_CXX17 = _Arg1;\n  using second_argument_type _LIBCUDACXX_DEPRECATED_IN_CXX17 = _Arg2;\n  using result_type _LIBCUDACXX_DEPRECATED_IN_CXX17 = _Result;\n#endif\n};\n\n#if _LIBCUDACXX_STD_VER <= 14 || defined(_LIBCUDACXX_ENABLE_CXX17_REMOVED_UNARY_BINARY_FUNCTION)\n_LIBCUDACXX_DIAGNOSTIC_PUSH\n_LIBCUDACXX_CLANG_DIAGNOSTIC_IGNORED(\"-Wdeprecated-declarations\")\ntemplate <class _Arg1, class _Arg2, class _Result>\nusing __binary_function = binary_function<_Arg1, _Arg2, _Result>;\n_LIBCUDACXX_DIAGNOSTIC_POP\n#else\ntemplate <class _Arg1, class _Arg2, class _Result>\nusing __binary_function = __binary_function_keep_layout_base<_Arg1, _Arg2, _Result>;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FUNCTIONAL_BINARY_FUNCTION_H\n", "__functional/binary_negate.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_BINARY_NEGATE_H\n#define _LIBCUDACXX___FUNCTIONAL_BINARY_NEGATE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__functional/binary_function.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER <= 17 || defined(_LIBCUDACXX_ENABLE_CXX20_REMOVED_NEGATORS)\n\ntemplate <class _Predicate>\nclass _LIBCUDACXX_TEMPLATE_VIS _LIBCUDACXX_DEPRECATED_IN_CXX17 binary_negate\n    : public __binary_function<typename _Predicate::first_argument_type,\n                               typename _Predicate::second_argument_type,\n                               bool>\n{\n    _Predicate __pred_;\npublic:\n    _LIBCUDACXX_INLINE_VISIBILITY explicit _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    binary_negate(const _Predicate& __pred) : __pred_(__pred) {}\n\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    bool operator()(const typename _Predicate::first_argument_type& __x,\n                    const typename _Predicate::second_argument_type& __y) const\n        {return !__pred_(__x, __y);}\n};\n\ntemplate <class _Predicate>\n_LIBCUDACXX_DEPRECATED_IN_CXX17 inline _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\nbinary_negate<_Predicate>\nnot2(const _Predicate& __pred) {return binary_negate<_Predicate>(__pred);}\n\n#endif // _LIBCUDACXX_STD_VER <= 17 || defined(_LIBCUDACXX_ENABLE_CXX20_REMOVED_NEGATORS)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FUNCTIONAL_BINARY_NEGATE_H\n", "__functional/bind.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_BIND_H\n#define _LIBCUDACXX___FUNCTIONAL_BIND_H\n\n// `cuda::std::bind` is not currently supported.\n\n#ifndef __cuda_std__\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__functional/invoke.h\"\n#include \"../__functional/reference_wrapper.h\"\n#include \"../__functional/weak_result_type.h\"\n#include \"../__fwd/get.h\"\n#include \"../__tuple_dir/tuple_element.h\"\n#include \"../__tuple_dir/tuple_indices.h\"\n#include \"../__tuple_dir/tuple_size.h\"\n#include \"../__type_traits/conditional.h\"\n#include \"../__type_traits/decay.h\"\n#include \"../__type_traits/enable_if.h\"\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_constructible.h\"\n#include \"../__type_traits/is_convertible.h\"\n#include \"../__type_traits/is_same.h\"\n#include \"../__type_traits/is_void.h\"\n#include \"../__type_traits/remove_cvref.h\"\n#include \"../__type_traits/remove_reference.h\"\n#include \"../__utility/forward.h\"\n\n#include \"../cstddef\"\n#include \"../tuple\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate<class _Tp>\nstruct is_bind_expression : _If<\n    _IsSame<_Tp, __remove_cvref_t<_Tp> >::value,\n    false_type,\n    is_bind_expression<__remove_cvref_t<_Tp> >\n> {};\n\n#if _LIBCUDACXX_STD_VER > 14\ntemplate <class _Tp>\ninline constexpr size_t is_bind_expression_v = is_bind_expression<_Tp>::value;\n#endif\n\ntemplate<class _Tp>\nstruct is_placeholder : _If<\n    _IsSame<_Tp, __remove_cvref_t<_Tp> >::value,\n    integral_constant<int, 0>,\n    is_placeholder<__remove_cvref_t<_Tp> >\n> {};\n\n#if _LIBCUDACXX_STD_VER > 14\ntemplate <class _Tp>\ninline constexpr size_t is_placeholder_v = is_placeholder<_Tp>::value;\n#endif\n\nnamespace placeholders\n{\n\ntemplate <int _Np> struct __ph {};\n\n#if defined(_LIBCUDACXX_BUILDING_LIBRARY)\n_LIBCUDACXX_FUNC_VIS extern const __ph<1>   _1;\n_LIBCUDACXX_FUNC_VIS extern const __ph<2>   _2;\n_LIBCUDACXX_FUNC_VIS extern const __ph<3>   _3;\n_LIBCUDACXX_FUNC_VIS extern const __ph<4>   _4;\n_LIBCUDACXX_FUNC_VIS extern const __ph<5>   _5;\n_LIBCUDACXX_FUNC_VIS extern const __ph<6>   _6;\n_LIBCUDACXX_FUNC_VIS extern const __ph<7>   _7;\n_LIBCUDACXX_FUNC_VIS extern const __ph<8>   _8;\n_LIBCUDACXX_FUNC_VIS extern const __ph<9>   _9;\n_LIBCUDACXX_FUNC_VIS extern const __ph<10> _10;\n#else\n/* _LIBCUDACXX_INLINE_VAR */ constexpr __ph<1>   _1{};\n/* _LIBCUDACXX_INLINE_VAR */ constexpr __ph<2>   _2{};\n/* _LIBCUDACXX_INLINE_VAR */ constexpr __ph<3>   _3{};\n/* _LIBCUDACXX_INLINE_VAR */ constexpr __ph<4>   _4{};\n/* _LIBCUDACXX_INLINE_VAR */ constexpr __ph<5>   _5{};\n/* _LIBCUDACXX_INLINE_VAR */ constexpr __ph<6>   _6{};\n/* _LIBCUDACXX_INLINE_VAR */ constexpr __ph<7>   _7{};\n/* _LIBCUDACXX_INLINE_VAR */ constexpr __ph<8>   _8{};\n/* _LIBCUDACXX_INLINE_VAR */ constexpr __ph<9>   _9{};\n/* _LIBCUDACXX_INLINE_VAR */ constexpr __ph<10> _10{};\n#endif // defined(_LIBCUDACXX_BUILDING_LIBRARY)\n\n} // namespace placeholders\n\ntemplate<int _Np>\nstruct is_placeholder<placeholders::__ph<_Np> >\n    : public integral_constant<int, _Np> {};\n\n\ntemplate <class _Tp, class _Uj>\ninline _LIBCUDACXX_INLINE_VISIBILITY\n_Tp&\n__mu(reference_wrapper<_Tp> __t, _Uj&)\n{\n    return __t.get();\n}\n\ntemplate <class _Ti, class ..._Uj, size_t ..._Indx>\ninline _LIBCUDACXX_INLINE_VISIBILITY\ntypename __invoke_of<_Ti&, _Uj...>::type\n__mu_expand(_Ti& __ti, tuple<_Uj...>& __uj, __tuple_indices<_Indx...>)\n{\n    return __ti(_CUDA_VSTD::forward<_Uj>(_CUDA_VSTD::get<_Indx>(__uj))...);\n}\n\ntemplate <class _Ti, class ..._Uj>\ninline _LIBCUDACXX_INLINE_VISIBILITY\n__enable_if_t\n<\n    is_bind_expression<_Ti>::value,\n    __invoke_of<_Ti&, _Uj...>\n>\n__mu(_Ti& __ti, tuple<_Uj...>& __uj)\n{\n    typedef typename __make_tuple_indices<sizeof...(_Uj)>::type __indices;\n    return _CUDA_VSTD::__mu_expand(__ti, __uj, __indices());\n}\n\ntemplate <bool IsPh, class _Ti, class _Uj>\nstruct __mu_return2 {};\n\ntemplate <class _Ti, class _Uj>\nstruct __mu_return2<true, _Ti, _Uj>\n{\n    typedef typename tuple_element<is_placeholder<_Ti>::value - 1, _Uj>::type type;\n};\n\ntemplate <class _Ti, class _Uj>\ninline _LIBCUDACXX_INLINE_VISIBILITY\n__enable_if_t\n<\n    0 < is_placeholder<_Ti>::value,\n    typename __mu_return2<0 < is_placeholder<_Ti>::value, _Ti, _Uj>::type\n>\n__mu(_Ti&, _Uj& __uj)\n{\n    const size_t _Indx = is_placeholder<_Ti>::value - 1;\n    return _CUDA_VSTD::forward<typename tuple_element<_Indx, _Uj>::type>(_CUDA_VSTD::get<_Indx>(__uj));\n}\n\ntemplate <class _Ti, class _Uj>\ninline _LIBCUDACXX_INLINE_VISIBILITY\n__enable_if_t\n<\n    !is_bind_expression<_Ti>::value &&\n    is_placeholder<_Ti>::value == 0 &&\n    !__is_reference_wrapper<_Ti>::value,\n    _Ti&\n>\n__mu(_Ti& __ti, _Uj&)\n{\n    return __ti;\n}\n\ntemplate <class _Ti, bool IsReferenceWrapper, bool IsBindEx, bool IsPh,\n          class _TupleUj>\nstruct __mu_return_impl;\n\ntemplate <bool _Invokable, class _Ti, class ..._Uj>\nstruct __mu_return_invokable  // false\n{\n    typedef __nat type;\n};\n\ntemplate <class _Ti, class ..._Uj>\nstruct __mu_return_invokable<true, _Ti, _Uj...>\n{\n    typedef typename __invoke_of<_Ti&, _Uj...>::type type;\n};\n\ntemplate <class _Ti, class ..._Uj>\nstruct __mu_return_impl<_Ti, false, true, false, tuple<_Uj...> >\n    : public __mu_return_invokable<__invokable<_Ti&, _Uj...>::value, _Ti, _Uj...>\n{\n};\n\ntemplate <class _Ti, class _TupleUj>\nstruct __mu_return_impl<_Ti, false, false, true, _TupleUj>\n{\n    typedef typename tuple_element<is_placeholder<_Ti>::value - 1,\n                                   _TupleUj>::type&& type;\n};\n\ntemplate <class _Ti, class _TupleUj>\nstruct __mu_return_impl<_Ti, true, false, false, _TupleUj>\n{\n    typedef typename _Ti::type& type;\n};\n\ntemplate <class _Ti, class _TupleUj>\nstruct __mu_return_impl<_Ti, false, false, false, _TupleUj>\n{\n    typedef _Ti& type;\n};\n\ntemplate <class _Ti, class _TupleUj>\nstruct __mu_return\n    : public __mu_return_impl<_Ti,\n                              __is_reference_wrapper<_Ti>::value,\n                              is_bind_expression<_Ti>::value,\n                              0 < is_placeholder<_Ti>::value &&\n                              is_placeholder<_Ti>::value <= tuple_size<_TupleUj>::value,\n                              _TupleUj>\n{\n};\n\ntemplate <class _Fp, class _BoundArgs, class _TupleUj>\nstruct __is_valid_bind_return\n{\n    static const bool value = false;\n};\n\ntemplate <class _Fp, class ..._BoundArgs, class _TupleUj>\nstruct __is_valid_bind_return<_Fp, tuple<_BoundArgs...>, _TupleUj>\n{\n    static const bool value = __invokable<_Fp,\n                    typename __mu_return<_BoundArgs, _TupleUj>::type...>::value;\n};\n\ntemplate <class _Fp, class ..._BoundArgs, class _TupleUj>\nstruct __is_valid_bind_return<_Fp, const tuple<_BoundArgs...>, _TupleUj>\n{\n    static const bool value = __invokable<_Fp,\n                    typename __mu_return<const _BoundArgs, _TupleUj>::type...>::value;\n};\n\ntemplate <class _Fp, class _BoundArgs, class _TupleUj,\n          bool = __is_valid_bind_return<_Fp, _BoundArgs, _TupleUj>::value>\nstruct __bind_return;\n\ntemplate <class _Fp, class ..._BoundArgs, class _TupleUj>\nstruct __bind_return<_Fp, tuple<_BoundArgs...>, _TupleUj, true>\n{\n    typedef typename __invoke_of\n    <\n        _Fp&,\n        typename __mu_return\n        <\n            _BoundArgs,\n            _TupleUj\n        >::type...\n    >::type type;\n};\n\ntemplate <class _Fp, class ..._BoundArgs, class _TupleUj>\nstruct __bind_return<_Fp, const tuple<_BoundArgs...>, _TupleUj, true>\n{\n    typedef typename __invoke_of\n    <\n        _Fp&,\n        typename __mu_return\n        <\n            const _BoundArgs,\n            _TupleUj\n        >::type...\n    >::type type;\n};\n\ntemplate <class _Fp, class _BoundArgs, class _TupleUj>\nusing __bind_return_t = typename __bind_return<_Fp, _BoundArgs, _TupleUj>::type;\n\ntemplate <class _Fp, class _BoundArgs, size_t ..._Indx, class _Args>\ninline _LIBCUDACXX_INLINE_VISIBILITY\n__bind_return_t<_Fp, _BoundArgs, _Args>\n__apply_functor(_Fp& __f, _BoundArgs& __bound_args, __tuple_indices<_Indx...>,\n                _Args&& __args)\n{\n    return _CUDA_VSTD::__invoke(__f, _CUDA_VSTD::__mu(_CUDA_VSTD::get<_Indx>(__bound_args), __args)...);\n}\n\ntemplate<class _Fp, class ..._BoundArgs>\nclass __bind : public __weak_result_type<__decay_t<_Fp>>\n{\nprotected:\n    typedef __decay_t<_Fp> _Fd;\n    typedef tuple<__decay_t<_BoundArgs>...> _Td;\nprivate:\n    _Fd __f_;\n    _Td __bound_args_;\n\n    typedef typename __make_tuple_indices<sizeof...(_BoundArgs)>::type __indices;\npublic:\n    template <class _Gp, class ..._BA,\n              class = __enable_if_t\n                               <\n                                  is_constructible<_Fd, _Gp>::value &&\n                                  !is_same<__libcpp_remove_reference_t<_Gp>,\n                                           __bind>::value\n                               >>\n      _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n      explicit __bind(_Gp&& __f, _BA&& ...__bound_args)\n        : __f_(_CUDA_VSTD::forward<_Gp>(__f)),\n          __bound_args_(_CUDA_VSTD::forward<_BA>(__bound_args)...) {}\n\n    template <class ..._Args>\n        _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n        __bind_return_t<_Fd, _Td, tuple<_Args&&...>>\n        operator()(_Args&& ...__args)\n        {\n            return _CUDA_VSTD::__apply_functor(__f_, __bound_args_, __indices(),\n                                  tuple<_Args&&...>(_CUDA_VSTD::forward<_Args>(__args)...));\n        }\n\n    template <class ..._Args>\n        _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n        __bind_return_t<const _Fd, const _Td, tuple<_Args&&...>>\n        operator()(_Args&& ...__args) const\n        {\n            return _CUDA_VSTD::__apply_functor(__f_, __bound_args_, __indices(),\n                                   tuple<_Args&&...>(_CUDA_VSTD::forward<_Args>(__args)...));\n        }\n};\n\ntemplate<class _Fp, class ..._BoundArgs>\nstruct is_bind_expression<__bind<_Fp, _BoundArgs...> > : public true_type {};\n\ntemplate<class _Rp, class _Fp, class ..._BoundArgs>\nclass __bind_r\n    : public __bind<_Fp, _BoundArgs...>\n{\n    typedef __bind<_Fp, _BoundArgs...> base;\n    typedef typename base::_Fd _Fd;\n    typedef typename base::_Td _Td;\npublic:\n    typedef _Rp result_type;\n\n\n    template <class _Gp, class ..._BA,\n              class = __enable_if_t\n                               <\n                                  is_constructible<_Fd, _Gp>::value &&\n                                  !is_same<__libcpp_remove_reference_t<_Gp>,\n                                           __bind_r>::value\n                               >>\n      _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n      explicit __bind_r(_Gp&& __f, _BA&& ...__bound_args)\n        : base(_CUDA_VSTD::forward<_Gp>(__f),\n               _CUDA_VSTD::forward<_BA>(__bound_args)...) {}\n\n    template <class ..._Args>\n        _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n        __enable_if_t\n        <\n            is_convertible<__bind_return_t<_Fd, _Td, tuple<_Args&&...>>,\n                           result_type>::value || is_void<_Rp>::value,\n            result_type\n        >\n        operator()(_Args&& ...__args)\n        {\n            typedef __invoke_void_return_wrapper<_Rp> _Invoker;\n            return _Invoker::__call(static_cast<base&>(*this), _CUDA_VSTD::forward<_Args>(__args)...);\n        }\n\n    template <class ..._Args>\n        _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n        __enable_if_t\n        <\n            is_convertible<__bind_return_t<const _Fd, const _Td, tuple<_Args&&...>>,\n                           result_type>::value || is_void<_Rp>::value,\n            result_type\n        >\n        operator()(_Args&& ...__args) const\n        {\n            typedef __invoke_void_return_wrapper<_Rp> _Invoker;\n            return _Invoker::__call(static_cast<base const&>(*this), _CUDA_VSTD::forward<_Args>(__args)...);\n        }\n};\n\ntemplate<class _Rp, class _Fp, class ..._BoundArgs>\nstruct is_bind_expression<__bind_r<_Rp, _Fp, _BoundArgs...> > : public true_type {};\n\ntemplate<class _Fp, class ..._BoundArgs>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n__bind<_Fp, _BoundArgs...>\nbind(_Fp&& __f, _BoundArgs&&... __bound_args)\n{\n    typedef __bind<_Fp, _BoundArgs...> type;\n    return type(_CUDA_VSTD::forward<_Fp>(__f), _CUDA_VSTD::forward<_BoundArgs>(__bound_args)...);\n}\n\ntemplate<class _Rp, class _Fp, class ..._BoundArgs>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n__bind_r<_Rp, _Fp, _BoundArgs...>\nbind(_Fp&& __f, _BoundArgs&&... __bound_args)\n{\n    typedef __bind_r<_Rp, _Fp, _BoundArgs...> type;\n    return type(_CUDA_VSTD::forward<_Fp>(__f), _CUDA_VSTD::forward<_BoundArgs>(__bound_args)...);\n}\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // __cuda_std__\n\n#endif // _LIBCUDACXX___FUNCTIONAL_BIND_H\n", "__functional/bind_back.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_BIND_BACK_H\n#define _LIBCUDACXX___FUNCTIONAL_BIND_BACK_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__functional/invoke.h\"\n#include \"../__functional/perfect_forward.h\"\n#include \"../__fwd/get.h\"\n#include \"../__tuple_dir/tuple_size.h\"\n#include \"../__type_traits/decay.h\"\n#include \"../__type_traits/enable_if.h\"\n#include \"../__type_traits/is_constructible.h\"\n#include \"../__type_traits/is_move_constructible.h\"\n#include \"../__utility/forward.h\"\n#include \"../__utility/integer_sequence.h\"\n\n#include \"../tuple\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 14\n\ntemplate <size_t _NBound, class = make_index_sequence<_NBound>>\nstruct __bind_back_op;\n\ntemplate <size_t _NBound, size_t ..._Ip>\nstruct __bind_back_op<_NBound, index_sequence<_Ip...>> {\n    template <class _Fn, class _BoundArgs, class... _Args>\n    _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr auto operator()(_Fn&& __f, _BoundArgs&& __bound_args, _Args&&... __args) const\n        noexcept(noexcept(_CUDA_VSTD::invoke(_CUDA_VSTD::forward<_Fn>(__f), _CUDA_VSTD::forward<_Args>(__args)..., _CUDA_VSTD::get<_Ip>(_CUDA_VSTD::forward<_BoundArgs>(__bound_args))...)))\n        -> decltype(      _CUDA_VSTD::invoke(_CUDA_VSTD::forward<_Fn>(__f), _CUDA_VSTD::forward<_Args>(__args)..., _CUDA_VSTD::get<_Ip>(_CUDA_VSTD::forward<_BoundArgs>(__bound_args))...))\n        { return          _CUDA_VSTD::invoke(_CUDA_VSTD::forward<_Fn>(__f), _CUDA_VSTD::forward<_Args>(__args)..., _CUDA_VSTD::get<_Ip>(_CUDA_VSTD::forward<_BoundArgs>(__bound_args))...); }\n};\n\ntemplate <class _Fn, class _BoundArgs>\nstruct __bind_back_t : __perfect_forward<__bind_back_op<tuple_size_v<_BoundArgs>>, _Fn, _BoundArgs> {\n    using __perfect_forward<__bind_back_op<tuple_size_v<_BoundArgs>>, _Fn, _BoundArgs>::__perfect_forward;\n};\n\ntemplate <class _Fn, class ..._Args, class = enable_if_t<\n    _And<\n        is_constructible<decay_t<_Fn>, _Fn>,\n        is_move_constructible<decay_t<_Fn>>,\n        is_constructible<decay_t<_Args>, _Args>...,\n        is_move_constructible<decay_t<_Args>>...\n    >::value\n>>\n_LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr auto __bind_back(_Fn&& __f, _Args&&... __args)\n    noexcept(noexcept(__bind_back_t<decay_t<_Fn>, tuple<decay_t<_Args>...>>(_CUDA_VSTD::forward<_Fn>(__f), _CUDA_VSTD::forward_as_tuple(_CUDA_VSTD::forward<_Args>(__args)...))))\n    -> decltype(      __bind_back_t<decay_t<_Fn>, tuple<decay_t<_Args>...>>(_CUDA_VSTD::forward<_Fn>(__f), _CUDA_VSTD::forward_as_tuple(_CUDA_VSTD::forward<_Args>(__args)...)))\n    { return          __bind_back_t<decay_t<_Fn>, tuple<decay_t<_Args>...>>(_CUDA_VSTD::forward<_Fn>(__f), _CUDA_VSTD::forward_as_tuple(_CUDA_VSTD::forward<_Args>(__args)...)); }\n\n#endif // _LIBCUDACXX_STD_VER > 14\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FUNCTIONAL_BIND_BACK_H\n", "__functional/bind_front.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_BIND_FRONT_H\n#define _LIBCUDACXX___FUNCTIONAL_BIND_FRONT_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__functional/invoke.h\"\n#include \"../__functional/perfect_forward.h\"\n#include \"../__type_traits/decay.h\"\n#include \"../__type_traits/enable_if.h\"\n#include \"../__type_traits/is_constructible.h\"\n#include \"../__type_traits/is_move_constructible.h\"\n#include \"../__type_traits/is_nothrow_constructible.h\"\n#include \"../__utility/forward.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 14\n\nstruct __bind_front_op {\n    template <class ..._Args>\n    _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr auto operator()(_Args&& ...__args) const\n        noexcept(noexcept(_CUDA_VSTD::invoke(_CUDA_VSTD::forward<_Args>(__args)...)))\n        -> decltype(      _CUDA_VSTD::invoke(_CUDA_VSTD::forward<_Args>(__args)...))\n        { return          _CUDA_VSTD::invoke(_CUDA_VSTD::forward<_Args>(__args)...); }\n};\n\ntemplate <class _Fn, class ..._BoundArgs>\nstruct __bind_front_t : __perfect_forward<__bind_front_op, _Fn, _BoundArgs...> {\n    using __base = __perfect_forward<__bind_front_op, _Fn, _BoundArgs...>;\n#if defined(_LIBCUDACXX_COMPILER_NVRTC)\n    constexpr __bind_front_t() noexcept = default;\n\n    template<class... _Args>\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    __bind_front_t(_Args&&... __args) noexcept(noexcept(__base(_CUDA_VSTD::declval<_Args>()...)))\n        : __base(_CUDA_VSTD::forward<_Args>(__args)...)\n    {}\n#else\n    using __base::__base;\n#endif\n};\n\ntemplate<class _Fn, class... _Args>\n_LIBCUDACXX_CONCEPT __can_bind_front = is_constructible_v<decay_t<_Fn>, _Fn> &&\n                                       is_move_constructible_v<decay_t<_Fn>> &&\n                                       (is_constructible_v<decay_t<_Args>, _Args> && ...) &&\n                                       (is_move_constructible_v<decay_t<_Args>> && ... );\n\n_LIBCUDACXX_TEMPLATE(class _Fn, class... _Args)\n  (requires __can_bind_front<_Fn, _Args...>)\n_LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr auto bind_front(_Fn&& __f, _Args&&... __args) noexcept(is_nothrow_constructible_v<tuple<decay_t<_Args>...>, _Args&&...>) {\n    return __bind_front_t<decay_t<_Fn>, decay_t<_Args>...>(_CUDA_VSTD::forward<_Fn>(__f), _CUDA_VSTD::forward<_Args>(__args)...);\n}\n\n#endif // _LIBCUDACXX_STD_VER > 14\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FUNCTIONAL_BIND_FRONT_H\n", "__functional/binder1st.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_BINDER1ST_H\n#define _LIBCUDACXX___FUNCTIONAL_BINDER1ST_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__functional/unary_function.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER <= 14 || defined(_LIBCUDACXX_ENABLE_CXX17_REMOVED_BINDERS)\n\ntemplate <class __Operation>\nclass _LIBCUDACXX_TEMPLATE_VIS _LIBCUDACXX_DEPRECATED_IN_CXX11 binder1st\n    : public __unary_function<typename __Operation::second_argument_type, typename __Operation::result_type>\n{\nprotected:\n    __Operation                               op;\n    typename __Operation::first_argument_type value;\npublic:\n    _LIBCUDACXX_INLINE_VISIBILITY binder1st(const __Operation& __x,\n                               const typename __Operation::first_argument_type __y)\n        : op(__x), value(__y) {}\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_INLINE_VISIBILITY typename __Operation::result_type operator()\n        (typename __Operation::second_argument_type& __x) const\n            {return op(value, __x);}\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_INLINE_VISIBILITY typename __Operation::result_type operator()\n        (const typename __Operation::second_argument_type& __x) const\n            {return op(value, __x);}\n};\n\ntemplate <class __Operation, class _Tp>\n_LIBCUDACXX_DEPRECATED_IN_CXX11 inline _LIBCUDACXX_INLINE_VISIBILITY\nbinder1st<__Operation>\nbind1st(const __Operation& __op, const _Tp& __x)\n    {return binder1st<__Operation>(__op, __x);}\n\n#endif // _LIBCUDACXX_STD_VER <= 14 || defined(_LIBCUDACXX_ENABLE_CXX17_REMOVED_BINDERS)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FUNCTIONAL_BINDER1ST_H\n", "__functional/binder2nd.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_BINDER2ND_H\n#define _LIBCUDACXX___FUNCTIONAL_BINDER2ND_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__functional/unary_function.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER <= 14 || defined(_LIBCUDACXX_ENABLE_CXX17_REMOVED_BINDERS)\n\ntemplate <class __Operation>\nclass _LIBCUDACXX_TEMPLATE_VIS _LIBCUDACXX_DEPRECATED_IN_CXX11 binder2nd\n    : public __unary_function<typename __Operation::first_argument_type, typename __Operation::result_type>\n{\nprotected:\n    __Operation                                op;\n    typename __Operation::second_argument_type value;\npublic:\n    _LIBCUDACXX_INLINE_VISIBILITY\n    binder2nd(const __Operation& __x, const typename __Operation::second_argument_type __y)\n        : op(__x), value(__y) {}\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_INLINE_VISIBILITY typename __Operation::result_type operator()\n        (      typename __Operation::first_argument_type& __x) const\n            {return op(__x, value);}\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_INLINE_VISIBILITY typename __Operation::result_type operator()\n        (const typename __Operation::first_argument_type& __x) const\n            {return op(__x, value);}\n};\n\ntemplate <class __Operation, class _Tp>\n_LIBCUDACXX_DEPRECATED_IN_CXX11 inline _LIBCUDACXX_INLINE_VISIBILITY\nbinder2nd<__Operation>\nbind2nd(const __Operation& __op, const _Tp& __x)\n    {return binder2nd<__Operation>(__op, __x);}\n\n#endif // _LIBCUDACXX_STD_VER <= 14 || defined(_LIBCUDACXX_ENABLE_CXX17_REMOVED_BINDERS)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FUNCTIONAL_BINDER2ND_H\n", "__functional/compose.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_COMPOSE_H\n#define _LIBCUDACXX___FUNCTIONAL_COMPOSE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__functional/invoke.h\"\n#include \"../__functional/perfect_forward.h\"\n#include \"../__type_traits/decay.h\"\n#include \"../__utility/forward.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 14\n\nstruct __compose_op {\n    template<class _Fn1, class _Fn2, class ..._Args>\n    _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr auto operator()(_Fn1&& __f1, _Fn2&& __f2, _Args&&... __args) const\n        noexcept(noexcept(_CUDA_VSTD::invoke(_CUDA_VSTD::forward<_Fn1>(__f1), _CUDA_VSTD::invoke(_CUDA_VSTD::forward<_Fn2>(__f2), _CUDA_VSTD::forward<_Args>(__args)...))))\n        -> decltype(      _CUDA_VSTD::invoke(_CUDA_VSTD::forward<_Fn1>(__f1), _CUDA_VSTD::invoke(_CUDA_VSTD::forward<_Fn2>(__f2), _CUDA_VSTD::forward<_Args>(__args)...)))\n        { return          _CUDA_VSTD::invoke(_CUDA_VSTD::forward<_Fn1>(__f1), _CUDA_VSTD::invoke(_CUDA_VSTD::forward<_Fn2>(__f2), _CUDA_VSTD::forward<_Args>(__args)...)); }\n};\n\ntemplate <class _Fn1, class _Fn2>\nstruct __compose_t : __perfect_forward<__compose_op, _Fn1, _Fn2> {\n    using __perfect_forward<__compose_op, _Fn1, _Fn2>::__perfect_forward;\n};\n\ntemplate <class _Fn1, class _Fn2>\n_LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr auto __compose(_Fn1&& __f1, _Fn2&& __f2)\n    noexcept(noexcept(__compose_t<decay_t<_Fn1>, decay_t<_Fn2>>(_CUDA_VSTD::forward<_Fn1>(__f1), _CUDA_VSTD::forward<_Fn2>(__f2))))\n    -> decltype(      __compose_t<decay_t<_Fn1>, decay_t<_Fn2>>(_CUDA_VSTD::forward<_Fn1>(__f1), _CUDA_VSTD::forward<_Fn2>(__f2)))\n    { return          __compose_t<decay_t<_Fn1>, decay_t<_Fn2>>(_CUDA_VSTD::forward<_Fn1>(__f1), _CUDA_VSTD::forward<_Fn2>(__f2)); }\n\n#endif // _LIBCUDACXX_STD_VER > 14\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FUNCTIONAL_COMPOSE_H\n", "__functional/default_searcher.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_DEFAULT_SEARCHER_H\n#define _LIBCUDACXX___FUNCTIONAL_DEFAULT_SEARCHER_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n// #include \"../__algorithm/search.h\"\n#include \"../__functional/identity.h\"\n#include \"../__functional/operations.h\"\n#include \"../__iterator/iterator_traits.h\"\n#include \"../__utility/pair.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _BinaryPredicate, class _ForwardIterator1, class _ForwardIterator2>\n_LIBCUDACXX_HOST_DEVICE _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\npair<_ForwardIterator1, _ForwardIterator1>\n__search(_ForwardIterator1 __first1, _ForwardIterator1 __last1,\n         _ForwardIterator2 __first2, _ForwardIterator2 __last2, _BinaryPredicate __pred,\n         forward_iterator_tag, forward_iterator_tag)\n{\n    if (__first2 == __last2)\n        return _CUDA_VSTD::make_pair(__first1, __first1);  // Everything matches an empty sequence\n    while (true)\n    {\n        // Find first element in sequence 1 that matchs *__first2, with a mininum of loop checks\n        while (true)\n        {\n            if (__first1 == __last1)  // return __last1 if no element matches *__first2\n                return _CUDA_VSTD::make_pair(__last1, __last1);\n            if (__pred(*__first1, *__first2))\n                break;\n            ++__first1;\n        }\n        // *__first1 matches *__first2, now match elements after here\n        _ForwardIterator1 __m1 = __first1;\n        _ForwardIterator2 __m2 = __first2;\n        while (true)\n        {\n            if (++__m2 == __last2)  // If pattern exhausted, __first1 is the answer (works for 1 element pattern)\n                return _CUDA_VSTD::make_pair(__first1, __m1);\n            if (++__m1 == __last1)  // Otherwise if source exhaused, pattern not found\n                return _CUDA_VSTD::make_pair(__last1, __last1);\n            if (!__pred(*__m1, *__m2))  // if there is a mismatch, restart with a new __first1\n            {\n                ++__first1;\n                break;\n            }  // else there is a match, check next elements\n        }\n    }\n}\n\ntemplate <class _BinaryPredicate, class _RandomAccessIterator1, class _RandomAccessIterator2>\n_LIBCUDACXX_HOST_DEVICE _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\npair<_RandomAccessIterator1, _RandomAccessIterator1>\n__search(_RandomAccessIterator1 __first1, _RandomAccessIterator1 __last1,\n         _RandomAccessIterator2 __first2, _RandomAccessIterator2 __last2, _BinaryPredicate __pred,\n           random_access_iterator_tag, random_access_iterator_tag)\n{\n    typedef typename iterator_traits<_RandomAccessIterator1>::difference_type _Diff1;\n    typedef typename iterator_traits<_RandomAccessIterator2>::difference_type _Diff2;\n    // Take advantage of knowing source and pattern lengths.  Stop short when source is smaller than pattern\n    const _Diff2 __len2 = __last2 - __first2;\n    if (__len2 == 0)\n        return _CUDA_VSTD::make_pair(__first1, __first1);\n    const _Diff1 __len1 = __last1 - __first1;\n    if (__len1 < __len2)\n        return _CUDA_VSTD::make_pair(__last1, __last1);\n    const _RandomAccessIterator1 __s = __last1 - (__len2 - 1);  // Start of pattern match can't go beyond here\n\n    while (true)\n    {\n        while (true)\n        {\n            if (__first1 == __s)\n                return _CUDA_VSTD::make_pair(__last1, __last1);\n            if (__pred(*__first1, *__first2))\n                break;\n            ++__first1;\n        }\n\n        _RandomAccessIterator1 __m1 = __first1;\n        _RandomAccessIterator2 __m2 = __first2;\n         while (true)\n         {\n             if (++__m2 == __last2)\n                 return _CUDA_VSTD::make_pair(__first1, __first1 + __len2);\n             ++__m1;          // no need to check range on __m1 because __s guarantees we have enough source\n             if (!__pred(*__m1, *__m2))\n             {\n                 ++__first1;\n                 break;\n             }\n         }\n    }\n}\n\n#ifndef __cuda_std__\n\n#if _LIBCUDACXX_STD_VER > 14\n\n// default searcher\ntemplate<class _ForwardIterator, class _BinaryPredicate = equal_to<>>\nclass _LIBCUDACXX_TEMPLATE_VIS default_searcher {\npublic:\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n    default_searcher(_ForwardIterator __f, _ForwardIterator __l,\n                       _BinaryPredicate __p = _BinaryPredicate())\n        : __first_(__f), __last_(__l), __pred_(__p) {}\n\n    template <typename _ForwardIterator2>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n    pair<_ForwardIterator2, _ForwardIterator2>\n    operator () (_ForwardIterator2 __f, _ForwardIterator2 __l) const\n    {\n        return _CUDA_VSTD::__search(__f, __l, __first_, __last_, __pred_,\n            typename _CUDA_VSTD::iterator_traits<_ForwardIterator>::iterator_category(),\n            typename _CUDA_VSTD::iterator_traits<_ForwardIterator2>::iterator_category());\n    }\n\nprivate:\n    _ForwardIterator __first_;\n    _ForwardIterator __last_;\n    _BinaryPredicate __pred_;\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(default_searcher);\n\n#endif // _LIBCUDACXX_STD_VER > 14\n#endif // __cuda_std__\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FUNCTIONAL_DEFAULT_SEARCHER_H\n", "__functional/function.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_FUNCTION_H\n#define _LIBCUDACXX___FUNCTIONAL_FUNCTION_H\n\n#ifndef __cuda_std__\n\n#ifndef __cuda_std__\n#include <__config>\n#include <exception>\n#include <memory>\n#include <new>\n#include <typeinfo>\n#endif // __cuda_std__\n\n#include \"__assert\"\n#include \"../__debug\"\n#include \"../__functional_base\"\n#include \"../__functional/binary_function.h\"\n#include \"../__functional/invoke.h\"\n#include \"../__functional/unary_function.h\"\n#include \"../__iterator/iterator_traits.h\"\n#include \"../__type_traits/conditional.h\"\n#include \"../__type_traits/decay.h\"\n#include \"../__type_traits/enable_if.h\"\n#include \"../__type_traits/is_nothrow_copy_constructible.h\"\n#include \"../__type_traits/is_same.h\"\n#include \"../__type_traits/is_scalar.h\"\n#include \"../__type_traits/is_trivially_copy_constructible.h\"\n#include \"../__type_traits/is_trivially_destructible.h\"\n#include \"../__type_traits/is_void.h\"\n#include \"../__type_traits/remove_cvref.h\"\n#include \"../__utility/forward.h\"\n#include \"../__utility/move.h\"\n#include \"../__utility/piecewise_construct.h\"\n#include \"../__utility/swap.h\"\n#include \"../tuple\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n// bad_function_call\n\nclass _LIBCUDACXX_EXCEPTION_ABI bad_function_call\n    : public exception\n{\n#ifdef _LIBCUDACXX_ABI_BAD_FUNCTION_CALL_KEY_FUNCTION\npublic:\n    virtual ~bad_function_call() noexcept;\n\n    virtual const char* what() const noexcept;\n#endif\n};\n\n_LIBCUDACXX_NORETURN inline _LIBCUDACXX_INLINE_VISIBILITY\nvoid __throw_bad_function_call()\n{\n#ifndef _LIBCUDACXX_NO_EXCEPTIONS\n    throw bad_function_call();\n#else\n    _CUDA_VSTD::abort();\n#endif\n}\n\ntemplate<class _Fp> class _LIBCUDACXX_TEMPLATE_VIS function; // undefined\n\nnamespace __function\n{\n\ntemplate<class _Rp>\nstruct __maybe_derive_from_unary_function\n{\n};\n\ntemplate<class _Rp, class _A1>\nstruct __maybe_derive_from_unary_function<_Rp(_A1)>\n    : public __unary_function<_A1, _Rp>\n{\n};\n\ntemplate<class _Rp>\nstruct __maybe_derive_from_binary_function\n{\n};\n\ntemplate<class _Rp, class _A1, class _A2>\nstruct __maybe_derive_from_binary_function<_Rp(_A1, _A2)>\n    : public __binary_function<_A1, _A2, _Rp>\n{\n};\n\ntemplate <class _Fp>\n_LIBCUDACXX_INLINE_VISIBILITY\nbool __not_null(_Fp const&) { return true; }\n\ntemplate <class _Fp>\n_LIBCUDACXX_INLINE_VISIBILITY\nbool __not_null(_Fp* __ptr) { return __ptr; }\n\ntemplate <class _Ret, class _Class>\n_LIBCUDACXX_INLINE_VISIBILITY\nbool __not_null(_Ret _Class::*__ptr) { return __ptr; }\n\ntemplate <class _Fp>\n_LIBCUDACXX_INLINE_VISIBILITY\nbool __not_null(function<_Fp> const& __f) { return !!__f; }\n\n#ifdef _LIBCUDACXX_HAS_EXTENSION_BLOCKS\ntemplate <class _Rp, class ..._Args>\n_LIBCUDACXX_INLINE_VISIBILITY\nbool __not_null(_Rp (^__p)(_Args...)) { return __p; }\n#endif\n\n} // namespace __function\n\nnamespace __function {\n\n// __alloc_func holds a functor and an allocator.\n\ntemplate <class _Fp, class _Ap, class _FB> class __alloc_func;\ntemplate <class _Fp, class _FB>\nclass __default_alloc_func;\n\ntemplate <class _Fp, class _Ap, class _Rp, class... _ArgTypes>\nclass __alloc_func<_Fp, _Ap, _Rp(_ArgTypes...)>\n{\n    __compressed_pair<_Fp, _Ap> __f_;\n\n  public:\n    typedef _LIBCUDACXX_NODEBUG_TYPE _Fp _Target;\n    typedef _LIBCUDACXX_NODEBUG_TYPE _Ap _Alloc;\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    const _Target& __target() const { return __f_.first(); }\n\n    // WIN32 APIs may define __allocator, so use __get_allocator instead.\n    _LIBCUDACXX_INLINE_VISIBILITY\n    const _Alloc& __get_allocator() const { return __f_.second(); }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    explicit __alloc_func(_Target&& __f)\n        : __f_(piecewise_construct, _CUDA_VSTD::forward_as_tuple(_CUDA_VSTD::move(__f)),\n               _CUDA_VSTD::forward_as_tuple())\n    {\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    explicit __alloc_func(const _Target& __f, const _Alloc& __a)\n        : __f_(piecewise_construct, _CUDA_VSTD::forward_as_tuple(__f),\n               _CUDA_VSTD::forward_as_tuple(__a))\n    {\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    explicit __alloc_func(const _Target& __f, _Alloc&& __a)\n        : __f_(piecewise_construct, _CUDA_VSTD::forward_as_tuple(__f),\n               _CUDA_VSTD::forward_as_tuple(_CUDA_VSTD::move(__a)))\n    {\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    explicit __alloc_func(_Target&& __f, _Alloc&& __a)\n        : __f_(piecewise_construct, _CUDA_VSTD::forward_as_tuple(_CUDA_VSTD::move(__f)),\n               _CUDA_VSTD::forward_as_tuple(_CUDA_VSTD::move(__a)))\n    {\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Rp operator()(_ArgTypes&&... __arg)\n    {\n        typedef __invoke_void_return_wrapper<_Rp> _Invoker;\n        return _Invoker::__call(__f_.first(),\n                                _CUDA_VSTD::forward<_ArgTypes>(__arg)...);\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    __alloc_func* __clone() const\n    {\n        typedef allocator_traits<_Alloc> __alloc_traits;\n        typedef typename __rebind_alloc_helper<__alloc_traits, __alloc_func>::type _AA;\n        _AA __a(__f_.second());\n        typedef __allocator_destructor<_AA> _Dp;\n        unique_ptr<__alloc_func, _Dp> __hold(__a.allocate(1), _Dp(__a, 1));\n        ::new ((void*)__hold.get()) __alloc_func(__f_.first(), _Alloc(__a));\n        return __hold.release();\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    void destroy() noexcept { __f_.~__compressed_pair<_Target, _Alloc>(); }\n\n    static void __destroy_and_delete(__alloc_func* __f) {\n      typedef allocator_traits<_Alloc> __alloc_traits;\n      typedef typename __rebind_alloc_helper<__alloc_traits, __alloc_func>::type _FunAlloc;\n      _FunAlloc __a(__f->__get_allocator());\n      __f->destroy();\n      __a.deallocate(__f, 1);\n    }\n};\n\ntemplate <class _Fp, class _Rp, class... _ArgTypes>\nclass __default_alloc_func<_Fp, _Rp(_ArgTypes...)> {\n  _Fp __f_;\n\npublic:\n  typedef _LIBCUDACXX_NODEBUG_TYPE _Fp _Target;\n\n  _LIBCUDACXX_INLINE_VISIBILITY\n  const _Target& __target() const { return __f_; }\n\n  _LIBCUDACXX_INLINE_VISIBILITY\n  explicit __default_alloc_func(_Target&& __f) : __f_(_CUDA_VSTD::move(__f)) {}\n\n  _LIBCUDACXX_INLINE_VISIBILITY\n  explicit __default_alloc_func(const _Target& __f) : __f_(__f) {}\n\n  _LIBCUDACXX_INLINE_VISIBILITY\n  _Rp operator()(_ArgTypes&&... __arg) {\n    typedef __invoke_void_return_wrapper<_Rp> _Invoker;\n    return _Invoker::__call(__f_, _CUDA_VSTD::forward<_ArgTypes>(__arg)...);\n  }\n\n  _LIBCUDACXX_INLINE_VISIBILITY\n  __default_alloc_func* __clone() const {\n      __builtin_new_allocator::__holder_t __hold =\n        __builtin_new_allocator::__allocate_type<__default_alloc_func>(1);\n    __default_alloc_func* __res =\n        ::new ((void*)__hold.get()) __default_alloc_func(__f_);\n    (void)__hold.release();\n    return __res;\n  }\n\n  _LIBCUDACXX_INLINE_VISIBILITY\n  void destroy() noexcept { __f_.~_Target(); }\n\n  static void __destroy_and_delete(__default_alloc_func* __f) {\n    __f->destroy();\n      __builtin_new_allocator::__deallocate_type<__default_alloc_func>(__f, 1);\n  }\n};\n\n// __base provides an abstract interface for copyable functors.\n\ntemplate<class _Fp> class _LIBCUDACXX_TEMPLATE_VIS __base;\n\ntemplate<class _Rp, class ..._ArgTypes>\nclass __base<_Rp(_ArgTypes...)>\n{\n    __base(const __base&);\n    __base& operator=(const __base&);\npublic:\n    _LIBCUDACXX_INLINE_VISIBILITY __base() {}\n    _LIBCUDACXX_INLINE_VISIBILITY virtual ~__base() {}\n    virtual __base* __clone() const = 0;\n    virtual void __clone(__base*) const = 0;\n    virtual void destroy() noexcept = 0;\n    virtual void destroy_deallocate() noexcept = 0;\n    virtual _Rp operator()(_ArgTypes&& ...) = 0;\n#ifndef _LIBCUDACXX_NO_RTTI\n    virtual const void* target(const type_info&) const noexcept = 0;\n    virtual const type_info& target_type() const noexcept = 0;\n#endif // _LIBCUDACXX_NO_RTTI\n};\n\n// __func implements __base for a given functor type.\n\ntemplate<class _FD, class _Alloc, class _FB> class __func;\n\ntemplate<class _Fp, class _Alloc, class _Rp, class ..._ArgTypes>\nclass __func<_Fp, _Alloc, _Rp(_ArgTypes...)>\n    : public  __base<_Rp(_ArgTypes...)>\n{\n    __alloc_func<_Fp, _Alloc, _Rp(_ArgTypes...)> __f_;\npublic:\n    _LIBCUDACXX_INLINE_VISIBILITY\n    explicit __func(_Fp&& __f)\n        : __f_(_CUDA_VSTD::move(__f)) {}\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    explicit __func(const _Fp& __f, const _Alloc& __a)\n        : __f_(__f, __a) {}\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    explicit __func(const _Fp& __f, _Alloc&& __a)\n        : __f_(__f, _CUDA_VSTD::move(__a)) {}\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    explicit __func(_Fp&& __f, _Alloc&& __a)\n        : __f_(_CUDA_VSTD::move(__f), _CUDA_VSTD::move(__a)) {}\n\n    virtual __base<_Rp(_ArgTypes...)>* __clone() const;\n    virtual void __clone(__base<_Rp(_ArgTypes...)>*) const;\n    virtual void destroy() noexcept;\n    virtual void destroy_deallocate() noexcept;\n    virtual _Rp operator()(_ArgTypes&&... __arg);\n#ifndef _LIBCUDACXX_NO_RTTI\n    virtual const void* target(const type_info&) const noexcept;\n    virtual const type_info& target_type() const noexcept;\n#endif // _LIBCUDACXX_NO_RTTI\n};\n\ntemplate<class _Fp, class _Alloc, class _Rp, class ..._ArgTypes>\n__base<_Rp(_ArgTypes...)>*\n__func<_Fp, _Alloc, _Rp(_ArgTypes...)>::__clone() const\n{\n    typedef allocator_traits<_Alloc> __alloc_traits;\n    typedef typename __rebind_alloc_helper<__alloc_traits, __func>::type _Ap;\n    _Ap __a(__f_.__get_allocator());\n    typedef __allocator_destructor<_Ap> _Dp;\n    unique_ptr<__func, _Dp> __hold(__a.allocate(1), _Dp(__a, 1));\n    ::new ((void*)__hold.get()) __func(__f_.__target(), _Alloc(__a));\n    return __hold.release();\n}\n\ntemplate<class _Fp, class _Alloc, class _Rp, class ..._ArgTypes>\nvoid\n__func<_Fp, _Alloc, _Rp(_ArgTypes...)>::__clone(__base<_Rp(_ArgTypes...)>* __p) const\n{\n    ::new ((void*)__p) __func(__f_.__target(), __f_.__get_allocator());\n}\n\ntemplate<class _Fp, class _Alloc, class _Rp, class ..._ArgTypes>\nvoid\n__func<_Fp, _Alloc, _Rp(_ArgTypes...)>::destroy() noexcept\n{\n    __f_.destroy();\n}\n\ntemplate<class _Fp, class _Alloc, class _Rp, class ..._ArgTypes>\nvoid\n__func<_Fp, _Alloc, _Rp(_ArgTypes...)>::destroy_deallocate() noexcept\n{\n    typedef allocator_traits<_Alloc> __alloc_traits;\n    typedef typename __rebind_alloc_helper<__alloc_traits, __func>::type _Ap;\n    _Ap __a(__f_.__get_allocator());\n    __f_.destroy();\n    __a.deallocate(this, 1);\n}\n\ntemplate<class _Fp, class _Alloc, class _Rp, class ..._ArgTypes>\n_Rp\n__func<_Fp, _Alloc, _Rp(_ArgTypes...)>::operator()(_ArgTypes&& ... __arg)\n{\n    return __f_(_CUDA_VSTD::forward<_ArgTypes>(__arg)...);\n}\n\n#ifndef _LIBCUDACXX_NO_RTTI\n\ntemplate<class _Fp, class _Alloc, class _Rp, class ..._ArgTypes>\nconst void*\n__func<_Fp, _Alloc, _Rp(_ArgTypes...)>::target(const type_info& __ti) const noexcept\n{\n    if (__ti == typeid(_Fp))\n        return _CUDA_VSTD::addressof(__f_.__target());\n    return nullptr;\n}\n\ntemplate<class _Fp, class _Alloc, class _Rp, class ..._ArgTypes>\nconst type_info&\n__func<_Fp, _Alloc, _Rp(_ArgTypes...)>::target_type() const noexcept\n{\n    return typeid(_Fp);\n}\n\n#endif // _LIBCUDACXX_NO_RTTI\n\n// __value_func creates a value-type from a __func.\n\ntemplate <class _Fp> class __value_func;\n\ntemplate <class _Rp, class... _ArgTypes> class __value_func<_Rp(_ArgTypes...)>\n{\n    typename aligned_storage<3 * sizeof(void*)>::type __buf_;\n\n    typedef __base<_Rp(_ArgTypes...)> __func;\n    __func* __f_;\n\n    _LIBCUDACXX_NO_CFI static __func* __as_base(void* __p)\n    {\n        return reinterpret_cast<__func*>(__p);\n    }\n\n  public:\n    _LIBCUDACXX_INLINE_VISIBILITY\n    __value_func() noexcept : __f_(nullptr) {}\n\n    template <class _Fp, class _Alloc>\n    _LIBCUDACXX_INLINE_VISIBILITY __value_func(_Fp&& __f, const _Alloc& __a)\n        : __f_(nullptr)\n    {\n        typedef allocator_traits<_Alloc> __alloc_traits;\n        typedef __function::__func<_Fp, _Alloc, _Rp(_ArgTypes...)> _Fun;\n        typedef typename __rebind_alloc_helper<__alloc_traits, _Fun>::type _FunAlloc;\n\n        if (__function::__not_null(__f))\n        {\n            _FunAlloc __af(__a);\n            if (sizeof(_Fun) <= sizeof(__buf_) &&\n                is_nothrow_copy_constructible<_Fp>::value &&\n                is_nothrow_copy_constructible<_FunAlloc>::value)\n            {\n                __f_ =\n                    ::new ((void*)&__buf_) _Fun(_CUDA_VSTD::move(__f), _Alloc(__af));\n            }\n            else\n            {\n                typedef __allocator_destructor<_FunAlloc> _Dp;\n                unique_ptr<__func, _Dp> __hold(__af.allocate(1), _Dp(__af, 1));\n                ::new ((void*)__hold.get()) _Fun(_CUDA_VSTD::move(__f), _Alloc(__a));\n                __f_ = __hold.release();\n            }\n        }\n    }\n\n    template <class _Fp,\n        class = __enable_if_t<!is_same<__decay_t<_Fp>, __value_func>::value>>\n    _LIBCUDACXX_INLINE_VISIBILITY explicit __value_func(_Fp&& __f)\n        : __value_func(_CUDA_VSTD::forward<_Fp>(__f), allocator<_Fp>()) {}\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    __value_func(const __value_func& __f)\n    {\n        if (__f.__f_ == nullptr)\n            __f_ = nullptr;\n        else if ((void*)__f.__f_ == &__f.__buf_)\n        {\n            __f_ = __as_base(&__buf_);\n            __f.__f_->__clone(__f_);\n        }\n        else\n            __f_ = __f.__f_->__clone();\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    __value_func(__value_func&& __f) noexcept\n    {\n        if (__f.__f_ == nullptr)\n            __f_ = nullptr;\n        else if ((void*)__f.__f_ == &__f.__buf_)\n        {\n            __f_ = __as_base(&__buf_);\n            __f.__f_->__clone(__f_);\n        }\n        else\n        {\n            __f_ = __f.__f_;\n            __f.__f_ = nullptr;\n        }\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    ~__value_func()\n    {\n        if ((void*)__f_ == &__buf_)\n            __f_->destroy();\n        else if (__f_)\n            __f_->destroy_deallocate();\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    __value_func& operator=(__value_func&& __f)\n    {\n        *this = nullptr;\n        if (__f.__f_ == nullptr)\n            __f_ = nullptr;\n        else if ((void*)__f.__f_ == &__f.__buf_)\n        {\n            __f_ = __as_base(&__buf_);\n            __f.__f_->__clone(__f_);\n        }\n        else\n        {\n            __f_ = __f.__f_;\n            __f.__f_ = nullptr;\n        }\n        return *this;\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    __value_func& operator=(nullptr_t)\n    {\n        __func* __f = __f_;\n        __f_ = nullptr;\n        if ((void*)__f == &__buf_)\n            __f->destroy();\n        else if (__f)\n            __f->destroy_deallocate();\n        return *this;\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Rp operator()(_ArgTypes&&... __args) const\n    {\n        if (__f_ == nullptr)\n            __throw_bad_function_call();\n        return (*__f_)(_CUDA_VSTD::forward<_ArgTypes>(__args)...);\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    void swap(__value_func& __f) noexcept\n    {\n        if (&__f == this)\n            return;\n        if ((void*)__f_ == &__buf_ && (void*)__f.__f_ == &__f.__buf_)\n        {\n            typename aligned_storage<sizeof(__buf_)>::type __tempbuf;\n            __func* __t = __as_base(&__tempbuf);\n            __f_->__clone(__t);\n            __f_->destroy();\n            __f_ = nullptr;\n            __f.__f_->__clone(__as_base(&__buf_));\n            __f.__f_->destroy();\n            __f.__f_ = nullptr;\n            __f_ = __as_base(&__buf_);\n            __t->__clone(__as_base(&__f.__buf_));\n            __t->destroy();\n            __f.__f_ = __as_base(&__f.__buf_);\n        }\n        else if ((void*)__f_ == &__buf_)\n        {\n            __f_->__clone(__as_base(&__f.__buf_));\n            __f_->destroy();\n            __f_ = __f.__f_;\n            __f.__f_ = __as_base(&__f.__buf_);\n        }\n        else if ((void*)__f.__f_ == &__f.__buf_)\n        {\n            __f.__f_->__clone(__as_base(&__buf_));\n            __f.__f_->destroy();\n            __f.__f_ = __f_;\n            __f_ = __as_base(&__buf_);\n        }\n        else\n            _CUDA_VSTD::swap(__f_, __f.__f_);\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _LIBCUDACXX_EXPLICIT operator bool() const noexcept { return __f_ != nullptr; }\n\n#ifndef _LIBCUDACXX_NO_RTTI\n    _LIBCUDACXX_INLINE_VISIBILITY\n    const type_info& target_type() const noexcept\n    {\n        if (__f_ == nullptr)\n            return typeid(void);\n        return __f_->target_type();\n    }\n\n    template <typename _Tp>\n    _LIBCUDACXX_INLINE_VISIBILITY const _Tp* target() const noexcept\n    {\n        if (__f_ == nullptr)\n            return nullptr;\n        return (const _Tp*)__f_->target(typeid(_Tp));\n    }\n#endif // _LIBCUDACXX_NO_RTTI\n};\n\n// Storage for a functor object, to be used with __policy to manage copy and\n// destruction.\nunion __policy_storage\n{\n    mutable char __small[sizeof(void*) * 2];\n    void* __large;\n};\n\n// True if _Fun can safely be held in __policy_storage.__small.\ntemplate <typename _Fun>\nstruct __use_small_storage\n    : public integral_constant<\n          bool, sizeof(_Fun) <= sizeof(__policy_storage) &&\n                    _LIBCUDACXX_ALIGNOF(_Fun) <= _LIBCUDACXX_ALIGNOF(__policy_storage) &&\n                    is_trivially_copy_constructible<_Fun>::value &&\n                    is_trivially_destructible<_Fun>::value> {};\n\n// Policy contains information about how to copy, destroy, and move the\n// underlying functor. You can think of it as a vtable of sorts.\nstruct __policy\n{\n    // Used to copy or destroy __large values. null for trivial objects.\n    void* (*const __clone)(const void*);\n    void (*const __destroy)(void*);\n\n    // True if this is the null policy (no value).\n    const bool __is_null;\n\n    // The target type. May be null if RTTI is disabled.\n    const type_info* const __type_info;\n\n    // Returns a pointer to a static policy object suitable for the functor\n    // type.\n    template <typename _Fun>\n    _LIBCUDACXX_INLINE_VISIBILITY static const __policy* __create()\n    {\n        return __choose_policy<_Fun>(__use_small_storage<_Fun>());\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    static const __policy* __create_empty()\n    {\n        static const constexpr __policy __policy_ = {nullptr, nullptr,\n                                                             true,\n#ifndef _LIBCUDACXX_NO_RTTI\n                                                             &typeid(void)\n#else\n                                                             nullptr\n#endif\n        };\n        return &__policy_;\n    }\n\n  private:\n    template <typename _Fun> static void* __large_clone(const void* __s)\n    {\n        const _Fun* __f = static_cast<const _Fun*>(__s);\n        return __f->__clone();\n    }\n\n    template <typename _Fun>\n    static void __large_destroy(void* __s) {\n      _Fun::__destroy_and_delete(static_cast<_Fun*>(__s));\n    }\n\n    template <typename _Fun>\n    _LIBCUDACXX_INLINE_VISIBILITY static const __policy*\n    __choose_policy(/* is_small = */ false_type) {\n      static const constexpr __policy __policy_ = {\n          &__large_clone<_Fun>, &__large_destroy<_Fun>, false,\n#ifndef _LIBCUDACXX_NO_RTTI\n          &typeid(typename _Fun::_Target)\n#else\n          nullptr\n#endif\n      };\n        return &__policy_;\n    }\n\n    template <typename _Fun>\n    _LIBCUDACXX_INLINE_VISIBILITY static const __policy*\n        __choose_policy(/* is_small = */ true_type)\n    {\n        static const constexpr __policy __policy_ = {\n            nullptr, nullptr, false,\n#ifndef _LIBCUDACXX_NO_RTTI\n            &typeid(typename _Fun::_Target)\n#else\n            nullptr\n#endif\n        };\n        return &__policy_;\n    }\n};\n\n// Used to choose between perfect forwarding or pass-by-value. Pass-by-value is\n// faster for types that can be passed in registers.\ntemplate <typename _Tp>\nusing __fast_forward = __conditional_t<is_scalar<_Tp>::value, _Tp, _Tp&&>;\n\n// __policy_invoker calls an instance of __alloc_func held in __policy_storage.\n\ntemplate <class _Fp> struct __policy_invoker;\n\ntemplate <class _Rp, class... _ArgTypes>\nstruct __policy_invoker<_Rp(_ArgTypes...)>\n{\n    typedef _Rp (*__Call)(const __policy_storage*,\n                          __fast_forward<_ArgTypes>...);\n\n    __Call __call_;\n\n    // Creates an invoker that throws bad_function_call.\n    _LIBCUDACXX_INLINE_VISIBILITY\n    __policy_invoker() : __call_(&__call_empty) {}\n\n    // Creates an invoker that calls the given instance of __func.\n    template <typename _Fun>\n    _LIBCUDACXX_INLINE_VISIBILITY static __policy_invoker __create()\n    {\n        return __policy_invoker(&__call_impl<_Fun>);\n    }\n\n  private:\n    _LIBCUDACXX_INLINE_VISIBILITY\n    explicit __policy_invoker(__Call __c) : __call_(__c) {}\n\n    static _Rp __call_empty(const __policy_storage*,\n                            __fast_forward<_ArgTypes>...)\n    {\n        __throw_bad_function_call();\n    }\n\n    template <typename _Fun>\n    static _Rp __call_impl(const __policy_storage* __buf,\n                           __fast_forward<_ArgTypes>... __args)\n    {\n        _Fun* __f = reinterpret_cast<_Fun*>(__use_small_storage<_Fun>::value\n                                                ? &__buf->__small\n                                                : __buf->__large);\n        return (*__f)(_CUDA_VSTD::forward<_ArgTypes>(__args)...);\n    }\n};\n\n// __policy_func uses a __policy and __policy_invoker to create a type-erased,\n// copyable functor.\n\ntemplate <class _Fp> class __policy_func;\n\ntemplate <class _Rp, class... _ArgTypes> class __policy_func<_Rp(_ArgTypes...)>\n{\n    // Inline storage for small objects.\n    __policy_storage __buf_;\n\n    // Calls the value stored in __buf_. This could technically be part of\n    // policy, but storing it here eliminates a level of indirection inside\n    // operator().\n    typedef __function::__policy_invoker<_Rp(_ArgTypes...)> __invoker;\n    __invoker __invoker_;\n\n    // The policy that describes how to move / copy / destroy __buf_. Never\n    // null, even if the function is empty.\n    const __policy* __policy_;\n\n  public:\n    _LIBCUDACXX_INLINE_VISIBILITY\n    __policy_func() : __policy_(__policy::__create_empty()) {}\n\n    template <class _Fp, class _Alloc>\n    _LIBCUDACXX_INLINE_VISIBILITY __policy_func(_Fp&& __f, const _Alloc& __a)\n        : __policy_(__policy::__create_empty())\n    {\n        typedef __alloc_func<_Fp, _Alloc, _Rp(_ArgTypes...)> _Fun;\n        typedef allocator_traits<_Alloc> __alloc_traits;\n        typedef typename __rebind_alloc_helper<__alloc_traits, _Fun>::type _FunAlloc;\n\n        if (__function::__not_null(__f))\n        {\n            __invoker_ = __invoker::template __create<_Fun>();\n            __policy_ = __policy::__create<_Fun>();\n\n            _FunAlloc __af(__a);\n            if (__use_small_storage<_Fun>())\n            {\n                ::new ((void*)&__buf_.__small)\n                    _Fun(_CUDA_VSTD::move(__f), _Alloc(__af));\n            }\n            else\n            {\n                typedef __allocator_destructor<_FunAlloc> _Dp;\n                unique_ptr<_Fun, _Dp> __hold(__af.allocate(1), _Dp(__af, 1));\n                ::new ((void*)__hold.get())\n                    _Fun(_CUDA_VSTD::move(__f), _Alloc(__af));\n                __buf_.__large = __hold.release();\n            }\n        }\n    }\n\n    template <class _Fp, class = __enable_if_t<!is_same<__decay_t<_Fp>, __policy_func>::value>>\n    _LIBCUDACXX_INLINE_VISIBILITY explicit __policy_func(_Fp&& __f)\n        : __policy_(__policy::__create_empty()) {\n      typedef __default_alloc_func<_Fp, _Rp(_ArgTypes...)> _Fun;\n\n      if (__function::__not_null(__f)) {\n        __invoker_ = __invoker::template __create<_Fun>();\n        __policy_ = __policy::__create<_Fun>();\n        if (__use_small_storage<_Fun>()) {\n          ::new ((void*)&__buf_.__small) _Fun(_CUDA_VSTD::move(__f));\n        } else {\n          __builtin_new_allocator::__holder_t __hold =\n              __builtin_new_allocator::__allocate_type<_Fun>(1);\n          __buf_.__large = ::new ((void*)__hold.get()) _Fun(_CUDA_VSTD::move(__f));\n          (void)__hold.release();\n        }\n      }\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    __policy_func(const __policy_func& __f)\n        : __buf_(__f.__buf_), __invoker_(__f.__invoker_),\n          __policy_(__f.__policy_)\n    {\n        if (__policy_->__clone)\n            __buf_.__large = __policy_->__clone(__f.__buf_.__large);\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    __policy_func(__policy_func&& __f)\n        : __buf_(__f.__buf_), __invoker_(__f.__invoker_),\n          __policy_(__f.__policy_)\n    {\n        if (__policy_->__destroy)\n        {\n            __f.__policy_ = __policy::__create_empty();\n            __f.__invoker_ = __invoker();\n        }\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    ~__policy_func()\n    {\n        if (__policy_->__destroy)\n            __policy_->__destroy(__buf_.__large);\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    __policy_func& operator=(__policy_func&& __f)\n    {\n        *this = nullptr;\n        __buf_ = __f.__buf_;\n        __invoker_ = __f.__invoker_;\n        __policy_ = __f.__policy_;\n        __f.__policy_ = __policy::__create_empty();\n        __f.__invoker_ = __invoker();\n        return *this;\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    __policy_func& operator=(nullptr_t)\n    {\n        const __policy* __p = __policy_;\n        __policy_ = __policy::__create_empty();\n        __invoker_ = __invoker();\n        if (__p->__destroy)\n            __p->__destroy(__buf_.__large);\n        return *this;\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Rp operator()(_ArgTypes&&... __args) const\n    {\n        return __invoker_.__call_(_CUDA_VSTD::addressof(__buf_),\n                                  _CUDA_VSTD::forward<_ArgTypes>(__args)...);\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    void swap(__policy_func& __f)\n    {\n        _CUDA_VSTD::swap(__invoker_, __f.__invoker_);\n        _CUDA_VSTD::swap(__policy_, __f.__policy_);\n        _CUDA_VSTD::swap(__buf_, __f.__buf_);\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    explicit operator bool() const noexcept\n    {\n        return !__policy_->__is_null;\n    }\n\n#ifndef _LIBCUDACXX_NO_RTTI\n    _LIBCUDACXX_INLINE_VISIBILITY\n    const type_info& target_type() const noexcept\n    {\n        return *__policy_->__type_info;\n    }\n\n    template <typename _Tp>\n    _LIBCUDACXX_INLINE_VISIBILITY const _Tp* target() const noexcept\n    {\n        if (__policy_->__is_null || typeid(_Tp) != *__policy_->__type_info)\n            return nullptr;\n        if (__policy_->__clone) // Out of line storage.\n            return reinterpret_cast<const _Tp*>(__buf_.__large);\n        else\n            return reinterpret_cast<const _Tp*>(&__buf_.__small);\n    }\n#endif // _LIBCUDACXX_NO_RTTI\n};\n\n#if defined(_LIBCUDACXX_HAS_BLOCKS_RUNTIME)\n\nextern \"C\" void *_Block_copy(const void *);\nextern \"C\" void _Block_release(const void *);\n\ntemplate<class _Rp1, class ..._ArgTypes1, class _Alloc, class _Rp, class ..._ArgTypes>\nclass __func<_Rp1(^)(_ArgTypes1...), _Alloc, _Rp(_ArgTypes...)>\n    : public  __base<_Rp(_ArgTypes...)>\n{\n    typedef _Rp1(^__block_type)(_ArgTypes1...);\n    __block_type __f_;\n\npublic:\n    _LIBCUDACXX_INLINE_VISIBILITY\n    explicit __func(__block_type const& __f)\n#ifdef _LIBCUDACXX_HAS_OBJC_ARC\n        : __f_(__f)\n#else\n        : __f_(reinterpret_cast<__block_type>(__f ? _Block_copy(__f) : nullptr))\n#endif\n    { }\n\n    // [TODO] add && to save on a retain\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    explicit __func(__block_type __f, const _Alloc& /* unused */)\n#ifdef _LIBCUDACXX_HAS_OBJC_ARC\n        : __f_(__f)\n#else\n        : __f_(reinterpret_cast<__block_type>(__f ? _Block_copy(__f) : nullptr))\n#endif\n    { }\n\n    virtual __base<_Rp(_ArgTypes...)>* __clone() const {\n        _LIBCUDACXX_ASSERT(false,\n            \"Block pointers are just pointers, so they should always fit into \"\n            \"std::function's small buffer optimization. This function should \"\n            \"never be invoked.\");\n        return nullptr;\n    }\n\n    virtual void __clone(__base<_Rp(_ArgTypes...)>* __p) const {\n        ::new ((void*)__p) __func(__f_);\n    }\n\n    virtual void destroy() noexcept {\n#ifndef _LIBCUDACXX_HAS_OBJC_ARC\n        if (__f_)\n            _Block_release(__f_);\n#endif\n        __f_ = 0;\n    }\n\n    virtual void destroy_deallocate() noexcept {\n        _LIBCUDACXX_ASSERT(false,\n            \"Block pointers are just pointers, so they should always fit into \"\n            \"std::function's small buffer optimization. This function should \"\n            \"never be invoked.\");\n    }\n\n    virtual _Rp operator()(_ArgTypes&& ... __arg) {\n        return _CUDA_VSTD::__invoke(__f_, _CUDA_VSTD::forward<_ArgTypes>(__arg)...);\n    }\n\n#ifndef _LIBCUDACXX_NO_RTTI\n    virtual const void* target(type_info const& __ti) const noexcept {\n        if (__ti == typeid(__func::__block_type))\n            return &__f_;\n        return (const void*)nullptr;\n    }\n\n    virtual const type_info& target_type() const noexcept {\n        return typeid(__func::__block_type);\n    }\n#endif // _LIBCUDACXX_NO_RTTI\n};\n\n#endif // _LIBCUDACXX_HAS_EXTENSION_BLOCKS\n\n} // namespace __function\n\ntemplate<class _Rp, class ..._ArgTypes>\nclass _LIBCUDACXX_TEMPLATE_VIS function<_Rp(_ArgTypes...)>\n    : public __function::__maybe_derive_from_unary_function<_Rp(_ArgTypes...)>,\n      public __function::__maybe_derive_from_binary_function<_Rp(_ArgTypes...)>\n{\n#ifndef _LIBCUDACXX_ABI_OPTIMIZED_FUNCTION\n    typedef __function::__value_func<_Rp(_ArgTypes...)> __func;\n#else\n    typedef __function::__policy_func<_Rp(_ArgTypes...)> __func;\n#endif\n\n    __func __f_;\n\n    template <class _Fp, bool = _And<\n        _IsNotSame<__remove_cvref_t<_Fp>, function>,\n        __invokable<_Fp, _ArgTypes...>\n    >::value>\n    struct __callable;\n    template <class _Fp>\n        struct __callable<_Fp, true>\n        {\n            static const bool value = is_void<_Rp>::value ||\n                __is_core_convertible<typename __invoke_of<_Fp, _ArgTypes...>::type,\n                                      _Rp>::value;\n        };\n    template <class _Fp>\n        struct __callable<_Fp, false>\n        {\n            static const bool value = false;\n        };\n\n  template <class _Fp>\n  using _EnableIfLValueCallable = __enable_if_t<__callable<_Fp&>::value>;\npublic:\n    typedef _Rp result_type;\n\n    // construct/copy/destroy:\n    _LIBCUDACXX_INLINE_VISIBILITY\n    function() noexcept { }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    function(nullptr_t) noexcept {}\n    function(const function&);\n    function(function&&) noexcept;\n    template<class _Fp, class = _EnableIfLValueCallable<_Fp>>\n    function(_Fp);\n\n#if _LIBCUDACXX_STD_VER <= 14\n    template<class _Alloc>\n      _LIBCUDACXX_INLINE_VISIBILITY\n      function(allocator_arg_t, const _Alloc&) noexcept {}\n    template<class _Alloc>\n      _LIBCUDACXX_INLINE_VISIBILITY\n      function(allocator_arg_t, const _Alloc&, nullptr_t) noexcept {}\n    template<class _Alloc>\n      function(allocator_arg_t, const _Alloc&, const function&);\n    template<class _Alloc>\n      function(allocator_arg_t, const _Alloc&, function&&);\n    template<class _Fp, class _Alloc, class = _EnableIfLValueCallable<_Fp>>\n      function(allocator_arg_t, const _Alloc& __a, _Fp __f);\n#endif\n\n    function& operator=(const function&);\n    function& operator=(function&&) noexcept;\n    function& operator=(nullptr_t) noexcept;\n    template<class _Fp, class = _EnableIfLValueCallable<__decay_t<_Fp>>>\n    function& operator=(_Fp&&);\n\n    ~function();\n\n    // function modifiers:\n    void swap(function&) noexcept;\n\n#if _LIBCUDACXX_STD_VER <= 14\n    template<class _Fp, class _Alloc>\n      _LIBCUDACXX_INLINE_VISIBILITY\n      void assign(_Fp&& __f, const _Alloc& __a)\n        {function(allocator_arg, __a, _CUDA_VSTD::forward<_Fp>(__f)).swap(*this);}\n#endif\n\n    // function capacity:\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _LIBCUDACXX_EXPLICIT operator bool() const noexcept {\n      return static_cast<bool>(__f_);\n    }\n\n    // deleted overloads close possible hole in the type system\n    template<class _R2, class... _ArgTypes2>\n      bool operator==(const function<_R2(_ArgTypes2...)>&) const = delete;\n    template<class _R2, class... _ArgTypes2>\n      bool operator!=(const function<_R2(_ArgTypes2...)>&) const = delete;\npublic:\n    // function invocation:\n    _Rp operator()(_ArgTypes...) const;\n\n#ifndef _LIBCUDACXX_NO_RTTI\n    // function target access:\n    const type_info& target_type() const noexcept;\n    template <typename _Tp> _Tp* target() noexcept;\n    template <typename _Tp> const _Tp* target() const noexcept;\n#endif // _LIBCUDACXX_NO_RTTI\n};\n\n#if _LIBCUDACXX_STD_VER > 14\ntemplate<class _Rp, class ..._Ap>\nfunction(_Rp(*)(_Ap...)) -> function<_Rp(_Ap...)>;\n\ntemplate<class _Fp>\nstruct __strip_signature;\n\ntemplate<class _Rp, class _Gp, class ..._Ap>\nstruct __strip_signature<_Rp (_Gp::*) (_Ap...)> { using type = _Rp(_Ap...); };\ntemplate<class _Rp, class _Gp, class ..._Ap>\nstruct __strip_signature<_Rp (_Gp::*) (_Ap...) const> { using type = _Rp(_Ap...); };\ntemplate<class _Rp, class _Gp, class ..._Ap>\nstruct __strip_signature<_Rp (_Gp::*) (_Ap...) volatile> { using type = _Rp(_Ap...); };\ntemplate<class _Rp, class _Gp, class ..._Ap>\nstruct __strip_signature<_Rp (_Gp::*) (_Ap...) const volatile> { using type = _Rp(_Ap...); };\n\ntemplate<class _Rp, class _Gp, class ..._Ap>\nstruct __strip_signature<_Rp (_Gp::*) (_Ap...) &> { using type = _Rp(_Ap...); };\ntemplate<class _Rp, class _Gp, class ..._Ap>\nstruct __strip_signature<_Rp (_Gp::*) (_Ap...) const &> { using type = _Rp(_Ap...); };\ntemplate<class _Rp, class _Gp, class ..._Ap>\nstruct __strip_signature<_Rp (_Gp::*) (_Ap...) volatile &> { using type = _Rp(_Ap...); };\ntemplate<class _Rp, class _Gp, class ..._Ap>\nstruct __strip_signature<_Rp (_Gp::*) (_Ap...) const volatile &> { using type = _Rp(_Ap...); };\n\ntemplate<class _Rp, class _Gp, class ..._Ap>\nstruct __strip_signature<_Rp (_Gp::*) (_Ap...) noexcept> { using type = _Rp(_Ap...); };\ntemplate<class _Rp, class _Gp, class ..._Ap>\nstruct __strip_signature<_Rp (_Gp::*) (_Ap...) const noexcept> { using type = _Rp(_Ap...); };\ntemplate<class _Rp, class _Gp, class ..._Ap>\nstruct __strip_signature<_Rp (_Gp::*) (_Ap...) volatile noexcept> { using type = _Rp(_Ap...); };\ntemplate<class _Rp, class _Gp, class ..._Ap>\nstruct __strip_signature<_Rp (_Gp::*) (_Ap...) const volatile noexcept> { using type = _Rp(_Ap...); };\n\ntemplate<class _Rp, class _Gp, class ..._Ap>\nstruct __strip_signature<_Rp (_Gp::*) (_Ap...) & noexcept> { using type = _Rp(_Ap...); };\ntemplate<class _Rp, class _Gp, class ..._Ap>\nstruct __strip_signature<_Rp (_Gp::*) (_Ap...) const & noexcept> { using type = _Rp(_Ap...); };\ntemplate<class _Rp, class _Gp, class ..._Ap>\nstruct __strip_signature<_Rp (_Gp::*) (_Ap...) volatile & noexcept> { using type = _Rp(_Ap...); };\ntemplate<class _Rp, class _Gp, class ..._Ap>\nstruct __strip_signature<_Rp (_Gp::*) (_Ap...) const volatile & noexcept> { using type = _Rp(_Ap...); };\n\ntemplate<class _Fp, class _Stripped = typename __strip_signature<decltype(&_Fp::operator())>::type>\nfunction(_Fp) -> function<_Stripped>;\n#endif // _LIBCUDACXX_STD_VER > 14\n\ntemplate<class _Rp, class ..._ArgTypes>\nfunction<_Rp(_ArgTypes...)>::function(const function& __f) : __f_(__f.__f_) {}\n\n#if _LIBCUDACXX_STD_VER <= 14\ntemplate<class _Rp, class ..._ArgTypes>\ntemplate <class _Alloc>\nfunction<_Rp(_ArgTypes...)>::function(allocator_arg_t, const _Alloc&,\n                                     const function& __f) : __f_(__f.__f_) {}\n#endif\n\ntemplate <class _Rp, class... _ArgTypes>\nfunction<_Rp(_ArgTypes...)>::function(function&& __f) noexcept\n    : __f_(_CUDA_VSTD::move(__f.__f_)) {}\n\n#if _LIBCUDACXX_STD_VER <= 14\ntemplate<class _Rp, class ..._ArgTypes>\ntemplate <class _Alloc>\nfunction<_Rp(_ArgTypes...)>::function(allocator_arg_t, const _Alloc&,\n                                      function&& __f)\n    : __f_(_CUDA_VSTD::move(__f.__f_)) {}\n#endif\n\ntemplate <class _Rp, class... _ArgTypes>\ntemplate <class _Fp, class>\nfunction<_Rp(_ArgTypes...)>::function(_Fp __f) : __f_(_CUDA_VSTD::move(__f)) {}\n\n#if _LIBCUDACXX_STD_VER <= 14\ntemplate <class _Rp, class... _ArgTypes>\ntemplate <class _Fp, class _Alloc, class>\nfunction<_Rp(_ArgTypes...)>::function(allocator_arg_t, const _Alloc& __a,\n                                      _Fp __f)\n    : __f_(_CUDA_VSTD::move(__f), __a) {}\n#endif\n\ntemplate<class _Rp, class ..._ArgTypes>\nfunction<_Rp(_ArgTypes...)>&\nfunction<_Rp(_ArgTypes...)>::operator=(const function& __f)\n{\n    function(__f).swap(*this);\n    return *this;\n}\n\ntemplate<class _Rp, class ..._ArgTypes>\nfunction<_Rp(_ArgTypes...)>&\nfunction<_Rp(_ArgTypes...)>::operator=(function&& __f) noexcept\n{\n    __f_ = _CUDA_VSTD::move(__f.__f_);\n    return *this;\n}\n\ntemplate<class _Rp, class ..._ArgTypes>\nfunction<_Rp(_ArgTypes...)>&\nfunction<_Rp(_ArgTypes...)>::operator=(nullptr_t) noexcept\n{\n    __f_ = nullptr;\n    return *this;\n}\n\ntemplate<class _Rp, class ..._ArgTypes>\ntemplate <class _Fp, class>\nfunction<_Rp(_ArgTypes...)>&\nfunction<_Rp(_ArgTypes...)>::operator=(_Fp&& __f)\n{\n    function(_CUDA_VSTD::forward<_Fp>(__f)).swap(*this);\n    return *this;\n}\n\ntemplate<class _Rp, class ..._ArgTypes>\nfunction<_Rp(_ArgTypes...)>::~function() {}\n\ntemplate<class _Rp, class ..._ArgTypes>\nvoid\nfunction<_Rp(_ArgTypes...)>::swap(function& __f) noexcept\n{\n    __f_.swap(__f.__f_);\n}\n\ntemplate<class _Rp, class ..._ArgTypes>\n_Rp\nfunction<_Rp(_ArgTypes...)>::operator()(_ArgTypes... __arg) const\n{\n    return __f_(_CUDA_VSTD::forward<_ArgTypes>(__arg)...);\n}\n\n#ifndef _LIBCUDACXX_NO_RTTI\n\ntemplate<class _Rp, class ..._ArgTypes>\nconst type_info&\nfunction<_Rp(_ArgTypes...)>::target_type() const noexcept\n{\n    return __f_.target_type();\n}\n\ntemplate<class _Rp, class ..._ArgTypes>\ntemplate <typename _Tp>\n_Tp*\nfunction<_Rp(_ArgTypes...)>::target() noexcept\n{\n    return (_Tp*)(__f_.template target<_Tp>());\n}\n\ntemplate<class _Rp, class ..._ArgTypes>\ntemplate <typename _Tp>\nconst _Tp*\nfunction<_Rp(_ArgTypes...)>::target() const noexcept\n{\n    return __f_.template target<_Tp>();\n}\n\n#endif // _LIBCUDACXX_NO_RTTI\n\ntemplate <class _Rp, class... _ArgTypes>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nbool\noperator==(const function<_Rp(_ArgTypes...)>& __f, nullptr_t) noexcept {return !__f;}\n\ntemplate <class _Rp, class... _ArgTypes>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nbool\noperator==(nullptr_t, const function<_Rp(_ArgTypes...)>& __f) noexcept {return !__f;}\n\ntemplate <class _Rp, class... _ArgTypes>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nbool\noperator!=(const function<_Rp(_ArgTypes...)>& __f, nullptr_t) noexcept {return (bool)__f;}\n\ntemplate <class _Rp, class... _ArgTypes>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nbool\noperator!=(nullptr_t, const function<_Rp(_ArgTypes...)>& __f) noexcept {return (bool)__f;}\n\ntemplate <class _Rp, class... _ArgTypes>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nvoid\nswap(function<_Rp(_ArgTypes...)>& __x, function<_Rp(_ArgTypes...)>& __y) noexcept\n{return __x.swap(__y);}\n\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // __cuda_std__\n\n#endif // _LIBCUDACXX___FUNCTIONAL_FUNCTION_H\n", "__functional/hash.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_HASH_H\n#define _LIBCUDACXX___FUNCTIONAL_HASH_H\n\n#ifndef __cuda_std__\n#include <__config>\n#include <cstring>\n#endif // __cuda_std__\n\n#include \"../__functional/invoke.h\"\n#include \"../__functional/unary_function.h\"\n#include \"../__fwd/hash.h\"\n#include \"../__type_traits/enable_if.h\"\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_enum.h\"\n#include \"../__type_traits/is_copy_constructible.h\"\n#include \"../__type_traits/is_default_constructible.h\"\n#include \"../__type_traits/is_move_constructible.h\"\n#include \"../__type_traits/underlying_type.h\"\n#include \"../__utility/forward.h\"\n#include \"../__utility/move.h\"\n#include \"../__utility/pair.h\"\n#include \"../__utility/swap.h\"\n#include \"../cstdint\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n#ifndef __cuda_std__\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Size>\ninline _LIBCUDACXX_INLINE_VISIBILITY\n_Size\n__loadword(const void* __p)\n{\n    _Size __r;\n    std::memcpy(&__r, __p, sizeof(__r));\n    return __r;\n}\n\n// We use murmur2 when size_t is 32 bits, and cityhash64 when size_t\n// is 64 bits.  This is because cityhash64 uses 64bit x 64bit\n// multiplication, which can be very slow on 32-bit systems.\ntemplate <class _Size, size_t = sizeof(_Size)*__CHAR_BIT__>\nstruct __murmur2_or_cityhash;\n\ntemplate <class _Size>\nstruct __murmur2_or_cityhash<_Size, 32>\n{\n    inline _Size operator()(const void* __key, _Size __len)\n         _LIBCUDACXX_DISABLE_UBSAN_UNSIGNED_INTEGER_CHECK;\n};\n\n// murmur2\ntemplate <class _Size>\n_Size\n__murmur2_or_cityhash<_Size, 32>::operator()(const void* __key, _Size __len)\n{\n    const _Size __m = 0x5bd1e995;\n    const _Size __r = 24;\n    _Size __h = __len;\n    const unsigned char* __data = static_cast<const unsigned char*>(__key);\n    for (; __len >= 4; __data += 4, __len -= 4)\n    {\n        _Size __k = __loadword<_Size>(__data);\n        __k *= __m;\n        __k ^= __k >> __r;\n        __k *= __m;\n        __h *= __m;\n        __h ^= __k;\n    }\n    switch (__len)\n    {\n    case 3:\n        __h ^= static_cast<_Size>(__data[2] << 16);\n        _LIBCUDACXX_FALLTHROUGH();\n    case 2:\n        __h ^= static_cast<_Size>(__data[1] << 8);\n        _LIBCUDACXX_FALLTHROUGH();\n    case 1:\n        __h ^= __data[0];\n        __h *= __m;\n    }\n    __h ^= __h >> 13;\n    __h *= __m;\n    __h ^= __h >> 15;\n    return __h;\n}\n\ntemplate <class _Size>\nstruct __murmur2_or_cityhash<_Size, 64>\n{\n    inline _Size operator()(const void* __key, _Size __len)  _LIBCUDACXX_DISABLE_UBSAN_UNSIGNED_INTEGER_CHECK;\n\n private:\n  // Some primes between 2^63 and 2^64.\n  static const _Size __k0 = 0xc3a5c85c97cb3127ULL;\n  static const _Size __k1 = 0xb492b66fbe98f273ULL;\n  static const _Size __k2 = 0x9ae16a3b2f90404fULL;\n  static const _Size __k3 = 0xc949d7c7509e6557ULL;\n\n  static _Size __rotate(_Size __val, int __shift) {\n    return __shift == 0 ? __val : ((__val >> __shift) | (__val << (64 - __shift)));\n  }\n\n  static _Size __rotate_by_at_least_1(_Size __val, int __shift) {\n    return (__val >> __shift) | (__val << (64 - __shift));\n  }\n\n  static _Size __shift_mix(_Size __val) {\n    return __val ^ (__val >> 47);\n  }\n\n  static _Size __hash_len_16(_Size __u, _Size __v)\n     _LIBCUDACXX_DISABLE_UBSAN_UNSIGNED_INTEGER_CHECK\n  {\n    const _Size __mul = 0x9ddfea08eb382d69ULL;\n    _Size __a = (__u ^ __v) * __mul;\n    __a ^= (__a >> 47);\n    _Size __b = (__v ^ __a) * __mul;\n    __b ^= (__b >> 47);\n    __b *= __mul;\n    return __b;\n  }\n\n  static _Size __hash_len_0_to_16(const char* __s, _Size __len)\n     _LIBCUDACXX_DISABLE_UBSAN_UNSIGNED_INTEGER_CHECK\n  {\n    if (__len > 8) {\n      const _Size __a = __loadword<_Size>(__s);\n      const _Size __b = __loadword<_Size>(__s + __len - 8);\n      return __hash_len_16(__a, __rotate_by_at_least_1(__b + __len, __len)) ^ __b;\n    }\n    if (__len >= 4) {\n      const uint32_t __a = __loadword<uint32_t>(__s);\n      const uint32_t __b = __loadword<uint32_t>(__s + __len - 4);\n      return __hash_len_16(__len + (static_cast<_Size>(__a) << 3), __b);\n    }\n    if (__len > 0) {\n      const unsigned char __a = static_cast<unsigned char>(__s[0]);\n      const unsigned char __b = static_cast<unsigned char>(__s[__len >> 1]);\n      const unsigned char __c = static_cast<unsigned char>(__s[__len - 1]);\n      const uint32_t __y = static_cast<uint32_t>(__a) +\n                           (static_cast<uint32_t>(__b) << 8);\n      const uint32_t __z = __len + (static_cast<uint32_t>(__c) << 2);\n      return __shift_mix(__y * __k2 ^ __z * __k3) * __k2;\n    }\n    return __k2;\n  }\n\n  static _Size __hash_len_17_to_32(const char *__s, _Size __len)\n     _LIBCUDACXX_DISABLE_UBSAN_UNSIGNED_INTEGER_CHECK\n  {\n    const _Size __a = __loadword<_Size>(__s) * __k1;\n    const _Size __b = __loadword<_Size>(__s + 8);\n    const _Size __c = __loadword<_Size>(__s + __len - 8) * __k2;\n    const _Size __d = __loadword<_Size>(__s + __len - 16) * __k0;\n    return __hash_len_16(__rotate(__a - __b, 43) + __rotate(__c, 30) + __d,\n                         __a + __rotate(__b ^ __k3, 20) - __c + __len);\n  }\n\n  // Return a 16-byte hash for 48 bytes.  Quick and dirty.\n  // Callers do best to use \"random-looking\" values for a and b.\n  static pair<_Size, _Size> __weak_hash_len_32_with_seeds(\n      _Size __w, _Size __x, _Size __y, _Size __z, _Size __a, _Size __b)\n        _LIBCUDACXX_DISABLE_UBSAN_UNSIGNED_INTEGER_CHECK\n  {\n    __a += __w;\n    __b = __rotate(__b + __a + __z, 21);\n    const _Size __c = __a;\n    __a += __x;\n    __a += __y;\n    __b += __rotate(__a, 44);\n    return pair<_Size, _Size>(__a + __z, __b + __c);\n  }\n\n  // Return a 16-byte hash for s[0] ... s[31], a, and b.  Quick and dirty.\n  static pair<_Size, _Size> __weak_hash_len_32_with_seeds(\n      const char* __s, _Size __a, _Size __b)\n    _LIBCUDACXX_DISABLE_UBSAN_UNSIGNED_INTEGER_CHECK\n  {\n    return __weak_hash_len_32_with_seeds(__loadword<_Size>(__s),\n                                         __loadword<_Size>(__s + 8),\n                                         __loadword<_Size>(__s + 16),\n                                         __loadword<_Size>(__s + 24),\n                                         __a,\n                                         __b);\n  }\n\n  // Return an 8-byte hash for 33 to 64 bytes.\n  static _Size __hash_len_33_to_64(const char *__s, size_t __len)\n    _LIBCUDACXX_DISABLE_UBSAN_UNSIGNED_INTEGER_CHECK\n  {\n    _Size __z = __loadword<_Size>(__s + 24);\n    _Size __a = __loadword<_Size>(__s) +\n                (__len + __loadword<_Size>(__s + __len - 16)) * __k0;\n    _Size __b = __rotate(__a + __z, 52);\n    _Size __c = __rotate(__a, 37);\n    __a += __loadword<_Size>(__s + 8);\n    __c += __rotate(__a, 7);\n    __a += __loadword<_Size>(__s + 16);\n    _Size __vf = __a + __z;\n    _Size __vs = __b + __rotate(__a, 31) + __c;\n    __a = __loadword<_Size>(__s + 16) + __loadword<_Size>(__s + __len - 32);\n    __z += __loadword<_Size>(__s + __len - 8);\n    __b = __rotate(__a + __z, 52);\n    __c = __rotate(__a, 37);\n    __a += __loadword<_Size>(__s + __len - 24);\n    __c += __rotate(__a, 7);\n    __a += __loadword<_Size>(__s + __len - 16);\n    _Size __wf = __a + __z;\n    _Size __ws = __b + __rotate(__a, 31) + __c;\n    _Size __r = __shift_mix((__vf + __ws) * __k2 + (__wf + __vs) * __k0);\n    return __shift_mix(__r * __k0 + __vs) * __k2;\n  }\n};\n\n// cityhash64\ntemplate <class _Size>\n_Size\n__murmur2_or_cityhash<_Size, 64>::operator()(const void* __key, _Size __len)\n{\n  const char* __s = static_cast<const char*>(__key);\n  if (__len <= 32) {\n    if (__len <= 16) {\n      return __hash_len_0_to_16(__s, __len);\n    } else {\n      return __hash_len_17_to_32(__s, __len);\n    }\n  } else if (__len <= 64) {\n    return __hash_len_33_to_64(__s, __len);\n  }\n\n  // For strings over 64 bytes we hash the end first, and then as we\n  // loop we keep 56 bytes of state: v, w, x, y, and z.\n  _Size __x = __loadword<_Size>(__s + __len - 40);\n  _Size __y = __loadword<_Size>(__s + __len - 16) +\n              __loadword<_Size>(__s + __len - 56);\n  _Size __z = __hash_len_16(__loadword<_Size>(__s + __len - 48) + __len,\n                          __loadword<_Size>(__s + __len - 24));\n  pair<_Size, _Size> __v = __weak_hash_len_32_with_seeds(__s + __len - 64, __len, __z);\n  pair<_Size, _Size> __w = __weak_hash_len_32_with_seeds(__s + __len - 32, __y + __k1, __x);\n  __x = __x * __k1 + __loadword<_Size>(__s);\n\n  // Decrease len to the nearest multiple of 64, and operate on 64-byte chunks.\n  __len = (__len - 1) & ~static_cast<_Size>(63);\n  do {\n    __x = __rotate(__x + __y + __v.first + __loadword<_Size>(__s + 8), 37) * __k1;\n    __y = __rotate(__y + __v.second + __loadword<_Size>(__s + 48), 42) * __k1;\n    __x ^= __w.second;\n    __y += __v.first + __loadword<_Size>(__s + 40);\n    __z = __rotate(__z + __w.first, 33) * __k1;\n    __v = __weak_hash_len_32_with_seeds(__s, __v.second * __k1, __x + __w.first);\n    __w = __weak_hash_len_32_with_seeds(__s + 32, __z + __w.second,\n                                        __y + __loadword<_Size>(__s + 16));\n    _CUDA_VSTD::swap(__z, __x);\n    __s += 64;\n    __len -= 64;\n  } while (__len != 0);\n  return __hash_len_16(\n      __hash_len_16(__v.first, __w.first) + __shift_mix(__y) * __k1 + __z,\n      __hash_len_16(__v.second, __w.second) + __x);\n}\n\ntemplate <class _Tp, size_t = sizeof(_Tp) / sizeof(size_t)>\nstruct __scalar_hash;\n\ntemplate <class _Tp>\nstruct __scalar_hash<_Tp, 0>\n    : public __unary_function<_Tp, size_t>\n{\n    _LIBCUDACXX_INLINE_VISIBILITY\n    size_t operator()(_Tp __v) const noexcept\n    {\n        union\n        {\n            _Tp    __t;\n            size_t __a;\n        } __u;\n        __u.__a = 0;\n        __u.__t = __v;\n        return __u.__a;\n    }\n};\n\ntemplate <class _Tp>\nstruct __scalar_hash<_Tp, 1>\n    : public __unary_function<_Tp, size_t>\n{\n    _LIBCUDACXX_INLINE_VISIBILITY\n    size_t operator()(_Tp __v) const noexcept\n    {\n        union\n        {\n            _Tp    __t;\n            size_t __a;\n        } __u;\n        __u.__t = __v;\n        return __u.__a;\n    }\n};\n\ntemplate <class _Tp>\nstruct __scalar_hash<_Tp, 2>\n    : public __unary_function<_Tp, size_t>\n{\n    _LIBCUDACXX_INLINE_VISIBILITY\n    size_t operator()(_Tp __v) const noexcept\n    {\n        union\n        {\n            _Tp __t;\n            struct\n            {\n                size_t __a;\n                size_t __b;\n            } __s;\n        } __u;\n        __u.__t = __v;\n        return __murmur2_or_cityhash<size_t>()(&__u, sizeof(__u));\n    }\n};\n\ntemplate <class _Tp>\nstruct __scalar_hash<_Tp, 3>\n    : public __unary_function<_Tp, size_t>\n{\n    _LIBCUDACXX_INLINE_VISIBILITY\n    size_t operator()(_Tp __v) const noexcept\n    {\n        union\n        {\n            _Tp __t;\n            struct\n            {\n                size_t __a;\n                size_t __b;\n                size_t __c;\n            } __s;\n        } __u;\n        __u.__t = __v;\n        return __murmur2_or_cityhash<size_t>()(&__u, sizeof(__u));\n    }\n};\n\ntemplate <class _Tp>\nstruct __scalar_hash<_Tp, 4>\n    : public __unary_function<_Tp, size_t>\n{\n    _LIBCUDACXX_INLINE_VISIBILITY\n    size_t operator()(_Tp __v) const noexcept\n    {\n        union\n        {\n            _Tp __t;\n            struct\n            {\n                size_t __a;\n                size_t __b;\n                size_t __c;\n                size_t __d;\n            } __s;\n        } __u;\n        __u.__t = __v;\n        return __murmur2_or_cityhash<size_t>()(&__u, sizeof(__u));\n    }\n};\n\nstruct _PairT {\n  size_t first;\n  size_t second;\n};\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline size_t __hash_combine(size_t __lhs, size_t __rhs) noexcept {\n    typedef __scalar_hash<_PairT> _HashT;\n    const _PairT __p = {__lhs, __rhs};\n    return _HashT()(__p);\n}\n\ntemplate<class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS hash<_Tp*>\n    : public __unary_function<_Tp*, size_t>\n{\n    _LIBCUDACXX_INLINE_VISIBILITY\n    size_t operator()(_Tp* __v) const noexcept\n    {\n        union\n        {\n            _Tp* __t;\n            size_t __a;\n        } __u;\n        __u.__t = __v;\n        return __murmur2_or_cityhash<size_t>()(&__u, sizeof(__u));\n    }\n};\n\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS hash<bool>\n    : public __unary_function<bool, size_t>\n{\n    _LIBCUDACXX_INLINE_VISIBILITY\n    size_t operator()(bool __v) const noexcept {return static_cast<size_t>(__v);}\n};\n\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS hash<char>\n    : public __unary_function<char, size_t>\n{\n    _LIBCUDACXX_INLINE_VISIBILITY\n    size_t operator()(char __v) const noexcept {return static_cast<size_t>(__v);}\n};\n\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS hash<signed char>\n    : public __unary_function<signed char, size_t>\n{\n    _LIBCUDACXX_INLINE_VISIBILITY\n    size_t operator()(signed char __v) const noexcept {return static_cast<size_t>(__v);}\n};\n\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS hash<unsigned char>\n    : public __unary_function<unsigned char, size_t>\n{\n    _LIBCUDACXX_INLINE_VISIBILITY\n    size_t operator()(unsigned char __v) const noexcept {return static_cast<size_t>(__v);}\n};\n\n#ifndef _LIBCUDACXX_HAS_NO_UNICODE_CHARS\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS hash<char16_t>\n    : public __unary_function<char16_t, size_t>\n{\n    _LIBCUDACXX_INLINE_VISIBILITY\n    size_t operator()(char16_t __v) const noexcept {return static_cast<size_t>(__v);}\n};\n\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS hash<char32_t>\n    : public __unary_function<char32_t, size_t>\n{\n    _LIBCUDACXX_INLINE_VISIBILITY\n    size_t operator()(char32_t __v) const noexcept {return static_cast<size_t>(__v);}\n};\n#endif  // _LIBCUDACXX_HAS_NO_UNICODE_CHARS\n\n#ifndef _LIBCUDACXX_HAS_NO_WIDE_CHARACTERS\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS hash<wchar_t>\n    : public __unary_function<wchar_t, size_t>\n{\n    _LIBCUDACXX_INLINE_VISIBILITY\n    size_t operator()(wchar_t __v) const noexcept {return static_cast<size_t>(__v);}\n};\n#endif // _LIBCUDACXX_HAS_NO_WIDE_CHARACTERS\n\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS hash<short>\n    : public __unary_function<short, size_t>\n{\n    _LIBCUDACXX_INLINE_VISIBILITY\n    size_t operator()(short __v) const noexcept {return static_cast<size_t>(__v);}\n};\n\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS hash<unsigned short>\n    : public __unary_function<unsigned short, size_t>\n{\n    _LIBCUDACXX_INLINE_VISIBILITY\n    size_t operator()(unsigned short __v) const noexcept {return static_cast<size_t>(__v);}\n};\n\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS hash<int>\n    : public __unary_function<int, size_t>\n{\n    _LIBCUDACXX_INLINE_VISIBILITY\n    size_t operator()(int __v) const noexcept {return static_cast<size_t>(__v);}\n};\n\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS hash<unsigned int>\n    : public __unary_function<unsigned int, size_t>\n{\n    _LIBCUDACXX_INLINE_VISIBILITY\n    size_t operator()(unsigned int __v) const noexcept {return static_cast<size_t>(__v);}\n};\n\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS hash<long>\n    : public __unary_function<long, size_t>\n{\n    _LIBCUDACXX_INLINE_VISIBILITY\n    size_t operator()(long __v) const noexcept {return static_cast<size_t>(__v);}\n};\n\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS hash<unsigned long>\n    : public __unary_function<unsigned long, size_t>\n{\n    _LIBCUDACXX_INLINE_VISIBILITY\n    size_t operator()(unsigned long __v) const noexcept {return static_cast<size_t>(__v);}\n};\n\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS hash<long long>\n    : public __scalar_hash<long long>\n{\n};\n\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS hash<unsigned long long>\n    : public __scalar_hash<unsigned long long>\n{\n};\n\n#ifndef _LIBCUDACXX_HAS_NO_INT128\n\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS hash<__int128_t>\n    : public __scalar_hash<__int128_t>\n{\n};\n\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS hash<__uint128_t>\n    : public __scalar_hash<__uint128_t>\n{\n};\n\n#endif\n\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS hash<float>\n    : public __scalar_hash<float>\n{\n    _LIBCUDACXX_INLINE_VISIBILITY\n    size_t operator()(float __v) const noexcept\n    {\n        // -0.0 and 0.0 should return same hash\n       if (__v == 0.0f)\n           return 0;\n        return __scalar_hash<float>::operator()(__v);\n    }\n};\n\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS hash<double>\n    : public __scalar_hash<double>\n{\n    _LIBCUDACXX_INLINE_VISIBILITY\n    size_t operator()(double __v) const noexcept\n    {\n        // -0.0 and 0.0 should return same hash\n       if (__v == 0.0)\n           return 0;\n        return __scalar_hash<double>::operator()(__v);\n    }\n};\n\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS hash<long double>\n    : public __scalar_hash<long double>\n{\n    _LIBCUDACXX_INLINE_VISIBILITY\n    size_t operator()(long double __v) const noexcept\n    {\n        // -0.0 and 0.0 should return same hash\n        if (__v == 0.0L)\n            return 0;\n#if defined(__i386__) || (defined(__x86_64__) && defined(__ILP32__))\n        // Zero out padding bits\n        union\n        {\n            long double __t;\n            struct\n            {\n                size_t __a;\n                size_t __b;\n                size_t __c;\n                size_t __d;\n            } __s;\n        } __u;\n        __u.__s.__a = 0;\n        __u.__s.__b = 0;\n        __u.__s.__c = 0;\n        __u.__s.__d = 0;\n        __u.__t = __v;\n        return __u.__s.__a ^ __u.__s.__b ^ __u.__s.__c ^ __u.__s.__d;\n#elif defined(__x86_64__)\n        // Zero out padding bits\n        union\n        {\n            long double __t;\n            struct\n            {\n                size_t __a;\n                size_t __b;\n            } __s;\n        } __u;\n        __u.__s.__a = 0;\n        __u.__s.__b = 0;\n        __u.__t = __v;\n        return __u.__s.__a ^ __u.__s.__b;\n#else\n        return __scalar_hash<long double>::operator()(__v);\n#endif\n    }\n};\n\ntemplate <class _Tp, bool = is_enum<_Tp>::value>\nstruct _LIBCUDACXX_TEMPLATE_VIS __enum_hash\n    : public __unary_function<_Tp, size_t>\n{\n    _LIBCUDACXX_INLINE_VISIBILITY\n    size_t operator()(_Tp __v) const noexcept\n    {\n        typedef typename underlying_type<_Tp>::type type;\n        return hash<type>()(static_cast<type>(__v));\n    }\n};\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS __enum_hash<_Tp, false> {\n    __enum_hash() = delete;\n    __enum_hash(__enum_hash const&) = delete;\n    __enum_hash& operator=(__enum_hash const&) = delete;\n};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS hash : public __enum_hash<_Tp>\n{\n};\n\n#if _LIBCUDACXX_STD_VER > 14\n\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS hash<nullptr_t>\n  : public __unary_function<nullptr_t, size_t>\n{\n  _LIBCUDACXX_INLINE_VISIBILITY\n  size_t operator()(nullptr_t) const noexcept {\n    return 662607004ull;\n  }\n};\n#endif\n\ntemplate <class _Key, class _Hash>\nusing __check_hash_requirements _LIBCUDACXX_NODEBUG_TYPE = integral_constant<bool,\n    is_copy_constructible<_Hash>::value &&\n    is_move_constructible<_Hash>::value &&\n    __invokable_r<size_t, _Hash, _Key const&>::value\n>;\n\ntemplate <class _Key, class _Hash = hash<_Key> >\nusing __has_enabled_hash _LIBCUDACXX_NODEBUG_TYPE = integral_constant<bool,\n    __check_hash_requirements<_Key, _Hash>::value &&\n    is_default_constructible<_Hash>::value\n>;\n\n#if _LIBCUDACXX_STD_VER > 14\ntemplate <class _Type, class>\nusing __enable_hash_helper_imp _LIBCUDACXX_NODEBUG_TYPE = _Type;\n\ntemplate <class _Type, class ..._Keys>\nusing __enable_hash_helper _LIBCUDACXX_NODEBUG_TYPE = __enable_hash_helper_imp<_Type,\n  __enable_if_t<__all<__has_enabled_hash<_Keys>::value...>::value>\n>;\n#else\ntemplate <class _Type, class ...>\nusing __enable_hash_helper _LIBCUDACXX_NODEBUG_TYPE = _Type;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // __cuda_std__\n\n#endif // _LIBCUDACXX___FUNCTIONAL_HASH_H\n", "__functional/identity.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_IDENTITY_H\n#define _LIBCUDACXX___FUNCTIONAL_IDENTITY_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__utility/forward.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\nstruct __identity {\n  template <class _Tp>\n  _LIBCUDACXX_NODISCARD_EXT _LIBCUDACXX_INLINE_VISIBILITY constexpr _Tp&& operator()(_Tp&& __t) const noexcept {\n    return _CUDA_VSTD::forward<_Tp>(__t);\n  }\n\n  using is_transparent = void;\n};\n\n#if _LIBCUDACXX_STD_VER > 11\n\nstruct identity {\n    template<class _Tp>\n    _LIBCUDACXX_NODISCARD_EXT _LIBCUDACXX_INLINE_VISIBILITY constexpr _Tp&& operator()(_Tp&& __t) const noexcept\n    {\n        return _CUDA_VSTD::forward<_Tp>(__t);\n    }\n\n    using is_transparent = void;\n};\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FUNCTIONAL_IDENTITY_H\n", "__functional/invoke.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_INVOKE_H\n#define _LIBCUDACXX___FUNCTIONAL_INVOKE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_lvalue_reference.h\"\n#include \"../__type_traits/apply_cv.h\"\n#include \"../__type_traits/conditional.h\"\n#include \"../__type_traits/decay.h\"\n#include \"../__type_traits/enable_if.h\"\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_base_of.h\"\n#include \"../__type_traits/is_core_convertible.h\"\n#include \"../__type_traits/is_member_function_pointer.h\"\n#include \"../__type_traits/is_member_object_pointer.h\"\n#include \"../__type_traits/is_reference_wrapper.h\"\n#include \"../__type_traits/is_same.h\"\n#include \"../__type_traits/is_void.h\"\n#include \"../__type_traits/nat.h\"\n#include \"../__type_traits/remove_cv.h\"\n#include \"../__utility/declval.h\"\n#include \"../__utility/forward.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n// TODO: Disentangle the type traits and _CUDA_VSTD::invoke properly\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\nstruct __any\n{\n    _LIBCUDACXX_INLINE_VISIBILITY __any(...);\n};\n\ntemplate <class _MP, bool _IsMemberFunctionPtr, bool _IsMemberObjectPtr>\nstruct __member_pointer_traits_imp\n{\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param...), true, false>\n{\n    typedef _Class _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param..., ...), true, false>\n{\n    typedef _Class _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param..., ...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param...) const, true, false>\n{\n    typedef _Class const _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param..., ...) const, true, false>\n{\n    typedef _Class const _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param..., ...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param...) volatile, true, false>\n{\n    typedef _Class volatile _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param..., ...) volatile, true, false>\n{\n    typedef _Class volatile _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param..., ...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param...) const volatile, true, false>\n{\n    typedef _Class const volatile _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param..., ...) const volatile, true, false>\n{\n    typedef _Class const volatile _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param..., ...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param...) &, true, false>\n{\n    typedef _Class& _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param..., ...) &, true, false>\n{\n    typedef _Class& _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param..., ...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param...) const&, true, false>\n{\n    typedef _Class const& _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param..., ...) const&, true, false>\n{\n    typedef _Class const& _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param..., ...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param...) volatile&, true, false>\n{\n    typedef _Class volatile& _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param..., ...) volatile&, true, false>\n{\n    typedef _Class volatile& _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param..., ...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param...) const volatile&, true, false>\n{\n    typedef _Class const volatile& _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param..., ...) const volatile&, true, false>\n{\n    typedef _Class const volatile& _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param..., ...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param...) &&, true, false>\n{\n    typedef _Class&& _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param..., ...) &&, true, false>\n{\n    typedef _Class&& _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param..., ...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param...) const&&, true, false>\n{\n    typedef _Class const&& _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param..., ...) const&&, true, false>\n{\n    typedef _Class const&& _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param..., ...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param...) volatile&&, true, false>\n{\n    typedef _Class volatile&& _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param..., ...) volatile&&, true, false>\n{\n    typedef _Class volatile&& _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param..., ...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param...) const volatile&&, true, false>\n{\n    typedef _Class const volatile&& _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param...);\n};\n\ntemplate <class _Rp, class _Class, class ..._Param>\nstruct __member_pointer_traits_imp<_Rp (_Class::*)(_Param..., ...) const volatile&&, true, false>\n{\n    typedef _Class const volatile&& _ClassType;\n    typedef _Rp _ReturnType;\n    typedef _Rp (_FnType) (_Param..., ...);\n};\n\ntemplate <class _Rp, class _Class>\nstruct __member_pointer_traits_imp<_Rp _Class::*, false, true>\n{\n    typedef _Class _ClassType;\n    typedef _Rp _ReturnType;\n};\n\ntemplate <class _MP>\nstruct __member_pointer_traits\n    : public __member_pointer_traits_imp<__remove_cv_t<_MP>,\n                    is_member_function_pointer<_MP>::value,\n                    is_member_object_pointer<_MP>::value>\n{\n//     typedef ... _ClassType;\n//     typedef ... _ReturnType;\n//     typedef ... _FnType;\n};\n\ntemplate <class _DecayedFp>\nstruct __member_pointer_class_type {};\n\ntemplate <class _Ret, class _ClassType>\nstruct __member_pointer_class_type<_Ret _ClassType::*> {\n  typedef _ClassType type;\n};\n\ntemplate <class _Fp, class _A0,\n         class _DecayFp = __decay_t<_Fp>,\n         class _DecayA0 = typename decay<_A0>::type,\n         class _ClassT = typename __member_pointer_class_type<_DecayFp>::type>\nusing __enable_if_bullet1 = __enable_if_t\n    <\n        is_member_function_pointer<_DecayFp>::value\n        && is_base_of<_ClassT, _DecayA0>::value\n    >;\n\ntemplate <class _Fp, class _A0,\n         class _DecayFp = __decay_t<_Fp>,\n         class _DecayA0 = typename decay<_A0>::type>\nusing __enable_if_bullet2 = __enable_if_t\n    <\n        is_member_function_pointer<_DecayFp>::value\n        && __is_reference_wrapper<_DecayA0>::value\n    >;\n\ntemplate <class _Fp, class _A0,\n         class _DecayFp = __decay_t<_Fp>,\n         class _DecayA0 = typename decay<_A0>::type,\n         class _ClassT = typename __member_pointer_class_type<_DecayFp>::type>\nusing __enable_if_bullet3 =__enable_if_t\n    <\n        is_member_function_pointer<_DecayFp>::value\n        && !is_base_of<_ClassT, _DecayA0>::value\n        && !__is_reference_wrapper<_DecayA0>::value\n    >;\n\ntemplate <class _Fp, class _A0,\n         class _DecayFp = __decay_t<_Fp>,\n         class _DecayA0 = typename decay<_A0>::type,\n         class _ClassT = typename __member_pointer_class_type<_DecayFp>::type>\nusing __enable_if_bullet4 = __enable_if_t\n    <\n        is_member_object_pointer<_DecayFp>::value\n        && is_base_of<_ClassT, _DecayA0>::value\n    >;\n\ntemplate <class _Fp, class _A0,\n         class _DecayFp = __decay_t<_Fp>,\n         class _DecayA0 = typename decay<_A0>::type>\nusing __enable_if_bullet5 = __enable_if_t\n    <\n        is_member_object_pointer<_DecayFp>::value\n        && __is_reference_wrapper<_DecayA0>::value\n    >;\n\ntemplate <class _Fp, class _A0,\n         class _DecayFp = __decay_t<_Fp>,\n         class _DecayA0 = typename decay<_A0>::type,\n         class _ClassT = typename __member_pointer_class_type<_DecayFp>::type>\nusing __enable_if_bullet6 = __enable_if_t\n    <\n        is_member_object_pointer<_DecayFp>::value\n        && !is_base_of<_ClassT, _DecayA0>::value\n        && !__is_reference_wrapper<_DecayA0>::value\n    >;\n\n// __invoke forward declarations\n\n// fall back - none of the bullets\n\ntemplate <class ..._Args>\n_LIBCUDACXX_INLINE_VISIBILITY __nat __invoke(__any, _Args&& ...__args);\n\n// bullets 1, 2 and 3\n\n_LIBCUDACXX_DISABLE_EXEC_CHECK\ntemplate <class _Fp, class _A0, class ..._Args,\n          class = __enable_if_bullet1<_Fp, _A0> >\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr decltype((_CUDA_VSTD::declval<_A0>().*_CUDA_VSTD::declval<_Fp>())(_CUDA_VSTD::declval<_Args>()...))\n__invoke(_Fp&& __f, _A0&& __a0, _Args&& ...__args)\n    noexcept(noexcept((static_cast<_A0&&>(__a0).*__f)(static_cast<_Args&&>(__args)...)))\n    { return           (static_cast<_A0&&>(__a0).*__f)(static_cast<_Args&&>(__args)...); }\n\n_LIBCUDACXX_DISABLE_EXEC_CHECK\ntemplate <class _Fp, class _A0, class ..._Args,\n          class = __enable_if_bullet2<_Fp, _A0> >\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr decltype((_CUDA_VSTD::declval<_A0>().get().*_CUDA_VSTD::declval<_Fp>())(_CUDA_VSTD::declval<_Args>()...))\n__invoke(_Fp&& __f, _A0&& __a0, _Args&& ...__args)\n    noexcept(noexcept((__a0.get().*__f)(static_cast<_Args&&>(__args)...)))\n    { return          (__a0.get().*__f)(static_cast<_Args&&>(__args)...); }\n\n_LIBCUDACXX_DISABLE_EXEC_CHECK\ntemplate <class _Fp, class _A0, class ..._Args,\n          class = __enable_if_bullet3<_Fp, _A0> >\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr decltype(((*_CUDA_VSTD::declval<_A0>()).*_CUDA_VSTD::declval<_Fp>())(_CUDA_VSTD::declval<_Args>()...))\n__invoke(_Fp&& __f, _A0&& __a0, _Args&& ...__args)\n    noexcept(noexcept(((*static_cast<_A0&&>(__a0)).*__f)(static_cast<_Args&&>(__args)...)))\n    { return          ((*static_cast<_A0&&>(__a0)).*__f)(static_cast<_Args&&>(__args)...); }\n\n// bullets 4, 5 and 6\n\n_LIBCUDACXX_DISABLE_EXEC_CHECK\ntemplate <class _Fp, class _A0,\n          class = __enable_if_bullet4<_Fp, _A0> >\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr decltype(_CUDA_VSTD::declval<_A0>().*_CUDA_VSTD::declval<_Fp>())\n__invoke(_Fp&& __f, _A0&& __a0)\n    noexcept(noexcept(static_cast<_A0&&>(__a0).*__f))\n    { return          static_cast<_A0&&>(__a0).*__f; }\n\n_LIBCUDACXX_DISABLE_EXEC_CHECK\ntemplate <class _Fp, class _A0,\n          class = __enable_if_bullet5<_Fp, _A0> >\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr decltype(_CUDA_VSTD::declval<_A0>().get().*_CUDA_VSTD::declval<_Fp>())\n__invoke(_Fp&& __f, _A0&& __a0)\n    noexcept(noexcept(__a0.get().*__f))\n    { return          __a0.get().*__f; }\n\n_LIBCUDACXX_DISABLE_EXEC_CHECK\ntemplate <class _Fp, class _A0,\n          class = __enable_if_bullet6<_Fp, _A0> >\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr decltype((*_CUDA_VSTD::declval<_A0>()).*_CUDA_VSTD::declval<_Fp>())\n__invoke(_Fp&& __f, _A0&& __a0)\n    noexcept(noexcept((*static_cast<_A0&&>(__a0)).*__f))\n    { return          (*static_cast<_A0&&>(__a0)).*__f; }\n\n// bullet 7\n\n_LIBCUDACXX_DISABLE_EXEC_CHECK\ntemplate <class _Fp, class ..._Args>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr decltype(_CUDA_VSTD::declval<_Fp>()(_CUDA_VSTD::declval<_Args>()...))\n__invoke(_Fp&& __f, _Args&& ...__args)\n    noexcept(noexcept(static_cast<_Fp&&>(__f)(static_cast<_Args&&>(__args)...)))\n    { return          static_cast<_Fp&&>(__f)(static_cast<_Args&&>(__args)...); }\n\n// __invokable\ntemplate <class _Ret, class _Fp, class ..._Args>\nstruct __invokable_r\n{\n  template <class _XFp, class ..._XArgs>\n  _LIBCUDACXX_INLINE_VISIBILITY\n  static decltype(_CUDA_VSTD::__invoke(_CUDA_VSTD::declval<_XFp>(), _CUDA_VSTD::declval<_XArgs>()...)) __try_call(int);\n\n  template <class _XFp, class ..._XArgs>\n  _LIBCUDACXX_INLINE_VISIBILITY\n  static __nat __try_call(...);\n\n  // FIXME: Check that _Ret, _Fp, and _Args... are all complete types, cv void,\n  // or incomplete array types as required by the standard.\n  using _Result = decltype(__try_call<_Fp, _Args...>(0));\n\n  using type = __conditional_t<\n      _IsNotSame<_Result, __nat>::value,\n      __conditional_t<is_void<_Ret>::value, true_type, __is_core_convertible<_Result, _Ret> >,\n      false_type>;\n  static const bool value = type::value;\n};\ntemplate <class _Fp, class ..._Args>\nusing __invokable = __invokable_r<void, _Fp, _Args...>;\n\ntemplate <bool _IsInvokable, bool _IsCVVoid, class _Ret, class _Fp, class ..._Args>\nstruct __nothrow_invokable_r_imp {\n  static const bool value = false;\n};\n\ntemplate <class _Ret, class _Fp, class ..._Args>\nstruct __nothrow_invokable_r_imp<true, false, _Ret, _Fp, _Args...>\n{\n    typedef __nothrow_invokable_r_imp _ThisT;\n\n    template <class _Tp>\n    _LIBCUDACXX_INLINE_VISIBILITY\n    static void __test_noexcept(_Tp) noexcept;\n\n    static const bool value = noexcept(_ThisT::__test_noexcept<_Ret>(\n        _CUDA_VSTD::__invoke(declval<_Fp>(), _CUDA_VSTD::declval<_Args>()...)));\n};\n\ntemplate <class _Ret, class _Fp, class ..._Args>\nstruct __nothrow_invokable_r_imp<true, true, _Ret, _Fp, _Args...>\n{\n    static const bool value = noexcept(\n        _CUDA_VSTD::__invoke(_CUDA_VSTD::declval<_Fp>(), _CUDA_VSTD::declval<_Args>()...));\n};\n\ntemplate <class _Ret, class _Fp, class ..._Args>\nusing __nothrow_invokable_r =\n    __nothrow_invokable_r_imp<\n            __invokable_r<_Ret, _Fp, _Args...>::value,\n            is_void<_Ret>::value,\n            _Ret, _Fp, _Args...\n    >;\n\ntemplate <class _Fp, class ..._Args>\nusing __nothrow_invokable =\n    __nothrow_invokable_r_imp<\n            __invokable<_Fp, _Args...>::value,\n            true, void, _Fp, _Args...\n    >;\n\ntemplate <class _Fp, class ..._Args>\nstruct __invoke_of\n    : public enable_if<\n        __invokable<_Fp, _Args...>::value,\n        typename __invokable_r<void, _Fp, _Args...>::_Result>\n{\n#if defined(__NVCC__) && defined(__CUDACC_EXTENDED_LAMBDA__) && \\\n   !defined(__CUDA_ARCH__)\n  static_assert(!__nv_is_extended_device_lambda_closure_type(_Fp),\n                \"Attempt to use an extended __device__ lambda in a context \"\n                \"that requires querying its return type in host code. Use a \"\n                \"named function object, a __host__ __device__ lambda, or \"\n                \"cuda::proclaim_return_type instead.\");\n#endif\n};\n\ntemplate <class _Ret, bool = is_void<_Ret>::value>\nstruct __invoke_void_return_wrapper\n{\n    template <class ..._Args>\n    _LIBCUDACXX_INLINE_VISIBILITY static _Ret __call(_Args&&... __args) {\n        return _CUDA_VSTD::__invoke(_CUDA_VSTD::forward<_Args>(__args)...);\n    }\n};\n\ntemplate <class _Ret>\nstruct __invoke_void_return_wrapper<_Ret, true>\n{\n    template <class ..._Args>\n    _LIBCUDACXX_INLINE_VISIBILITY static void __call(_Args&&... __args) {\n        _CUDA_VSTD::__invoke(_CUDA_VSTD::forward<_Args>(__args)...);\n    }\n};\n\n#if _LIBCUDACXX_STD_VER > 11\n\n// is_invocable\n\ntemplate <class _Fn, class ..._Args>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_invocable\n    : integral_constant<bool, __invokable<_Fn, _Args...>::value> {};\n\ntemplate <class _Ret, class _Fn, class ..._Args>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_invocable_r\n    : integral_constant<bool, __invokable_r<_Ret, _Fn, _Args...>::value> {};\n\ntemplate <class _Fn, class ..._Args>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_invocable_v = is_invocable<_Fn, _Args...>::value;\n\ntemplate <class _Ret, class _Fn, class ..._Args>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_invocable_r_v = is_invocable_r<_Ret, _Fn, _Args...>::value;\n\n// is_nothrow_invocable\n\ntemplate <class _Fn, class ..._Args>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_invocable\n    : integral_constant<bool, __nothrow_invokable<_Fn, _Args...>::value> {};\n\ntemplate <class _Ret, class _Fn, class ..._Args>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_invocable_r\n    : integral_constant<bool, __nothrow_invokable_r<_Ret, _Fn, _Args...>::value> {};\n\ntemplate <class _Fn, class ..._Args>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_invocable_v = is_nothrow_invocable<_Fn, _Args...>::value;\n\ntemplate <class _Ret, class _Fn, class ..._Args>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_invocable_r_v = is_nothrow_invocable_r<_Ret, _Fn, _Args...>::value;\n\ntemplate <class _Fn, class... _Args>\nstruct _LIBCUDACXX_TEMPLATE_VIS invoke_result\n    : __invoke_of<_Fn, _Args...>\n{\n};\n\ntemplate <class _Fn, class... _Args>\nusing invoke_result_t = typename invoke_result<_Fn, _Args...>::type;\n\ntemplate <class _Fn, class ..._Args>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr invoke_result_t<_Fn, _Args...>\ninvoke(_Fn&& __f, _Args&&... __args)\n    noexcept(is_nothrow_invocable_v<_Fn, _Args...>)\n{\n    return _CUDA_VSTD::__invoke(_CUDA_VSTD::forward<_Fn>(__f), _CUDA_VSTD::forward<_Args>(__args)...);\n}\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FUNCTIONAL_INVOKE_H\n", "__functional/is_transparent.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_IS_TRANSPARENT\n#define _LIBCUDACXX___FUNCTIONAL_IS_TRANSPARENT\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/void_t.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 11\n\ntemplate <class _Tp, class, class = void>\nstruct __is_transparent : false_type {};\n\ntemplate <class _Tp, class _Up>\nstruct __is_transparent<_Tp, _Up, __void_t<typename _Tp::is_transparent> >\n   : true_type {};\n\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FUNCTIONAL_IS_TRANSPARENT\n", "__functional/mem_fn.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_MEM_FN_H\n#define _LIBCUDACXX___FUNCTIONAL_MEM_FN_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__functional/binary_function.h\"\n#include \"../__functional/invoke.h\"\n#include \"../__functional/weak_result_type.h\"\n#include \"../__utility/forward.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp>\nclass __mem_fn : public __weak_result_type<_Tp>\n{\npublic:\n    // types\n    typedef _Tp type;\nprivate:\n    type __f_;\n\npublic:\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n    __mem_fn(type __f) noexcept : __f_(__f) {}\n\n    // invoke\n    template <class... _ArgTypes>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n    typename __invoke_return<type, _ArgTypes...>::type\n    operator() (_ArgTypes&&... __args) const {\n        return _CUDA_VSTD::__invoke(__f_, _CUDA_VSTD::forward<_ArgTypes>(__args)...);\n    }\n};\n\ntemplate<class _Rp, class _Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n__mem_fn<_Rp _Tp::*>\nmem_fn(_Rp _Tp::* __pm) noexcept\n{\n    return __mem_fn<_Rp _Tp::*>(__pm);\n}\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FUNCTIONAL_MEM_FN_H\n", "__functional/mem_fun_ref.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_MEM_FUN_REF_H\n#define _LIBCUDACXX___FUNCTIONAL_MEM_FUN_REF_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__functional/binary_function.h\"\n#include \"../__functional/unary_function.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER <= 14 || defined(_LIBCUDACXX_ENABLE_CXX17_REMOVED_BINDERS)\n\ntemplate<class _Sp, class _Tp>\nclass _LIBCUDACXX_TEMPLATE_VIS _LIBCUDACXX_DEPRECATED_IN_CXX11 mem_fun_t\n    : public __unary_function<_Tp*, _Sp>\n{\n    _Sp (_Tp::*__p_)();\npublic:\n    _LIBCUDACXX_INLINE_VISIBILITY explicit mem_fun_t(_Sp (_Tp::*__p)())\n        : __p_(__p) {}\n    _LIBCUDACXX_INLINE_VISIBILITY _Sp operator()(_Tp* __p) const\n        {return (__p->*__p_)();}\n};\n\ntemplate<class _Sp, class _Tp, class _Ap>\nclass _LIBCUDACXX_TEMPLATE_VIS _LIBCUDACXX_DEPRECATED_IN_CXX11 mem_fun1_t\n    : public __binary_function<_Tp*, _Ap, _Sp>\n{\n    _Sp (_Tp::*__p_)(_Ap);\npublic:\n    _LIBCUDACXX_INLINE_VISIBILITY explicit mem_fun1_t(_Sp (_Tp::*__p)(_Ap))\n        : __p_(__p) {}\n    _LIBCUDACXX_INLINE_VISIBILITY _Sp operator()(_Tp* __p, _Ap __x) const\n        {return (__p->*__p_)(__x);}\n};\n\ntemplate<class _Sp, class _Tp>\n_LIBCUDACXX_DEPRECATED_IN_CXX11 inline _LIBCUDACXX_INLINE_VISIBILITY\nmem_fun_t<_Sp,_Tp>\nmem_fun(_Sp (_Tp::*__f)())\n    {return mem_fun_t<_Sp,_Tp>(__f);}\n\ntemplate<class _Sp, class _Tp, class _Ap>\n_LIBCUDACXX_DEPRECATED_IN_CXX11 inline _LIBCUDACXX_INLINE_VISIBILITY\nmem_fun1_t<_Sp,_Tp,_Ap>\nmem_fun(_Sp (_Tp::*__f)(_Ap))\n    {return mem_fun1_t<_Sp,_Tp,_Ap>(__f);}\n\ntemplate<class _Sp, class _Tp>\nclass _LIBCUDACXX_TEMPLATE_VIS _LIBCUDACXX_DEPRECATED_IN_CXX11 mem_fun_ref_t\n    : public __unary_function<_Tp, _Sp>\n{\n    _Sp (_Tp::*__p_)();\npublic:\n    _LIBCUDACXX_INLINE_VISIBILITY explicit mem_fun_ref_t(_Sp (_Tp::*__p)())\n        : __p_(__p) {}\n    _LIBCUDACXX_INLINE_VISIBILITY _Sp operator()(_Tp& __p) const\n        {return (__p.*__p_)();}\n};\n\ntemplate<class _Sp, class _Tp, class _Ap>\nclass _LIBCUDACXX_TEMPLATE_VIS _LIBCUDACXX_DEPRECATED_IN_CXX11 mem_fun1_ref_t\n    : public __binary_function<_Tp, _Ap, _Sp>\n{\n    _Sp (_Tp::*__p_)(_Ap);\npublic:\n    _LIBCUDACXX_INLINE_VISIBILITY explicit mem_fun1_ref_t(_Sp (_Tp::*__p)(_Ap))\n        : __p_(__p) {}\n    _LIBCUDACXX_INLINE_VISIBILITY _Sp operator()(_Tp& __p, _Ap __x) const\n        {return (__p.*__p_)(__x);}\n};\n\ntemplate<class _Sp, class _Tp>\n_LIBCUDACXX_DEPRECATED_IN_CXX11 inline _LIBCUDACXX_INLINE_VISIBILITY\nmem_fun_ref_t<_Sp,_Tp>\nmem_fun_ref(_Sp (_Tp::*__f)())\n    {return mem_fun_ref_t<_Sp,_Tp>(__f);}\n\ntemplate<class _Sp, class _Tp, class _Ap>\n_LIBCUDACXX_DEPRECATED_IN_CXX11 inline _LIBCUDACXX_INLINE_VISIBILITY\nmem_fun1_ref_t<_Sp,_Tp,_Ap>\nmem_fun_ref(_Sp (_Tp::*__f)(_Ap))\n    {return mem_fun1_ref_t<_Sp,_Tp,_Ap>(__f);}\n\ntemplate <class _Sp, class _Tp>\nclass _LIBCUDACXX_TEMPLATE_VIS _LIBCUDACXX_DEPRECATED_IN_CXX11 const_mem_fun_t\n    : public __unary_function<const _Tp*, _Sp>\n{\n    _Sp (_Tp::*__p_)() const;\npublic:\n    _LIBCUDACXX_INLINE_VISIBILITY explicit const_mem_fun_t(_Sp (_Tp::*__p)() const)\n        : __p_(__p) {}\n    _LIBCUDACXX_INLINE_VISIBILITY _Sp operator()(const _Tp* __p) const\n        {return (__p->*__p_)();}\n};\n\ntemplate <class _Sp, class _Tp, class _Ap>\nclass _LIBCUDACXX_TEMPLATE_VIS _LIBCUDACXX_DEPRECATED_IN_CXX11 const_mem_fun1_t\n    : public __binary_function<const _Tp*, _Ap, _Sp>\n{\n    _Sp (_Tp::*__p_)(_Ap) const;\npublic:\n    _LIBCUDACXX_INLINE_VISIBILITY explicit const_mem_fun1_t(_Sp (_Tp::*__p)(_Ap) const)\n        : __p_(__p) {}\n    _LIBCUDACXX_INLINE_VISIBILITY _Sp operator()(const _Tp* __p, _Ap __x) const\n        {return (__p->*__p_)(__x);}\n};\n\ntemplate <class _Sp, class _Tp>\n_LIBCUDACXX_DEPRECATED_IN_CXX11 inline _LIBCUDACXX_INLINE_VISIBILITY\nconst_mem_fun_t<_Sp,_Tp>\nmem_fun(_Sp (_Tp::*__f)() const)\n    {return const_mem_fun_t<_Sp,_Tp>(__f);}\n\ntemplate <class _Sp, class _Tp, class _Ap>\n_LIBCUDACXX_DEPRECATED_IN_CXX11 inline _LIBCUDACXX_INLINE_VISIBILITY\nconst_mem_fun1_t<_Sp,_Tp,_Ap>\nmem_fun(_Sp (_Tp::*__f)(_Ap) const)\n    {return const_mem_fun1_t<_Sp,_Tp,_Ap>(__f);}\n\ntemplate <class _Sp, class _Tp>\nclass _LIBCUDACXX_TEMPLATE_VIS _LIBCUDACXX_DEPRECATED_IN_CXX11 const_mem_fun_ref_t\n    : public __unary_function<_Tp, _Sp>\n{\n    _Sp (_Tp::*__p_)() const;\npublic:\n    _LIBCUDACXX_INLINE_VISIBILITY explicit const_mem_fun_ref_t(_Sp (_Tp::*__p)() const)\n        : __p_(__p) {}\n    _LIBCUDACXX_INLINE_VISIBILITY _Sp operator()(const _Tp& __p) const\n        {return (__p.*__p_)();}\n};\n\ntemplate <class _Sp, class _Tp, class _Ap>\nclass _LIBCUDACXX_TEMPLATE_VIS _LIBCUDACXX_DEPRECATED_IN_CXX11 const_mem_fun1_ref_t\n    : public __binary_function<_Tp, _Ap, _Sp>\n{\n    _Sp (_Tp::*__p_)(_Ap) const;\npublic:\n    _LIBCUDACXX_INLINE_VISIBILITY explicit const_mem_fun1_ref_t(_Sp (_Tp::*__p)(_Ap) const)\n        : __p_(__p) {}\n    _LIBCUDACXX_INLINE_VISIBILITY _Sp operator()(const _Tp& __p, _Ap __x) const\n        {return (__p.*__p_)(__x);}\n};\n\ntemplate <class _Sp, class _Tp>\n_LIBCUDACXX_DEPRECATED_IN_CXX11 inline _LIBCUDACXX_INLINE_VISIBILITY\nconst_mem_fun_ref_t<_Sp,_Tp>\nmem_fun_ref(_Sp (_Tp::*__f)() const)\n    {return const_mem_fun_ref_t<_Sp,_Tp>(__f);}\n\ntemplate <class _Sp, class _Tp, class _Ap>\n_LIBCUDACXX_DEPRECATED_IN_CXX11 inline _LIBCUDACXX_INLINE_VISIBILITY\nconst_mem_fun1_ref_t<_Sp,_Tp,_Ap>\nmem_fun_ref(_Sp (_Tp::*__f)(_Ap) const)\n    {return const_mem_fun1_ref_t<_Sp,_Tp,_Ap>(__f);}\n\n#endif // _LIBCUDACXX_STD_VER <= 14 || defined(_LIBCUDACXX_ENABLE_CXX17_REMOVED_BINDERS)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FUNCTIONAL_MEM_FUN_REF_H\n", "__functional/not_fn.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_NOT_FN_H\n#define _LIBCUDACXX___FUNCTIONAL_NOT_FN_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__functional/invoke.h\"\n#include \"../__functional/perfect_forward.h\"\n#include \"../__type_traits/decay.h\"\n#include \"../__type_traits/enable_if.h\"\n#include \"../__type_traits/is_constructible.h\"\n#include \"../__type_traits/is_move_constructible.h\"\n#include \"../__utility/forward.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 14\n\nstruct __not_fn_op {\n    template <class... _Args>\n    _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX17 auto operator()(_Args&&... __args) const\n        noexcept(noexcept(!_CUDA_VSTD::invoke(_CUDA_VSTD::forward<_Args>(__args)...)))\n        -> decltype(      !_CUDA_VSTD::invoke(_CUDA_VSTD::forward<_Args>(__args)...))\n        { return          !_CUDA_VSTD::invoke(_CUDA_VSTD::forward<_Args>(__args)...); }\n};\n\ntemplate <class _Fn>\nstruct __not_fn_t : __perfect_forward<__not_fn_op, _Fn> {\n    using __base = __perfect_forward<__not_fn_op, _Fn>;\n#if defined(_LIBCUDACXX_COMPILER_NVRTC) // nvbug 3961621\n    constexpr __not_fn_t() noexcept = default;\n\n    _LIBCUDACXX_TEMPLATE(class _OrigFn)\n        (requires _LIBCUDACXX_TRAIT(is_same, _Fn, __decay_t<_OrigFn>))\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    __not_fn_t(_OrigFn&& __fn) noexcept(noexcept(__base(_CUDA_VSTD::declval<_OrigFn>())))\n        : __base(_CUDA_VSTD::forward<_OrigFn>(__fn))\n    {}\n#else\n    using __base::__base;\n#endif\n};\n\ntemplate <class _Fn, class = enable_if_t<\n    is_constructible_v<decay_t<_Fn>, _Fn> &&\n    is_move_constructible_v<decay_t<_Fn>>\n>>\ninline _LIBCUDACXX_INLINE_VISIBILITY\n_LIBCUDACXX_CONSTEXPR_AFTER_CXX17 auto not_fn(_Fn&& __f) {\n    return __not_fn_t<decay_t<_Fn>>(_CUDA_VSTD::forward<_Fn>(__f));\n}\n\n#endif // _LIBCUDACXX_STD_VER > 14\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FUNCTIONAL_NOT_FN_H\n", "__functional/operations.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_OPERATIONS_H\n#define _LIBCUDACXX___FUNCTIONAL_OPERATIONS_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__functional/binary_function.h\"\n#include \"../__functional/unary_function.h\"\n#include \"../__utility/forward.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n// Arithmetic operations\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS plus\n    : __binary_function<_Tp, _Tp, _Tp>\n{\n    typedef _Tp __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator()(const _Tp& __x, const _Tp& __y) const\n        {return __x + __y;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(plus);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS plus<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _T1, class _T2>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_T1&& __t, _T2&& __u) const\n        noexcept(noexcept(_CUDA_VSTD::forward<_T1>(__t) + _CUDA_VSTD::forward<_T2>(__u)))\n        -> decltype(      _CUDA_VSTD::forward<_T1>(__t) + _CUDA_VSTD::forward<_T2>(__u))\n        { return          _CUDA_VSTD::forward<_T1>(__t) + _CUDA_VSTD::forward<_T2>(__u); }\n    typedef void is_transparent;\n};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS minus\n    : __binary_function<_Tp, _Tp, _Tp>\n{\n    typedef _Tp __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator()(const _Tp& __x, const _Tp& __y) const\n        {return __x - __y;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(minus);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS minus<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _T1, class _T2>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_T1&& __t, _T2&& __u) const\n        noexcept(noexcept(_CUDA_VSTD::forward<_T1>(__t) - _CUDA_VSTD::forward<_T2>(__u)))\n        -> decltype(      _CUDA_VSTD::forward<_T1>(__t) - _CUDA_VSTD::forward<_T2>(__u))\n        { return          _CUDA_VSTD::forward<_T1>(__t) - _CUDA_VSTD::forward<_T2>(__u); }\n    typedef void is_transparent;\n};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS multiplies\n    : __binary_function<_Tp, _Tp, _Tp>\n{\n    typedef _Tp __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator()(const _Tp& __x, const _Tp& __y) const\n        {return __x * __y;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(multiplies);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS multiplies<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _T1, class _T2>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_T1&& __t, _T2&& __u) const\n        noexcept(noexcept(_CUDA_VSTD::forward<_T1>(__t) * _CUDA_VSTD::forward<_T2>(__u)))\n        -> decltype(      _CUDA_VSTD::forward<_T1>(__t) * _CUDA_VSTD::forward<_T2>(__u))\n        { return          _CUDA_VSTD::forward<_T1>(__t) * _CUDA_VSTD::forward<_T2>(__u); }\n    typedef void is_transparent;\n};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS divides\n    : __binary_function<_Tp, _Tp, _Tp>\n{\n    typedef _Tp __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator()(const _Tp& __x, const _Tp& __y) const\n        {return __x / __y;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(divides);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS divides<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _T1, class _T2>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_T1&& __t, _T2&& __u) const\n        noexcept(noexcept(_CUDA_VSTD::forward<_T1>(__t) / _CUDA_VSTD::forward<_T2>(__u)))\n        -> decltype(      _CUDA_VSTD::forward<_T1>(__t) / _CUDA_VSTD::forward<_T2>(__u))\n        { return          _CUDA_VSTD::forward<_T1>(__t) / _CUDA_VSTD::forward<_T2>(__u); }\n    typedef void is_transparent;\n};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS modulus\n    : __binary_function<_Tp, _Tp, _Tp>\n{\n    typedef _Tp __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator()(const _Tp& __x, const _Tp& __y) const\n        {return __x % __y;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(modulus);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS modulus<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _T1, class _T2>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_T1&& __t, _T2&& __u) const\n        noexcept(noexcept(_CUDA_VSTD::forward<_T1>(__t) % _CUDA_VSTD::forward<_T2>(__u)))\n        -> decltype(      _CUDA_VSTD::forward<_T1>(__t) % _CUDA_VSTD::forward<_T2>(__u))\n        { return          _CUDA_VSTD::forward<_T1>(__t) % _CUDA_VSTD::forward<_T2>(__u); }\n    typedef void is_transparent;\n};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS negate\n    : __unary_function<_Tp, _Tp>\n{\n    typedef _Tp __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator()(const _Tp& __x) const\n        {return -__x;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(negate);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS negate<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _Tp>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_Tp&& __x) const\n        noexcept(noexcept(- _CUDA_VSTD::forward<_Tp>(__x)))\n        -> decltype(      - _CUDA_VSTD::forward<_Tp>(__x))\n        { return          - _CUDA_VSTD::forward<_Tp>(__x); }\n    typedef void is_transparent;\n};\n#endif\n\n// Bitwise operations\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS bit_and\n    : __binary_function<_Tp, _Tp, _Tp>\n{\n    typedef _Tp __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator()(const _Tp& __x, const _Tp& __y) const\n        {return __x & __y;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(bit_and);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS bit_and<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _T1, class _T2>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_T1&& __t, _T2&& __u) const\n        noexcept(noexcept(_CUDA_VSTD::forward<_T1>(__t) & _CUDA_VSTD::forward<_T2>(__u)))\n        -> decltype(      _CUDA_VSTD::forward<_T1>(__t) & _CUDA_VSTD::forward<_T2>(__u))\n        { return          _CUDA_VSTD::forward<_T1>(__t) & _CUDA_VSTD::forward<_T2>(__u); }\n    typedef void is_transparent;\n};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\nstruct _LIBCUDACXX_TEMPLATE_VIS bit_not\n    : __unary_function<_Tp, _Tp>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator()(const _Tp& __x) const\n        {return ~__x;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(bit_not);\n\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS bit_not<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _Tp>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_Tp&& __x) const\n        noexcept(noexcept(~_CUDA_VSTD::forward<_Tp>(__x)))\n        -> decltype(      ~_CUDA_VSTD::forward<_Tp>(__x))\n        { return          ~_CUDA_VSTD::forward<_Tp>(__x); }\n    typedef void is_transparent;\n};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS bit_or\n    : __binary_function<_Tp, _Tp, _Tp>\n{\n    typedef _Tp __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator()(const _Tp& __x, const _Tp& __y) const\n        {return __x | __y;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(bit_or);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS bit_or<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _T1, class _T2>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_T1&& __t, _T2&& __u) const\n        noexcept(noexcept(_CUDA_VSTD::forward<_T1>(__t) | _CUDA_VSTD::forward<_T2>(__u)))\n        -> decltype(      _CUDA_VSTD::forward<_T1>(__t) | _CUDA_VSTD::forward<_T2>(__u))\n        { return          _CUDA_VSTD::forward<_T1>(__t) | _CUDA_VSTD::forward<_T2>(__u); }\n    typedef void is_transparent;\n};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS bit_xor\n    : __binary_function<_Tp, _Tp, _Tp>\n{\n    typedef _Tp __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator()(const _Tp& __x, const _Tp& __y) const\n        {return __x ^ __y;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(bit_xor);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS bit_xor<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _T1, class _T2>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_T1&& __t, _T2&& __u) const\n        noexcept(noexcept(_CUDA_VSTD::forward<_T1>(__t) ^ _CUDA_VSTD::forward<_T2>(__u)))\n        -> decltype(      _CUDA_VSTD::forward<_T1>(__t) ^ _CUDA_VSTD::forward<_T2>(__u))\n        { return          _CUDA_VSTD::forward<_T1>(__t) ^ _CUDA_VSTD::forward<_T2>(__u); }\n    typedef void is_transparent;\n};\n#endif\n\n// Comparison operations\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS equal_to\n    : __binary_function<_Tp, _Tp, bool>\n{\n    typedef bool __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    bool operator()(const _Tp& __x, const _Tp& __y) const\n        {return __x == __y;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(equal_to);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS equal_to<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _T1, class _T2>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_T1&& __t, _T2&& __u) const\n        noexcept(noexcept(_CUDA_VSTD::forward<_T1>(__t) == _CUDA_VSTD::forward<_T2>(__u)))\n        -> decltype(      _CUDA_VSTD::forward<_T1>(__t) == _CUDA_VSTD::forward<_T2>(__u))\n        { return          _CUDA_VSTD::forward<_T1>(__t) == _CUDA_VSTD::forward<_T2>(__u); }\n    typedef void is_transparent;\n};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS not_equal_to\n    : __binary_function<_Tp, _Tp, bool>\n{\n    typedef bool __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    bool operator()(const _Tp& __x, const _Tp& __y) const\n        {return __x != __y;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(not_equal_to);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS not_equal_to<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _T1, class _T2>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_T1&& __t, _T2&& __u) const\n        noexcept(noexcept(_CUDA_VSTD::forward<_T1>(__t) != _CUDA_VSTD::forward<_T2>(__u)))\n        -> decltype(      _CUDA_VSTD::forward<_T1>(__t) != _CUDA_VSTD::forward<_T2>(__u))\n        { return          _CUDA_VSTD::forward<_T1>(__t) != _CUDA_VSTD::forward<_T2>(__u); }\n    typedef void is_transparent;\n};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS less\n    : __binary_function<_Tp, _Tp, bool>\n{\n    typedef bool __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    bool operator()(const _Tp& __x, const _Tp& __y) const\n        {return __x < __y;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(less);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS less<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _T1, class _T2>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_T1&& __t, _T2&& __u) const\n        noexcept(noexcept(_CUDA_VSTD::forward<_T1>(__t) < _CUDA_VSTD::forward<_T2>(__u)))\n        -> decltype(      _CUDA_VSTD::forward<_T1>(__t) < _CUDA_VSTD::forward<_T2>(__u))\n        { return          _CUDA_VSTD::forward<_T1>(__t) < _CUDA_VSTD::forward<_T2>(__u); }\n    typedef void is_transparent;\n};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS less_equal\n    : __binary_function<_Tp, _Tp, bool>\n{\n    typedef bool __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    bool operator()(const _Tp& __x, const _Tp& __y) const\n        {return __x <= __y;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(less_equal);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS less_equal<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _T1, class _T2>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_T1&& __t, _T2&& __u) const\n        noexcept(noexcept(_CUDA_VSTD::forward<_T1>(__t) <= _CUDA_VSTD::forward<_T2>(__u)))\n        -> decltype(      _CUDA_VSTD::forward<_T1>(__t) <= _CUDA_VSTD::forward<_T2>(__u))\n        { return          _CUDA_VSTD::forward<_T1>(__t) <= _CUDA_VSTD::forward<_T2>(__u); }\n    typedef void is_transparent;\n};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS greater_equal\n    : __binary_function<_Tp, _Tp, bool>\n{\n    typedef bool __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    bool operator()(const _Tp& __x, const _Tp& __y) const\n        {return __x >= __y;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(greater_equal);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS greater_equal<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _T1, class _T2>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_T1&& __t, _T2&& __u) const\n        noexcept(noexcept(_CUDA_VSTD::forward<_T1>(__t) >= _CUDA_VSTD::forward<_T2>(__u)))\n        -> decltype(      _CUDA_VSTD::forward<_T1>(__t) >= _CUDA_VSTD::forward<_T2>(__u))\n        { return          _CUDA_VSTD::forward<_T1>(__t) >= _CUDA_VSTD::forward<_T2>(__u); }\n    typedef void is_transparent;\n};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS greater\n    : __binary_function<_Tp, _Tp, bool>\n{\n    typedef bool __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    bool operator()(const _Tp& __x, const _Tp& __y) const\n        {return __x > __y;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(greater);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS greater<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _T1, class _T2>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_T1&& __t, _T2&& __u) const\n        noexcept(noexcept(_CUDA_VSTD::forward<_T1>(__t) > _CUDA_VSTD::forward<_T2>(__u)))\n        -> decltype(      _CUDA_VSTD::forward<_T1>(__t) > _CUDA_VSTD::forward<_T2>(__u))\n        { return          _CUDA_VSTD::forward<_T1>(__t) > _CUDA_VSTD::forward<_T2>(__u); }\n    typedef void is_transparent;\n};\n#endif\n\n// Logical operations\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS logical_and\n    : __binary_function<_Tp, _Tp, bool>\n{\n    typedef bool __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    bool operator()(const _Tp& __x, const _Tp& __y) const\n        {return __x && __y;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(logical_and);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS logical_and<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _T1, class _T2>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_T1&& __t, _T2&& __u) const\n        noexcept(noexcept(_CUDA_VSTD::forward<_T1>(__t) && _CUDA_VSTD::forward<_T2>(__u)))\n        -> decltype(      _CUDA_VSTD::forward<_T1>(__t) && _CUDA_VSTD::forward<_T2>(__u))\n        { return          _CUDA_VSTD::forward<_T1>(__t) && _CUDA_VSTD::forward<_T2>(__u); }\n    typedef void is_transparent;\n};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS logical_not\n    : __unary_function<_Tp, bool>\n{\n    typedef bool __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    bool operator()(const _Tp& __x) const\n        {return !__x;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(logical_not);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS logical_not<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _Tp>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_Tp&& __x) const\n        noexcept(noexcept(!_CUDA_VSTD::forward<_Tp>(__x)))\n        -> decltype(      !_CUDA_VSTD::forward<_Tp>(__x))\n        { return          !_CUDA_VSTD::forward<_Tp>(__x); }\n    typedef void is_transparent;\n};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp = void>\n#else\ntemplate <class _Tp>\n#endif\nstruct _LIBCUDACXX_TEMPLATE_VIS logical_or\n    : __binary_function<_Tp, _Tp, bool>\n{\n    typedef bool __result_type;  // used by valarray\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    bool operator()(const _Tp& __x, const _Tp& __y) const\n        {return __x || __y;}\n};\n_LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(logical_or);\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS logical_or<void>\n{\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    template <class _T1, class _T2>\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    auto operator()(_T1&& __t, _T2&& __u) const\n        noexcept(noexcept(_CUDA_VSTD::forward<_T1>(__t) || _CUDA_VSTD::forward<_T2>(__u)))\n        -> decltype(      _CUDA_VSTD::forward<_T1>(__t) || _CUDA_VSTD::forward<_T2>(__u))\n        { return          _CUDA_VSTD::forward<_T1>(__t) || _CUDA_VSTD::forward<_T2>(__u); }\n    typedef void is_transparent;\n};\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FUNCTIONAL_OPERATIONS_H\n", "__functional/perfect_forward.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_PERFECT_FORWARD_H\n#define _LIBCUDACXX___FUNCTIONAL_PERFECT_FORWARD_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__concepts/__concept_macros.h\"\n#include \"../__functional/invoke.h\"\n#include \"../__type_traits/enable_if.h\"\n#include \"../__type_traits/is_constructible.h\"\n#include \"../__type_traits/is_nothrow_constructible.h\"\n#include \"../__utility/declval.h\"\n#include \"../__utility/forward.h\"\n#include \"../__utility/integer_sequence.h\"\n#include \"../__utility/move.h\"\n\n#include \"../tuple\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 14\n\ntemplate <class _Op, class _Indices, class... _BoundArgs>\nstruct __perfect_forward_impl;\n\ntemplate <class _Op, size_t... _Idx, class... _BoundArgs>\nstruct __perfect_forward_impl<_Op, index_sequence<_Idx...>, _BoundArgs...> {\nprivate:\n  tuple<_BoundArgs...> __bound_args_;\n\npublic:\n  _LIBCUDACXX_TEMPLATE(class... _Args)\n    (requires is_constructible_v<tuple<_BoundArgs...>, _Args&&...>)\n  _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY\n  explicit constexpr __perfect_forward_impl(_Args&&... __bound_args)\n    noexcept(is_nothrow_constructible_v<tuple<_BoundArgs...>, _Args&&...>)\n    : __bound_args_(_CUDA_VSTD::forward<_Args>(__bound_args)...) {}\n\n  __perfect_forward_impl(__perfect_forward_impl const&) = default;\n  __perfect_forward_impl(__perfect_forward_impl&&) = default;\n\n  __perfect_forward_impl& operator=(__perfect_forward_impl const&) = default;\n  __perfect_forward_impl& operator=(__perfect_forward_impl&&) = default;\n\n  _LIBCUDACXX_TEMPLATE(class... _Args)\n    (requires is_invocable_v<_Op, _BoundArgs&..., _Args...>)\n  _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY constexpr auto operator()(_Args&&... __args) &\n    noexcept(noexcept(_Op()(_CUDA_VSTD::get<_Idx>(__bound_args_)..., _CUDA_VSTD::forward<_Args>(__args)...)))\n    -> decltype(      _Op()(_CUDA_VSTD::get<_Idx>(__bound_args_)..., _CUDA_VSTD::forward<_Args>(__args)...))\n    { return          _Op()(_CUDA_VSTD::get<_Idx>(__bound_args_)..., _CUDA_VSTD::forward<_Args>(__args)...); }\n\n  _LIBCUDACXX_TEMPLATE(class... _Args)\n    (requires (!is_invocable_v<_Op, _BoundArgs&..., _Args...>))\n  _LIBCUDACXX_INLINE_VISIBILITY auto operator()(_Args&&...) & = delete;\n\n  _LIBCUDACXX_TEMPLATE(class... _Args)\n    (requires is_invocable_v<_Op, _BoundArgs const&..., _Args...>)\n  _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY constexpr auto operator()(_Args&&... __args) const&\n    noexcept(noexcept(_Op()(_CUDA_VSTD::get<_Idx>(__bound_args_)..., _CUDA_VSTD::forward<_Args>(__args)...)))\n    -> decltype(      _Op()(_CUDA_VSTD::get<_Idx>(__bound_args_)..., _CUDA_VSTD::forward<_Args>(__args)...))\n    { return          _Op()(_CUDA_VSTD::get<_Idx>(__bound_args_)..., _CUDA_VSTD::forward<_Args>(__args)...); }\n\n  _LIBCUDACXX_TEMPLATE(class... _Args)\n    (requires (!is_invocable_v<_Op, _BoundArgs const&..., _Args...>))\n  _LIBCUDACXX_INLINE_VISIBILITY auto operator()(_Args&&...) const& = delete;\n\n  _LIBCUDACXX_TEMPLATE(class... _Args)\n    (requires is_invocable_v<_Op, _BoundArgs..., _Args...>)\n  _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY constexpr auto operator()(_Args&&... __args) &&\n    noexcept(noexcept(_Op()(_CUDA_VSTD::get<_Idx>(_CUDA_VSTD::move(__bound_args_))..., _CUDA_VSTD::forward<_Args>(__args)...)))\n    -> decltype(      _Op()(_CUDA_VSTD::get<_Idx>(_CUDA_VSTD::move(__bound_args_))..., _CUDA_VSTD::forward<_Args>(__args)...))\n    { return          _Op()(_CUDA_VSTD::get<_Idx>(_CUDA_VSTD::move(__bound_args_))..., _CUDA_VSTD::forward<_Args>(__args)...); }\n\n  _LIBCUDACXX_TEMPLATE(class... _Args)\n    (requires (!is_invocable_v<_Op, _BoundArgs..., _Args...>))\n  _LIBCUDACXX_INLINE_VISIBILITY auto operator()(_Args&&...) && = delete;\n\n  _LIBCUDACXX_TEMPLATE(class... _Args)\n    (requires is_invocable_v<_Op, _BoundArgs const..., _Args...>)\n  _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY constexpr auto operator()(_Args&&... __args) const&&\n    noexcept(noexcept(_Op()(_CUDA_VSTD::get<_Idx>(_CUDA_VSTD::move(__bound_args_))..., _CUDA_VSTD::forward<_Args>(__args)...)))\n    -> decltype(      _Op()(_CUDA_VSTD::get<_Idx>(_CUDA_VSTD::move(__bound_args_))..., _CUDA_VSTD::forward<_Args>(__args)...))\n    { return          _Op()(_CUDA_VSTD::get<_Idx>(_CUDA_VSTD::move(__bound_args_))..., _CUDA_VSTD::forward<_Args>(__args)...); }\n\n  _LIBCUDACXX_TEMPLATE(class... _Args)\n    (requires (!is_invocable_v<_Op, _BoundArgs const..., _Args...>))\n  _LIBCUDACXX_INLINE_VISIBILITY auto operator()(_Args&&...) const&& = delete;\n};\n\n// __perfect_forward implements a perfect-forwarding call wrapper as explained in [func.require].\ntemplate <class _Op, class ..._Args>\nusing __perfect_forward = __perfect_forward_impl<_Op, index_sequence_for<_Args...>, _Args...>;\n\n#endif // _LIBCUDACXX_STD_VER > 14\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FUNCTIONAL_PERFECT_FORWARD_H\n", "__functional/pointer_to_binary_function.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_POINTER_TO_BINARY_FUNCTION_H\n#define _LIBCUDACXX___FUNCTIONAL_POINTER_TO_BINARY_FUNCTION_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__functional/binary_function.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER <= 14 || defined(_LIBCUDACXX_ENABLE_CXX17_REMOVED_BINDERS)\n\ntemplate <class _Arg1, class _Arg2, class _Result>\nclass _LIBCUDACXX_TEMPLATE_VIS _LIBCUDACXX_DEPRECATED_IN_CXX11 pointer_to_binary_function\n    : public __binary_function<_Arg1, _Arg2, _Result>\n{\n    _Result (*__f_)(_Arg1, _Arg2);\npublic:\n    _LIBCUDACXX_INLINE_VISIBILITY explicit pointer_to_binary_function(_Result (*__f)(_Arg1, _Arg2))\n        : __f_(__f) {}\n    _LIBCUDACXX_INLINE_VISIBILITY _Result operator()(_Arg1 __x, _Arg2 __y) const\n        {return __f_(__x, __y);}\n};\n\ntemplate <class _Arg1, class _Arg2, class _Result>\n_LIBCUDACXX_DEPRECATED_IN_CXX11 inline _LIBCUDACXX_INLINE_VISIBILITY\npointer_to_binary_function<_Arg1,_Arg2,_Result>\nptr_fun(_Result (*__f)(_Arg1,_Arg2))\n    {return pointer_to_binary_function<_Arg1,_Arg2,_Result>(__f);}\n\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FUNCTIONAL_POINTER_TO_BINARY_FUNCTION_H\n", "__functional/pointer_to_unary_function.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_POINTER_TO_UNARY_FUNCTION_H\n#define _LIBCUDACXX___FUNCTIONAL_POINTER_TO_UNARY_FUNCTION_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__functional/unary_function.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER <= 14 || defined(_LIBCUDACXX_ENABLE_CXX17_REMOVED_BINDERS)\n\ntemplate <class _Arg, class _Result>\nclass _LIBCUDACXX_TEMPLATE_VIS _LIBCUDACXX_DEPRECATED_IN_CXX11 pointer_to_unary_function\n    : public __unary_function<_Arg, _Result>\n{\n    _Result (*__f_)(_Arg);\npublic:\n    _LIBCUDACXX_INLINE_VISIBILITY explicit pointer_to_unary_function(_Result (*__f)(_Arg))\n        : __f_(__f) {}\n    _LIBCUDACXX_INLINE_VISIBILITY _Result operator()(_Arg __x) const\n        {return __f_(__x);}\n};\n\ntemplate <class _Arg, class _Result>\n_LIBCUDACXX_DEPRECATED_IN_CXX11 inline _LIBCUDACXX_INLINE_VISIBILITY\npointer_to_unary_function<_Arg,_Result>\nptr_fun(_Result (*__f)(_Arg))\n    {return pointer_to_unary_function<_Arg,_Result>(__f);}\n\n#endif // _LIBCUDACXX_STD_VER <= 14 || defined(_LIBCUDACXX_ENABLE_CXX17_REMOVED_BINDERS)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FUNCTIONAL_POINTER_TO_UNARY_FUNCTION_H\n", "__functional/reference_wrapper.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_REFERENCE_WRAPPER_H\n#define _LIBCUDACXX___FUNCTIONAL_REFERENCE_WRAPPER_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__functional/weak_result_type.h\"\n#include \"../__memory/addressof.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp>\nclass _LIBCUDACXX_TEMPLATE_VIS reference_wrapper : public __weak_result_type<_Tp>\n{\npublic:\n    // types\n    typedef _Tp type;\nprivate:\n    type* __f_;\n\n    static _LIBCUDACXX_INLINE_VISIBILITY void __fun(_Tp&) noexcept;\n    static void __fun(_Tp&&) = delete;\n\npublic:\n    template <class _Up, class = __enable_if_t<!__is_same_uncvref<_Up, reference_wrapper>::value, decltype(__fun(declval<_Up>())) > >\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n    reference_wrapper(_Up&& __u) noexcept(noexcept(__fun(declval<_Up>()))) {\n        type& __f = static_cast<_Up&&>(__u);\n        __f_ = _CUDA_VSTD::addressof(__f);\n    }\n\n    // access\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n    operator type&() const noexcept {return *__f_;}\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n    type& get() const noexcept {return *__f_;}\n\n    // invoke\n    template <class... _ArgTypes>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n    typename __invoke_of<type&, _ArgTypes...>::type\n    operator() (_ArgTypes&&... __args) const {\n        return _CUDA_VSTD::__invoke(get(), _CUDA_VSTD::forward<_ArgTypes>(__args)...);\n    }\n};\n\n#if _LIBCUDACXX_STD_VER > 14 && !defined(_LIBCUDACXX_HAS_NO_DEDUCTION_GUIDES)\ntemplate <class _Tp>\nreference_wrapper(_Tp&) -> reference_wrapper<_Tp>;\n#endif\n\ntemplate <class _Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\nreference_wrapper<_Tp>\nref(_Tp& __t) noexcept\n{\n    return reference_wrapper<_Tp>(__t);\n}\n\ntemplate <class _Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\nreference_wrapper<_Tp>\nref(reference_wrapper<_Tp> __t) noexcept\n{\n    return __t;\n}\n\ntemplate <class _Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\nreference_wrapper<const _Tp>\ncref(const _Tp& __t) noexcept\n{\n    return reference_wrapper<const _Tp>(__t);\n}\n\ntemplate <class _Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\nreference_wrapper<const _Tp>\ncref(reference_wrapper<_Tp> __t) noexcept\n{\n    return __t;\n}\n\ntemplate <class _Tp> void ref(const _Tp&&) = delete;\ntemplate <class _Tp> void cref(const _Tp&&) = delete;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FUNCTIONAL_REFERENCE_WRAPPER_H\n", "__functional/unary_function.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_UNARY_FUNCTION_H\n#define _LIBCUDACXX___FUNCTIONAL_UNARY_FUNCTION_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER <= 14 || defined(_LIBCUDACXX_ENABLE_CXX17_REMOVED_UNARY_BINARY_FUNCTION)\n\ntemplate <class _Arg, class _Result>\nstruct _LIBCUDACXX_TEMPLATE_VIS _LIBCUDACXX_DEPRECATED_IN_CXX11 unary_function\n{\n    typedef _Arg    argument_type;\n    typedef _Result result_type;\n};\n\n#endif // _LIBCUDACXX_STD_VER <= 14\n\ntemplate <class _Arg, class _Result> struct __unary_function_keep_layout_base {\n#if _LIBCUDACXX_STD_VER <= 17 || defined(_LIBCUDACXX_ENABLE_CXX20_REMOVED_BINDER_TYPEDEFS)\n  using argument_type _LIBCUDACXX_DEPRECATED_IN_CXX17 = _Arg;\n  using result_type _LIBCUDACXX_DEPRECATED_IN_CXX17 = _Result;\n#endif\n};\n\n#if _LIBCUDACXX_STD_VER <= 14 || defined(_LIBCUDACXX_ENABLE_CXX17_REMOVED_UNARY_BINARY_FUNCTION)\n_LIBCUDACXX_DIAGNOSTIC_PUSH\n_LIBCUDACXX_CLANG_DIAGNOSTIC_IGNORED(\"-Wdeprecated-declarations\")\ntemplate <class _Arg, class _Result>\nusing __unary_function = unary_function<_Arg, _Result>;\n_LIBCUDACXX_DIAGNOSTIC_POP\n#else\ntemplate <class _Arg, class _Result>\nusing __unary_function = __unary_function_keep_layout_base<_Arg, _Result>;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FUNCTIONAL_UNARY_FUNCTION_H\n", "__functional/unary_negate.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_UNARY_NEGATE_H\n#define _LIBCUDACXX___FUNCTIONAL_UNARY_NEGATE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__functional/unary_function.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER <= 17 || defined(_LIBCUDACXX_ENABLE_CXX20_REMOVED_NEGATORS)\n\ntemplate <class _Predicate>\nclass _LIBCUDACXX_TEMPLATE_VIS _LIBCUDACXX_DEPRECATED_IN_CXX17 unary_negate\n    : public __unary_function<typename _Predicate::argument_type, bool>\n{\n    _Predicate __pred_;\npublic:\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    explicit unary_negate(const _Predicate& __pred)\n        : __pred_(__pred) {}\n    _LIBCUDACXX_DISABLE_EXEC_CHECK\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n    bool operator()(const typename _Predicate::argument_type& __x) const\n        {return !__pred_(__x);}\n};\n\ntemplate <class _Predicate>\n_LIBCUDACXX_DEPRECATED_IN_CXX17 inline _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\nunary_negate<_Predicate>\nnot1(const _Predicate& __pred) {return unary_negate<_Predicate>(__pred);}\n\n#endif // _LIBCUDACXX_STD_VER <= 17 || defined(_LIBCUDACXX_ENABLE_CXX20_REMOVED_NEGATORS)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FUNCTIONAL_UNARY_NEGATE_H\n", "__functional/unwrap_ref.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_UNWRAP_REF_H\n#define _LIBCUDACXX___FUNCTIONAL_UNWRAP_REF_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp>\nstruct __unwrap_reference { typedef _LIBCUDACXX_NODEBUG_TYPE _Tp type; };\n\ntemplate <class _Tp>\nclass reference_wrapper;\n\ntemplate <class _Tp>\nstruct __unwrap_reference<reference_wrapper<_Tp> > { typedef _LIBCUDACXX_NODEBUG_TYPE _Tp& type; };\n\ntemplate <class _Tp>\nstruct decay;\n\n#if _LIBCUDACXX_STD_VER > 17\ntemplate <class _Tp>\nstruct unwrap_reference : __unwrap_reference<_Tp> { };\n\ntemplate <class _Tp>\nusing unwrap_reference_t = typename unwrap_reference<_Tp>::type;\n\ntemplate <class _Tp>\nstruct unwrap_ref_decay : unwrap_reference<typename decay<_Tp>::type> { };\n\ntemplate <class _Tp>\nusing unwrap_ref_decay_t = typename unwrap_ref_decay<_Tp>::type;\n#endif // _LIBCUDACXX_STD_VER > 17\n\ntemplate <class _Tp>\nstruct __unwrap_ref_decay\n#if _LIBCUDACXX_STD_VER > 17\n    : unwrap_ref_decay<_Tp>\n#else\n    : __unwrap_reference<typename decay<_Tp>::type>\n#endif\n{ };\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FUNCTIONAL_UNWRAP_REF_H\n", "__functional/weak_result_type.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FUNCTIONAL_WEAK_RESULT_TYPE_H\n#define _LIBCUDACXX___FUNCTIONAL_WEAK_RESULT_TYPE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__functional/binary_function.h\"\n#include \"../__functional/invoke.h\"\n#include \"../__functional/unary_function.h\"\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_same.h\"\n#include \"../__utility/declval.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp>\nstruct __has_result_type\n{\nprivate:\n    template <class _Up> _LIBCUDACXX_INLINE_VISIBILITY static false_type __test(...);\n    template <class _Up> _LIBCUDACXX_INLINE_VISIBILITY static true_type __test(typename _Up::result_type* = 0);\npublic:\n    static const bool value = decltype(__test<_Tp>(0))::value;\n};\n\n// __weak_result_type\n\ntemplate <class _Tp>\nstruct __derives_from_unary_function\n{\nprivate:\n    struct __two {char __lx; char __lxx;};\n    static _LIBCUDACXX_INLINE_VISIBILITY __two __test(...);\n    template <class _Ap, class _Rp>\n        static _LIBCUDACXX_INLINE_VISIBILITY __unary_function<_Ap, _Rp>\n        __test(const volatile __unary_function<_Ap, _Rp>*);\n\npublic:\n    static const bool value = !is_same<decltype(__test((_Tp*)0)), __two>::value;\n    typedef decltype(__test((_Tp*)0)) type;\n};\n\ntemplate <class _Tp>\nstruct __derives_from_binary_function\n{\nprivate:\n    struct __two {char __lx; char __lxx;};\n    static __two _LIBCUDACXX_INLINE_VISIBILITY __test(...);\n    template <class _A1, class _A2, class _Rp>\n        static _LIBCUDACXX_INLINE_VISIBILITY __binary_function<_A1, _A2, _Rp>\n        __test(const volatile __binary_function<_A1, _A2, _Rp>*);\n\npublic:\n    static const bool value = !is_same<decltype(__test((_Tp*)0)), __two>::value;\n    typedef decltype(__test((_Tp*)0)) type;\n};\n\ntemplate <class _Tp, bool = __derives_from_unary_function<_Tp>::value>\nstruct __maybe_derive_from_unary_function  // bool is true\n    : public __derives_from_unary_function<_Tp>::type\n{\n};\n\ntemplate <class _Tp>\nstruct __maybe_derive_from_unary_function<_Tp, false>\n{\n};\n\ntemplate <class _Tp, bool = __derives_from_binary_function<_Tp>::value>\nstruct __maybe_derive_from_binary_function  // bool is true\n    : public __derives_from_binary_function<_Tp>::type\n{\n};\n\ntemplate <class _Tp>\nstruct __maybe_derive_from_binary_function<_Tp, false>\n{\n};\n\ntemplate <class _Tp, bool = __has_result_type<_Tp>::value>\nstruct __weak_result_type_imp // bool is true\n    : public __maybe_derive_from_unary_function<_Tp>,\n      public __maybe_derive_from_binary_function<_Tp>\n{\n#if _LIBCUDACXX_STD_VER <= 17 || defined(_LIBCUDACXX_ENABLE_CXX20_REMOVED_BINDER_TYPEDEFS)\n    using result_type _LIBCUDACXX_NODEBUG_TYPE _LIBCUDACXX_DEPRECATED_IN_CXX17 = typename _Tp::result_type;\n#endif\n};\n\ntemplate <class _Tp>\nstruct __weak_result_type_imp<_Tp, false>\n    : public __maybe_derive_from_unary_function<_Tp>,\n      public __maybe_derive_from_binary_function<_Tp>\n{\n};\n\ntemplate <class _Tp>\nstruct __weak_result_type\n    : public __weak_result_type_imp<_Tp>\n{\n};\n\n// 0 argument case\n\ntemplate <class _Rp>\nstruct __weak_result_type<_Rp ()>\n{\n#if _LIBCUDACXX_STD_VER <= 17 || defined(_LIBCUDACXX_ENABLE_CXX20_REMOVED_BINDER_TYPEDEFS)\n    using result_type _LIBCUDACXX_NODEBUG_TYPE _LIBCUDACXX_DEPRECATED_IN_CXX17 = _Rp;\n#endif\n};\n\ntemplate <class _Rp>\nstruct __weak_result_type<_Rp (&)()>\n{\n#if _LIBCUDACXX_STD_VER <= 17 || defined(_LIBCUDACXX_ENABLE_CXX20_REMOVED_BINDER_TYPEDEFS)\n    using result_type _LIBCUDACXX_NODEBUG_TYPE _LIBCUDACXX_DEPRECATED_IN_CXX17 = _Rp;\n#endif\n};\n\ntemplate <class _Rp>\nstruct __weak_result_type<_Rp (*)()>\n{\n#if _LIBCUDACXX_STD_VER <= 17 || defined(_LIBCUDACXX_ENABLE_CXX20_REMOVED_BINDER_TYPEDEFS)\n    using result_type _LIBCUDACXX_NODEBUG_TYPE _LIBCUDACXX_DEPRECATED_IN_CXX17 = _Rp;\n#endif\n};\n\n// 1 argument case\n\ntemplate <class _Rp, class _A1>\nstruct __weak_result_type<_Rp (_A1)>\n    : public __unary_function<_A1, _Rp>\n{\n};\n\ntemplate <class _Rp, class _A1>\nstruct __weak_result_type<_Rp (&)(_A1)>\n    : public __unary_function<_A1, _Rp>\n{\n};\n\ntemplate <class _Rp, class _A1>\nstruct __weak_result_type<_Rp (*)(_A1)>\n    : public __unary_function<_A1, _Rp>\n{\n};\n\ntemplate <class _Rp, class _Cp>\nstruct __weak_result_type<_Rp (_Cp::*)()>\n    : public __unary_function<_Cp*, _Rp>\n{\n};\n\ntemplate <class _Rp, class _Cp>\nstruct __weak_result_type<_Rp (_Cp::*)() const>\n    : public __unary_function<const _Cp*, _Rp>\n{\n};\n\ntemplate <class _Rp, class _Cp>\nstruct __weak_result_type<_Rp (_Cp::*)() volatile>\n    : public __unary_function<volatile _Cp*, _Rp>\n{\n};\n\ntemplate <class _Rp, class _Cp>\nstruct __weak_result_type<_Rp (_Cp::*)() const volatile>\n    : public __unary_function<const volatile _Cp*, _Rp>\n{\n};\n\n// 2 argument case\n\ntemplate <class _Rp, class _A1, class _A2>\nstruct __weak_result_type<_Rp (_A1, _A2)>\n    : public __binary_function<_A1, _A2, _Rp>\n{\n};\n\ntemplate <class _Rp, class _A1, class _A2>\nstruct __weak_result_type<_Rp (*)(_A1, _A2)>\n    : public __binary_function<_A1, _A2, _Rp>\n{\n};\n\ntemplate <class _Rp, class _A1, class _A2>\nstruct __weak_result_type<_Rp (&)(_A1, _A2)>\n    : public __binary_function<_A1, _A2, _Rp>\n{\n};\n\ntemplate <class _Rp, class _Cp, class _A1>\nstruct __weak_result_type<_Rp (_Cp::*)(_A1)>\n    : public __binary_function<_Cp*, _A1, _Rp>\n{\n};\n\ntemplate <class _Rp, class _Cp, class _A1>\nstruct __weak_result_type<_Rp (_Cp::*)(_A1) const>\n    : public __binary_function<const _Cp*, _A1, _Rp>\n{\n};\n\ntemplate <class _Rp, class _Cp, class _A1>\nstruct __weak_result_type<_Rp (_Cp::*)(_A1) volatile>\n    : public __binary_function<volatile _Cp*, _A1, _Rp>\n{\n};\n\ntemplate <class _Rp, class _Cp, class _A1>\nstruct __weak_result_type<_Rp (_Cp::*)(_A1) const volatile>\n    : public __binary_function<const volatile _Cp*, _A1, _Rp>\n{\n};\n\n// 3 or more arguments\n\ntemplate <class _Rp, class _A1, class _A2, class _A3, class ..._A4>\nstruct __weak_result_type<_Rp (_A1, _A2, _A3, _A4...)>\n{\n#if _LIBCUDACXX_STD_VER <= 17 || defined(_LIBCUDACXX_ENABLE_CXX20_REMOVED_BINDER_TYPEDEFS)\n    using result_type _LIBCUDACXX_NODEBUG_TYPE _LIBCUDACXX_DEPRECATED_IN_CXX17 = _Rp;\n#endif\n};\n\ntemplate <class _Rp, class _A1, class _A2, class _A3, class ..._A4>\nstruct __weak_result_type<_Rp (&)(_A1, _A2, _A3, _A4...)>\n{\n#if _LIBCUDACXX_STD_VER <= 17 || defined(_LIBCUDACXX_ENABLE_CXX20_REMOVED_BINDER_TYPEDEFS)\n    using result_type _LIBCUDACXX_NODEBUG_TYPE _LIBCUDACXX_DEPRECATED_IN_CXX17 = _Rp;\n#endif\n};\n\ntemplate <class _Rp, class _A1, class _A2, class _A3, class ..._A4>\nstruct __weak_result_type<_Rp (*)(_A1, _A2, _A3, _A4...)>\n{\n#if _LIBCUDACXX_STD_VER <= 17 || defined(_LIBCUDACXX_ENABLE_CXX20_REMOVED_BINDER_TYPEDEFS)\n    using result_type _LIBCUDACXX_NODEBUG_TYPE _LIBCUDACXX_DEPRECATED_IN_CXX17 = _Rp;\n#endif\n};\n\ntemplate <class _Rp, class _Cp, class _A1, class _A2, class ..._A3>\nstruct __weak_result_type<_Rp (_Cp::*)(_A1, _A2, _A3...)>\n{\n#if _LIBCUDACXX_STD_VER <= 17 || defined(_LIBCUDACXX_ENABLE_CXX20_REMOVED_BINDER_TYPEDEFS)\n    using result_type _LIBCUDACXX_NODEBUG_TYPE _LIBCUDACXX_DEPRECATED_IN_CXX17 = _Rp;\n#endif\n};\n\ntemplate <class _Rp, class _Cp, class _A1, class _A2, class ..._A3>\nstruct __weak_result_type<_Rp (_Cp::*)(_A1, _A2, _A3...) const>\n{\n#if _LIBCUDACXX_STD_VER <= 17 || defined(_LIBCUDACXX_ENABLE_CXX20_REMOVED_BINDER_TYPEDEFS)\n    using result_type _LIBCUDACXX_NODEBUG_TYPE _LIBCUDACXX_DEPRECATED_IN_CXX17 = _Rp;\n#endif\n};\n\ntemplate <class _Rp, class _Cp, class _A1, class _A2, class ..._A3>\nstruct __weak_result_type<_Rp (_Cp::*)(_A1, _A2, _A3...) volatile>\n{\n#if _LIBCUDACXX_STD_VER <= 17 || defined(_LIBCUDACXX_ENABLE_CXX20_REMOVED_BINDER_TYPEDEFS)\n    using result_type _LIBCUDACXX_NODEBUG_TYPE _LIBCUDACXX_DEPRECATED_IN_CXX17 = _Rp;\n#endif\n};\n\ntemplate <class _Rp, class _Cp, class _A1, class _A2, class ..._A3>\nstruct __weak_result_type<_Rp (_Cp::*)(_A1, _A2, _A3...) const volatile>\n{\n#if _LIBCUDACXX_STD_VER <= 17 || defined(_LIBCUDACXX_ENABLE_CXX20_REMOVED_BINDER_TYPEDEFS)\n    using result_type _LIBCUDACXX_NODEBUG_TYPE _LIBCUDACXX_DEPRECATED_IN_CXX17 = _Rp;\n#endif\n};\n\ntemplate <class _Tp, class ..._Args>\nstruct __invoke_return\n{\n    typedef decltype(_CUDA_VSTD::__invoke(declval<_Tp>(), declval<_Args>()...)) type;\n};\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FUNCTIONAL_WEAK_RESULT_TYPE_H\n", "__functional_base": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX_FUNCTIONAL_BASE\n#define _LIBCUDACXX_FUNCTIONAL_BASE\n\n#ifndef __cuda_std__\n#include <__config>\n#include <typeinfo>\n#include <exception>\n#include <new>\n#endif // __cuda_std__\n\n#include \"__functional/binary_function.h\"\n#include \"__functional/operations.h\"\n#include \"__functional/reference_wrapper.h\"\n#include \"__functional/unary_function.h\"\n#include \"__functional/weak_result_type.h\"\n#include \"__type_traits/integral_constant.h\"\n#include \"__type_traits/is_constructible.h\"\n#include \"__type_traits/is_convertible.h\"\n#include \"__type_traits/remove_cvref.h\"\n#include \"__utility/forward.h\"\n\n#ifndef __cuda_std__\n#include <__pragma_push>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n// allocator_arg_t\n\nstruct _LIBCUDACXX_TEMPLATE_VIS allocator_arg_t { explicit allocator_arg_t() = default; };\n\n#if defined(_LIBCUDACXX_BUILDING_LIBRARY)\nextern _LIBCUDACXX_EXPORTED_FROM_ABI const allocator_arg_t allocator_arg;\n#else\n/* _LIBCUDACXX_INLINE_VAR */ constexpr allocator_arg_t allocator_arg = allocator_arg_t();\n#endif\n\n// uses_allocator\n\ntemplate <class _Tp>\nstruct __has_allocator_type\n{\nprivate:\n    struct __two {char __lx; char __lxx;};\n    template <class _Up> _LIBCUDACXX_INLINE_VISIBILITY static __two __test(...);\n    template <class _Up> _LIBCUDACXX_INLINE_VISIBILITY static char __test(typename _Up::allocator_type* = 0);\npublic:\n    static const bool value = sizeof(__test<_Tp>(0)) == 1;\n};\n\ntemplate <class _Tp, class _Alloc, bool = __has_allocator_type<_Tp>::value>\nstruct __uses_allocator\n    : public integral_constant<bool,\n        is_convertible<_Alloc, typename _Tp::allocator_type>::value>\n{\n};\n\ntemplate <class _Tp, class _Alloc>\nstruct __uses_allocator<_Tp, _Alloc, false>\n    : public false_type\n{\n};\n\ntemplate <class _Tp, class _Alloc>\nstruct _LIBCUDACXX_TEMPLATE_VIS uses_allocator\n    : public __uses_allocator<_Tp, _Alloc>\n{\n};\n\n#if _LIBCUDACXX_STD_VER > 14\ntemplate <class _Tp, class _Alloc>\n_LIBCUDACXX_INLINE_VAR constexpr size_t uses_allocator_v = uses_allocator<_Tp, _Alloc>::value;\n#endif\n\n// allocator construction\n\ntemplate <class _Tp, class _Alloc, class ..._Args>\nstruct __uses_alloc_ctor_imp\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE __remove_cvref_t<_Alloc> _RawAlloc;\n    static const bool __ua = uses_allocator<_Tp, _RawAlloc>::value;\n    static const bool __ic =\n        is_constructible<_Tp, allocator_arg_t, _Alloc, _Args...>::value;\n    static const int value = __ua ? 2 - __ic : 0;\n};\n\ntemplate <class _Tp, class _Alloc, class ..._Args>\nstruct __uses_alloc_ctor\n    : integral_constant<int, __uses_alloc_ctor_imp<_Tp, _Alloc, _Args...>::value>\n    {};\n\ntemplate <class _Tp, class _Allocator, class... _Args>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nvoid __user_alloc_construct_impl (integral_constant<int, 0>, _Tp *__storage, const _Allocator &, _Args &&... __args )\n{\n    new (__storage) _Tp (_CUDA_VSTD::forward<_Args>(__args)...);\n}\n\n// FIXME: This should have a version which takes a non-const alloc.\ntemplate <class _Tp, class _Allocator, class... _Args>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nvoid __user_alloc_construct_impl (integral_constant<int, 1>, _Tp *__storage, const _Allocator &__a, _Args &&... __args )\n{\n    new (__storage) _Tp (allocator_arg, __a, _CUDA_VSTD::forward<_Args>(__args)...);\n}\n\n// FIXME: This should have a version which takes a non-const alloc.\ntemplate <class _Tp, class _Allocator, class... _Args>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nvoid __user_alloc_construct_impl (integral_constant<int, 2>, _Tp *__storage, const _Allocator &__a, _Args &&... __args )\n{\n    new (__storage) _Tp (_CUDA_VSTD::forward<_Args>(__args)..., __a);\n}\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#ifndef __cuda_std__\n#include <__pragma_pop>\n#endif // __cuda_std__\n\n#endif  // _LIBCUDACXX_FUNCTIONAL_BASE\n", "__fwd/array.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===---------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===---------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FWD_ARRAY_H\n#define _LIBCUDACXX___FWD_ARRAY_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp, size_t _Size>\nstruct _LIBCUDACXX_TEMPLATE_VIS array;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FWD_ARRAY_H\n", "__fwd/get.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FWD_GET_H\n#define _LIBCUDACXX___FWD_GET_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__fwd/array.h\"\n#include \"../__fwd/pair.h\"\n#include \"../__fwd/tuple.h\"\n#include \"../__tuple_dir/tuple_element.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <size_t _Ip, class ..._Tp>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\ntypename tuple_element<_Ip, tuple<_Tp...> >::type&\nget(tuple<_Tp...>&) noexcept;\n\ntemplate <size_t _Ip, class ..._Tp>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nconst typename tuple_element<_Ip, tuple<_Tp...> >::type&\nget(const tuple<_Tp...>&) noexcept;\n\ntemplate <size_t _Ip, class ..._Tp>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\ntypename tuple_element<_Ip, tuple<_Tp...> >::type&&\nget(tuple<_Tp...>&&) noexcept;\n\ntemplate <size_t _Ip, class ..._Tp>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nconst typename tuple_element<_Ip, tuple<_Tp...> >::type&&\nget(const tuple<_Tp...>&&) noexcept;\n\ntemplate <size_t _Ip, class _T1, class _T2>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\ntypename tuple_element<_Ip, pair<_T1, _T2> >::type&\nget(pair<_T1, _T2>&) noexcept;\n\ntemplate <size_t _Ip, class _T1, class _T2>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nconst typename tuple_element<_Ip, pair<_T1, _T2> >::type&\nget(const pair<_T1, _T2>&) noexcept;\n\ntemplate <size_t _Ip, class _T1, class _T2>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\ntypename tuple_element<_Ip, pair<_T1, _T2> >::type&&\nget(pair<_T1, _T2>&&) noexcept;\n\ntemplate <size_t _Ip, class _T1, class _T2>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nconst typename tuple_element<_Ip, pair<_T1, _T2> >::type&&\nget(const pair<_T1, _T2>&&) noexcept;\n\ntemplate <size_t _Ip, class _Tp, size_t _Size>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n_Tp&\nget(array<_Tp, _Size>&) noexcept;\n\ntemplate <size_t _Ip, class _Tp, size_t _Size>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nconst _Tp&\nget(const array<_Tp, _Size>&) noexcept;\n\ntemplate <size_t _Ip, class _Tp, size_t _Size>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n_Tp&&\nget(array<_Tp, _Size>&&) noexcept;\n\ntemplate <size_t _Ip, class _Tp, size_t _Size>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nconst _Tp&&\nget(const array<_Tp, _Size>&&) noexcept;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FWD_GET_H\n", "__fwd/string.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___FWD_STRING_H\n#define _LIBCUDACXX___FWD_STRING_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__fwd/memory_resource.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _CharT>\nstruct _LIBCUDACXX_TEMPLATE_VIS char_traits;\ntemplate <>\nstruct char_traits<char>;\n\n#ifndef _LIBCUDACXX_NO_HAS_CHAR8_T\ntemplate <>\nstruct char_traits<char8_t>;\n#endif\n\ntemplate <>\nstruct char_traits<char16_t>;\ntemplate <>\nstruct char_traits<char32_t>;\n\n#ifndef _LIBCUDACXX_HAS_NO_WIDE_CHARACTERS\ntemplate <>\nstruct char_traits<wchar_t>;\n#endif\n\ntemplate <class _Tp>\nclass _LIBCUDACXX_TEMPLATE_VIS allocator;\n\ntemplate <class _CharT, class _Traits = char_traits<_CharT>, class _Allocator = allocator<_CharT> >\nclass _LIBCUDACXX_TEMPLATE_VIS basic_string;\n\nusing string = basic_string<char>;\n\n#ifndef _LIBCUDACXX_HAS_NO_WIDE_CHARACTERS\nusing wstring = basic_string<wchar_t>;\n#endif\n\n#ifndef _LIBCUDACXX_NO_HAS_CHAR8_T\nusing u8string = basic_string<char8_t>;\n#endif\n\nusing u16string = basic_string<char16_t>;\nusing u32string = basic_string<char32_t>;\n\n#if _LIBCUDACXX_STD_VER >= 17\n\nnamespace pmr {\ntemplate <class _CharT, class _Traits = char_traits<_CharT>>\nusing basic_string = std::basic_string<_CharT, _Traits, polymorphic_allocator<_CharT>>;\n\nusing string = basic_string<char>;\n\n#  ifndef _LIBCUDACXX_HAS_NO_WIDE_CHARACTERS\nusing wstring = basic_string<wchar_t>;\n#  endif\n\n#  ifndef _LIBCUDACXX_NO_HAS_CHAR8_T\nusing u8string = basic_string<char8_t>;\n#  endif\n\nusing u16string = basic_string<char16_t>;\nusing u32string = basic_string<char32_t>;\n\n} // namespace pmr\n\n#endif // _LIBCUDACXX_STD_VER >= 17\n\n// clang-format off\ntemplate <class _CharT, class _Traits, class _Allocator>\nclass _LIBCUDACXX_PREFERRED_NAME(string)\n#ifndef _LIBCUDACXX_HAS_NO_WIDE_CHARACTERS\n      _LIBCUDACXX_PREFERRED_NAME(wstring)\n#endif\n#ifndef _LIBCUDACXX_NO_HAS_CHAR8_T\n      _LIBCUDACXX_PREFERRED_NAME(u8string)\n#endif\n      _LIBCUDACXX_PREFERRED_NAME(u16string)\n      _LIBCUDACXX_PREFERRED_NAME(u32string)\n#if _LIBCUDACXX_STD_VER >= 17\n      _LIBCUDACXX_PREFERRED_NAME(pmr::string)\n#  ifndef _LIBCUDACXX_HAS_NO_WIDE_CHARACTERS\n      _LIBCUDACXX_PREFERRED_NAME(pmr::wstring)\n#  endif\n#  ifndef _LIBCUDACXX_NO_HAS_CHAR8_T\n      _LIBCUDACXX_PREFERRED_NAME(pmr::u8string)\n#  endif\n      _LIBCUDACXX_PREFERRED_NAME(pmr::u16string)\n      _LIBCUDACXX_PREFERRED_NAME(pmr::u32string)\n#endif\n      basic_string;\n// clang-format on\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___FWD_STRING_H\n", "__memory/addressof.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___MEMORY_ADDRESSOF_H\n#define _LIBCUDACXX___MEMORY_ADDRESSOF_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n// addressof\n// NVCXX has the builtin defined but did not mark it as supported\n#if defined(_LIBCUDACXX_ADDRESSOF)\n\ntemplate <class _Tp>\ninline _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n_LIBCUDACXX_NO_CFI _LIBCUDACXX_INLINE_VISIBILITY\n_Tp*\naddressof(_Tp& __x) noexcept\n{\n    return __builtin_addressof(__x);\n}\n\n#else\n\ntemplate <class _Tp>\ninline _LIBCUDACXX_NO_CFI _LIBCUDACXX_INLINE_VISIBILITY\n_Tp*\naddressof(_Tp& __x) noexcept\n{\n  return reinterpret_cast<_Tp *>(\n      const_cast<char *>(&reinterpret_cast<const volatile char &>(__x)));\n}\n\n#endif // defined(_LIBCUDACXX_ADDRESSOF)\n\n#if defined(_LIBCUDACXX_HAS_OBJC_ARC) && !defined(_LIBCUDACXX_PREDEFINED_OBJC_ARC_ADDRESSOF)\n// Objective-C++ Automatic Reference Counting uses qualified pointers\n// that require special addressof() signatures. When\n// _LIBCUDACXX_PREDEFINED_OBJC_ARC_ADDRESSOF is defined, the compiler\n// itself is providing these definitions. Otherwise, we provide them.\ntemplate <class _Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY\n__strong _Tp*\naddressof(__strong _Tp& __x) noexcept\n{\n  return &__x;\n}\n\n#ifdef _LIBCUDACXX_HAS_OBJC_ARC_WEAK\ntemplate <class _Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY\n__weak _Tp*\naddressof(__weak _Tp& __x) noexcept\n{\n  return &__x;\n}\n#endif\n\ntemplate <class _Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY\n__autoreleasing _Tp*\naddressof(__autoreleasing _Tp& __x) noexcept\n{\n  return &__x;\n}\n\ntemplate <class _Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY\n__unsafe_unretained _Tp*\naddressof(__unsafe_unretained _Tp& __x) noexcept\n{\n  return &__x;\n}\n#endif\n\ntemplate <class _Tp> _Tp* addressof(const _Tp&&) noexcept = delete;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___MEMORY_ADDRESSOF_H\n", "__memory/construct_at.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___MEMORY_CONSTRUCT_AT_H\n#define _LIBCUDACXX___MEMORY_CONSTRUCT_AT_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__assert\"\n#include \"../__iterator/access.h\"\n#include \"../__memory/addressof.h\"\n#include \"../__memory/voidify.h\"\n#include \"../__type_traits/enable_if.h\"\n#include \"../__type_traits/is_array.h\"\n#include \"../__type_traits/is_constant_evaluated.h\"\n#include \"../__type_traits/is_trivially_move_assignable.h\"\n#include \"../__type_traits/is_trivially_constructible.h\"\n#include \"../__utility/forward.h\"\n#include \"../__utility/move.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n#if defined(__cuda_std__) && _LIBCUDACXX_STD_VER > 17 // need to backfill ::std::construct_at\n#ifndef _LIBCUDACXX_COMPILER_NVRTC\n#include <memory>\n#endif // _LIBCUDACXX_COMPILER_NVRTC\n\n#ifndef __cpp_lib_constexpr_dynamic_alloc\nnamespace std {\ntemplate <class _Tp, class... _Args, class = decltype(::new(_CUDA_VSTD::declval<void*>()) _Tp(_CUDA_VSTD::declval<_Args>()...))>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr _Tp* construct_at(_Tp* __location, _Args&&... __args) {\n#if defined(_LIBCUDACXX_ADDRESSOF)\n  return ::new (_CUDA_VSTD::__voidify(*__location)) _Tp(_CUDA_VSTD::forward<_Args>(__args)...);\n#else\n  return ::new (const_cast<void*>(static_cast<const volatile void*>(__location))) _Tp(_CUDA_VSTD::forward<_Args>(__args)...);\n#endif\n}\n} // namespace std\n#endif // __cpp_lib_constexpr_dynamic_alloc\n#endif // __cuda_std__ && _LIBCUDACXX_STD_VER > 17\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n// There is a performance issue with placement new, where EDG based compiler insert a nullptr check that is superfluous\n// Because this is a noticable performance regression, we specialize for trivially constructible types\n// This is possible because we are calling ::new ignoring any user defined overloads of operator placement new\n\n// construct_at\n#if _LIBCUDACXX_STD_VER > 17\n\n_LIBCUDACXX_DISABLE_EXEC_CHECK\ntemplate <class _Tp, class... _Args, class = decltype(::new(_CUDA_VSTD::declval<void*>()) _Tp(_CUDA_VSTD::declval<_Args>()...))>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n__enable_if_t<!is_trivially_constructible_v<_Tp, _Args...> ||\n              !is_trivially_move_assignable_v<_Tp>, _Tp*>\nconstruct_at(_Tp* __location, _Args&&... __args) {\n  _LIBCUDACXX_ASSERT(__location != nullptr, \"null pointer given to construct_at\");\n#if defined(__cuda_std__)\n  // Need to go through `std::construct_at` as that is the explicitly blessed function\n  if (__libcpp_is_constant_evaluated()) {\n    return ::std::construct_at(__location, _CUDA_VSTD::forward<_Args>(__args)...);\n  }\n#endif // __cuda_std__\n  return ::new (_CUDA_VSTD::__voidify(*__location)) _Tp(_CUDA_VSTD::forward<_Args>(__args)...);\n}\n\n_LIBCUDACXX_DISABLE_EXEC_CHECK\ntemplate <class _Tp, class... _Args, class = decltype(::new(_CUDA_VSTD::declval<void*>()) _Tp(_CUDA_VSTD::declval<_Args>()...))>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n__enable_if_t<is_trivially_constructible_v<_Tp, _Args...> &&\n              is_trivially_move_assignable_v<_Tp>, _Tp*>\nconstruct_at(_Tp* __location, _Args&&... __args) {\n  _LIBCUDACXX_ASSERT(__location != nullptr, \"null pointer given to construct_at\");\n#if defined(__cuda_std__)\n  // Need to go through `std::construct_at` as that is the explicitly blessed function\n  if (__libcpp_is_constant_evaluated()) {\n    return ::std::construct_at(__location, _CUDA_VSTD::forward<_Args>(__args)...);\n  }\n  *__location = _Tp{_CUDA_VSTD::forward<_Args>(__args)...};\n  return __location;\n#else // ^^^ __cuda_std__ ^^^ / vvv !__cuda_std__ vvv\n  // NVCC always considers construction + move assignment, other compilers are smarter using copy construction\n  // So rather than adding all kinds of workarounds simply fall back to the correct implementation for libcxx mode\n  return ::new (_CUDA_VSTD::__voidify(*__location)) _Tp(_CUDA_VSTD::forward<_Args>(__args)...);\n#endif // !__cuda_std__\n}\n\n#endif // _LIBCUDACXX_STD_VER > 17\n\n_LIBCUDACXX_DISABLE_EXEC_CHECK\ntemplate <class _Tp, class... _Args>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n__enable_if_t<!_LIBCUDACXX_TRAIT(is_trivially_constructible, _Tp, _Args...) || !_LIBCUDACXX_TRAIT(is_trivially_move_assignable, _Tp), _Tp*>\n__construct_at(_Tp* __location, _Args&&... __args) {\n  _LIBCUDACXX_ASSERT(__location != nullptr, \"null pointer given to construct_at\");\n#if defined(__cuda_std__) && _LIBCUDACXX_STD_VER > 17\n  // Need to go through `std::construct_at` as that is the explicitly blessed function\n  if (__libcpp_is_constant_evaluated()) {\n    return ::std::construct_at(__location, _CUDA_VSTD::forward<_Args>(__args)...);\n  }\n#endif // __cuda_std__ && _LIBCUDACXX_STD_VER > 17\n  return ::new (_CUDA_VSTD::__voidify(*__location)) _Tp(_CUDA_VSTD::forward<_Args>(__args)...);\n}\n\n_LIBCUDACXX_DISABLE_EXEC_CHECK\ntemplate <class _Tp, class... _Args>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n__enable_if_t<_LIBCUDACXX_TRAIT(is_trivially_constructible, _Tp, _Args...) && _LIBCUDACXX_TRAIT(is_trivially_move_assignable, _Tp), _Tp*>\n__construct_at(_Tp* __location, _Args&&... __args) {\n  _LIBCUDACXX_ASSERT(__location != nullptr, \"null pointer given to construct_at\");\n#if defined(__cuda_std__) && _LIBCUDACXX_STD_VER > 17\n  // Need to go through `std::construct_at` as that is the explicitly blessed function\n  if (__libcpp_is_constant_evaluated()) {\n    return ::std::construct_at(__location, _CUDA_VSTD::forward<_Args>(__args)...);\n  }\n#endif // __cuda_std__ && _LIBCUDACXX_STD_VER > 17\n  *__location = _Tp{_CUDA_VSTD::forward<_Args>(__args)...};\n  return __location;\n}\n\n// destroy_at\n\n// The internal functions are available regardless of the language version (with the exception of the `__destroy_at`\n// taking an array).\ntemplate <class _ForwardIterator>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n_ForwardIterator __destroy(_ForwardIterator, _ForwardIterator);\n\ntemplate <class _Tp, __enable_if_t<!is_array<_Tp>::value, int> = 0>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\nvoid __destroy_at(_Tp* __loc) {\n    _LIBCUDACXX_ASSERT(__loc != nullptr, \"null pointer given to destroy_at\");\n    __loc->~_Tp();\n}\n\n#if _LIBCUDACXX_STD_VER > 17\ntemplate <class _Tp, __enable_if_t<is_array<_Tp>::value, int> = 0>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\nvoid __destroy_at(_Tp* __loc) {\n    _LIBCUDACXX_ASSERT(__loc != nullptr, \"null pointer given to destroy_at\");\n    _CUDA_VSTD::__destroy(_CUDA_VSTD::begin(*__loc), _CUDA_VSTD::end(*__loc));\n}\n#endif\n\ntemplate <class _ForwardIterator>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n_ForwardIterator __destroy(_ForwardIterator __first, _ForwardIterator __last) {\n    for (; __first != __last; ++__first)\n        _CUDA_VSTD::__destroy_at(_CUDA_VSTD::addressof(*__first));\n    return __first;\n}\n\ntemplate <class _BidirectionalIterator>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n_BidirectionalIterator __reverse_destroy(_BidirectionalIterator __first, _BidirectionalIterator __last) {\n    while (__last != __first) {\n        --__last;\n        _CUDA_VSTD::__destroy_at(_CUDA_VSTD::addressof(*__last));\n    }\n    return __last;\n}\n\n#if _LIBCUDACXX_STD_VER > 14\n\ntemplate <class _Tp, enable_if_t<!is_array_v<_Tp>, int> = 0>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\nvoid destroy_at(_Tp* __loc) {\n  _LIBCUDACXX_ASSERT(__loc != nullptr, \"null pointer given to destroy_at\");\n  __loc->~_Tp();\n}\n\n#if _LIBCUDACXX_STD_VER > 17\ntemplate <class _Tp, enable_if_t<is_array_v<_Tp>, int> = 0>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\nvoid destroy_at(_Tp* __loc) {\n  _CUDA_VSTD::__destroy_at(__loc);\n}\n#endif // _LIBCUDACXX_STD_VER > 17\n\ntemplate <class _ForwardIterator>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\nvoid destroy(_ForwardIterator __first, _ForwardIterator __last) {\n  (void)_CUDA_VSTD::__destroy(_CUDA_VSTD::move(__first), _CUDA_VSTD::move(__last));\n}\n\ntemplate <class _ForwardIterator, class _Size>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n_ForwardIterator destroy_n(_ForwardIterator __first, _Size __n) {\n    for (; __n > 0; (void)++__first, --__n)\n        _CUDA_VSTD::__destroy_at(_CUDA_VSTD::addressof(*__first));\n    return __first;\n}\n\n#endif // _LIBCUDACXX_STD_VER > 14\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___MEMORY_CONSTRUCT_AT_H\n", "__memory/voidify.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___MEMORY_VOIDIFY_H\n#define _LIBCUDACXX___MEMORY_VOIDIFY_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"../__memory/addressof.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <typename _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17 void* __voidify(_Tp& __from) {\n  // Cast away cv-qualifiers to allow modifying elements of a range through const iterators.\n  return const_cast<void*>(static_cast<const volatile void*>(_CUDA_VSTD::addressof(__from)));\n}\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___MEMORY_VOIDIFY_H\n", "__threading_support": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX_THREADING_SUPPORT\n#define _LIBCUDACXX_THREADING_SUPPORT\n\n#ifndef __cuda_std__\n#include <__config>\n#include <errno.h>\n#else\n#include \"__cuda/atomic_prelude.h\"\n#endif // __cuda_std__\n\n#include \"__assert\" // all public C++ headers provide the assertion handler\n#include \"__functional/hash.h\"\n#include \"chrono\"\n#include \"climits\"\n#include \"iosfwd\"\n\n#ifndef __cuda_std__\n#include <__pragma_push>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n#if defined(_LIBCUDACXX_HAS_THREAD_API_EXTERNAL)\n# ifndef __cuda_std__\n#  include <__external_threading>\n# else\n#  define _LIBCUDACXX_THREAD_ABI_VISIBILITY inline _LIBCUDACXX_INLINE_VISIBILITY\n# endif\n#elif !defined(_LIBCUDACXX_HAS_NO_THREADS)\n\n#if defined(_LIBCUDACXX_HAS_THREAD_API_PTHREAD)\n# include <pthread.h>\n# include <sched.h>\n# include <semaphore.h>\n# if defined(__APPLE__)\n#  include <dispatch/dispatch.h>\n# endif\n# if defined(__linux__)\n#  include <unistd.h>\n#  include <linux/futex.h>\n#  include <sys/syscall.h>\n# endif\n#endif\n\n#if defined(_LIBCUDACXX_HAS_THREAD_API_WIN32)\n# include <process.h>\n# include <windows.h>\n#endif\n\n#if defined(_LIBCUDACXX_HAS_THREAD_LIBRARY_EXTERNAL) || \\\n    defined(_LIBCUDACXX_BUILDING_THREAD_LIBRARY_EXTERNAL)\n#define _LIBCUDACXX_THREAD_ABI_VISIBILITY _LIBCUDACXX_FUNC_VIS\n#else\n#define _LIBCUDACXX_THREAD_ABI_VISIBILITY inline _LIBCUDACXX_INLINE_VISIBILITY\n#endif\n\n#if defined(__FreeBSD__) && defined(_LIBCUDACXX_COMPILER_CLANG) && __has_attribute(no_thread_safety_analysis)\n#define _LIBCUDACXX_NO_THREAD_SAFETY_ANALYSIS __attribute__((no_thread_safety_analysis))\n#else\n#define _LIBCUDACXX_NO_THREAD_SAFETY_ANALYSIS\n#endif\n\ntypedef ::timespec __libcpp_timespec_t;\n#endif // !defined(_LIBCUDACXX_HAS_NO_THREADS)\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if !defined(_LIBCUDACXX_HAS_NO_THREADS)\n\n#define _LIBCUDACXX_POLLING_COUNT 16\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline void __libcpp_thread_yield_processor()\n{\n#if defined(__aarch64__)\n# define __LIBCUDACXX_ASM_THREAD_YIELD (asm volatile (\"yield\" :::);)\n#elif defined(__x86_64__)\n# define __LIBCUDACXX_ASM_THREAD_YIELD (asm volatile (\"pause\" :::);)\n#elif defined (__powerpc__)\n# define __LIBCUDACXX_ASM_THREAD_YIELD (asm volatile (\"or 27,27,27\":::);)\n#else\n# define __LIBCUDACXX_ASM_THREAD_YIELD (;)\n#endif\n    NV_IF_TARGET(\n        NV_IS_HOST,\n            __LIBCUDACXX_ASM_THREAD_YIELD\n    )\n}\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nvoid __libcpp_thread_yield();\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nvoid __libcpp_thread_sleep_for(chrono::nanoseconds __ns);\n\ntemplate<class _Fn>\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nbool __libcpp_thread_poll_with_backoff(_Fn && __f, chrono::nanoseconds __max = chrono::nanoseconds::zero());\n\n#if defined(_LIBCUDACXX_HAS_THREAD_API_PTHREAD)\n// Mutex\ntypedef pthread_mutex_t __libcpp_mutex_t;\n#define _LIBCUDACXX_MUTEX_INITIALIZER PTHREAD_MUTEX_INITIALIZER\n\ntypedef pthread_mutex_t __libcpp_recursive_mutex_t;\n\n// Condition Variable\ntypedef pthread_cond_t __libcpp_condvar_t;\n#define _LIBCUDACXX_CONDVAR_INITIALIZER PTHREAD_COND_INITIALIZER\n\n// Semaphore\n#if defined(__APPLE__)\ntypedef dispatch_semaphore_t __libcpp_semaphore_t;\n# define _LIBCUDACXX_SEMAPHORE_MAX numeric_limits<long>::max()\n#else\ntypedef sem_t __libcpp_semaphore_t;\n# define _LIBCUDACXX_SEMAPHORE_MAX SEM_VALUE_MAX\n#endif\n\n// Execute once\ntypedef pthread_once_t __libcpp_exec_once_flag;\n#define _LIBCUDACXX_EXEC_ONCE_INITIALIZER PTHREAD_ONCE_INIT\n\n// Thread id\ntypedef pthread_t __libcpp_thread_id;\n\n// Thread\n#define _LIBCUDACXX_NULL_THREAD 0U\n\ntypedef pthread_t __libcpp_thread_t;\n\n// Thread Local Storage\ntypedef pthread_key_t __libcpp_tls_key;\n\n#define _LIBCUDACXX_TLS_DESTRUCTOR_CC\n#elif !defined(_LIBCUDACXX_HAS_THREAD_API_EXTERNAL)\n// Mutex\ntypedef void* __libcpp_mutex_t;\n#define _LIBCUDACXX_MUTEX_INITIALIZER 0\n\n#if defined(_M_IX86) || defined(__i386__) || defined(_M_ARM) || defined(__arm__)\ntypedef void* __libcpp_recursive_mutex_t[6];\n#elif defined(_M_AMD64) || defined(__x86_64__) || defined(_M_ARM64) || defined(__aarch64__)\ntypedef void* __libcpp_recursive_mutex_t[5];\n#else\n# error Unsupported architecture\n#endif\n\n// Condition Variable\ntypedef void* __libcpp_condvar_t;\n#define _LIBCUDACXX_CONDVAR_INITIALIZER 0\n\n// Semaphore\ntypedef void* __libcpp_semaphore_t;\n\n// Execute Once\ntypedef void* __libcpp_exec_once_flag;\n#define _LIBCUDACXX_EXEC_ONCE_INITIALIZER 0\n\n// Thread ID\ntypedef long __libcpp_thread_id;\n\n// Thread\n#define _LIBCUDACXX_NULL_THREAD 0U\n\ntypedef void* __libcpp_thread_t;\n\n// Thread Local Storage\ntypedef long __libcpp_tls_key;\n\n#define _LIBCUDACXX_TLS_DESTRUCTOR_CC __stdcall\n#endif // !defined(_LIBCUDACXX_HAS_THREAD_API_PTHREAD) && !defined(_LIBCUDACXX_HAS_THREAD_API_EXTERNAL)\n\n#if !defined(_LIBCUDACXX_HAS_THREAD_API_EXTERNAL)\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\n__libcpp_timespec_t __libcpp_to_timespec(const chrono::nanoseconds& __ns);\n\n// Mutex\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nint __libcpp_recursive_mutex_init(__libcpp_recursive_mutex_t *__m);\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY _LIBCUDACXX_NO_THREAD_SAFETY_ANALYSIS\nint __libcpp_recursive_mutex_lock(__libcpp_recursive_mutex_t *__m);\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY _LIBCUDACXX_NO_THREAD_SAFETY_ANALYSIS\nbool __libcpp_recursive_mutex_trylock(__libcpp_recursive_mutex_t *__m);\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY _LIBCUDACXX_NO_THREAD_SAFETY_ANALYSIS\nint __libcpp_recursive_mutex_unlock(__libcpp_recursive_mutex_t *__m);\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nint __libcpp_recursive_mutex_destroy(__libcpp_recursive_mutex_t *__m);\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY _LIBCUDACXX_NO_THREAD_SAFETY_ANALYSIS\nint __libcpp_mutex_lock(__libcpp_mutex_t *__m);\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY _LIBCUDACXX_NO_THREAD_SAFETY_ANALYSIS\nbool __libcpp_mutex_trylock(__libcpp_mutex_t *__m);\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY _LIBCUDACXX_NO_THREAD_SAFETY_ANALYSIS\nint __libcpp_mutex_unlock(__libcpp_mutex_t *__m);\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nint __libcpp_mutex_destroy(__libcpp_mutex_t *__m);\n\n// Condition variable\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nint __libcpp_condvar_signal(__libcpp_condvar_t* __cv);\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nint __libcpp_condvar_broadcast(__libcpp_condvar_t* __cv);\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY _LIBCUDACXX_NO_THREAD_SAFETY_ANALYSIS\nint __libcpp_condvar_wait(__libcpp_condvar_t* __cv, __libcpp_mutex_t* __m);\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY _LIBCUDACXX_NO_THREAD_SAFETY_ANALYSIS\nint __libcpp_condvar_timedwait(__libcpp_condvar_t *__cv, __libcpp_mutex_t *__m,\n                               __libcpp_timespec_t *__ts);\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nint __libcpp_condvar_destroy(__libcpp_condvar_t* __cv);\n\n// Semaphore\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nbool __libcpp_semaphore_init(__libcpp_semaphore_t* __sem, int __init);\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nbool __libcpp_semaphore_destroy(__libcpp_semaphore_t* __sem);\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nbool __libcpp_semaphore_post(__libcpp_semaphore_t* __sem);\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nbool __libcpp_semaphore_wait(__libcpp_semaphore_t* __sem);\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nbool __libcpp_semaphore_wait_timed(__libcpp_semaphore_t* __sem, chrono::nanoseconds const& __ns);\n\n// Execute once\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nint __libcpp_execute_once(__libcpp_exec_once_flag *flag,\n                          void (*init_routine)());\n\n// Thread id\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nbool __libcpp_thread_id_equal(__libcpp_thread_id t1, __libcpp_thread_id t2);\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nbool __libcpp_thread_id_less(__libcpp_thread_id t1, __libcpp_thread_id t2);\n\n// Thread\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nbool __libcpp_thread_isnull(const __libcpp_thread_t *__t);\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nint __libcpp_thread_create(__libcpp_thread_t *__t, void *(*__func)(void *),\n                           void *__arg);\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\n__libcpp_thread_id __libcpp_thread_get_current_id();\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\n__libcpp_thread_id __libcpp_thread_get_id(const __libcpp_thread_t *__t);\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nint __libcpp_thread_join(__libcpp_thread_t *__t);\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nint __libcpp_thread_detach(__libcpp_thread_t *__t);\n\n// Thread local storage\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nint __libcpp_tls_create(__libcpp_tls_key* __key,\n                        void(_LIBCUDACXX_TLS_DESTRUCTOR_CC* __at_exit)(void*));\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nvoid *__libcpp_tls_get(__libcpp_tls_key __key);\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nint __libcpp_tls_set(__libcpp_tls_key __key, void *__p);\n\n#endif // !defined(_LIBCUDACXX_HAS_THREAD_API_EXTERNAL)\n\n#if !defined(_LIBCUDACXX_HAS_THREAD_LIBRARY_EXTERNAL) || defined(_LIBCUDACXX_BUILDING_THREAD_LIBRARY_EXTERNAL)\n\n#if defined(_LIBCUDACXX_HAS_THREAD_API_CUDA)\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nvoid __libcpp_thread_yield() {}\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nvoid __libcpp_thread_sleep_for(chrono::nanoseconds __ns)\n{\n    NV_IF_TARGET(\n        NV_IS_DEVICE, (\n            auto const __step = __ns.count();\n            assert(__step < numeric_limits<unsigned>::max());\n            asm volatile(\"nanosleep.u32 %0;\"::\"r\"((unsigned)__step):);\n        )\n    )\n}\n\n#elif defined(_LIBCUDACXX_HAS_THREAD_API_PTHREAD)\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\n__libcpp_timespec_t __libcpp_to_timespec(const chrono::nanoseconds& __ns)\n{\n     using namespace chrono;\n     seconds __s = duration_cast<seconds>(__ns);\n     __libcpp_timespec_t __ts;\n     typedef decltype(__ts.tv_sec) ts_sec;\n     constexpr ts_sec __ts_sec_max = numeric_limits<ts_sec>::max();\n\n     if (__s.count() < __ts_sec_max)\n     {\n         __ts.tv_sec = static_cast<ts_sec>(__s.count());\n         __ts.tv_nsec = static_cast<decltype(__ts.tv_nsec)>((__ns - __s).count());\n     }\n     else\n     {\n         __ts.tv_sec = __ts_sec_max;\n         __ts.tv_nsec = 999999999; // (10^9 - 1)\n     }\n     return __ts;\n}\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nint __libcpp_recursive_mutex_init(__libcpp_recursive_mutex_t *__m)\n{\n    pthread_mutexattr_t attr;\n    int __ec = pthread_mutexattr_init(&attr);\n    if (__ec)\n        return __ec;\n    __ec = pthread_mutexattr_settype(&attr, PTHREAD_MUTEX_RECURSIVE);\n    if (__ec) {\n        pthread_mutexattr_destroy(&attr);\n        return __ec;\n    }\n    __ec = pthread_mutex_init(__m, &attr);\n    if (__ec) {\n        pthread_mutexattr_destroy(&attr);\n        return __ec;\n    }\n    __ec = pthread_mutexattr_destroy(&attr);\n    if (__ec) {\n        pthread_mutex_destroy(__m);\n        return __ec;\n    }\n    return 0;\n}\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nint __libcpp_recursive_mutex_lock(__libcpp_recursive_mutex_t *__m)\n{\n    return pthread_mutex_lock(__m);\n}\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nbool __libcpp_recursive_mutex_trylock(__libcpp_recursive_mutex_t *__m)\n{\n    return pthread_mutex_trylock(__m) == 0;\n}\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nint __libcpp_recursive_mutex_unlock(__libcpp_mutex_t *__m)\n{\n    return pthread_mutex_unlock(__m);\n}\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nint __libcpp_recursive_mutex_destroy(__libcpp_recursive_mutex_t *__m)\n{\n    return pthread_mutex_destroy(__m);\n}\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nint __libcpp_mutex_lock(__libcpp_mutex_t *__m)\n{\n    return pthread_mutex_lock(__m);\n}\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nbool __libcpp_mutex_trylock(__libcpp_mutex_t *__m)\n{\n    return pthread_mutex_trylock(__m) == 0;\n}\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nint __libcpp_mutex_unlock(__libcpp_mutex_t *__m)\n{\n    return pthread_mutex_unlock(__m);\n}\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nint __libcpp_mutex_destroy(__libcpp_mutex_t *__m)\n{\n  return pthread_mutex_destroy(__m);\n}\n\n// Condition Variable\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nint __libcpp_condvar_signal(__libcpp_condvar_t *__cv)\n{\n    return pthread_cond_signal(__cv);\n}\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nint __libcpp_condvar_broadcast(__libcpp_condvar_t *__cv)\n{\n    return pthread_cond_broadcast(__cv);\n}\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nint __libcpp_condvar_wait(__libcpp_condvar_t *__cv, __libcpp_mutex_t *__m)\n{\n    return pthread_cond_wait(__cv, __m);\n}\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nint __libcpp_condvar_timedwait(__libcpp_condvar_t *__cv, __libcpp_mutex_t *__m,\n                               __libcpp_timespec_t *__ts)\n{\n    return pthread_cond_timedwait(__cv, __m, __ts);\n}\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nint __libcpp_condvar_destroy(__libcpp_condvar_t *__cv)\n{\n    return pthread_cond_destroy(__cv);\n}\n\n// Semaphore\n#if defined(__APPLE__)\n\nbool __libcpp_semaphore_init(__libcpp_semaphore_t* __sem, int __init)\n{\n    return (*__sem = dispatch_semaphore_create(__init)) != NULL;\n}\n\nbool __libcpp_semaphore_destroy(__libcpp_semaphore_t* __sem)\n{\n    dispatch_release(*__sem);\n    return true;\n}\n\nbool __libcpp_semaphore_post(__libcpp_semaphore_t* __sem)\n{\n    dispatch_semaphore_signal(*__sem);\n    return true;\n}\n\nbool __libcpp_semaphore_wait(__libcpp_semaphore_t* __sem)\n{\n    return dispatch_semaphore_wait(*__sem, DISPATCH_TIME_FOREVER) == 0;\n}\n\nbool __libcpp_semaphore_wait_timed(__libcpp_semaphore_t* __sem, chrono::nanoseconds const& __ns)\n{\n    return dispatch_semaphore_wait(*__sem, dispatch_time(DISPATCH_TIME_NOW, __ns.count())) == 0;\n}\n\n#else\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nbool __libcpp_semaphore_init(__libcpp_semaphore_t* __sem, int __init)\n{\n    return sem_init(__sem, 0, __init) == 0;\n}\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nbool __libcpp_semaphore_destroy(__libcpp_semaphore_t* __sem)\n{\n    return sem_destroy(__sem) == 0;\n}\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nbool __libcpp_semaphore_post(__libcpp_semaphore_t* __sem)\n{\n    return sem_post(__sem) == 0;\n}\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nbool __libcpp_semaphore_wait(__libcpp_semaphore_t* __sem)\n{\n    return sem_wait(__sem) == 0;\n}\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nbool __libcpp_semaphore_wait_timed(__libcpp_semaphore_t* __sem, chrono::nanoseconds const& __ns)\n{\n    __libcpp_timespec_t __ts = __libcpp_to_timespec(__ns);\n    return sem_timedwait(__sem, &__ts) == 0;\n}\n\n#endif //__APPLE__\n\n// Execute once\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nint __libcpp_execute_once(__libcpp_exec_once_flag *flag, void (*init_routine)())\n{\n    return pthread_once(flag, init_routine);\n}\n\n// Thread id\n// Returns non-zero if the thread ids are equal, otherwise 0\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nbool __libcpp_thread_id_equal(__libcpp_thread_id t1, __libcpp_thread_id t2)\n{\n    return pthread_equal(t1, t2) != 0;\n}\n\n// Returns non-zero if t1 < t2, otherwise 0\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nbool __libcpp_thread_id_less(__libcpp_thread_id t1, __libcpp_thread_id t2)\n{\n    return t1 < t2;\n}\n\n// Thread\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nbool __libcpp_thread_isnull(const __libcpp_thread_t *__t)\n{\n    return *__t == 0;\n}\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nint __libcpp_thread_create(__libcpp_thread_t *__t, void *(*__func)(void *),\n                           void *__arg)\n{\n    return pthread_create(__t, 0, __func, __arg);\n}\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\n__libcpp_thread_id __libcpp_thread_get_current_id()\n{\n    return pthread_self();\n}\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\n__libcpp_thread_id __libcpp_thread_get_id(const __libcpp_thread_t *__t)\n{\n    return *__t;\n}\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nint __libcpp_thread_join(__libcpp_thread_t *__t)\n{\n    return pthread_join(*__t, 0);\n}\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nint __libcpp_thread_detach(__libcpp_thread_t *__t)\n{\n    return pthread_detach(*__t);\n}\n\n// Thread local storage\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nint __libcpp_tls_create(__libcpp_tls_key *__key, void (*__at_exit)(void *))\n{\n    return pthread_key_create(__key, __at_exit);\n}\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nvoid *__libcpp_tls_get(__libcpp_tls_key __key)\n{\n    return pthread_getspecific(__key);\n}\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nint __libcpp_tls_set(__libcpp_tls_key __key, void *__p)\n{\n    return pthread_setspecific(__key, __p);\n}\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nvoid __libcpp_thread_yield()\n{\n    sched_yield();\n}\n\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nvoid __libcpp_thread_sleep_for(chrono::nanoseconds __ns)\n{\n    __libcpp_timespec_t __ts = __libcpp_to_timespec(__ns);\n    while (nanosleep(&__ts, &__ts) == -1 && errno == EINTR);\n}\n\n#if defined(__linux__) && !defined(_LIBCUDACXX_HAS_NO_PLATFORM_WAIT)\n\n#define _LIBCUDACXX_HAS_PLATFORM_WAIT\n\ntypedef int __libcpp_platform_wait_t;\n\ntemplate<typename _Tp>\nstruct __libcpp_platform_wait_uses_type {\n    enum { __value = is_same<__remove_cv_t<_Tp>, __libcpp_platform_wait_t>::value };\n};\n\ntemplate <class _Tp, typename enable_if<__libcpp_platform_wait_uses_type<_Tp>::__value, int>::type = 1>\nvoid __libcpp_platform_wait(_Tp const* ptr, _Tp val, void const* timeout) {\n    syscall(SYS_futex, ptr, FUTEX_WAIT_PRIVATE, val, timeout, 0, 0);\n}\n\ntemplate <class _Tp, typename enable_if<__libcpp_platform_wait_uses_type<_Tp>::__value, int>::type = 1>\nvoid __libcpp_platform_wake(_Tp const* ptr, bool all) {\n    syscall(SYS_futex, ptr, FUTEX_WAKE_PRIVATE, all ? INT_MAX : 1, 0, 0, 0);\n}\n\n#endif // defined(__linux__) && !defined(_LIBCUDACXX_HAS_NO_PLATFORM_WAIT)\n\n#elif defined(_LIBCUDACXX_HAS_THREAD_API_WIN32)\n\nvoid __libcpp_thread_yield()\n{\n    SwitchToThread();\n}\n\nvoid __libcpp_thread_sleep_for(chrono::nanoseconds __ns)\n{\n    using namespace chrono;\n    // round-up to the nearest milisecond\n    milliseconds __ms =\n        duration_cast<milliseconds>(__ns + chrono::nanoseconds(999999));\n    Sleep(static_cast<DWORD>(__ms.count()));\n}\n\n#endif // defined(_LIBCUDACXX_HAS_THREAD_API_WIN32)\n\n#endif // !defined(_LIBCUDACXX_HAS_THREAD_LIBRARY_EXTERNAL) || defined(_LIBCUDACXX_BUILDING_THREAD_LIBRARY_EXTERNAL)\n\ntemplate<class _Fn>\n_LIBCUDACXX_THREAD_ABI_VISIBILITY\nbool __libcpp_thread_poll_with_backoff(_Fn && __f, chrono::nanoseconds __max)\n{\n    chrono::high_resolution_clock::time_point const __start = chrono::high_resolution_clock::now();\n    for(int __count = 0;;) {\n      if(__f())\n        return true;\n      if(__count < _LIBCUDACXX_POLLING_COUNT) {\n        if(__count > (_LIBCUDACXX_POLLING_COUNT >> 1))\n          __libcpp_thread_yield_processor();\n        __count += 1;\n        continue;\n      }\n      chrono::high_resolution_clock::duration const __elapsed = chrono::high_resolution_clock::now() - __start;\n      if(__max != chrono::nanoseconds::zero() &&\n         __max < __elapsed)\n         return false;\n      chrono::nanoseconds const __step = __elapsed / 4;\n      if(__step >= chrono::milliseconds(1))\n        __libcpp_thread_sleep_for(chrono::milliseconds(1));\n      else if(__step >= chrono::microseconds(10))\n        __libcpp_thread_sleep_for(__step);\n      else\n        __libcpp_thread_yield();\n    }\n}\n\n#ifndef _LIBCUDACXX_HAS_NO_THREAD_CONTENTION_TABLE\n\nstruct alignas(64) __libcpp_contention_t {\n#if defined(_LIBCUDACXX_HAS_PLATFORM_WAIT)\n    ptrdiff_t                __waiters = 0;\n    __libcpp_platform_wait_t __version = 0;\n#else\n    ptrdiff_t                __credit = 0;\n    __libcpp_mutex_t         __mutex = _LIBCUDACXX_MUTEX_INITIALIZER;\n    __libcpp_condvar_t       __condvar = _LIBCUDACXX_CONDVAR_INITIALIZER;\n#endif\n};\n\n_LIBCUDACXX_FUNC_VIS\n__libcpp_contention_t * __libcpp_contention_state(void const volatile * p) noexcept;\n\n#endif // _LIBCUDACXX_HAS_NO_THREAD_CONTENTION_TABLE\n\n#if !defined(_LIBCUDACXX_HAS_NO_TREE_BARRIER) && !defined(_LIBCUDACXX_HAS_NO_THREAD_FAVORITE_BARRIER_INDEX)\n\n_LIBCUDACXX_EXPORTED_FROM_ABI\nextern thread_local ptrdiff_t __libcpp_thread_favorite_barrier_index;\n\n#endif\n\n#ifndef __cuda_std__\n\nclass _LIBCUDACXX_TYPE_VIS thread;\nclass _LIBCUDACXX_TYPE_VIS __thread_id;\n\nnamespace this_thread\n{\n\n_LIBCUDACXX_INLINE_VISIBILITY __thread_id get_id() noexcept;\n\n}  // this_thread\n\ntemplate<> struct hash<__thread_id>;\n\nclass _LIBCUDACXX_TEMPLATE_VIS __thread_id\n{\n    // FIXME: pthread_t is a pointer on Darwin but a long on Linux.\n    // NULL is the no-thread value on Darwin.  Someone needs to check\n    // on other platforms.  We assume 0 works everywhere for now.\n    __libcpp_thread_id __id_;\n\npublic:\n    _LIBCUDACXX_INLINE_VISIBILITY\n    __thread_id() noexcept : __id_(0) {}\n\n    friend _LIBCUDACXX_INLINE_VISIBILITY\n        bool operator==(__thread_id __x, __thread_id __y) noexcept\n        { // don't pass id==0 to underlying routines\n        if (__x.__id_ == 0) return __y.__id_ == 0;\n        if (__y.__id_ == 0) return false;\n        return __libcpp_thread_id_equal(__x.__id_, __y.__id_);\n        }\n    friend _LIBCUDACXX_INLINE_VISIBILITY\n        bool operator!=(__thread_id __x, __thread_id __y) noexcept\n        {return !(__x == __y);}\n    friend _LIBCUDACXX_INLINE_VISIBILITY\n        bool operator< (__thread_id __x, __thread_id __y) noexcept\n        { // id==0 is always less than any other thread_id\n        if (__x.__id_ == 0) return __y.__id_ != 0;\n        if (__y.__id_ == 0) return false;\n        return  __libcpp_thread_id_less(__x.__id_, __y.__id_);\n        }\n    friend _LIBCUDACXX_INLINE_VISIBILITY\n        bool operator<=(__thread_id __x, __thread_id __y) noexcept\n        {return !(__y < __x);}\n    friend _LIBCUDACXX_INLINE_VISIBILITY\n        bool operator> (__thread_id __x, __thread_id __y) noexcept\n        {return   __y < __x ;}\n    friend _LIBCUDACXX_INLINE_VISIBILITY\n        bool operator>=(__thread_id __x, __thread_id __y) noexcept\n        {return !(__x < __y);}\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    void __reset() { __id_ = 0; }\n\n#ifndef __cuda_std__\n    template<class _CharT, class _Traits>\n    friend\n    _LIBCUDACXX_INLINE_VISIBILITY\n    basic_ostream<_CharT, _Traits>&\n    operator<<(basic_ostream<_CharT, _Traits>& __os, __thread_id __id);\n#endif\n\nprivate:\n    _LIBCUDACXX_INLINE_VISIBILITY\n    __thread_id(__libcpp_thread_id __id) : __id_(__id) {}\n\n    friend __thread_id this_thread::get_id() noexcept;\n    friend class _LIBCUDACXX_TYPE_VIS thread;\n    friend struct _LIBCUDACXX_TEMPLATE_VIS hash<__thread_id>;\n};\n\nnamespace this_thread\n{\n\ninline _LIBCUDACXX_INLINE_VISIBILITY\n__thread_id\nget_id() noexcept\n{\n    return __libcpp_thread_get_current_id();\n}\n\n}  // this_thread\n\n#endif // __cuda_std__\n\n#endif // !_LIBCUDACXX_HAS_NO_THREADS\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#ifndef __cuda_std__\n#include <__pragma_pop>\n#endif\n\n#endif // _LIBCUDACXX_THREADING_SUPPORT\n", "__tuple_dir/apply_cv.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TUPLE_APPLY_CV_H\n#define _LIBCUDACXX___TUPLE_APPLY_CV_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/is_const.h\"\n#include \"../__type_traits/is_reference.h\"\n#include \"../__type_traits/is_volatile.h\"\n#include \"../__type_traits/remove_reference.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <bool _ApplyLV, bool _ApplyConst, bool _ApplyVolatile>\nstruct __apply_cv_mf;\ntemplate <>\nstruct __apply_cv_mf<false, false, false> {\n  template <class _Tp> using __apply = _Tp;\n};\ntemplate <>\nstruct __apply_cv_mf<false, true, false> {\n  template <class _Tp> using __apply _LIBCUDACXX_NODEBUG_TYPE = const _Tp;\n};\ntemplate <>\nstruct __apply_cv_mf<false, false, true> {\n  template <class _Tp> using __apply _LIBCUDACXX_NODEBUG_TYPE = volatile _Tp;\n};\ntemplate <>\nstruct __apply_cv_mf<false, true, true> {\n  template <class _Tp> using __apply _LIBCUDACXX_NODEBUG_TYPE = const volatile _Tp;\n};\ntemplate <>\nstruct __apply_cv_mf<true, false, false> {\n  template <class _Tp> using __apply _LIBCUDACXX_NODEBUG_TYPE = _Tp&;\n};\ntemplate <>\nstruct __apply_cv_mf<true, true, false> {\n  template <class _Tp> using __apply _LIBCUDACXX_NODEBUG_TYPE = const _Tp&;\n};\ntemplate <>\nstruct __apply_cv_mf<true, false, true> {\n  template <class _Tp> using __apply _LIBCUDACXX_NODEBUG_TYPE = volatile _Tp&;\n};\ntemplate <>\nstruct __apply_cv_mf<true, true, true> {\n  template <class _Tp> using __apply _LIBCUDACXX_NODEBUG_TYPE = const volatile _Tp&;\n};\ntemplate <class _Tp, class _RawTp = __libcpp_remove_reference_t<_Tp> >\nusing __apply_cv_t _LIBCUDACXX_NODEBUG_TYPE = __apply_cv_mf<\n    is_lvalue_reference<_Tp>::value,\n    is_const<_RawTp>::value,\n    is_volatile<_RawTp>::value>;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TUPLE_APPLY_CV_H\n", "__tuple_dir/make_tuple_types.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TUPLE_MAKE_TUPLE_TYPES_H\n#define _LIBCUDACXX___TUPLE_MAKE_TUPLE_TYPES_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__fwd/array.h\"\n#include \"../__fwd/tuple.h\"\n#include \"../__tuple_dir/apply_cv.h\"\n#include \"../__tuple_dir/tuple_element.h\"\n#include \"../__tuple_dir/tuple_indices.h\"\n#include \"../__tuple_dir/tuple_size.h\"\n#include \"../__tuple_dir/tuple_types.h\"\n#include \"../__type_traits/remove_cv.h\"\n#include \"../__type_traits/remove_reference.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n// __make_tuple_types<_Tuple<_Types...>, _Ep, _Sp>::type is a\n// __tuple_types<_Types...> using only those _Types in the range [_Sp, _Ep).\n// _Sp defaults to 0 and _Ep defaults to tuple_size<_Tuple>.  If _Tuple is a\n// lvalue_reference type, then __tuple_types<_Types&...> is the result.\n\ntemplate <class _TupleTypes, class _TupleIndices>\nstruct __make_tuple_types_flat;\n\ntemplate <template <class...> class _Tuple, class ..._Types, size_t ..._Idx>\nstruct __make_tuple_types_flat<_Tuple<_Types...>, __tuple_indices<_Idx...>> {\n  // Specialization for pair, tuple, and __tuple_types\n  template <class _Tp, class _ApplyFn = __apply_cv_t<_Tp>>\n  using __apply_quals _LIBCUDACXX_NODEBUG_TYPE = __tuple_types<\n      typename _ApplyFn::template __apply<__type_pack_element<_Idx, _Types...>>...\n    >;\n};\n\ntemplate <class _Vt, size_t _Np, size_t ..._Idx>\nstruct __make_tuple_types_flat<array<_Vt, _Np>, __tuple_indices<_Idx...>> {\n  template <size_t>\n  using __value_type = _Vt;\n  template <class _Tp, class _ApplyFn = __apply_cv_t<_Tp>>\n  using __apply_quals = __tuple_types<\n      typename _ApplyFn::template __apply<__value_type<_Idx>>...\n    >;\n};\n\ntemplate <class _Tp, size_t _Ep = tuple_size<__libcpp_remove_reference_t<_Tp> >::value,\n          size_t _Sp = 0,\n          bool _SameSize = (_Ep == tuple_size<__libcpp_remove_reference_t<_Tp> >::value)>\nstruct __make_tuple_types\n{\n    static_assert(_Sp <= _Ep, \"__make_tuple_types input error\");\n    using _RawTp = __remove_cv_t<__libcpp_remove_reference_t<_Tp>>;\n    using _Maker = __make_tuple_types_flat<_RawTp, typename __make_tuple_indices<_Ep, _Sp>::type>;\n    using type = typename _Maker::template __apply_quals<_Tp>;\n};\n\ntemplate <class ..._Types, size_t _Ep>\nstruct __make_tuple_types<tuple<_Types...>, _Ep, 0, true> {\n  typedef _LIBCUDACXX_NODEBUG_TYPE __tuple_types<_Types...> type;\n};\n\ntemplate <class ..._Types, size_t _Ep>\nstruct __make_tuple_types<__tuple_types<_Types...>, _Ep, 0, true> {\n  typedef _LIBCUDACXX_NODEBUG_TYPE __tuple_types<_Types...> type;\n};\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TUPLE_MAKE_TUPLE_TYPES_H\n", "__tuple_dir/sfinae_helpers.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TUPLE_SFINAE_HELPERS_H\n#define _LIBCUDACXX___TUPLE_SFINAE_HELPERS_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__fwd/tuple.h\"\n#include \"../__tuple_dir/make_tuple_types.h\"\n#include \"../__tuple_dir/tuple_element.h\"\n#include \"../__tuple_dir/tuple_like.h\"\n#include \"../__tuple_dir/tuple_size.h\"\n#include \"../__tuple_dir/tuple_types.h\"\n#include \"../__type_traits/enable_if.h\"\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_assignable.h\"\n#include \"../__type_traits/is_constructible.h\"\n#include \"../__type_traits/is_convertible.h\"\n#include \"../__type_traits/is_same.h\"\n#include \"../__type_traits/remove_cvref.h\"\n#include \"../__type_traits/remove_reference.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <bool ..._Preds>\nstruct __all_dummy;\n\ntemplate <bool ..._Pred>\nusing __all = _IsSame<__all_dummy<_Pred...>, __all_dummy<((void)_Pred, true)...>>;\n\nstruct __tuple_sfinae_base {\n  template <class, class>\n  struct __test_size : false_type {};\n\n  template <class ..._Tp, class ..._Up>\n  struct __test_size<__tuple_types<_Tp...>, __tuple_types<_Up...>>\n    : _BoolConstant<sizeof...(_Tp) == sizeof...(_Up)> {};\n\n  template <template <class, class...> class,\n            class _Tp, class _Up, bool = __test_size<_Tp, _Up>::value>\n  struct __test : false_type {};\n\n  template <template <class, class...> class _Trait,\n            class ..._LArgs, class ..._RArgs>\n  struct __test<_Trait, __tuple_types<_LArgs...>, __tuple_types<_RArgs...>, true>\n      : __all<_Trait<_LArgs, _RArgs>::value...> {};\n\n  template <class _FromArgs, class _ToArgs>\n  using __constructible = __test<is_constructible, _ToArgs, _FromArgs>;\n  template <class _FromArgs, class _ToArgs>\n  using __convertible = __test<is_convertible, _FromArgs, _ToArgs>;\n  template <class _FromArgs, class _ToArgs>\n  using __assignable = __test<is_assignable, _ToArgs, _FromArgs>;\n};\n\n// __tuple_convertible\n\ntemplate <class _Tp, class _Up, bool = __tuple_like<__libcpp_remove_reference_t<_Tp>>::value,\n                                bool = __tuple_like<_Up>::value>\nstruct __tuple_convertible\n    : public false_type {};\n\ntemplate <class _Tp, class _Up>\nstruct __tuple_convertible<_Tp, _Up, true, true>\n    : public __tuple_sfinae_base::__convertible<\n      typename __make_tuple_types<_Tp>::type\n    , typename __make_tuple_types<_Up>::type\n    >\n{};\n\n// __tuple_constructible\n\ntemplate <class _Tp, class _Up, bool = __tuple_like<__libcpp_remove_reference_t<_Tp>>::value,\n                                bool = __tuple_like<_Up>::value>\nstruct __tuple_constructible\n    : public false_type {};\n\ntemplate <class _Tp, class _Up>\nstruct __tuple_constructible<_Tp, _Up, true, true>\n    : public __tuple_sfinae_base::__constructible<\n      typename __make_tuple_types<_Tp>::type\n    , typename __make_tuple_types<_Up>::type\n    >\n{};\n\n// __tuple_assignable\n\ntemplate <class _Tp, class _Up, bool = __tuple_like<__libcpp_remove_reference_t<_Tp>>::value,\n                                bool = __tuple_like<_Up>::value>\nstruct __tuple_assignable\n    : public false_type {};\n\ntemplate <class _Tp, class _Up>\nstruct __tuple_assignable<_Tp, _Up, true, true>\n    : public __tuple_sfinae_base::__assignable<\n      typename __make_tuple_types<_Tp>::type\n    , typename __make_tuple_types<_Up&>::type\n    >\n{};\n\n\ntemplate <size_t _Ip, class ..._Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS tuple_element<_Ip, tuple<_Tp...> >\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE typename tuple_element<_Ip, __tuple_types<_Tp...> >::type type;\n};\n\ntemplate <bool _IsTuple, class _SizeTrait, size_t _Expected>\nstruct __tuple_like_with_size_imp : false_type {};\n\ntemplate <class _SizeTrait, size_t _Expected>\nstruct __tuple_like_with_size_imp<true, _SizeTrait, _Expected>\n    : integral_constant<bool, _SizeTrait::value == _Expected> {};\n\ntemplate <class _Tuple, size_t _ExpectedSize,\n          class _RawTuple = __remove_cvref_t<_Tuple>>\nusing __tuple_like_with_size _LIBCUDACXX_NODEBUG_TYPE = __tuple_like_with_size_imp<\n                                   __tuple_like<_RawTuple>::value,\n                                   tuple_size<_RawTuple>, _ExpectedSize\n                              >;\n\nstruct _LIBCUDACXX_TYPE_VIS __check_tuple_constructor_fail {\n    template <int&...>\n    using __enable_explicit_default = false_type;\n    template <int&...>\n    using __enable_implicit_default = false_type;\n    template <class ...>\n    using __enable_explicit = false_type;\n    template <class ...>\n    using __enable_implicit = false_type;\n    template <class ...>\n    using __enable_assign = false_type;\n};\n\n#if _LIBCUDACXX_STD_VER > 11\n\ntemplate <bool _CanCopy, bool _CanMove>\nstruct __sfinae_ctor_base {};\ntemplate <>\nstruct __sfinae_ctor_base<false, false> {\n  __sfinae_ctor_base() = default;\n  __sfinae_ctor_base(__sfinae_ctor_base const&) = delete;\n  __sfinae_ctor_base(__sfinae_ctor_base &&) = delete;\n  __sfinae_ctor_base& operator=(__sfinae_ctor_base const&) = default;\n  __sfinae_ctor_base& operator=(__sfinae_ctor_base&&) = default;\n};\ntemplate <>\nstruct __sfinae_ctor_base<true, false> {\n  __sfinae_ctor_base() = default;\n  __sfinae_ctor_base(__sfinae_ctor_base const&) = default;\n  __sfinae_ctor_base(__sfinae_ctor_base &&) = delete;\n  __sfinae_ctor_base& operator=(__sfinae_ctor_base const&) = default;\n  __sfinae_ctor_base& operator=(__sfinae_ctor_base&&) = default;\n};\ntemplate <>\nstruct __sfinae_ctor_base<false, true> {\n  __sfinae_ctor_base() = default;\n  __sfinae_ctor_base(__sfinae_ctor_base const&) = delete;\n  __sfinae_ctor_base(__sfinae_ctor_base &&) = default;\n  __sfinae_ctor_base& operator=(__sfinae_ctor_base const&) = default;\n  __sfinae_ctor_base& operator=(__sfinae_ctor_base&&) = default;\n};\n\ntemplate <bool _CanCopy, bool _CanMove>\nstruct __sfinae_assign_base {};\ntemplate <>\nstruct __sfinae_assign_base<false, false> {\n  __sfinae_assign_base() = default;\n  __sfinae_assign_base(__sfinae_assign_base const&) = default;\n  __sfinae_assign_base(__sfinae_assign_base &&) = default;\n  __sfinae_assign_base& operator=(__sfinae_assign_base const&) = delete;\n  __sfinae_assign_base& operator=(__sfinae_assign_base&&) = delete;\n};\ntemplate <>\nstruct __sfinae_assign_base<true, false> {\n  __sfinae_assign_base() = default;\n  __sfinae_assign_base(__sfinae_assign_base const&) = default;\n  __sfinae_assign_base(__sfinae_assign_base &&) = default;\n  __sfinae_assign_base& operator=(__sfinae_assign_base const&) = default;\n  __sfinae_assign_base& operator=(__sfinae_assign_base&&) = delete;\n};\ntemplate <>\nstruct __sfinae_assign_base<false, true> {\n  __sfinae_assign_base() = default;\n  __sfinae_assign_base(__sfinae_assign_base const&) = default;\n  __sfinae_assign_base(__sfinae_assign_base &&) = default;\n  __sfinae_assign_base& operator=(__sfinae_assign_base const&) = delete;\n  __sfinae_assign_base& operator=(__sfinae_assign_base&&) = default;\n};\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TUPLE_SFINAE_HELPERS_H\n", "__tuple_dir/structured_bindings.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TUPLE_STRUCTURED_BINDINGS_H\n#define _LIBCUDACXX___TUPLE_STRUCTURED_BINDINGS_H\n\n#ifdef __cuda_std__\n\n#if defined(_LIBCUDACXX_COMPILER_CLANG)\n_Pragma(\"clang diagnostic push\")\n_Pragma(\"clang diagnostic ignored \\\"-Wmismatched-tags\\\"\")\n#endif // _LIBCUDACXX_COMPILER_CLANG\n\n#if !defined(__CUDACC_RTC__)\n// Fetch utility to get primary template for ::std::tuple_size necessary for the specialization of\n// ::std::tuple_size<cuda::std::tuple> to enable structured bindings.\n// See https://github.com/NVIDIA/libcudacxx/issues/316\n#include <utility>\n#endif\n\n#include \"../__fwd/array.h\"\n#include \"../__fwd/pair.h\"\n#include \"../__fwd/tuple.h\"\n#include \"../__tuple_dir/tuple_element.h\"\n#include \"../__tuple_dir/tuple_size.h\"\n#include \"../__type_traits/integral_constant.h\"\n\n// This is a workaround for the fact that structured bindings require that the specializations of\n// `tuple_size` and `tuple_element` reside in namespace std (https://eel.is/c++draft/dcl.struct.bind#4).\n// See https://github.com/NVIDIA/libcudacxx/issues/316 for a short discussion\n#if _LIBCUDACXX_STD_VER > 14\nnamespace std {\n#if defined(__CUDACC_RTC__)\n    template <class... _Tp>\n    struct tuple_size;\n\n    template<size_t _Ip, class... _Tp>\n    struct tuple_element;\n#endif\n\n    template <class _Tp, size_t _Size>\n    struct tuple_size<_CUDA_VSTD::array<_Tp, _Size>>\n      : _CUDA_VSTD::tuple_size<_CUDA_VSTD::array<_Tp, _Size>>\n    {};\n\n    template <class _Tp, size_t _Size>\n    struct tuple_size<const _CUDA_VSTD::array<_Tp, _Size>>\n      : _CUDA_VSTD::tuple_size<_CUDA_VSTD::array<_Tp, _Size>>\n    {};\n\n    template <class _Tp, size_t _Size>\n    struct tuple_size<volatile _CUDA_VSTD::array<_Tp, _Size>>\n      : _CUDA_VSTD::tuple_size<_CUDA_VSTD::array<_Tp, _Size>>\n    {};\n\n    template <class _Tp, size_t _Size>\n    struct tuple_size<const volatile _CUDA_VSTD::array<_Tp, _Size>>\n      : _CUDA_VSTD::tuple_size<_CUDA_VSTD::array<_Tp, _Size>>\n    {};\n\n    template<size_t _Ip, class _Tp, size_t _Size>\n    struct tuple_element<_Ip, _CUDA_VSTD::array<_Tp, _Size>>\n      : _CUDA_VSTD::tuple_element<_Ip, _CUDA_VSTD::array<_Tp, _Size>>\n    {};\n\n    template<size_t _Ip, class _Tp, size_t _Size>\n    struct tuple_element<_Ip, const _CUDA_VSTD::array<_Tp, _Size>>\n      : _CUDA_VSTD::tuple_element<_Ip, const _CUDA_VSTD::array<_Tp, _Size>>\n    {};\n\n    template<size_t _Ip, class _Tp, size_t _Size>\n    struct tuple_element<_Ip, volatile _CUDA_VSTD::array<_Tp, _Size>>\n      : _CUDA_VSTD::tuple_element<_Ip, volatile _CUDA_VSTD::array<_Tp, _Size>>\n    {};\n\n    template<size_t _Ip, class _Tp, size_t _Size>\n    struct tuple_element<_Ip, const volatile _CUDA_VSTD::array<_Tp, _Size>>\n      : _CUDA_VSTD::tuple_element<_Ip, const volatile _CUDA_VSTD::array<_Tp, _Size>>\n    {};\n\n    template <class _Tp, class _Up>\n    struct tuple_size<_CUDA_VSTD::pair<_Tp, _Up>>\n      : _CUDA_VSTD::tuple_size<_CUDA_VSTD::pair<_Tp, _Up>>\n    {};\n\n    template <class _Tp, class _Up>\n    struct tuple_size<const _CUDA_VSTD::pair<_Tp, _Up>>\n      : _CUDA_VSTD::tuple_size<_CUDA_VSTD::pair<_Tp, _Up>>\n    {};\n\n    template <class _Tp, class _Up>\n    struct tuple_size<volatile _CUDA_VSTD::pair<_Tp, _Up>>\n      : _CUDA_VSTD::tuple_size<_CUDA_VSTD::pair<_Tp, _Up>>\n    {};\n\n    template <class _Tp, class _Up>\n    struct tuple_size<const volatile _CUDA_VSTD::pair<_Tp, _Up>>\n      : _CUDA_VSTD::tuple_size<_CUDA_VSTD::pair<_Tp, _Up>>\n    {};\n\n    template<size_t _Ip, class _Tp, class _Up>\n    struct tuple_element<_Ip, _CUDA_VSTD::pair<_Tp, _Up>>\n      : _CUDA_VSTD::tuple_element<_Ip, _CUDA_VSTD::pair<_Tp, _Up>>\n    {};\n\n    template<size_t _Ip, class _Tp, class _Up>\n    struct tuple_element<_Ip, const _CUDA_VSTD::pair<_Tp, _Up>>\n      : _CUDA_VSTD::tuple_element<_Ip, const _CUDA_VSTD::pair<_Tp, _Up>>\n    {};\n\n    template<size_t _Ip, class _Tp, class _Up>\n    struct tuple_element<_Ip, volatile _CUDA_VSTD::pair<_Tp, _Up>>\n      : _CUDA_VSTD::tuple_element<_Ip, volatile _CUDA_VSTD::pair<_Tp, _Up>>\n    {};\n\n    template<size_t _Ip, class _Tp, class _Up>\n    struct tuple_element<_Ip, const volatile _CUDA_VSTD::pair<_Tp, _Up>>\n      : _CUDA_VSTD::tuple_element<_Ip, const volatile _CUDA_VSTD::pair<_Tp, _Up>>\n    {};\n\n    template <class... _Tp>\n    struct tuple_size<_CUDA_VSTD::tuple<_Tp...>>\n      : _CUDA_VSTD::tuple_size<_CUDA_VSTD::tuple<_Tp...>>\n    {};\n\n    template <class... _Tp>\n    struct tuple_size<const _CUDA_VSTD::tuple<_Tp...>>\n      : _CUDA_VSTD::tuple_size<_CUDA_VSTD::tuple<_Tp...>>\n    {};\n\n    template <class... _Tp>\n    struct tuple_size<volatile _CUDA_VSTD::tuple<_Tp...>>\n      : _CUDA_VSTD::tuple_size<_CUDA_VSTD::tuple<_Tp...>>\n    {};\n\n    template <class... _Tp>\n    struct tuple_size<const volatile _CUDA_VSTD::tuple<_Tp...>>\n      : _CUDA_VSTD::tuple_size<_CUDA_VSTD::tuple<_Tp...>>\n    {};\n\n    template<size_t _Ip, class... _Tp>\n    struct tuple_element<_Ip, _CUDA_VSTD::tuple<_Tp...>>\n      : _CUDA_VSTD::tuple_element<_Ip, _CUDA_VSTD::tuple<_Tp...>>\n    {};\n\n    template<size_t _Ip, class... _Tp>\n    struct tuple_element<_Ip, const _CUDA_VSTD::tuple<_Tp...>>\n      : _CUDA_VSTD::tuple_element<_Ip, const _CUDA_VSTD::tuple<_Tp...>>\n    {};\n\n    template<size_t _Ip, class... _Tp>\n    struct tuple_element<_Ip, volatile _CUDA_VSTD::tuple<_Tp...>>\n      : _CUDA_VSTD::tuple_element<_Ip, volatile _CUDA_VSTD::tuple<_Tp...>>\n    {};\n\n    template<size_t _Ip, class... _Tp>\n    struct tuple_element<_Ip, const volatile _CUDA_VSTD::tuple<_Tp...>>\n      : _CUDA_VSTD::tuple_element<_Ip, const volatile _CUDA_VSTD::tuple<_Tp...>>\n    {};\n}\n#endif // _LIBCUDACXX_STD_VER > 14\n\n#if defined(_LIBCUDACXX_COMPILER_CLANG)\n_Pragma(\"clang diagnostic pop\")\n# endif // _LIBCUDACXX_COMPILER_CLANG\n\n#endif // __cuda_std__\n\n#endif // _LIBCUDACXX___TUPLE_STRUCTURED_BINDINGS_H\n", "__tuple_dir/tuple_element.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TUPLE_TUPLE_ELEMENT_H\n#define _LIBCUDACXX___TUPLE_TUPLE_ELEMENT_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__tuple_dir/tuple_indices.h\"\n#include \"../__tuple_dir/tuple_types.h\"\n#include \"../__type_traits/add_const.h\"\n#include \"../__type_traits/add_cv.h\"\n#include \"../__type_traits/add_volatile.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <size_t _Ip, class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS tuple_element;\n\ntemplate <size_t _Ip, class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS tuple_element<_Ip, const _Tp>\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE typename add_const<typename tuple_element<_Ip, _Tp>::type>::type type;\n};\n\ntemplate <size_t _Ip, class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS tuple_element<_Ip, volatile _Tp>\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE typename add_volatile<typename tuple_element<_Ip, _Tp>::type>::type type;\n};\n\ntemplate <size_t _Ip, class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS tuple_element<_Ip, const volatile _Tp>\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE typename add_cv<typename tuple_element<_Ip, _Tp>::type>::type type;\n};\n\n#ifdef _LIBCUDACXX_COMPILER_MSVC\n\nnamespace __indexer_detail {\n\ntemplate <size_t _Idx, class ..._Types>\nstruct _nth_of;\n\ntemplate <class _Head, class ..._Tail>\nstruct _nth_of<0, _Head, _Tail...> {\n    using type = _Head;\n};\n\ntemplate <size_t _Idx, class _Head, class ..._Tail>\nstruct _nth_of<_Idx, _Head, _Tail...> {\n    using type = typename _nth_of<_Idx-1, _Tail...>::type;\n};\n\ntemplate <size_t _Idx, class ..._Types>\nstruct nth_of {\n    static_assert(_Idx < sizeof...(_Types), \"\");\n    using _impl = _nth_of<_Idx, _Types...>;\n    using type = typename _impl::type;\n};\n\n} // namespace __indexer_detail\n\ntemplate <size_t _Idx, class ..._Types>\nusing __type_pack_element _LIBCUDACXX_NODEBUG_TYPE = typename __indexer_detail::nth_of<_Idx, _Types...>::type;\n\n#elif !__has_builtin(__type_pack_element)\n\nnamespace __indexer_detail {\n\ntemplate <size_t _Idx, class _Tp>\nstruct __indexed { using type _LIBCUDACXX_NODEBUG_TYPE = _Tp; };\n\ntemplate <class _Types, class _Indexes> struct __indexer;\n\ntemplate <class ..._Types, size_t ..._Idx>\nstruct __indexer<__tuple_types<_Types...>, __tuple_indices<_Idx...>>\n    : __indexed<_Idx, _Types>...\n{};\n\ntemplate <size_t _Idx, class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n__indexed<_Idx, _Tp> __at_index(__indexed<_Idx, _Tp> const&);\n\n} // namespace __indexer_detail\n\ntemplate <size_t _Idx, class ..._Types>\nusing __type_pack_element _LIBCUDACXX_NODEBUG_TYPE = typename decltype(\n    __indexer_detail::__at_index<_Idx>(\n        __indexer_detail::__indexer<\n            __tuple_types<_Types...>,\n            typename __make_tuple_indices<sizeof...(_Types)>::type\n        >{})\n  )::type;\n#endif\n\ntemplate <size_t _Ip, class ..._Types>\nstruct _LIBCUDACXX_TEMPLATE_VIS tuple_element<_Ip, __tuple_types<_Types...> >\n{\n    static_assert(_Ip < sizeof...(_Types), \"tuple_element index out of range\");\n    typedef _LIBCUDACXX_NODEBUG_TYPE __type_pack_element<_Ip, _Types...> type;\n};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <size_t _Ip, class ..._Tp>\nusing tuple_element_t _LIBCUDACXX_NODEBUG_TYPE = typename tuple_element <_Ip, _Tp...>::type;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TUPLE_TUPLE_ELEMENT_H\n", "__tuple_dir/tuple_indices.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TUPLE_MAKE_TUPLE_INDICES_H\n#define _LIBCUDACXX___TUPLE_MAKE_TUPLE_INDICES_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__utility/integer_sequence.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <size_t _Ep, size_t _Sp = 0>\nstruct __make_tuple_indices\n{\n    static_assert(_Sp <= _Ep, \"__make_tuple_indices input error\");\n    typedef __make_indices_imp<_Ep, _Sp> type;\n};\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TUPLE_MAKE_TUPLE_INDICES_H\n", "__tuple_dir/tuple_like.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TUPLE_TUPLE_LIKE_H\n#define _LIBCUDACXX___TUPLE_TUPLE_LIKE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__fwd/array.h\"\n#include \"../__fwd/pair.h\"\n#include \"../__fwd/tuple.h\"\n#include \"../__tuple_dir/tuple_types.h\"\n#include \"../__type_traits/integral_constant.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp> struct __tuple_like : false_type {};\n\ntemplate <class _Tp> struct __tuple_like<const _Tp> : public __tuple_like<_Tp> {};\ntemplate <class _Tp> struct __tuple_like<volatile _Tp> : public __tuple_like<_Tp> {};\ntemplate <class _Tp> struct __tuple_like<const volatile _Tp> : public __tuple_like<_Tp> {};\n\ntemplate <class... _Tp> struct __tuple_like<tuple<_Tp...> > : true_type {};\n\ntemplate <class _T1, class _T2> struct __tuple_like<pair<_T1, _T2> > : true_type {};\n\ntemplate <class _Tp, size_t _Size> struct __tuple_like<array<_Tp, _Size> > : true_type {};\n\ntemplate <class... _Tp> struct __tuple_like<__tuple_types<_Tp...> > : true_type {};\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TUPLE_TUPLE_LIKE_H\n", "__tuple_dir/tuple_size.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TUPLE_TUPLE_SIZE_H\n#define _LIBCUDACXX___TUPLE_TUPLE_SIZE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__fwd/tuple.h\"\n#include \"../__tuple_dir/tuple_types.h\"\n#include \"../__type_traits/enable_if.h\"\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_const.h\"\n#include \"../__type_traits/is_volatile.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS tuple_size;\n\ntemplate <class _Tp, class...>\nusing __enable_if_tuple_size_imp = _Tp;\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS tuple_size<__enable_if_tuple_size_imp<\n    const _Tp,\n    __enable_if_t<!is_volatile<_Tp>::value>,\n    integral_constant<size_t, sizeof(tuple_size<_Tp>)>>>\n    : public integral_constant<size_t, tuple_size<_Tp>::value> {};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS tuple_size<__enable_if_tuple_size_imp<\n    volatile _Tp,\n    __enable_if_t<!is_const<_Tp>::value>,\n    integral_constant<size_t, sizeof(tuple_size<_Tp>)>>>\n    : public integral_constant<size_t, tuple_size<_Tp>::value> {};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS tuple_size<__enable_if_tuple_size_imp<\n    const volatile _Tp,\n    integral_constant<size_t, sizeof(tuple_size<_Tp>)>>>\n    : public integral_constant<size_t, tuple_size<_Tp>::value> {};\n\ntemplate <class ..._Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS tuple_size<tuple<_Tp...> >\n    : public integral_constant<size_t, sizeof...(_Tp)>\n{\n};\n\ntemplate <class ..._Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS tuple_size<__tuple_types<_Tp...> >\n    : public integral_constant<size_t, sizeof...(_Tp)>\n{\n};\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TUPLE_TUPLE_SIZE_H\n", "__tuple_dir/tuple_types.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TUPLE_TUPLE_TYPES_H\n#define _LIBCUDACXX___TUPLE_TUPLE_TYPES_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class ..._Tp> struct __tuple_types {};\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TUPLE_TUPLE_TYPES_H\n", "__type_traits/add_const.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_ADD_CONST_H\n#define _LIBCUDACXX___TYPE_TRAITS_ADD_CONST_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS add_const {\n  typedef _LIBCUDACXX_NODEBUG_TYPE const _Tp type;\n};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> using add_const_t = typename add_const<_Tp>::type;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_ADD_CONST_H\n", "__type_traits/add_cv.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_ADD_CV_H\n#define _LIBCUDACXX___TYPE_TRAITS_ADD_CV_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS add_cv {\n  typedef _LIBCUDACXX_NODEBUG_TYPE const volatile _Tp type;\n};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> using add_cv_t = typename add_cv<_Tp>::type;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_ADD_CV_H\n", "__type_traits/add_lvalue_reference.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_ADD_LVALUE_REFERENCE_H\n#define _LIBCUDACXX___TYPE_TRAITS_ADD_LVALUE_REFERENCE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/is_referenceable.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_ADD_LVALUE_REFERENCE) && !defined(_LIBCUDACXX_USE_ADD_LVALUE_REFERENCE_FALLBACK)\n\ntemplate <class _Tp>\nusing __add_lvalue_reference_t = _LIBCUDACXX_ADD_LVALUE_REFERENCE(_Tp);\n\n#else\n\ntemplate <class _Tp, bool = __libcpp_is_referenceable<_Tp>::value>\nstruct __add_lvalue_reference_impl {\n  typedef _LIBCUDACXX_NODEBUG_TYPE _Tp type;\n};\ntemplate <class _Tp >\nstruct __add_lvalue_reference_impl<_Tp, true> {\n  typedef _LIBCUDACXX_NODEBUG_TYPE _Tp& type;\n};\n\ntemplate <class _Tp>\nusing __add_lvalue_reference_t = typename __add_lvalue_reference_impl<_Tp>::type;\n\n#endif // defined(_LIBCUDACXX_ADD_LVALUE_REFERENCE) && !defined(_LIBCUDACXX_USE_ADD_LVALUE_REFERENCE_FALLBACK)\n\ntemplate <class _Tp>\nstruct add_lvalue_reference {\n  using type _LIBCUDACXX_NODEBUG_TYPE = __add_lvalue_reference_t<_Tp>;\n};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> using add_lvalue_reference_t = __add_lvalue_reference_t<_Tp>;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_ADD_LVALUE_REFERENCE_H\n", "__type_traits/add_pointer.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_ADD_POINTER_H\n#define _LIBCUDACXX___TYPE_TRAITS_ADD_POINTER_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/is_referenceable.h\"\n#include \"../__type_traits/is_same.h\"\n#include \"../__type_traits/is_void.h\"\n#include \"../__type_traits/remove_cv.h\"\n#include \"../__type_traits/remove_reference.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_ADD_POINTER) && !defined(_LIBCUDACXX_USE_ADD_POINTER_FALLBACK)\n\ntemplate <class _Tp>\nusing __add_pointer_t = _LIBCUDACXX_ADD_POINTER(_Tp);\n\n#else\ntemplate <class _Tp,\n          bool = __libcpp_is_referenceable<_Tp>::value || is_void<_Tp>::value>\nstruct __add_pointer_impl {\n  typedef _LIBCUDACXX_NODEBUG_TYPE __libcpp_remove_reference_t<_Tp>* type;\n};\ntemplate <class _Tp> struct __add_pointer_impl<_Tp, false>\n    {typedef _LIBCUDACXX_NODEBUG_TYPE _Tp type;};\n\ntemplate <class _Tp>\nusing __add_pointer_t = typename __add_pointer_impl<_Tp>::type;\n\n#endif // defined(_LIBCUDACXX_ADD_POINTER) && !defined(_LIBCUDACXX_USE_ADD_POINTER_FALLBACK)\n\ntemplate <class _Tp>\nstruct add_pointer {\n  using type _LIBCUDACXX_NODEBUG_TYPE = __add_pointer_t<_Tp>;\n};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> using add_pointer_t = __add_pointer_t<_Tp>;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_ADD_POINTER_H\n", "__type_traits/add_rvalue_reference.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_ADD_RVALUE_REFERENCE_H\n#define _LIBCUDACXX___TYPE_TRAITS_ADD_RVALUE_REFERENCE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/is_referenceable.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_ADD_RVALUE_REFERENCE) && !defined(_LIBCUDACXX_USE_ADD_RVALUE_REFERENCE_FALLBACK)\n\ntemplate <class _Tp>\nusing __add_rvalue_reference_t = _LIBCUDACXX_ADD_RVALUE_REFERENCE(_Tp);\n\n#else\n\ntemplate <class _Tp, bool = __libcpp_is_referenceable<_Tp>::value>\nstruct __add_rvalue_reference_impl {\n  typedef _LIBCUDACXX_NODEBUG_TYPE _Tp type;\n};\ntemplate <class _Tp >\nstruct __add_rvalue_reference_impl<_Tp, true> {\n  typedef _LIBCUDACXX_NODEBUG_TYPE _Tp&& type;\n};\n\ntemplate <class _Tp>\nusing __add_rvalue_reference_t = typename __add_rvalue_reference_impl<_Tp>::type;\n\n#endif // defined(_LIBCUDACXX_ADD_RVALUE_REFERENCE) && !defined(_LIBCUDACXX_USE_ADD_RVALUE_REFERENCE_FALLBACK)\n\ntemplate <class _Tp>\nstruct add_rvalue_reference {\n  using type = __add_rvalue_reference_t<_Tp>;\n};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp>\nusing add_rvalue_reference_t = __add_rvalue_reference_t<_Tp>;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_ADD_RVALUE_REFERENCE_H\n", "__type_traits/add_volatile.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_ADD_VOLATILE_H\n#define _LIBCUDACXX___TYPE_TRAITS_ADD_VOLATILE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS add_volatile {\n  typedef _LIBCUDACXX_NODEBUG_TYPE volatile _Tp type;\n};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> using add_volatile_t = typename add_volatile<_Tp>::type;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_ADD_VOLATILE_H\n", "__type_traits/aligned_storage.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_ALIGNED_STORAGE_H\n#define _LIBCUDACXX___TYPE_TRAITS_ALIGNED_STORAGE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/conditional.h\"\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/nat.h\"\n#include \"../__type_traits/type_list.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp>\nstruct __align_type\n{\n    static const size_t value = _LIBCUDACXX_PREFERRED_ALIGNOF(_Tp);\n    typedef _Tp type;\n};\n\nstruct __struct_double {long double __lx;};\nstruct __struct_double4 {double __lx[4];};\n\ntypedef\n    __type_list<__align_type<unsigned char>,\n    __type_list<__align_type<unsigned short>,\n    __type_list<__align_type<unsigned int>,\n    __type_list<__align_type<unsigned long>,\n    __type_list<__align_type<unsigned long long>,\n    __type_list<__align_type<double>,\n    __type_list<__align_type<long double>,\n    __type_list<__align_type<__struct_double>,\n    __type_list<__align_type<__struct_double4>,\n    __type_list<__align_type<int*>,\n    __nat\n    > > > > > > > > > > __all_types;\n\ntemplate <size_t _Align>\nstruct _ALIGNAS(_Align) __fallback_overaligned {};\n\ntemplate <class _TL, size_t _Align> struct __find_pod;\n\ntemplate <class _Hp, size_t _Align>\nstruct __find_pod<__type_list<_Hp, __nat>, _Align>\n{\n    typedef __conditional_t<_Align == _Hp::value, typename _Hp::type, __fallback_overaligned<_Align> > type;\n};\n\ntemplate <class _Hp, class _Tp, size_t _Align>\nstruct __find_pod<__type_list<_Hp, _Tp>, _Align>\n{\n    typedef __conditional_t<_Align == _Hp::value, typename _Hp::type, typename __find_pod<_Tp, _Align>::type> type;\n};\n\ntemplate <class _TL, size_t _Len> struct __find_max_align;\n\ntemplate <class _Hp, size_t _Len>\nstruct __find_max_align<__type_list<_Hp, __nat>, _Len> : public integral_constant<size_t, _Hp::value> {};\n\ntemplate <size_t _Len, size_t _A1, size_t _A2>\nstruct __select_align\n{\nprivate:\n    static const size_t __min = _A2 < _A1 ? _A2 : _A1;\n    static const size_t __max = _A1 < _A2 ? _A2 : _A1;\npublic:\n    static const size_t value = _Len < __max ? __min : __max;\n};\n\ntemplate <class _Hp, class _Tp, size_t _Len>\nstruct __find_max_align<__type_list<_Hp, _Tp>, _Len>\n    : public integral_constant<size_t, __select_align<_Len, _Hp::value, __find_max_align<_Tp, _Len>::value>::value> {};\n\ntemplate <size_t _Len, size_t _Align = __find_max_align<__all_types, _Len>::value>\nstruct _LIBCUDACXX_TEMPLATE_VIS aligned_storage\n{\n    typedef typename __find_pod<__all_types, _Align>::type _Aligner;\n    union type\n    {\n        _Aligner __align;\n        unsigned char __data[(_Len + _Align - 1)/_Align * _Align];\n    };\n};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <size_t _Len, size_t _Align = __find_max_align<__all_types, _Len>::value>\n    using aligned_storage_t = typename aligned_storage<_Len, _Align>::type;\n#endif\n\n#define _CREATE_ALIGNED_STORAGE_SPECIALIZATION(n) \\\ntemplate <size_t _Len>\\\nstruct _LIBCUDACXX_TEMPLATE_VIS aligned_storage<_Len, n>\\\n{\\\n    struct _ALIGNAS(n) type\\\n    {\\\n        unsigned char __lx[(_Len + n - 1)/n * n];\\\n    };\\\n}\n\n_CREATE_ALIGNED_STORAGE_SPECIALIZATION(0x1);\n_CREATE_ALIGNED_STORAGE_SPECIALIZATION(0x2);\n_CREATE_ALIGNED_STORAGE_SPECIALIZATION(0x4);\n_CREATE_ALIGNED_STORAGE_SPECIALIZATION(0x8);\n_CREATE_ALIGNED_STORAGE_SPECIALIZATION(0x10);\n_CREATE_ALIGNED_STORAGE_SPECIALIZATION(0x20);\n_CREATE_ALIGNED_STORAGE_SPECIALIZATION(0x40);\n_CREATE_ALIGNED_STORAGE_SPECIALIZATION(0x80);\n_CREATE_ALIGNED_STORAGE_SPECIALIZATION(0x100);\n_CREATE_ALIGNED_STORAGE_SPECIALIZATION(0x200);\n_CREATE_ALIGNED_STORAGE_SPECIALIZATION(0x400);\n_CREATE_ALIGNED_STORAGE_SPECIALIZATION(0x800);\n_CREATE_ALIGNED_STORAGE_SPECIALIZATION(0x1000);\n_CREATE_ALIGNED_STORAGE_SPECIALIZATION(0x2000);\n// PE/COFF does not support alignment beyond 8192 (=0x2000)\n#if !defined(_LIBCUDACXX_OBJECT_FORMAT_COFF)\n_CREATE_ALIGNED_STORAGE_SPECIALIZATION(0x4000);\n#endif // !defined(_LIBCUDACXX_OBJECT_FORMAT_COFF)\n\n#undef _CREATE_ALIGNED_STORAGE_SPECIALIZATION\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_ALIGNED_STORAGE_H\n", "__type_traits/aligned_union.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_ALIGNED_UNION_H\n#define _LIBCUDACXX___TYPE_TRAITS_ALIGNED_UNION_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/aligned_storage.h\"\n#include \"../__type_traits/integral_constant.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <size_t _I0, size_t ..._In>\nstruct __static_max;\n\ntemplate <size_t _I0>\nstruct __static_max<_I0>\n{\n    static const size_t value = _I0;\n};\n\ntemplate <size_t _I0, size_t _I1, size_t ..._In>\nstruct __static_max<_I0, _I1, _In...>\n{\n    static const size_t value = _I0 >= _I1 ? __static_max<_I0, _In...>::value :\n                                             __static_max<_I1, _In...>::value;\n};\n\ntemplate <size_t _Len, class _Type0, class ..._Types>\nstruct aligned_union\n{\n    static const size_t alignment_value = __static_max<_LIBCUDACXX_PREFERRED_ALIGNOF(_Type0),\n                                                       _LIBCUDACXX_PREFERRED_ALIGNOF(_Types)...>::value;\n    static const size_t __len = __static_max<_Len, sizeof(_Type0),\n                                             sizeof(_Types)...>::value;\n    typedef typename aligned_storage<__len, alignment_value>::type type;\n};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <size_t _Len, class ..._Types> using aligned_union_t = typename aligned_union<_Len, _Types...>::type;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_ALIGNED_UNION_H\n", "__type_traits/alignment_of.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_ALIGNMENT_OF_H\n#define _LIBCUDACXX___TYPE_TRAITS_ALIGNMENT_OF_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS alignment_of\n    : public integral_constant<size_t, _LIBCUDACXX_ALIGNOF(_Tp)> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr size_t alignment_of_v = _LIBCUDACXX_ALIGNOF(_Tp);\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_ALIGNMENT_OF_H\n", "__type_traits/apply_cv.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_APPLY_CV_H\n#define _LIBCUDACXX___TYPE_TRAITS_APPLY_CV_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/is_const.h\"\n#include \"../__type_traits/is_volatile.h\"\n#include \"../__type_traits/remove_reference.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp, class _Up, bool = is_const<__libcpp_remove_reference_t<_Tp> >::value,\n                             bool = is_volatile<__libcpp_remove_reference_t<_Tp> >::value>\nstruct __apply_cv\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE _Up type;\n};\n\ntemplate <class _Tp, class _Up>\nstruct __apply_cv<_Tp, _Up, true, false>\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE const _Up type;\n};\n\ntemplate <class _Tp, class _Up>\nstruct __apply_cv<_Tp, _Up, false, true>\n{\n    typedef volatile _Up type;\n};\n\ntemplate <class _Tp, class _Up>\nstruct __apply_cv<_Tp, _Up, true, true>\n{\n    typedef const volatile _Up type;\n};\n\ntemplate <class _Tp, class _Up>\nstruct __apply_cv<_Tp&, _Up, false, false>\n{\n    typedef _Up& type;\n};\n\ntemplate <class _Tp, class _Up>\nstruct __apply_cv<_Tp&, _Up, true, false>\n{\n    typedef const _Up& type;\n};\n\ntemplate <class _Tp, class _Up>\nstruct __apply_cv<_Tp&, _Up, false, true>\n{\n    typedef volatile _Up& type;\n};\n\ntemplate <class _Tp, class _Up>\nstruct __apply_cv<_Tp&, _Up, true, true>\n{\n    typedef const volatile _Up& type;\n};\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_APPLY_CV_H\n", "__type_traits/can_extract_key.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_CAN_EXTRACT_KEY_H\n#define _LIBCUDACXX___TYPE_TRAITS_CAN_EXTRACT_KEY_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__fwd/pair.h\"\n#include \"../__type_traits/conditional.h\"\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_same.h\"\n#include \"../__type_traits/remove_const.h\"\n#include \"../__type_traits/remove_const_ref.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n// These traits are used in __tree and __hash_table\nstruct __extract_key_fail_tag {};\nstruct __extract_key_self_tag {};\nstruct __extract_key_first_tag {};\n\ntemplate <class _ValTy, class _Key, class _RawValTy = __remove_const_ref_t<_ValTy> >\nstruct __can_extract_key\n    : __conditional_t<_IsSame<_RawValTy, _Key>::value, __extract_key_self_tag, __extract_key_fail_tag> {};\n\ntemplate <class _Pair, class _Key, class _First, class _Second>\nstruct __can_extract_key<_Pair, _Key, pair<_First, _Second> >\n    : __conditional_t<_IsSame<__remove_const_t<_First>, _Key>::value, __extract_key_first_tag, __extract_key_fail_tag> {\n};\n\n// __can_extract_map_key uses true_type/false_type instead of the tags.\n// It returns true if _Key != _ContainerValueTy (the container is a map not a set)\n// and _ValTy == _Key.\ntemplate <class _ValTy, class _Key, class _ContainerValueTy,\n          class _RawValTy = __remove_const_ref_t<_ValTy> >\nstruct __can_extract_map_key\n    : integral_constant<bool, _IsSame<_RawValTy, _Key>::value> {};\n\n// This specialization returns __extract_key_fail_tag for non-map containers\n// because _Key == _ContainerValueTy\ntemplate <class _ValTy, class _Key, class _RawValTy>\nstruct __can_extract_map_key<_ValTy, _Key, _Key, _RawValTy>\n    : false_type {};\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_CAN_EXTRACT_KEY_H\n", "__type_traits/common_reference.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n// SPDX-FileCopyrightText: Copyright (c) Microsoft Corporation.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_COMMON_REFERENCE_H\n#define _LIBCUDACXX___TYPE_TRAITS_COMMON_REFERENCE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/common_type.h\"\n#include \"../__type_traits/conditional.h\"\n#include \"../__type_traits/copy_cv.h\"\n#include \"../__type_traits/copy_cvref.h\"\n#include \"../__type_traits/disjunction.h\"\n#include \"../__type_traits/enable_if.h\"\n#include \"../__type_traits/is_array.h\"\n#include \"../__type_traits/is_convertible.h\"\n#include \"../__type_traits/is_reference.h\"\n#include \"../__type_traits/is_same.h\"\n#include \"../__type_traits/is_scalar.h\"\n#include \"../__type_traits/remove_reference.h\"\n#include \"../__type_traits/remove_cvref.h\"\n#include \"../__type_traits/void_t.h\"\n#include \"../__utility/declval.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n// common_reference\n#if _LIBCUDACXX_STD_VER > 11\n\n// Let COND_RES(X, Y) be:\n#ifdef _LIBCUDACXX_COMPILER_MSVC // Workaround for DevCom-1627396\ntemplate <class _Tp>\n_Tp __returns_exactly() noexcept; // not defined\n\ntemplate <class _Xp, class _Yp>\nusing __cond_res_if_right = decltype(false ? __returns_exactly<_Xp>() : __returns_exactly<_Yp>());\n\ntemplate <class _Tp, class _Up, class = void>\nstruct __cond_res_workaround {};\n\ntemplate <class _Tp, class _Up>\nstruct __cond_res_workaround<_Tp, _Up, void_t<__cond_res_if_right<_Tp, _Up>>> {\n    using _RTp = remove_cvref_t<_Tp>;\n    using type = conditional_t<is_same_v<_RTp, remove_cvref_t<_Up>> &&\n                               (is_scalar_v<_RTp> || is_array_v<_RTp>) &&\n                               ((is_lvalue_reference_v<_Tp> && is_rvalue_reference_v<_Up>) || (is_rvalue_reference_v<_Tp> && is_lvalue_reference_v<_Up>)),\n                 decay_t<__copy_cv_t<remove_reference_t<_Tp>, remove_reference_t<_Up>>>, __cond_res_if_right<_Tp, _Up>>;\n};\n\ntemplate <class _Xp, class _Yp>\nusing __cond_res = typename __cond_res_workaround<_Xp, _Yp>::type;\n#else // ^^^ MSVC ^^^ / vvv !MSVC vvv\ntemplate <class _Xp, class _Yp>\nusing __cond_res =\n    decltype(false ? _CUDA_VSTD::declval<_Xp(&)()>()() : _CUDA_VSTD::declval<_Yp(&)()>()());\n#endif // !MSVC\n\n// Let `XREF(A)` denote a unary alias template `T` such that `T<U>` denotes the same type as `U`\n// with the addition of `A`'s cv and reference qualifiers, for a non-reference cv-unqualified type\n// `U`.\n// [Note: `XREF(A)` is `__xref<A>::template __apply`]\ntemplate <class _Tp>\nstruct __xref {\n  template<class _Up>\n  using __apply = __copy_cvref_t<_Tp, _Up>;\n};\n\n// Given types A and B, let X be remove_reference_t<A>, let Y be remove_reference_t<B>,\n// and let COMMON-REF(A, B) be:\ntemplate<class _Ap, class _Bp, class = void>\nstruct __common_ref;\n\ntemplate<class _Xp, class _Yp>\nusing __common_ref_t = typename __common_ref<_Xp, _Yp>::__type;\n\ntemplate<class _Xp, class _Yp>\nusing __cv_cond_res = __cond_res<__copy_cv_t<_Xp, _Yp>&, __copy_cv_t<_Yp, _Xp>&>;\n\n\n//    If A and B are both lvalue reference types, COMMON-REF(A, B) is\n//    COND-RES(COPYCV(X, Y)&, COPYCV(Y, X)&) if that type exists and is a reference type.\ntemplate<class _Ap, class _Bp>\nstruct __common_ref<_Ap&, _Bp&, enable_if_t<is_reference_v<__cv_cond_res<_Ap, _Bp>>>>\n{\n    using __type = __cv_cond_res<_Ap, _Bp>;\n};\n\n//    Otherwise, let C be remove_reference_t<COMMON-REF(X&, Y&)>&&. ...\ntemplate <class _Xp, class _Yp>\nusing __common_ref_C = remove_reference_t<__common_ref_t<_Xp&, _Yp&>>&&;\n\n\n//    .... If A and B are both rvalue reference types, C is well-formed, and\n//    is_convertible_v<A, C> && is_convertible_v<B, C> is true, then COMMON-REF(A, B) is C.\ntemplate<class _Ap, class _Bp, class = void>\nstruct __common_ref_rr {};\n\ntemplate<class _Ap, class _Bp>\nstruct __common_ref_rr<_Ap&&, _Bp&&, enable_if_t<\n                            is_convertible_v<_Ap&&, __common_ref_C<_Ap, _Bp>>\n                         && is_convertible_v<_Bp&&, __common_ref_C<_Ap, _Bp>>>>\n{\n    using __type = __common_ref_C<_Ap, _Bp>;\n};\n\ntemplate<class _Ap, class _Bp>\nstruct __common_ref<_Ap&&, _Bp&&> : __common_ref_rr<_Ap&&, _Bp&&> {};\n\n//    Otherwise, let D be COMMON-REF(const X&, Y&). ...\ntemplate <class _Tp, class _Up>\nusing __common_ref_D = __common_ref_t<const _Tp&, _Up&>;\n\n//    ... If A is an rvalue reference and B is an lvalue reference and D is well-formed and\n//    is_convertible_v<A, D> is true, then COMMON-REF(A, B) is D.\ntemplate<class _Ap, class _Bp, class = void>\nstruct __common_ref_lr {};\n\ntemplate<class _Ap, class _Bp>\nstruct __common_ref_lr<_Ap&&, _Bp&, enable_if_t<is_convertible_v<_Ap&&, __common_ref_D<_Ap, _Bp>>>>\n{\n    using __type = __common_ref_D<_Ap, _Bp>;\n};\n\ntemplate<class _Ap, class _Bp>\nstruct __common_ref<_Ap&&, _Bp&> : __common_ref_lr<_Ap&&, _Bp&> {};\n\n//    Otherwise, if A is an lvalue reference and B is an rvalue reference, then\n//    COMMON-REF(A, B) is COMMON-REF(B, A).\ntemplate<class _Ap, class _Bp>\nstruct __common_ref<_Ap&, _Bp&&> : __common_ref_lr<_Bp&&, _Ap&> {};\n\n//    Otherwise, COMMON-REF(A, B) is ill-formed.\ntemplate<class _Ap, class _Bp, class>\nstruct __common_ref {};\n\n// Note C: For the common_reference trait applied to a parameter pack [...]\n\ntemplate <class...>\nstruct common_reference;\n\ntemplate <class... _Types>\nusing common_reference_t = typename common_reference<_Types...>::type;\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate<class, class, class = void>\n_LIBCUDACXX_INLINE_VAR constexpr bool __has_common_reference = false;\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_INLINE_VAR constexpr bool __has_common_reference<_Tp, _Up, void_t<common_reference_t<_Tp, _Up>>> = true;\n#endif  // _LIBCUDACXX_STD_VER > 11\n\n// bullet 1 - sizeof...(T) == 0\ntemplate<>\nstruct common_reference<> {};\n\n// bullet 2 - sizeof...(T) == 1\ntemplate <class _Tp>\nstruct common_reference<_Tp>\n{\n    using type = _Tp;\n};\n\n// bullet 3 - sizeof...(T) == 2\ntemplate <class _Tp, class _Up, class = void> struct __common_reference_sub_bullet3;\ntemplate <class _Tp, class _Up, class = void> struct __common_reference_sub_bullet2\n    : __common_reference_sub_bullet3<_Tp, _Up> {};\ntemplate <class _Tp, class _Up, class = void> struct __common_reference_sub_bullet1\n    : __common_reference_sub_bullet2<_Tp, _Up> {};\n\n// sub-bullet 1 - If T1 and T2 are reference types and COMMON-REF(T1, T2) is well-formed, then\n// the member typedef `type` denotes that type.\ntemplate <class _Tp, class _Up> struct common_reference<_Tp, _Up> : __common_reference_sub_bullet1<_Tp, _Up> {};\n\ntemplate <class _Tp, class _Up>\nstruct __common_reference_sub_bullet1<_Tp, _Up, void_t<__common_ref_t<_Tp, _Up>,\n    enable_if_t<is_reference_v<_Tp> && is_reference_v<_Up>>>>\n{\n    using type = __common_ref_t<_Tp, _Up>;\n};\n\n// sub-bullet 2 - Otherwise, if basic_common_reference<remove_cvref_t<T1>, remove_cvref_t<T2>, XREF(T1), XREF(T2)>::type\n// is well-formed, then the member typedef `type` denotes that type.\ntemplate <class, class, template <class> class, template <class> class> struct basic_common_reference {};\n\ntemplate <class _Tp, class _Up>\nusing __basic_common_reference_t = typename basic_common_reference<\n    remove_cvref_t<_Tp>, remove_cvref_t<_Up>,\n    __xref<_Tp>::template __apply, __xref<_Up>::template __apply>::type;\n\ntemplate <class _Tp, class _Up>\nstruct __common_reference_sub_bullet2<_Tp, _Up, void_t<__basic_common_reference_t<_Tp, _Up>>>\n{\n    using type = __basic_common_reference_t<_Tp, _Up>;\n};\n\n// sub-bullet 3 - Otherwise, if COND-RES(T1, T2) is well-formed,\n// then the member typedef `type` denotes that type.\ntemplate <class _Tp, class _Up>\nstruct __common_reference_sub_bullet3<_Tp, _Up, void_t<__cond_res<_Tp, _Up>>>\n{\n    using type = __cond_res<_Tp, _Up>;\n};\n\n\n// sub-bullet 4 & 5 - Otherwise, if common_type_t<T1, T2> is well-formed,\n//                    then the member typedef `type` denotes that type.\n//                  - Otherwise, there shall be no member `type`.\ntemplate <class _Tp, class _Up, class> struct __common_reference_sub_bullet3 : common_type<_Tp, _Up> {};\n\n// bullet 4 - If there is such a type `C`, the member typedef type shall denote the same type, if\n//            any, as `common_reference_t<C, Rest...>`.\ntemplate <class _Tp, class _Up, class _Vp, class... _Rest>\nstruct common_reference<_Tp, _Up, _Vp, void_t<common_reference_t<_Tp, _Up>>, _Rest...>\n    : common_reference<common_reference_t<_Tp, _Up>, _Vp, _Rest...>\n{};\n\n// bullet 5 - Otherwise, there shall be no member `type`.\ntemplate <class...> struct common_reference {};\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_COMMON_REFERENCE_H\n", "__type_traits/common_type.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_COMMON_TYPE_H\n#define _LIBCUDACXX___TYPE_TRAITS_COMMON_TYPE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/decay.h\"\n#include \"../__type_traits/is_same.h\"\n#include \"../__type_traits/remove_cvref.h\"\n#include \"../__type_traits/void_t.h\"\n#include \"../__utility/declval.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class... _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS common_type;\n\ntemplate <class ..._Tp>\nusing __common_type_t = typename common_type<_Tp...>::type;\n\n// Let COND_RES(X, Y) be:\ntemplate <class _Tp, class _Up>\nusing __cond_type = decltype(false ? declval<_Tp>() : declval<_Up>());\n\n#if _LIBCUDACXX_STD_VER > 17\ntemplate <class _Tp, class _Up, class = void>\nstruct __common_type3 {};\n\n// sub-bullet 4 - \"if COND_RES(CREF(D1), CREF(D2)) denotes a type...\"\ntemplate <class _Tp, class _Up>\nstruct __common_type3<_Tp, _Up, void_t<__cond_type<const _Tp&, const _Up&>>>\n{\n    using type = remove_cvref_t<__cond_type<const _Tp&, const _Up&>>;\n};\n\ntemplate <class _Tp, class _Up, class = void>\nstruct __common_type2_imp : __common_type3<_Tp, _Up> {};\n#else\ntemplate <class _Tp, class _Up, class = void>\nstruct __common_type2_imp {};\n#endif\n\n// sub-bullet 3 - \"if decay_t<decltype(false ? declval<D1>() : declval<D2>())> ...\"\ntemplate <class _Tp, class _Up>\nstruct __common_type2_imp<_Tp, _Up, __void_t<__cond_type<_Tp, _Up>>>\n{\n  typedef _LIBCUDACXX_NODEBUG_TYPE __decay_t<__cond_type<_Tp, _Up>> type;\n};\n\ntemplate <class, class = void>\nstruct __common_type_impl {};\n\ntemplate <class... _Tp>\nstruct __common_types;\n\ntemplate <class _Tp, class _Up>\nstruct __common_type_impl<\n    __common_types<_Tp, _Up>, __void_t<__common_type_t<_Tp, _Up>> >\n{\n  typedef __common_type_t<_Tp, _Up> type;\n};\n\ntemplate <class _Tp, class _Up, class _Vp, class... _Rest>\nstruct __common_type_impl<__common_types<_Tp, _Up, _Vp, _Rest...>, __void_t<__common_type_t<_Tp, _Up>> >\n    : __common_type_impl<__common_types<__common_type_t<_Tp, _Up>, _Vp, _Rest...>> {};\n\n// bullet 1 - sizeof...(Tp) == 0\n\ntemplate <>\nstruct _LIBCUDACXX_TEMPLATE_VIS common_type<> {};\n\n// bullet 2 - sizeof...(Tp) == 1\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS common_type<_Tp>\n    : public common_type<_Tp, _Tp> {};\n\n// bullet 3 - sizeof...(Tp) == 2\n\n// sub-bullet 1 - \"If is_same_v<T1, D1> is false or ...\"\ntemplate <class _Tp, class _Up, class _D1 = __decay_t<_Tp>, class _D2 = __decay_t<_Up>>\nstruct __common_type2 : common_type<_D1, _D2> {};\n\ntemplate <class _Tp, class _Up>\nstruct __common_type2<_Tp, _Up, _Tp, _Up> : __common_type2_imp<_Tp, _Up> {};\n\ntemplate <class _Tp, class _Up>\nstruct _LIBCUDACXX_TEMPLATE_VIS common_type<_Tp, _Up>\n    : __common_type2<_Tp, _Up> {};\n\n// bullet 4 - sizeof...(Tp) > 2\n\ntemplate <class _Tp, class _Up, class _Vp, class... _Rest>\nstruct _LIBCUDACXX_TEMPLATE_VIS common_type<_Tp, _Up, _Vp, _Rest...>\n    : __common_type_impl<__common_types<_Tp, _Up, _Vp, _Rest...> > {};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class ..._Tp> using common_type_t = typename common_type<_Tp...>::type;\n\ntemplate<class, class, class = void>\n_LIBCUDACXX_INLINE_VAR constexpr bool __has_common_type = false;\n\ntemplate<class _Tp, class _Up>\n_LIBCUDACXX_INLINE_VAR constexpr bool __has_common_type<_Tp, _Up, void_t<common_type_t<_Tp, _Up>>> = true;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_COMMON_TYPE_H\n", "__type_traits/conditional.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_CONDITIONAL_H\n#define _LIBCUDACXX___TYPE_TRAITS_CONDITIONAL_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <bool>\nstruct _IfImpl;\n\ntemplate <>\nstruct _IfImpl<true> {\n  template <class _IfRes, class _ElseRes>\n  using _Select _LIBCUDACXX_NODEBUG_TYPE = _IfRes;\n};\n\ntemplate <>\nstruct _IfImpl<false> {\n  template <class _IfRes, class _ElseRes>\n  using _Select _LIBCUDACXX_NODEBUG_TYPE = _ElseRes;\n};\n\ntemplate <bool _Cond, class _IfRes, class _ElseRes>\nusing _If _LIBCUDACXX_NODEBUG_TYPE = typename _IfImpl<_Cond>::template _Select<_IfRes, _ElseRes>;\n\ntemplate <bool _Bp, class _If, class _Then>\n    struct _LIBCUDACXX_TEMPLATE_VIS conditional {typedef _If type;};\ntemplate <class _If, class _Then>\n    struct _LIBCUDACXX_TEMPLATE_VIS conditional<false, _If, _Then> {typedef _Then type;};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <bool _Bp, class _IfRes, class _ElseRes>\nusing conditional_t = typename conditional<_Bp, _IfRes, _ElseRes>::type;\n#endif\n\n// Helper so we can use \"conditional_t\" in all language versions.\ntemplate <bool _Bp, class _If, class _Then> using __conditional_t = typename conditional<_Bp, _If, _Then>::type;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_CONDITIONAL_H\n", "__type_traits/conjunction.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_CONJUNCTION_H\n#define _LIBCUDACXX___TYPE_TRAITS_CONJUNCTION_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/conditional.h\"\n#include \"../__type_traits/enable_if.h\"\n#include \"../__type_traits/integral_constant.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class...>\nusing __expand_to_true = true_type;\n\ntemplate <class... _Pred>\n_LIBCUDACXX_INLINE_VISIBILITY __expand_to_true<__enable_if_t<_Pred::value>...> __and_helper(int);\n\ntemplate <class...>\n_LIBCUDACXX_INLINE_VISIBILITY false_type __and_helper(...);\n\n// _And always performs lazy evaluation of its arguments.\n//\n// However, `_And<_Pred...>` itself will evaluate its result immediately (without having to\n// be instantiated) since it is an alias, unlike `conjunction<_Pred...>`, which is a struct.\n// If you want to defer the evaluation of `_And<_Pred...>` itself, use `_Lazy<_And, _Pred...>`.\ntemplate <class... _Pred>\nusing _And _LIBCUDACXX_NODEBUG_TYPE = decltype(__and_helper<_Pred...>(0));\n\n#if _LIBCUDACXX_STD_VER > 11\n\ntemplate <class...>\nstruct conjunction : true_type {};\n\ntemplate <class _Arg>\nstruct conjunction<_Arg> : _Arg {};\n\ntemplate <class _Arg, class... _Args>\nstruct conjunction<_Arg, _Args...> : conditional_t<!bool(_Arg::value), _Arg, conjunction<_Args...>> {};\n\ntemplate <class... _Args>\n_LIBCUDACXX_INLINE_VAR constexpr bool conjunction_v = conjunction<_Args...>::value;\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_CONJUNCTION_H\n", "__type_traits/copy_cv.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_COPY_CV_H\n#define _LIBCUDACXX___TYPE_TRAITS_COPY_CV_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_const.h\"\n#include \"../__type_traits/add_cv.h\"\n#include \"../__type_traits/add_volatile.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n// Let COPYCV(FROM, TO) be an alias for type TO with the addition of FROM's\n// top-level cv-qualifiers.\ntemplate <class _From, class _To>\nstruct __copy_cv\n{\n    using type = _To;\n};\n\ntemplate <class _From, class _To>\nstruct __copy_cv<const _From, _To>\n{\n    using type = typename add_const<_To>::type;\n};\n\ntemplate <class _From, class _To>\nstruct __copy_cv<volatile _From, _To>\n{\n    using type = typename add_volatile<_To>::type;\n};\n\ntemplate <class _From, class _To>\nstruct __copy_cv<const volatile _From, _To>\n{\n    using type = typename add_cv<_To>::type;\n};\n\ntemplate <class _From, class _To>\nusing __copy_cv_t = typename __copy_cv<_From, _To>::type;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_COPY_CV_H\n", "__type_traits/copy_cvref.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_COPY_CVREF_H\n#define _LIBCUDACXX___TYPE_TRAITS_COPY_CVREF_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_lvalue_reference.h\"\n#include \"../__type_traits/add_rvalue_reference.h\"\n#include \"../__type_traits/copy_cv.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _From, class _To>\nstruct __copy_cvref\n{\n    using type = __copy_cv_t<_From, _To>;\n};\n\ntemplate <class _From, class _To>\nstruct __copy_cvref<_From&, _To>\n{\n    using type = __add_lvalue_reference_t<__copy_cv_t<_From, _To> >;\n};\n\ntemplate <class _From, class _To>\nstruct __copy_cvref<_From&&, _To>\n{\n    using type = __add_rvalue_reference_t<__copy_cv_t<_From, _To> >;\n};\n\ntemplate <class _From, class _To>\nusing __copy_cvref_t = typename __copy_cvref<_From, _To>::type;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_COPY_CVREF_H\n", "__type_traits/decay.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_DECAY_H\n#define _LIBCUDACXX___TYPE_TRAITS_DECAY_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_pointer.h\"\n#include \"../__type_traits/conditional.h\"\n#include \"../__type_traits/is_array.h\"\n#include \"../__type_traits/is_function.h\"\n#include \"../__type_traits/is_referenceable.h\"\n#include \"../__type_traits/remove_cv.h\"\n#include \"../__type_traits/remove_extent.h\"\n#include \"../__type_traits/remove_reference.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_DECAY) && !defined(_LIBCUDACXX_USE_DECAY_FALLBACK)\ntemplate <class _Tp>\nstruct decay {\n  using type _LIBCUDACXX_NODEBUG_TYPE = _LIBCUDACXX_DECAY(_Tp);\n};\n\n#else\n\ntemplate <class _Up, bool>\nstruct __decay_impl {\n    typedef _LIBCUDACXX_NODEBUG_TYPE __remove_cv_t<_Up> type;\n};\n\ntemplate <class _Up>\nstruct __decay_impl<_Up, true> {\npublic:\n    typedef _LIBCUDACXX_NODEBUG_TYPE __conditional_t\n                     <\n                         is_array<_Up>::value,\n                         __remove_extent_t<_Up>*,\n                         __conditional_t\n                         <\n                              is_function<_Up>::value,\n                              __add_pointer_t<_Up>,\n                              __remove_cv_t<_Up>\n                         >\n                     > type;\n};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS decay\n{\nprivate:\n    typedef _LIBCUDACXX_NODEBUG_TYPE __libcpp_remove_reference_t<_Tp> _Up;\npublic:\n  typedef _LIBCUDACXX_NODEBUG_TYPE typename __decay_impl<_Up, __libcpp_is_referenceable<_Up>::value>::type type;\n};\n#endif // defined(_LIBCUDACXX_DECAY) && !defined(_LIBCUDACXX_USE_DECAY_FALLBACK)\n\ntemplate <class _Tp> using __decay_t = typename decay<_Tp>::type;\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> using decay_t = typename decay<_Tp>::type;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_DECAY_H\n", "__type_traits/dependent_type.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_DEPENDENT_TYPE_H\n#define _LIBCUDACXX___TYPE_TRAITS_DEPENDENT_TYPE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp, bool>\nstruct _LIBCUDACXX_TEMPLATE_VIS __dependent_type : public _Tp {};\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_DEPENDENT_TYPE_H\n", "__type_traits/disjunction.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_DISJUNCTION_H\n#define _LIBCUDACXX___TYPE_TRAITS_DISJUNCTION_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <bool>\nstruct _OrImpl;\n\ntemplate <>\nstruct _OrImpl<true> {\n  template <class _Res, class _First, class... _Rest>\n  using _Result _LIBCUDACXX_NODEBUG_TYPE =\n      typename _OrImpl<!bool(_First::value) && sizeof...(_Rest) != 0>::template _Result<_First, _Rest...>;\n};\n\ntemplate <>\nstruct _OrImpl<false> {\n  template <class _Res, class...>\n  using _Result = _Res;\n};\n\n// _Or always performs lazy evaluation of its arguments.\n//\n// However, `_Or<_Pred...>` itself will evaluate its result immediately (without having to\n// be instantiated) since it is an alias, unlike `disjunction<_Pred...>`, which is a struct.\n// If you want to defer the evaluation of `_Or<_Pred...>` itself, use `_Lazy<_Or, _Pred...>`\n// or `disjunction<_Pred...>` directly.\ntemplate <class... _Args>\nusing _Or _LIBCUDACXX_NODEBUG_TYPE = typename _OrImpl<sizeof...(_Args) != 0>::template _Result<false_type, _Args...>;\n\n#if _LIBCUDACXX_STD_VER > 11\n\n#ifdef _LIBCUDACXX_COMPILER_MSVC\ntemplate <class... _Args>\nstruct disjunction : false_type {};\n\ntemplate <class _First, class... _Rest>\nstruct disjunction<_First, _Rest...> : _OrImpl<true>::template _Result<false_type, _First, _Rest...> {};\n#else\ntemplate <class... _Args>\nstruct disjunction : _Or<_Args...> {};\n#endif // !MSVC\n\ntemplate <class... _Args>\n_LIBCUDACXX_INLINE_VAR constexpr bool disjunction_v = _Or<_Args...>::value;\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_DISJUNCTION_H\n", "__type_traits/enable_if.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_ENABLE_IF_H\n#define _LIBCUDACXX___TYPE_TRAITS_ENABLE_IF_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <bool, class _Tp = void> struct _LIBCUDACXX_TEMPLATE_VIS enable_if {};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS enable_if<true, _Tp> {typedef _Tp type;};\n\ntemplate <bool _Bp, class _Tp = void> using __enable_if_t _LIBCUDACXX_NODEBUG_TYPE = typename enable_if<_Bp, _Tp>::type;\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <bool _Bp, class _Tp = void> using enable_if_t = typename enable_if<_Bp, _Tp>::type;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_ENABLE_IF_H\n", "__type_traits/extent.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_EXTENT_H\n#define _LIBCUDACXX___TYPE_TRAITS_EXTENT_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_ARRAY_EXTENT) && !defined(_LIBCUDACXX_USE_ARRAY_EXTENT_FALLBACK)\n\ntemplate<class _Tp, size_t _Dim = 0>\nstruct _LIBCUDACXX_TEMPLATE_VIS extent\n    : integral_constant<size_t, _LIBCUDACXX_ARRAY_EXTENT(_Tp, _Dim)> { };\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, unsigned _Ip = 0>\n_LIBCUDACXX_INLINE_VAR constexpr size_t extent_v = _LIBCUDACXX_ARRAY_EXTENT(_Tp, _Ip);\n#endif\n\n#else\n\ntemplate <class _Tp, unsigned _Ip = 0> struct _LIBCUDACXX_TEMPLATE_VIS extent\n    : public integral_constant<size_t, 0> {};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS extent<_Tp[], 0>\n    : public integral_constant<size_t, 0> {};\ntemplate <class _Tp, unsigned _Ip> struct _LIBCUDACXX_TEMPLATE_VIS extent<_Tp[], _Ip>\n    : public integral_constant<size_t, extent<_Tp, _Ip-1>::value> {};\ntemplate <class _Tp, size_t _Np> struct _LIBCUDACXX_TEMPLATE_VIS extent<_Tp[_Np], 0>\n    : public integral_constant<size_t, _Np> {};\ntemplate <class _Tp, size_t _Np, unsigned _Ip> struct _LIBCUDACXX_TEMPLATE_VIS extent<_Tp[_Np], _Ip>\n    : public integral_constant<size_t, extent<_Tp, _Ip-1>::value> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, unsigned _Ip = 0>\n_LIBCUDACXX_INLINE_VAR constexpr size_t extent_v = extent<_Tp, _Ip>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_ARRAY_EXTENT) && !defined(_LIBCUDACXX_USE_ARRAY_EXTENT_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_EXTENT_H\n", "__type_traits/has_unique_object_representation.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_HAS_UNIQUE_OBJECT_REPRESENTATION_H\n#define _LIBCUDACXX___TYPE_TRAITS_HAS_UNIQUE_OBJECT_REPRESENTATION_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/remove_all_extents.h\"\n#include \"../__type_traits/remove_cv.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 11 && defined(_LIBCUDACXX_HAS_UNIQUE_OBJECT_REPRESENTATIONS)\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS has_unique_object_representations\n    : public integral_constant<bool,\n       __has_unique_object_representations(remove_cv_t<remove_all_extents_t<_Tp>>)> {};\n\n#if !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool has_unique_object_representations_v = has_unique_object_representations<_Tp>::value;\n#endif\n\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_HAS_UNIQUE_OBJECT_REPRESENTATION_H\n", "__type_traits/has_virtual_destructor.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_HAS_VIRTUAL_DESTRUCTOR_H\n#define _LIBCUDACXX___TYPE_TRAITS_HAS_VIRTUAL_DESTRUCTOR_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_HAS_VIRTUAL_DESTRUCTOR) && !defined(_LIBCUDACXX_USE_HAS_VIRTUAL_DESTRUCTOR_FALLBACK)\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS has_virtual_destructor\n    : public integral_constant<bool, _LIBCUDACXX_HAS_VIRTUAL_DESTRUCTOR(_Tp)> {};\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS has_virtual_destructor\n    : public false_type {};\n\n#endif // defined(_LIBCUDACXX_HAS_VIRTUAL_DESTRUCTOR) && !defined(_LIBCUDACXX_USE_HAS_VIRTUAL_DESTRUCTOR_FALLBACK)\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool has_virtual_destructor_v\n    = has_virtual_destructor<_Tp>::value;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_HAS_VIRTUAL_DESTRUCTOR_H\n", "__type_traits/integral_constant.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_INTEGRAL_CONSTANT_H\n#define _LIBCUDACXX___TYPE_TRAITS_INTEGRAL_CONSTANT_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp, _Tp __v>\nstruct _LIBCUDACXX_TEMPLATE_VIS integral_constant\n{\n  static constexpr const _Tp      value = __v;\n  typedef _Tp               value_type;\n  typedef integral_constant type;\n  _LIBCUDACXX_INLINE_VISIBILITY\n  constexpr operator value_type() const noexcept {return value;}\n#if _LIBCUDACXX_STD_VER > 11\n  _LIBCUDACXX_INLINE_VISIBILITY\n  constexpr value_type operator ()() const noexcept {return value;}\n#endif\n};\n\ntemplate <class _Tp, _Tp __v>\nconstexpr const _Tp integral_constant<_Tp, __v>::value;\n\ntypedef integral_constant<bool, true>  true_type;\ntypedef integral_constant<bool, false> false_type;\n\ntemplate <bool _Val>\nusing _BoolConstant _LIBCUDACXX_NODEBUG_TYPE = integral_constant<bool, _Val>;\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <bool __b>\nusing bool_constant = integral_constant<bool, __b>;\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\n#define _LIBCUDACXX_BOOL_CONSTANT(__b) bool_constant<(__b)>\n#else\n#define _LIBCUDACXX_BOOL_CONSTANT(__b) integral_constant<bool,(__b)>\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_INTEGRAL_CONSTANT_H\n", "__type_traits/is_abstract.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_ABSTRACT_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_ABSTRACT_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_abstract\n    : public integral_constant<bool, __is_abstract(_Tp)> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_abstract_v = __is_abstract(_Tp);\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_ABSTRACT_H\n", "__type_traits/is_aggregate.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_AGGREGATE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_AGGREGATE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 11 && defined(_LIBCUDACXX_IS_AGGREGATE)\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS\nis_aggregate : public integral_constant<bool, _LIBCUDACXX_IS_AGGREGATE(_Tp)> {};\n\n#if !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_aggregate_v = _LIBCUDACXX_IS_AGGREGATE(_Tp);\n#endif\n\n#endif // _LIBCUDACXX_STD_VER > 11 && defined(_LIBCUDACXX_IS_AGGREGATE)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_AGGREGATE_H\n", "__type_traits/is_allocator.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_IS_ALLOCATOR_H\n#define _LIBCUDACXX___TYPE_IS_ALLOCATOR_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/void_t.h\"\n#include \"../__utility/declval.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate<typename _Alloc, typename = void, typename = void>\nstruct __is_allocator : false_type {};\n\ntemplate<typename _Alloc>\nstruct __is_allocator<_Alloc,\n       __void_t<typename _Alloc::value_type>,\n       __void_t<decltype(_CUDA_VSTD::declval<_Alloc&>().allocate(size_t(0)))>\n     >\n   : true_type {};\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_IS_ALLOCATOR_H\n", "__type_traits/is_arithmetic.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_ARITHMETIC_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_ARITHMETIC_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_floating_point.h\"\n#include \"../__type_traits/is_integral.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_arithmetic\n    : public integral_constant<bool, is_integral<_Tp>::value      ||\n                                     is_floating_point<_Tp>::value> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_arithmetic_v = is_arithmetic<_Tp>::value;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_ARITHMETIC_H\n", "__type_traits/is_array.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_ARRAY_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_ARRAY_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n// TODO: Clang incorrectly reports that __is_array is true for T[0].\n//       Re-enable the branch once https://llvm.org/PR54705 is fixed.\n#if defined(_LIBCUDACXX_IS_ARRAY) && !defined(_LIBCUDACXX_USE_IS_ARRAY_FALLBACK)\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_array\n    : public integral_constant<bool, _LIBCUDACXX_IS_ARRAY(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_array_v = _LIBCUDACXX_IS_ARRAY(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_array\n    : public false_type {};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_array<_Tp[]>\n    : public true_type {};\ntemplate <class _Tp, size_t _Np> struct _LIBCUDACXX_TEMPLATE_VIS is_array<_Tp[_Np]>\n    : public true_type {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_array_v = is_array<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_ARRAY) && !defined(_LIBCUDACXX_USE_IS_ARRAY_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_ARRAY_H\n", "__type_traits/is_assignable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_ASSIGNABLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_ASSIGNABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_void.h\"\n#include \"../__utility/declval.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate<typename, typename _Tp> struct __select_2nd { typedef _LIBCUDACXX_NODEBUG_TYPE _Tp type; };\n\n#if defined(_LIBCUDACXX_IS_ASSIGNABLE) && !defined(_LIBCUDACXX_USE_IS_ASSIGNABLE_FALLBACK)\n\ntemplate <class _T1, class _T2> struct _LIBCUDACXX_TEMPLATE_VIS is_assignable\n    : public integral_constant<bool, _LIBCUDACXX_IS_ASSIGNABLE(_T1, _T2)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _T1, class _T2>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_assignable_v = _LIBCUDACXX_IS_ASSIGNABLE(_T1, _T2);\n#endif\n\n#else\n\ntemplate <class _Tp, class _Arg>\n_LIBCUDACXX_INLINE_VISIBILITY\ntypename __select_2nd<decltype((_CUDA_VSTD::declval<_Tp>() = _CUDA_VSTD::declval<_Arg>())), true_type>::type\n__is_assignable_test(int);\n\ntemplate <class, class>\n_LIBCUDACXX_INLINE_VISIBILITY\nfalse_type __is_assignable_test(...);\n\ntemplate <class _Tp, class _Arg, bool = is_void<_Tp>::value || is_void<_Arg>::value>\nstruct __is_assignable_imp\n    : public decltype((_CUDA_VSTD::__is_assignable_test<_Tp, _Arg>(0))) {};\n\ntemplate <class _Tp, class _Arg>\nstruct __is_assignable_imp<_Tp, _Arg, true>\n    : public false_type\n{\n};\n\ntemplate <class _Tp, class _Arg>\nstruct is_assignable\n    : public __is_assignable_imp<_Tp, _Arg> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, class _Arg>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_assignable_v = is_assignable<_Tp, _Arg>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_ASSIGNABLE) && !defined(_LIBCUDACXX_USE_IS_ASSIGNABLE_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_ASSIGNABLE_H\n", "__type_traits/is_base_of.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_BASE_OF_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_BASE_OF_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_BASE_OF) && !defined(_LIBCUDACXX_USE_IS_BASE_OF_FALLBACK)\n\ntemplate <class _Bp, class _Dp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_base_of\n    : public integral_constant<bool, _LIBCUDACXX_IS_BASE_OF(_Bp, _Dp)> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Bp, class _Dp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_base_of_v = _LIBCUDACXX_IS_BASE_OF(_Bp, _Dp);\n#endif\n\n#else  // defined(_LIBCUDACXX_IS_BASE_OF) && !defined(_LIBCUDACXX_USE_IS_BASE_OF_FALLBACK)\n\nnamespace __is_base_of_imp\n{\ntemplate <class _Tp>\nstruct _Dst\n{\n    _Dst(const volatile _Tp &);\n};\ntemplate <class _Tp>\nstruct _Src\n{\n    operator const volatile _Tp &();\n    template <class _Up> operator const _Dst<_Up> &();\n};\ntemplate <size_t> struct __one { typedef char type; };\ntemplate <class _Bp, class _Dp> typename __one<sizeof(_Dst<_Bp>(declval<_Src<_Dp> >()))>::type __test(int);\ntemplate <class _Bp, class _Dp> __two __test(...);\n}\n\ntemplate <class _Bp, class _Dp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_base_of\n    : public integral_constant<bool, is_class<_Bp>::value &&\n                                     sizeof(__is_base_of_imp::__test<_Bp, _Dp>(0)) == 2> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Bp, class _Dp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_base_of_v = is_base_of<_Bp, _Dp>::value;\n#endif\n\n#endif  // defined(_LIBCUDACXX_IS_BASE_OF) && !defined(_LIBCUDACXX_USE_IS_BASE_OF_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_BASE_OF_H\n", "__type_traits/is_bounded_array.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_BOUNDED_ARRAY_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_BOUNDED_ARRAY_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class>                 struct _LIBCUDACXX_TEMPLATE_VIS __libcpp_is_bounded_array           : false_type {};\ntemplate <class _Tp, size_t _Np> struct _LIBCUDACXX_TEMPLATE_VIS __libcpp_is_bounded_array<_Tp[_Np]> : true_type {};\n\n#if _LIBCUDACXX_STD_VER > 11\n\ntemplate <class>                 struct _LIBCUDACXX_TEMPLATE_VIS is_bounded_array           : false_type {};\ntemplate <class _Tp, size_t _Np> struct _LIBCUDACXX_TEMPLATE_VIS is_bounded_array<_Tp[_Np]> : true_type {};\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_bounded_array_v  = is_bounded_array<_Tp>::value;\n\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_BOUNDED_ARRAY_H\n", "__type_traits/is_callable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_CALLABLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_CALLABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__utility/declval.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate<class _Func, class... _Args, class = decltype(std::declval<_Func>()(std::declval<_Args>()...))>\n_LIBCUDACXX_INLINE_VISIBILITY true_type __is_callable_helper(int);\ntemplate<class...>\n_LIBCUDACXX_INLINE_VISIBILITY false_type __is_callable_helper(...);\n\ntemplate<class _Func, class... _Args>\nstruct __is_callable : decltype(__is_callable_helper<_Func, _Args...>(0)) {};\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_CALLABLE_H\n", "__type_traits/is_char_like_type.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_CHAR_LIKE_TYPE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_CHAR_LIKE_TYPE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/conjunction.h\"\n#include \"../__type_traits/is_standard_layout.h\"\n#include \"../__type_traits/is_trivial.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _CharT>\nusing _IsCharLikeType = _And<is_standard_layout<_CharT>, is_trivial<_CharT> >;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_CHAR_LIKE_TYPE_H\n", "__type_traits/is_class.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_CLASS_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_CLASS_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_union.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\nstruct __two {char __lx[2];};\n\n#if defined(_LIBCUDACXX_IS_CLASS) && !defined(_LIBCUDACXX_USE_IS_CLASS_FALLBACK)\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_class\n    : public integral_constant<bool, _LIBCUDACXX_IS_CLASS(_Tp)> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_class_v = _LIBCUDACXX_IS_CLASS(_Tp);\n#endif\n\n#else\n\nnamespace __is_class_imp\n{\ntemplate <class _Tp> char  __test(int _Tp::*);\ntemplate <class _Tp> __two __test(...);\n}\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_class\n    : public integral_constant<bool, sizeof(__is_class_imp::__test<_Tp>(0)) == 1 && !is_union<_Tp>::value> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_class_v = is_class<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_CLASS) && !defined(_LIBCUDACXX_USE_IS_CLASS_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_CLASS_H\n", "__type_traits/is_compound.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_COMPOUND_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_COMPOUND_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_fundamental.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_COMPOUND) && !defined(_LIBCUDACXX_USE_IS_COMPOUND_FALLBACK)\n\ntemplate<class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_compound\n    : public integral_constant<bool, _LIBCUDACXX_IS_COMPOUND(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_compound_v = _LIBCUDACXX_IS_COMPOUND(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_compound\n    : public integral_constant<bool, !is_fundamental<_Tp>::value> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_compound_v = is_compound<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_COMPOUND) && !defined(_LIBCUDACXX_USE_IS_COMPOUND_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_COMPOUND_H\n", "__type_traits/is_const.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_CONST_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_CONST_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_CONST) && !defined(_LIBCUDACXX_USE_IS_CONST_FALLBACK)\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_const\n    : public integral_constant<bool, _LIBCUDACXX_IS_CONST(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_const_v = _LIBCUDACXX_IS_CONST(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_const            : public false_type {};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_const<_Tp const> : public true_type {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_const_v = is_const<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_CONST) && !defined(_LIBCUDACXX_USE_IS_CONST_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_CONST_H\n", "__type_traits/is_constant_evaluated.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_CONSTANT_EVALUATED_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_CONSTANT_EVALUATED_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_CONSTANT_EVALUATED)\n#if defined(__cuda_std__) || _LIBCUDACXX_STD_VER > 17\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr bool is_constant_evaluated() noexcept {\n  return _LIBCUDACXX_IS_CONSTANT_EVALUATED();\n}\n#endif\n\ninline constexpr _LIBCUDACXX_INLINE_VISIBILITY\nbool __libcpp_is_constant_evaluated() noexcept { return _LIBCUDACXX_IS_CONSTANT_EVALUATED(); }\n#else\ninline constexpr _LIBCUDACXX_INLINE_VISIBILITY\nbool __libcpp_is_constant_evaluated() noexcept { return false; }\n#endif // defined(_LIBCUDACXX_IS_CONSTANT_EVALUATED)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_CONSTANT_EVALUATED_H\n", "__type_traits/is_constructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_IS_CONSTRUCTIBLE_H\n#define _LIBCUDACXX___TYPE_IS_CONSTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/conjunction.h\"\n#include \"../__type_traits/disjunction.h\"\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_base_of.h\"\n#include \"../__type_traits/is_destructible.h\"\n#include \"../__type_traits/is_reference.h\"\n#include \"../__type_traits/is_same.h\"\n#include \"../__type_traits/is_void.h\"\n#include \"../__type_traits/negation.h\"\n#include \"../__type_traits/remove_cvref.h\"\n#include \"../__utility/declval.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n\nnamespace __is_construct\n{\nstruct __nat {};\n}\n\n// FIXME: This logic isn't awesome.\n#if (!defined(_LIBCUDACXX_IS_CONSTRUCTIBLE) || \\\n    defined(_LIBCUDACXX_TESTING_FALLBACK_IS_CONSTRUCTIBLE) || \\\n    defined(_LIBCUDACXX_USE_IS_CONSTRUCTIBLE_FALLBACK))\n\ntemplate <class _Tp, class... _Args>\nstruct __libcpp_is_constructible;\n\ntemplate <class _To, class _From>\nstruct __is_invalid_base_to_derived_cast {\n  static_assert(is_reference<_To>::value, \"Wrong specialization\");\n  using _RawFrom = __remove_cvref_t<_From>;\n  using _RawTo = __remove_cvref_t<_To>;\n  static const bool value = _And<\n        _IsNotSame<_RawFrom, _RawTo>,\n        is_base_of<_RawFrom, _RawTo>,\n        _Not<__libcpp_is_constructible<_RawTo, _From>>\n  >::value;\n};\n\ntemplate <class _To, class _From>\nstruct __is_invalid_lvalue_to_rvalue_cast : false_type {\n  static_assert(is_reference<_To>::value, \"Wrong specialization\");\n};\n\ntemplate <class _ToRef, class _FromRef>\nstruct __is_invalid_lvalue_to_rvalue_cast<_ToRef&&, _FromRef&> {\n  using _RawFrom = __remove_cvref_t<_FromRef>;\n  using _RawTo = __remove_cvref_t<_ToRef>;\n  static const bool value = _And<\n      _Not<is_function<_RawTo>>,\n      _Or<\n        _IsSame<_RawFrom, _RawTo>,\n        is_base_of<_RawTo, _RawFrom>>\n    >::value;\n};\n\nstruct __is_constructible_helper\n{\n    template <class _To>\n    _LIBCUDACXX_INLINE_VISIBILITY\n    static void __eat(_To);\n\n    // This overload is needed to work around a Clang bug that disallows\n    // static_cast<T&&>(e) for non-reference-compatible types.\n    // Example: static_cast<int&&>(declval<double>());\n    // NOTE: The static_cast implementation below is required to support\n    //  classes with explicit conversion operators.\n    template <class _To, class _From,\n              class = decltype(__eat<_To>(_CUDA_VSTD::declval<_From>()))>\n    _LIBCUDACXX_INLINE_VISIBILITY\n    static true_type __test_cast(int);\n\n    template <class _To, class _From,\n              class = decltype(static_cast<_To>(_CUDA_VSTD::declval<_From>()))>\n    _LIBCUDACXX_INLINE_VISIBILITY\n    static integral_constant<bool,\n        !__is_invalid_base_to_derived_cast<_To, _From>::value &&\n        !__is_invalid_lvalue_to_rvalue_cast<_To, _From>::value\n    > __test_cast(long);\n\n    template <class, class>\n    _LIBCUDACXX_INLINE_VISIBILITY\n    static false_type __test_cast(...);\n\n    template <class _Tp, class ..._Args,\n        class = decltype(_Tp(_CUDA_VSTD::declval<_Args>()...))>\n    _LIBCUDACXX_INLINE_VISIBILITY\n    static true_type __test_nary(int);\n    template <class _Tp, class...>\n    _LIBCUDACXX_INLINE_VISIBILITY\n    static false_type __test_nary(...);\n\n    template <class _Tp, class _A0, class = decltype(::new _Tp(_CUDA_VSTD::declval<_A0>()))>\n    _LIBCUDACXX_INLINE_VISIBILITY\n    static is_destructible<_Tp> __test_unary(int);\n    template <class, class>\n    _LIBCUDACXX_INLINE_VISIBILITY\n    static false_type __test_unary(...);\n};\n\ntemplate <class _Tp, bool = is_void<_Tp>::value>\nstruct __is_default_constructible\n    : decltype(__is_constructible_helper::__test_nary<_Tp>(0))\n{};\n\ntemplate <class _Tp>\nstruct __is_default_constructible<_Tp, true> : false_type {};\n\ntemplate <class _Tp>\nstruct __is_default_constructible<_Tp[], false> : false_type {};\n\ntemplate <class _Tp, size_t _Nx>\nstruct __is_default_constructible<_Tp[_Nx], false>\n    : __is_default_constructible<__remove_all_extents_t<_Tp>>  {};\n\ntemplate <class _Tp, class... _Args>\nstruct __libcpp_is_constructible\n{\n  static_assert(sizeof...(_Args) > 1, \"Wrong specialization\");\n  typedef decltype(__is_constructible_helper::__test_nary<_Tp, _Args...>(0))\n      type;\n};\n\ntemplate <class _Tp>\nstruct __libcpp_is_constructible<_Tp> : __is_default_constructible<_Tp> {};\n\ntemplate <class _Tp, class _A0>\nstruct __libcpp_is_constructible<_Tp, _A0>\n    : public decltype(__is_constructible_helper::__test_unary<_Tp, _A0>(0))\n{};\n\ntemplate <class _Tp, class _A0>\nstruct __libcpp_is_constructible<_Tp&, _A0>\n    : public decltype(__is_constructible_helper::\n    __test_cast<_Tp&, _A0>(0))\n{};\n\ntemplate <class _Tp, class _A0>\nstruct __libcpp_is_constructible<_Tp&&, _A0>\n    : public decltype(__is_constructible_helper::\n    __test_cast<_Tp&&, _A0>(0))\n{};\n\n#endif\n\n#if defined(_LIBCUDACXX_IS_CONSTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_CONSTRUCTIBLE_FALLBACK)\ntemplate <class _Tp, class ..._Args>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_constructible\n    : public integral_constant<bool, _LIBCUDACXX_IS_CONSTRUCTIBLE(_Tp, _Args...)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, class ..._Args>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_constructible_v = _LIBCUDACXX_IS_CONSTRUCTIBLE(_Tp, _Args...);\n#endif\n\n#else\ntemplate <class _Tp, class... _Args>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_constructible\n    : public __libcpp_is_constructible<_Tp, _Args...>::type {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, class ..._Args>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_constructible_v = is_constructible<_Tp, _Args...>::value;\n#endif\n\n#endif  // defined(_LIBCUDACXX_IS_CONSTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_CONSTRUCTIBLE_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_IS_CONSTRUCTIBLE_H\n", "__type_traits/is_convertible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n// SPDX-FileCopyrightText: Copyright (c) Microsoft Corporation.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_CONVERTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_CONVERTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_array.h\"\n#include \"../__type_traits/is_function.h\"\n#include \"../__type_traits/is_void.h\"\n#include \"../__type_traits/remove_reference.h\"\n#include \"../__utility/declval.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_CONVERTIBLE_TO) && !defined(_LIBCUDACXX_USE_IS_CONVERTIBLE_FALLBACK)\n\ntemplate <class _T1, class _T2> struct _LIBCUDACXX_TEMPLATE_VIS is_convertible\n    : public integral_constant<bool, _LIBCUDACXX_IS_CONVERTIBLE_TO(_T1, _T2)> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _T1, class _T2>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_convertible_v = _LIBCUDACXX_IS_CONVERTIBLE_TO(_T1, _T2);\n#endif\n\n#ifdef _LIBCUDACXX_COMPILER_MSVC // Workaround for DevCom-1627396\ntemplate <class _Ty>\nstruct is_convertible<_Ty&, volatile _Ty&> : true_type {};\n\ntemplate <class _Ty>\nstruct is_convertible<volatile _Ty&, volatile _Ty&> : true_type {};\n\ntemplate <class _Ty>\nstruct is_convertible<_Ty&, const volatile _Ty&> : true_type {};\n\ntemplate <class _Ty>\nstruct is_convertible<volatile _Ty&, const volatile _Ty&> : true_type {};\n\ntemplate <class _Ty>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_convertible_v<_Ty&, volatile _Ty&> = true;\n\ntemplate <class _Ty>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_convertible_v<volatile _Ty&, volatile _Ty&> = true;\n\ntemplate <class _Ty>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_convertible_v<_Ty&, const volatile _Ty&> = true;\n\ntemplate <class _Ty>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_convertible_v<volatile _Ty&, const volatile _Ty&> = true;\n#endif // _LIBCUDACXX_COMPILER_MSVC\n\n#else  // __has_builtin(__is_convertible_to) && !defined(_LIBCUDACXX_USE_IS_CONVERTIBLE_FALLBACK)\n\nnamespace __is_convertible_imp\n{\n\n_LIBCUDACXX_NV_DIAG_SUPPRESS(3013) // a volatile function parameter is deprecated\ntemplate <class _Tp> _LIBCUDACXX_INLINE_VISIBILITY void  __test_convert(_Tp);\n_LIBCUDACXX_NV_DIAG_DEFAULT(3013) // a volatile function parameter is deprecated\n\ntemplate <class _From, class _To, class = void>\nstruct __is_convertible_test : public false_type {};\n\ntemplate <class _From, class _To>\nstruct __is_convertible_test<_From, _To,\n    decltype(_CUDA_VSTD::__is_convertible_imp::__test_convert<_To>(_CUDA_VSTD::declval<_From>()))> : public true_type\n{};\n\ntemplate <class _Tp, bool _IsArray =    is_array<_Tp>::value,\n                     bool _IsFunction = is_function<_Tp>::value,\n                     bool _IsVoid =     is_void<_Tp>::value>\n                     struct __is_array_function_or_void                          {enum {value = 0};};\ntemplate <class _Tp> struct __is_array_function_or_void<_Tp, true, false, false> {enum {value = 1};};\ntemplate <class _Tp> struct __is_array_function_or_void<_Tp, false, true, false> {enum {value = 2};};\ntemplate <class _Tp> struct __is_array_function_or_void<_Tp, false, false, true> {enum {value = 3};};\n}\n\ntemplate <class _Tp,\n    unsigned = __is_convertible_imp::__is_array_function_or_void<__libcpp_remove_reference_t<_Tp>>::value>\nstruct __is_convertible_check\n{\n    static const size_t __v = 0;\n};\n\ntemplate <class _Tp>\nstruct __is_convertible_check<_Tp, 0>\n{\n    static const size_t __v = sizeof(_Tp);\n};\n\ntemplate <class _T1, class _T2,\n    unsigned _T1_is_array_function_or_void = __is_convertible_imp::__is_array_function_or_void<_T1>::value,\n    unsigned _T2_is_array_function_or_void = __is_convertible_imp::__is_array_function_or_void<_T2>::value>\nstruct __is_convertible_fallback\n    : public integral_constant<bool,\n        __is_convertible_imp::__is_convertible_test<_T1, _T2>::value\n    >\n{};\n\ntemplate <class _T1, class _T2> struct __is_convertible_fallback<_T1, _T2, 0, 1> : public false_type {};\ntemplate <class _T1, class _T2> struct __is_convertible_fallback<_T1, _T2, 1, 1> : public false_type {};\ntemplate <class _T1, class _T2> struct __is_convertible_fallback<_T1, _T2, 2, 1> : public false_type {};\ntemplate <class _T1, class _T2> struct __is_convertible_fallback<_T1, _T2, 3, 1> : public false_type {};\n\ntemplate <class _T1, class _T2> struct __is_convertible_fallback<_T1, _T2, 0, 2> : public false_type {};\ntemplate <class _T1, class _T2> struct __is_convertible_fallback<_T1, _T2, 1, 2> : public false_type {};\ntemplate <class _T1, class _T2> struct __is_convertible_fallback<_T1, _T2, 2, 2> : public false_type {};\ntemplate <class _T1, class _T2> struct __is_convertible_fallback<_T1, _T2, 3, 2> : public false_type {};\n\ntemplate <class _T1, class _T2> struct __is_convertible_fallback<_T1, _T2, 0, 3> : public false_type {};\ntemplate <class _T1, class _T2> struct __is_convertible_fallback<_T1, _T2, 1, 3> : public false_type {};\ntemplate <class _T1, class _T2> struct __is_convertible_fallback<_T1, _T2, 2, 3> : public false_type {};\ntemplate <class _T1, class _T2> struct __is_convertible_fallback<_T1, _T2, 3, 3> : public true_type {};\n\ntemplate <class _T1, class _T2> struct _LIBCUDACXX_TEMPLATE_VIS is_convertible\n    : public __is_convertible_fallback<_T1, _T2>\n{\n    static const size_t __complete_check1 = __is_convertible_check<_T1>::__v;\n    static const size_t __complete_check2 = __is_convertible_check<_T2>::__v;\n};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _From, class _To>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_convertible_v = is_convertible<_From, _To>::value;\n#endif\n\n#endif // __has_builtin(__is_convertible_to) && !defined(_LIBCUDACXX_USE_IS_CONVERTIBLE_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_CONVERTIBLE_H\n", "__type_traits/is_copy_assignable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_COPY_ASSIGNABLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_COPY_ASSIGNABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_const.h\"\n#include \"../__type_traits/add_lvalue_reference.h\"\n#include \"../__type_traits/is_assignable.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_copy_assignable\n    : public is_assignable<__add_lvalue_reference_t<_Tp>,\n                           __add_lvalue_reference_t<typename add_const<_Tp>::type>> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_copy_assignable_v = is_copy_assignable<_Tp>::value;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_COPY_ASSIGNABLE_H\n", "__type_traits/is_copy_constructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_COPY_CONSTRUCTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_COPY_CONSTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_const.h\"\n#include \"../__type_traits/add_lvalue_reference.h\"\n#include \"../__type_traits/is_constructible.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_copy_constructible\n    : public is_constructible<_Tp, __add_lvalue_reference_t<typename add_const<_Tp>::type>> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_copy_constructible_v = is_copy_constructible<_Tp>::value;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_COPY_CONSTRUCTIBLE_H\n", "__type_traits/is_core_convertible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_CORE_CONVERTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_CORE_CONVERTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n// [conv.general]/3 says \"E is convertible to T\" whenever \"T t=E;\" is well-formed.\n// We can't test for that, but we can test implicit convertibility by passing it\n// to a function. Notice that __is_core_convertible<void,void> is false,\n// and __is_core_convertible<immovable-type,immovable-type> is true in C++17 and later.\n\ntemplate <class _Tp, class _Up, class = void>\nstruct __is_core_convertible : public false_type {};\n\ntemplate <class _Tp, class _Up>\nstruct __is_core_convertible<_Tp, _Up, decltype(\n    static_cast<void(*)(_Up)>(0) ( static_cast<_Tp(*)()>(0)() )\n)> : public true_type {};\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_CORE_CONVERTIBLE_H\n", "__type_traits/is_default_constructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_DEFAULT_CONSTRUCTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_DEFAULT_CONSTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/is_constructible.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_default_constructible\n    : public is_constructible<_Tp>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_default_constructible_v\n    = is_constructible_v<_Tp>;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_DEFAULT_CONSTRUCTIBLE_H\n", "__type_traits/is_destructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_DESTRUCTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_DESTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_function.h\"\n#include \"../__type_traits/is_reference.h\"\n#include \"../__type_traits/remove_all_extents.h\"\n#include \"../__utility/declval.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_DESTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_DESTRUCTIBLE_FALLBACK)\n\ntemplate<class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_destructible\n   : public integral_constant<bool, _LIBCUDACXX_IS_DESTRUCTIBLE(_Tp)> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_destructible_v = _LIBCUDACXX_IS_DESTRUCTIBLE(_Tp);\n#endif\n\n#else // __has_builtin(__is_destructible)\n\n//  if it's a reference, return true\n//  if it's a function, return false\n//  if it's   void,     return false\n//  if it's an array of unknown bound, return false\n//  Otherwise, return \"declval<_Up&>().~_Up()\" is well-formed\n//    where _Up is remove_all_extents<_Tp>::type\n\ntemplate <class>\nstruct __is_destructible_apply { typedef int type; };\n\ntemplate <typename _Tp>\nstruct __is_destructor_wellformed {\n    template <typename _Tp1>\n    _LIBCUDACXX_INLINE_VISIBILITY static true_type  __test (\n        typename __is_destructible_apply<decltype(_CUDA_VSTD::declval<_Tp1&>().~_Tp1())>::type\n    );\n\n    template <typename _Tp1>\n    _LIBCUDACXX_INLINE_VISIBILITY static false_type __test (...);\n\n    static const bool value = decltype(__test<_Tp>(12))::value;\n};\n\ntemplate <class _Tp, bool>\nstruct __destructible_imp;\n\ntemplate <class _Tp>\nstruct __destructible_imp<_Tp, false>\n   : public integral_constant<bool,\n        __is_destructor_wellformed<__remove_all_extents_t<_Tp> >::value> {};\n\ntemplate <class _Tp>\nstruct __destructible_imp<_Tp, true>\n    : public true_type {};\n\ntemplate <class _Tp, bool>\nstruct __destructible_false;\n\ntemplate <class _Tp>\nstruct __destructible_false<_Tp, false> : public __destructible_imp<_Tp, is_reference<_Tp>::value> {};\n\ntemplate <class _Tp>\nstruct __destructible_false<_Tp, true> : public false_type {};\n\ntemplate <class _Tp>\nstruct is_destructible : public __destructible_false<_Tp, is_function<_Tp>::value> {};\n\ntemplate <class _Tp>\nstruct is_destructible<_Tp[]> : public false_type {};\n\ntemplate <>\nstruct is_destructible<void> : public false_type {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_destructible_v = is_destructible<_Tp>::value;\n#endif\n\n#endif // __has_builtin(__is_destructible)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_DESTRUCTIBLE_H\n", "__type_traits/is_empty.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_EMPTY_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_EMPTY_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_EMPTY) && !defined(_LIBCUDACXX_USE_IS_EMPTY_FALLBACK)\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_empty\n    : public integral_constant<bool, _LIBCUDACXX_IS_EMPTY(_Tp)> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_empty_v = _LIBCUDACXX_IS_EMPTY(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp>\nstruct __is_empty1\n    : public _Tp\n{\n    double __lx;\n};\n\nstruct __is_empty2\n{\n    double __lx;\n};\n\ntemplate <class _Tp, bool = is_class<_Tp>::value>\nstruct __libcpp_empty : public integral_constant<bool, sizeof(__is_empty1<_Tp>) == sizeof(__is_empty2)> {};\n\ntemplate <class _Tp> struct __libcpp_empty<_Tp, false> : public false_type {};\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_empty : public __libcpp_empty<_Tp> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_empty_v = is_empty<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_EMPTY) && !defined(_LIBCUDACXX_USE_IS_EMPTY_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_EMPTY_H\n", "__type_traits/is_enum.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_ENUM_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_ENUM_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_array.h\"\n#include \"../__type_traits/is_class.h\"\n#include \"../__type_traits/is_floating_point.h\"\n#include \"../__type_traits/is_function.h\"\n#include \"../__type_traits/is_integral.h\"\n#include \"../__type_traits/is_member_pointer.h\"\n#include \"../__type_traits/is_pointer.h\"\n#include \"../__type_traits/is_reference.h\"\n#include \"../__type_traits/is_union.h\"\n#include \"../__type_traits/is_void.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_ENUM) && !defined(_LIBCUDACXX_USE_IS_ENUM_FALLBACK)\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_enum\n    : public integral_constant<bool, _LIBCUDACXX_IS_ENUM(_Tp)> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_enum_v = _LIBCUDACXX_IS_ENUM(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_enum\n    : public integral_constant<bool, !is_void<_Tp>::value             &&\n                                     !is_integral<_Tp>::value         &&\n                                     !is_floating_point<_Tp>::value   &&\n                                     !is_array<_Tp>::value            &&\n                                     !is_pointer<_Tp>::value          &&\n                                     !is_reference<_Tp>::value        &&\n                                     !is_member_pointer<_Tp>::value   &&\n                                     !is_union<_Tp>::value            &&\n                                     !is_class<_Tp>::value            &&\n                                     !is_function<_Tp>::value         > {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_enum_v\n    = is_enum<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_ENUM) && !defined(_LIBCUDACXX_USE_IS_ENUM_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_ENUM_H\n", "__type_traits/is_final.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_FINAL_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_FINAL_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_FINAL)\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS\n__libcpp_is_final : public integral_constant<bool, _LIBCUDACXX_IS_FINAL(_Tp)> {};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS\nis_final : public integral_constant<bool, _LIBCUDACXX_IS_FINAL(_Tp)> {};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_final_v = _LIBCUDACXX_IS_FINAL(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS\n__libcpp_is_final : public false_type {};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS\nis_final :  public false_type {};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_final_v = false;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_FINAL)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_FINAL_H\n", "__type_traits/is_floating_point.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_FLOATING_POINT_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_FLOATING_POINT_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/remove_cv.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp> struct __libcpp_is_floating_point              : public false_type {};\ntemplate <>          struct __libcpp_is_floating_point<float>       : public true_type {};\ntemplate <>          struct __libcpp_is_floating_point<double>      : public true_type {};\ntemplate <>          struct __libcpp_is_floating_point<long double> : public true_type {};\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_floating_point\n    : public __libcpp_is_floating_point<__remove_cv_t<_Tp> > {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_floating_point_v = is_floating_point<_Tp>::value;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_FLOATING_POINT_H\n", "__type_traits/is_function.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_FUNCTIONAL_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_FUNCTIONAL_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_const.h\"\n#include \"../__type_traits/is_reference.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_FUNCTION) && !defined(_LIBCUDACXX_USE_IS_FUNCTION_FALLBACK)\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_function : integral_constant<bool, _LIBCUDACXX_IS_FUNCTION(_Tp)> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_function_v = _LIBCUDACXX_IS_FUNCTION(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_function\n    : public integral_constant<bool, !(is_reference<_Tp>::value || is_const<const _Tp>::value)> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_function_v = is_function<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_FUNCTION) && !defined(_LIBCUDACXX_USE_IS_FUNCTION_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_FUNCTIONAL_H\n", "__type_traits/is_fundamental.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_FUNDAMENTAL_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_FUNDAMENTAL_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_arithmetic.h\"\n#include \"../__type_traits/is_null_pointer.h\"\n#include \"../__type_traits/is_void.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_FUNDAMENTAL) && !defined(_LIBCUDACXX_USE_IS_FUNDAMENTAL_FALLBACK)\n\ntemplate<class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_fundamental\n    : public integral_constant<bool, _LIBCUDACXX_IS_FUNDAMENTAL(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_fundamental_v = _LIBCUDACXX_IS_FUNDAMENTAL(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_fundamental\n    : public integral_constant<bool, is_void<_Tp>::value        ||\n                                     __is_nullptr_t<_Tp>::value ||\n                                     is_arithmetic<_Tp>::value> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_fundamental_v = is_fundamental<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_FUNDAMENTAL) && !defined(_LIBCUDACXX_USE_IS_FUNDAMENTAL_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_FUNDAMENTAL_H\n", "__type_traits/is_implicitly_default_constructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_IMPLICITLY_DEFAULT_CONSTRUCTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_IMPLICITLY_DEFAULT_CONSTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_default_constructible.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n// First of all, we can't implement this check in C++03 mode because the {}\n// default initialization syntax isn't valid.\n// Second, we implement the trait in a funny manner with two defaulted template\n// arguments to workaround Clang's PR43454.\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY void __test_implicit_default_constructible(_Tp);\n\ntemplate <class _Tp, class = void, class = typename is_default_constructible<_Tp>::type>\nstruct __is_implicitly_default_constructible\n    : false_type\n{ };\n\ntemplate <class _Tp>\nstruct __is_implicitly_default_constructible<_Tp, decltype(__test_implicit_default_constructible<_Tp const&>({})), true_type>\n    : true_type\n{ };\n\ntemplate <class _Tp>\nstruct __is_implicitly_default_constructible<_Tp, decltype(__test_implicit_default_constructible<_Tp const&>({})), false_type>\n    : false_type\n{ };\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_IMPLICITLY_DEFAULT_CONSTRUCTIBLE_H\n", "__type_traits/is_integral.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_INTEGRAL_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_INTEGRAL_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/remove_cv.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_INTEGRAL) && !defined(_LIBCUDACXX_USE_IS_INTEGRAL_FALLBACK)\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_integral\n    : public integral_constant<bool, _LIBCUDACXX_IS_INTEGRAL(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_integral_v = _LIBCUDACXX_IS_INTEGRAL(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct __libcpp_is_integral                     : public false_type {};\ntemplate <>          struct __libcpp_is_integral<bool>               : public true_type {};\ntemplate <>          struct __libcpp_is_integral<char>               : public true_type {};\ntemplate <>          struct __libcpp_is_integral<signed char>        : public true_type {};\ntemplate <>          struct __libcpp_is_integral<unsigned char>      : public true_type {};\ntemplate <>          struct __libcpp_is_integral<wchar_t>            : public true_type {};\n#ifndef _LIBCUDACXX_NO_HAS_CHAR8_T\ntemplate <>          struct __libcpp_is_integral<char8_t>            : public true_type {};\n#endif\n#ifndef _LIBCUDACXX_HAS_NO_UNICODE_CHARS\ntemplate <>          struct __libcpp_is_integral<char16_t>           : public true_type {};\ntemplate <>          struct __libcpp_is_integral<char32_t>           : public true_type {};\n#endif  // _LIBCUDACXX_HAS_NO_UNICODE_CHARS\ntemplate <>          struct __libcpp_is_integral<short>              : public true_type {};\ntemplate <>          struct __libcpp_is_integral<unsigned short>     : public true_type {};\ntemplate <>          struct __libcpp_is_integral<int>                : public true_type {};\ntemplate <>          struct __libcpp_is_integral<unsigned int>       : public true_type {};\ntemplate <>          struct __libcpp_is_integral<long>               : public true_type {};\ntemplate <>          struct __libcpp_is_integral<unsigned long>      : public true_type {};\ntemplate <>          struct __libcpp_is_integral<long long>          : public true_type {};\ntemplate <>          struct __libcpp_is_integral<unsigned long long> : public true_type {};\n#ifndef _LIBCUDACXX_HAS_NO_INT128\ntemplate <>          struct __libcpp_is_integral<__int128_t>         : public true_type {};\ntemplate <>          struct __libcpp_is_integral<__uint128_t>        : public true_type {};\n#endif\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_integral\n    : public integral_constant<bool, __libcpp_is_integral<__remove_cv_t<_Tp> >::value>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_integral_v = is_integral<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_INTEGRAL) && !defined(_LIBCUDACXX_USE_IS_INTEGRAL_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_INTEGRAL_H\n", "__type_traits/is_literal_type.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_LITERAL_TYPE\n#define _LIBCUDACXX___TYPE_TRAITS_IS_LITERAL_TYPE\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_reference.h\"\n#include \"../__type_traits/is_scalar.h\"\n#include \"../__type_traits/remove_all_extents.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_LITERAL) && !defined(_LIBCUDACXX_USE_IS_LITERAL_FALLBACK)\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS _LIBCUDACXX_DEPRECATED_IN_CXX17 is_literal_type\n    : public integral_constant<bool, _LIBCUDACXX_IS_LITERAL(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_DEPRECATED_IN_CXX17 _LIBCUDACXX_INLINE_VAR constexpr bool is_literal_type_v = __is_literal_type(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS _LIBCUDACXX_DEPRECATED_IN_CXX17 is_literal_type\n    : public integral_constant<bool, is_scalar<__remove_all_extents_t<_Tp>>::value ||\n                                     is_reference<__remove_all_extents_t<_Tp>>::value>\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_DEPRECATED_IN_CXX17 _LIBCUDACXX_INLINE_VAR constexpr bool is_literal_type_v\n    = is_literal_type<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_LITERAL) && !defined(_LIBCUDACXX_USE_IS_LITERAL_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_LITERAL_TYPE\n", "__type_traits/is_member_function_pointer.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_MEMBER_FUNCTION_POINTER_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_MEMBER_FUNCTION_POINTER_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_function.h\"\n#include \"../__type_traits/remove_cv.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_MEMBER_FUNCTION_POINTER) && !defined(_LIBCUDACXX_USE_IS_MEMBER_FUNCTION_POINTER_FALLBACK)\n\ntemplate<class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_member_function_pointer\n    : public integral_constant<bool, _LIBCUDACXX_IS_MEMBER_FUNCTION_POINTER(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_member_function_pointer_v = _LIBCUDACXX_IS_MEMBER_FUNCTION_POINTER(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct __libcpp_is_member_pointer {\n  enum {\n    __is_member = false,\n    __is_func = false,\n    __is_obj = false\n  };\n};\ntemplate <class _Tp, class _Up> struct __libcpp_is_member_pointer<_Tp _Up::*> {\n  enum {\n    __is_member = true,\n    __is_func = is_function<_Tp>::value,\n    __is_obj = !__is_func,\n  };\n};\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_member_function_pointer\n    : public integral_constant<bool, __libcpp_is_member_pointer<__remove_cv_t<_Tp> >::__is_func >\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_member_function_pointer_v = is_member_function_pointer<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_MEMBER_FUNCTION_POINTER) && !defined(_LIBCUDACXX_USE_IS_MEMBER_FUNCTION_POINTER_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_MEMBER_FUNCTION_POINTER_H\n", "__type_traits/is_member_object_pointer.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_MEMBER_OBJECT_POINTER_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_MEMBER_OBJECT_POINTER_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_member_function_pointer.h\"\n#include \"../__type_traits/remove_cv.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_MEMBER_OBJECT_POINTER) && !defined(_LIBCUDACXX_USE_IS_MEMBER_OBJECT_POINTER_FALLBACK)\n\ntemplate<class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_member_object_pointer\n    : public integral_constant<bool, _LIBCUDACXX_IS_MEMBER_OBJECT_POINTER(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_member_object_pointer_v = _LIBCUDACXX_IS_MEMBER_OBJECT_POINTER(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_member_object_pointer\n    : public integral_constant<bool, __libcpp_is_member_pointer<__remove_cv_t<_Tp> >::__is_obj >\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_member_object_pointer_v = is_member_object_pointer<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_MEMBER_OBJECT_POINTER) && !defined(_LIBCUDACXX_USE_IS_MEMBER_OBJECT_POINTER_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_MEMBER_FUNCTION_POINTER_H\n", "__type_traits/is_member_pointer.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_MEMBER_POINTER_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_MEMBER_POINTER_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_member_function_pointer.h\"\n#include \"../__type_traits/remove_cv.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_MEMBER_POINTER) && !defined(_LIBCUDACXX_USE_IS_MEMBER_POINTER_FALLBACK)\n\ntemplate<class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_member_pointer\n    : public integral_constant<bool, _LIBCUDACXX_IS_MEMBER_POINTER(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_member_pointer_v = _LIBCUDACXX_IS_MEMBER_POINTER(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_member_pointer\n    : public integral_constant<bool, __libcpp_is_member_pointer<__remove_cv_t<_Tp> >::__is_member >\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_member_pointer_v = is_member_pointer<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_MEMBER_POINTER) && !defined(_LIBCUDACXX_USE_IS_MEMBER_POINTER_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_MEMBER_POINTER_H\n", "__type_traits/is_move_assignable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_MOVE_ASSIGNABLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_MOVE_ASSIGNABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_lvalue_reference.h\"\n#include \"../__type_traits/add_rvalue_reference.h\"\n#include \"../__type_traits/is_assignable.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_move_assignable\n    : public is_assignable<__add_lvalue_reference_t<_Tp>, __add_rvalue_reference_t<_Tp>> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_move_assignable_v = is_move_assignable<_Tp>::value;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_MOVE_ASSIGNABLE_H\n", "__type_traits/is_move_constructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_MOVE_CONSTRUCTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_MOVE_CONSTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_lvalue_reference.h\"\n#include \"../__type_traits/add_rvalue_reference.h\"\n#include \"../__type_traits/is_constructible.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_move_constructible\n    : public is_constructible<_Tp, __add_rvalue_reference_t<_Tp>>\n{ };\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_move_constructible_v = is_move_constructible<_Tp>::value;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_MOVE_CONSTRUCTIBLE_H\n", "__type_traits/is_nothrow_assignable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_ASSIGNABLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_ASSIGNABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_assignable.h\"\n#include \"../__type_traits/is_scalar.h\"\n#include \"../__utility/declval.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_NOTHROW_ASSIGNABLE) && !defined(_LIBCUDACXX_USE_IS_NOTHROW_ASSIGNABLE_FALLBACK)\n\ntemplate <class _Tp, class _Arg>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_assignable\n    : public integral_constant<bool, _LIBCUDACXX_IS_NOTHROW_ASSIGNABLE(_Tp, _Arg)> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, class _Arg>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_assignable_v = _LIBCUDACXX_IS_NOTHROW_ASSIGNABLE(_Tp, _Arg);\n#endif\n\n#elif !defined(_LIBCUDACXX_HAS_NO_NOEXCEPT) && !defined(_LIBCUDACXX_HAS_NO_NOEXCEPT_SFINAE)\n\ntemplate <bool, class _Tp, class _Arg> struct __libcpp_is_nothrow_assignable;\n\ntemplate <class _Tp, class _Arg>\nstruct __libcpp_is_nothrow_assignable<false, _Tp, _Arg>\n    : public false_type\n{ };\n\ntemplate <class _Tp, class _Arg>\nstruct __libcpp_is_nothrow_assignable<true, _Tp, _Arg>\n    : public integral_constant<bool, noexcept(_CUDA_VSTD::declval<_Tp>() = _CUDA_VSTD::declval<_Arg>()) >\n{ };\n\ntemplate <class _Tp, class _Arg>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_assignable\n    : public __libcpp_is_nothrow_assignable<is_assignable<_Tp, _Arg>::value, _Tp, _Arg>\n{ };\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, class _Arg>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_assignable_v = is_nothrow_assignable<_Tp, _Arg>::value;\n#endif\n\n#else\n\ntemplate <class _Tp, class _Arg>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_assignable\n    : public false_type {};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_assignable<_Tp&, _Tp>\n#if defined(_LIBCUDACXX_HAS_NOTHROW_ASSIGN) && !defined(_LIBCUDACXX_USE_HAS_NOTHROW_ASSIGN_FALLBACK)\n    : integral_constant<bool, _LIBCUDACXX_HAS_NOTHROW_ASSIGN(_Tp)> {};\n#else\n    : integral_constant<bool, is_scalar<_Tp>::value> {};\n#endif // defined(_LIBCUDACXX_HAS_NOTHROW_ASSIGN) && !defined(_LIBCUDACXX_USE_HAS_NOTHROW_ASSIGN_FALLBACK)\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_assignable<_Tp&, _Tp&>\n#if defined(_LIBCUDACXX_HAS_NOTHROW_ASSIGN) && !defined(_LIBCUDACXX_USE_HAS_NOTHROW_ASSIGN_FALLBACK)\n    : integral_constant<bool, _LIBCUDACXX_HAS_NOTHROW_ASSIGN(_Tp)> {};\n#else\n    : integral_constant<bool, is_scalar<_Tp>::value> {};\n#endif // defined(_LIBCUDACXX_HAS_NOTHROW_ASSIGN) && !defined(_LIBCUDACXX_USE_HAS_NOTHROW_ASSIGN_FALLBACK)\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_assignable<_Tp&, const _Tp&>\n#if defined(_LIBCUDACXX_HAS_NOTHROW_ASSIGN) && !defined(_LIBCUDACXX_USE_HAS_NOTHROW_ASSIGN_FALLBACK)\n    : integral_constant<bool, _LIBCUDACXX_HAS_NOTHROW_ASSIGN(_Tp)> {};\n#else\n    : integral_constant<bool, is_scalar<_Tp>::value> {};\n#endif // defined(_LIBCUDACXX_HAS_NOTHROW_ASSIGN) && !defined(_LIBCUDACXX_USE_HAS_NOTHROW_ASSIGN_FALLBACK)\n\n#ifndef _LIBCUDACXX_HAS_NO_RVALUE_REFERENCES\n\ntemplate <class _Tp>\nstruct is_nothrow_assignable<_Tp&, _Tp&&>\n#if defined(_LIBCUDACXX_HAS_NOTHROW_ASSIGN) && !defined(_LIBCUDACXX_USE_HAS_NOTHROW_ASSIGN_FALLBACK)\n    : integral_constant<bool, _LIBCUDACXX_HAS_NOTHROW_ASSIGN(_Tp)> {};\n#else\n    : integral_constant<bool, is_scalar<_Tp>::value> {};\n#endif // defined(_LIBCUDACXX_HAS_NOTHROW_ASSIGN) && !defined(_LIBCUDACXX_USE_HAS_NOTHROW_ASSIGN_FALLBACK)\n\n#endif // _LIBCUDACXX_HAS_NO_RVALUE_REFERENCES\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, class _Arg>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_assignable_v = is_nothrow_assignable<_Tp, _Arg>::value;\n#endif\n\n#endif // !defined(_LIBCUDACXX_HAS_NO_NOEXCEPT)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_ASSIGNABLE_H\n", "__type_traits/is_nothrow_constructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_CONSTRUCTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_CONSTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_constructible.h\"\n#include \"../__type_traits/is_scalar.h\"\n#include \"../__utility/declval.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_NOTHROW_CONSTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_NOTHROW_CONSTRUCTIBLE_FALLBACK)\n\ntemplate <class _Tp, class... _Args>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_constructible\n    : public integral_constant<bool, _LIBCUDACXX_IS_NOTHROW_CONSTRUCTIBLE(_Tp, _Args...)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, class ..._Args>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_constructible_v = _LIBCUDACXX_IS_NOTHROW_CONSTRUCTIBLE(_Tp, _Args...);\n#endif\n\n#else\n\n#if !defined(_LIBCUDACXX_HAS_NO_NOEXCEPT)\n\ntemplate <bool, bool, class _Tp, class... _Args> struct __libcpp_is_nothrow_constructible;\n\ntemplate <class _Tp, class... _Args>\nstruct __libcpp_is_nothrow_constructible</*is constructible*/true, /*is reference*/false, _Tp, _Args...>\n    : public integral_constant<bool, noexcept(_Tp(_CUDA_VSTD::declval<_Args>()...))>\n{\n};\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY void __implicit_conversion_to(_Tp) noexcept { }\n\ntemplate <class _Tp, class _Arg>\nstruct __libcpp_is_nothrow_constructible</*is constructible*/true, /*is reference*/true, _Tp, _Arg>\n    : public integral_constant<bool, noexcept(__implicit_conversion_to<_Tp>(_CUDA_VSTD::declval<_Arg>()))>\n{\n};\n\ntemplate <class _Tp, bool _IsReference, class... _Args>\nstruct __libcpp_is_nothrow_constructible</*is constructible*/false, _IsReference, _Tp, _Args...>\n    : public false_type\n{\n};\n\ntemplate <class _Tp, class... _Args>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_constructible\n    : __libcpp_is_nothrow_constructible<is_constructible<_Tp, _Args...>::value, is_reference<_Tp>::value, _Tp, _Args...>\n{\n};\n\ntemplate <class _Tp, size_t _Ns>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_constructible<_Tp[_Ns]>\n    : __libcpp_is_nothrow_constructible<is_constructible<_Tp>::value, is_reference<_Tp>::value, _Tp>\n{\n};\n\n#else\n\ntemplate <class _Tp, class... _Args>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_constructible\n    : false_type\n{\n};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_constructible<_Tp>\n#if defined(_LIBCUDACXX_HAS_NOTHROW_CONSTRUCTOR) && !defined(_LIBCUDACXX_USE_HAS_NOTHROW_CONSTRUCTOR_FALLBACK)\n    : integral_constant<bool, _LIBCUDACXX_HAS_NOTHROW_CONSTRUCTOR(_Tp)>\n#else\n    : integral_constant<bool, is_scalar<_Tp>::value>\n#endif // defined(_LIBCUDACXX_HAS_NOTHROW_CONSTRUCTOR) && !defined(_LIBCUDACXX_USE_HAS_NOTHROW_CONSTRUCTOR_FALLBACK)\n{\n};\n\ntemplate <class _Tp>\n#ifndef _LIBCUDACXX_HAS_NO_RVALUE_REFERENCES\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_constructible<_Tp, _Tp&&>\n#else\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_constructible<_Tp, _Tp>\n#endif\n#if defined(_LIBCUDACXX_HAS_NOTHROW_COPY) && !defined(_LIBCUDACXX_USE_HAS_NOTHROW_COPY_FALLBACK)\n    : integral_constant<bool, _LIBCUDACXX_HAS_NOTHROW_COPY(_Tp)>\n#else\n    : integral_constant<bool, is_scalar<_Tp>::value>\n#endif // defined(_LIBCUDACXX_HAS_NOTHROW_COPY) && !defined(_LIBCUDACXX_USE_HAS_NOTHROW_COPY_FALLBACK)\n{\n};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_constructible<_Tp, const _Tp&>\n#if defined(_LIBCUDACXX_HAS_NOTHROW_COPY) && !defined(_LIBCUDACXX_USE_HAS_NOTHROW_COPY_FALLBACK)\n    : integral_constant<bool, _LIBCUDACXX_HAS_NOTHROW_COPY(_Tp)>\n#else\n    : integral_constant<bool, is_scalar<_Tp>::value>\n#endif // defined(_LIBCUDACXX_HAS_NOTHROW_COPY) && !defined(_LIBCUDACXX_USE_HAS_NOTHROW_COPY_FALLBACK)\n{\n};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_constructible<_Tp, _Tp&>\n#if defined(_LIBCUDACXX_HAS_NOTHROW_COPY) && !defined(_LIBCUDACXX_USE_HAS_NOTHROW_COPY_FALLBACK)\n    : integral_constant<bool, _LIBCUDACXX_HAS_NOTHROW_COPY(_Tp)>\n#else\n    : integral_constant<bool, is_scalar<_Tp>::value>\n#endif // defined(_LIBCUDACXX_HAS_NOTHROW_COPY) && !defined(_LIBCUDACXX_USE_HAS_NOTHROW_COPY_FALLBACK)\n{\n};\n\n#endif // !defined(_LIBCUDACXX_HAS_NO_NOEXCEPT)\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, class ..._Args>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_constructible_v = is_nothrow_constructible<_Tp, _Args...>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_NOTHROW_CONSTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_NOTHROW_CONSTRUCTIBLE_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_CONSTRUCTIBLE_H\n", "__type_traits/is_nothrow_convertible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_CONVERTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_CONVERTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/conjunction.h\"\n#include \"../__type_traits/disjunction.h\"\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_convertible.h\"\n#include \"../__type_traits/is_void.h\"\n#include \"../__type_traits/lazy.h\"\n#include \"../__utility/declval.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 11\n\ntemplate <typename _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY static void __test_noexcept(_Tp) noexcept;\n\ntemplate<typename _Fm, typename _To>\n_LIBCUDACXX_INLINE_VISIBILITY static bool_constant<noexcept(_CUDA_VSTD::__test_noexcept<_To>(_CUDA_VSTD::declval<_Fm>()))>\n__is_nothrow_convertible_test();\n\ntemplate <typename _Fm, typename _To>\nstruct __is_nothrow_convertible_helper: decltype(__is_nothrow_convertible_test<_Fm, _To>())\n{ };\n\ntemplate <typename _Fm, typename _To>\nstruct is_nothrow_convertible : _Or<\n    _And<is_void<_To>, is_void<_Fm>>,\n    _Lazy<_And, is_convertible<_Fm, _To>, __is_nothrow_convertible_helper<_Fm, _To>>\n>::type { };\n\ntemplate <typename _Fm, typename _To>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_convertible_v = is_nothrow_convertible<_Fm, _To>::value;\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_CONVERTIBLE_H\n", "__type_traits/is_nothrow_copy_assignable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_COPY_ASSIGNABLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_COPY_ASSIGNABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_const.h\"\n#include \"../__type_traits/add_lvalue_reference.h\"\n#include \"../__type_traits/is_nothrow_assignable.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_NOTHROW_ASSIGNABLE) && !defined(_LIBCUDACXX_USE_IS_NOTHROW_ASSIGNABLE_FALLBACK)\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_copy_assignable\n    : public integral_constant<bool, _LIBCUDACXX_IS_NOTHROW_ASSIGNABLE(__add_lvalue_reference_t<_Tp>,\n                                                                       __add_lvalue_reference_t<typename add_const<_Tp>::type>)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_copy_assignable_v =\n    _LIBCUDACXX_IS_NOTHROW_ASSIGNABLE(__add_lvalue_reference_t<_Tp>,\n                                      __add_lvalue_reference_t<typename add_const<_Tp>::type>);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_copy_assignable\n    : public is_nothrow_assignable<__add_lvalue_reference_t<_Tp>,\n                                   __add_lvalue_reference_t<typename add_const<_Tp>::type>> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_copy_assignable_v = is_nothrow_copy_assignable<_Tp>::value;\n#endif\n\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_COPY_ASSIGNABLE_H\n", "__type_traits/is_nothrow_copy_constructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_COPY_CONSTRUCTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_COPY_CONSTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_const.h\"\n#include \"../__type_traits/add_lvalue_reference.h\"\n#include \"../__type_traits/is_nothrow_constructible.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_copy_constructible\n    : public is_nothrow_constructible<_Tp, __add_lvalue_reference_t<typename add_const<_Tp>::type>> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_copy_constructible_v\n    = is_nothrow_copy_constructible<_Tp>::value;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_COPY_CONSTRUCTIBLE_H\n", "__type_traits/is_nothrow_default_constructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_DEFAULT_CONSTRUCTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_DEFAULT_CONSTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/is_nothrow_constructible.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_NOTHROW_CONSTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_NOTHROW_CONSTRUCTIBLE_FALLBACK)\n\ntemplate <class _Tp, class... _Args>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_default_constructible\n    : public integral_constant<bool, _LIBCUDACXX_IS_NOTHROW_CONSTRUCTIBLE(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, class ..._Args>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_default_constructible_v = _LIBCUDACXX_IS_NOTHROW_CONSTRUCTIBLE(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_default_constructible\n    : public is_nothrow_constructible<_Tp>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_default_constructible_v = is_nothrow_constructible<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_NOTHROW_CONSTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_NOTHROW_CONSTRUCTIBLE_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_DEFAULT_CONSTRUCTIBLE_H\n", "__type_traits/is_nothrow_destructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_DESTRUCTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_DESTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_destructible.h\"\n#include \"../__type_traits/is_reference.h\"\n#include \"../__type_traits/is_scalar.h\"\n#include \"../__type_traits/remove_all_extents.h\"\n#include \"../__utility/declval.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n// is_nothrow_destructible\n\n#if defined(_LIBCUDACXX_IS_NOTHROW_DESTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_NOTHROW_DESTRUCTIBLE_FALLBACK)\n\ntemplate <class _Tp>\nstruct is_nothrow_destructible\n   : public integral_constant<bool, _LIBCUDACXX_IS_NOTHROW_DESTRUCTIBLE(_Tp)> {};\n\n#elif !defined(_LIBCUDACXX_HAS_NO_NOEXCEPT)\n\ntemplate <bool, class _Tp> struct __libcpp_is_nothrow_destructible;\n\ntemplate <class _Tp>\nstruct __libcpp_is_nothrow_destructible<false, _Tp>\n    : public false_type\n{\n};\n\ntemplate <class _Tp>\nstruct __libcpp_is_nothrow_destructible<true, _Tp>\n    : public integral_constant<bool, noexcept(_CUDA_VSTD::declval<_Tp>().~_Tp()) >\n{\n};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_destructible\n    : public __libcpp_is_nothrow_destructible<is_destructible<_Tp>::value, _Tp>\n{\n};\n\ntemplate <class _Tp, size_t _Ns>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_destructible<_Tp[_Ns]>\n    : public is_nothrow_destructible<_Tp>\n{\n};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_destructible<_Tp&>\n    : public true_type\n{\n};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_destructible<_Tp&&>\n    : public true_type\n{\n};\n\n#else\n\ntemplate <class _Tp> struct __libcpp_nothrow_destructor\n    : public integral_constant<bool, is_scalar<_Tp>::value ||\n                                     is_reference<_Tp>::value> {};\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_destructible\n    : public __libcpp_nothrow_destructor<__remove_all_extents_t<_Tp>> {};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_destructible<_Tp[]>\n    : public false_type {};\n\n#endif // defined(_LIBCUDACXX_IS_NOTHROW_DESTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_NOTHROW_DESTRUCTIBLE_FALLBACK)\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_destructible_v\n    = is_nothrow_destructible<_Tp>::value;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_DESTRUCTIBLE_H\n", "__type_traits/is_nothrow_move_assignable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_MOVE_ASSIGNABLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_MOVE_ASSIGNABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_lvalue_reference.h\"\n#include \"../__type_traits/add_rvalue_reference.h\"\n#include \"../__type_traits/is_nothrow_assignable.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_NOTHROW_ASSIGNABLE) && !defined(_LIBCUDACXX_USE_IS_NOTHROW_ASSIGNABLE_FALLBACK)\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_move_assignable\n    : public integral_constant<bool, _LIBCUDACXX_IS_NOTHROW_ASSIGNABLE(__add_lvalue_reference_t<_Tp>,\n                                                                       __add_rvalue_reference_t<_Tp>)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_move_assignable_v =\n    _LIBCUDACXX_IS_NOTHROW_ASSIGNABLE(__add_lvalue_reference_t<_Tp>, __add_rvalue_reference_t<_Tp>);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_move_assignable\n    : public is_nothrow_assignable<__add_lvalue_reference_t<_Tp>,\n                                   __add_rvalue_reference_t<_Tp>>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_move_assignable_v = is_nothrow_move_assignable<_Tp>::value;\n#endif\n\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_MOVE_ASSIGNABLE_H\n", "__type_traits/is_nothrow_move_constructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_MOVE_CONSTRUCTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_MOVE_CONSTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_rvalue_reference.h\"\n#include \"../__type_traits/is_nothrow_constructible.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_move_constructible\n    : public is_nothrow_constructible<_Tp, __add_rvalue_reference_t<_Tp>>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_move_constructible_v\n    = is_nothrow_move_constructible<_Tp>::value;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_NOTHROW_MOVE_CONSTRUCTIBLE_H\n", "__type_traits/is_null_pointer.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_NULL_POINTER_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_NULL_POINTER_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/remove_cv.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp> struct __is_nullptr_t_impl       : public false_type {};\ntemplate <>          struct __is_nullptr_t_impl<nullptr_t> : public true_type {};\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS __is_nullptr_t\n    : public __is_nullptr_t_impl<__remove_cv_t<_Tp> > {};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_null_pointer\n    : public __is_nullptr_t_impl<__remove_cv_t<_Tp> > {};\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_null_pointer_v = is_null_pointer<_Tp>::value;\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_NULL_POINTER_H\n", "__type_traits/is_object.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_OBJECT_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_OBJECT_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_array.h\"\n#include \"../__type_traits/is_class.h\"\n#include \"../__type_traits/is_scalar.h\"\n#include \"../__type_traits/is_union.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_OBJECT) && !defined(_LIBCUDACXX_USE_IS_OBJECT_FALLBACK)\n\ntemplate<class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_object\n    : public integral_constant<bool, _LIBCUDACXX_IS_OBJECT(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_object_v = _LIBCUDACXX_IS_OBJECT(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_object\n    : public integral_constant<bool, is_scalar<_Tp>::value ||\n                                     is_array<_Tp>::value  ||\n                                     is_union<_Tp>::value  ||\n                                     is_class<_Tp>::value  > {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_object_v = is_object<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_OBJECT) && !defined(_LIBCUDACXX_USE_IS_OBJECT_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_OBJECT_H\n", "__type_traits/is_pod.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_POD_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_POD_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_trivially_copy_constructible.h\"\n#include \"../__type_traits/is_trivially_copy_assignable.h\"\n#include \"../__type_traits/is_trivially_default_constructible.h\"\n#include \"../__type_traits/is_trivially_destructible.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_POD) && !defined(_LIBCUDACXX_USE_IS_POD_FALLBACK)\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_pod\n    : public integral_constant<bool, _LIBCUDACXX_IS_POD(_Tp)> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_pod_v = _LIBCUDACXX_IS_POD(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_pod\n    : public integral_constant<bool, is_trivially_default_constructible<_Tp>::value   &&\n                                     is_trivially_copy_constructible<_Tp>::value      &&\n                                     is_trivially_copy_assignable<_Tp>::value    &&\n                                     is_trivially_destructible<_Tp>::value> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_pod_v\n    = is_pod<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_POD) && !defined(_LIBCUDACXX_USE_IS_POD_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_POD_H\n", "__type_traits/is_pointer.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_POINTER_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_POINTER_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/remove_cv.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_POINTER) && !defined(_LIBCUDACXX_USE_IS_POINTER_FALLBACK)\n\ntemplate<class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_pointer\n    : public integral_constant<bool, _LIBCUDACXX_IS_POINTER(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_pointer_v = _LIBCUDACXX_IS_POINTER(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct __libcpp_is_pointer       : public false_type {};\ntemplate <class _Tp> struct __libcpp_is_pointer<_Tp*> : public true_type {};\n\ntemplate <class _Tp> struct __libcpp_remove_objc_qualifiers { typedef _Tp type; };\n#if defined(_LIBCUDACXX_HAS_OBJC_ARC)\ntemplate <class _Tp> struct __libcpp_remove_objc_qualifiers<_Tp __strong> { typedef _Tp type; };\ntemplate <class _Tp> struct __libcpp_remove_objc_qualifiers<_Tp __weak> { typedef _Tp type; };\ntemplate <class _Tp> struct __libcpp_remove_objc_qualifiers<_Tp __autoreleasing> { typedef _Tp type; };\ntemplate <class _Tp> struct __libcpp_remove_objc_qualifiers<_Tp __unsafe_unretained> { typedef _Tp type; };\n#endif\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_pointer\n    : public __libcpp_is_pointer<typename __libcpp_remove_objc_qualifiers<__remove_cv_t<_Tp> >::type> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_pointer_v = is_pointer<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_POINTER) && !defined(_LIBCUDACXX_USE_IS_POINTER_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_POINTER_H\n", "__type_traits/is_polymorphic.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_POLYMORPHIC_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_POLYMORPHIC_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/enable_if.h\"\n#include \"../__type_traits/integral_constant.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_POLYMORPHIC) && !defined(_LIBCUDACXX_USE_IS_POLYMORPHIC_FALLBACK)\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_polymorphic\n    : public integral_constant<bool, _LIBCUDACXX_IS_POLYMORPHIC(_Tp)> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_polymorphic_v = _LIBCUDACXX_IS_POLYMORPHIC(_Tp);\n#endif\n\n#else\n\ntemplate<typename _Tp> char &__is_polymorphic_impl(\n    __enable_if_t<sizeof((_Tp*)dynamic_cast<const volatile void*>(_CUDA_VSTD::declval<_Tp*>())) != 0, int>);\ntemplate<typename _Tp> __two &__is_polymorphic_impl(...);\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_polymorphic\n    : public integral_constant<bool, sizeof(__is_polymorphic_impl<_Tp>(0)) == 1> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_polymorphic_v = is_polymorphic<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_POLYMORPHIC) && !defined(_LIBCUDACXX_USE_IS_POLYMORPHIC_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_POLYMORPHIC_H\n", "__type_traits/is_primary_template.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_PRIMARY_TEMPLATE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_PRIMARY_TEMPLATE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/enable_if.h\"\n#include \"../__type_traits/is_same.h\"\n#include \"../__type_traits/is_valid_expansion.h\"\n#include \"../__type_traits/void_t.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_COMPILER_MSVC)\ntemplate<class _Tp, class = void>\nstruct __is_primary_template : false_type {};\n\ntemplate<class _Tp>\nstruct __is_primary_template<_Tp, void_t<typename _Tp::__primary_template>>\n  : public is_same<_Tp, typename _Tp::__primary_template> {};\n\n#else // ^^^ _LIBCUDACXX_COMPILER_MSVC ^^^ / vvv !_LIBCUDACXX_COMPILER_MSVC vvv\n\ntemplate <class _Tp>\nusing __test_for_primary_template = __enable_if_t<\n    _IsSame<_Tp, typename _Tp::__primary_template>::value\n  >;\ntemplate <class _Tp>\nusing __is_primary_template = _IsValidExpansion<\n    __test_for_primary_template, _Tp\n  >;\n#endif // !_LIBCUDACXX_COMPILER_MSVC\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_PRIMARY_TEMPLATE_H\n", "__type_traits/is_reference.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_REFERENCE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_REFERENCE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_LVALUE_REFERENCE) && !defined(_LIBCUDACXX_USE_IS_LVALUE_REFERENCE_FALLBACK) && \\\n    defined(_LIBCUDACXX_IS_RVALUE_REFERENCE) && !defined(_LIBCUDACXX_USE_IS_RVALUE_REFERENCE_FALLBACK) && \\\n    defined(_LIBCUDACXX_IS_REFERENCE) && !defined(_LIBCUDACXX_USE_IS_REFERENCE_FALLBACK)\n\ntemplate<class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_lvalue_reference\n    : public integral_constant<bool, _LIBCUDACXX_IS_LVALUE_REFERENCE(_Tp)>\n    {};\n\ntemplate<class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_rvalue_reference\n    : public integral_constant<bool, _LIBCUDACXX_IS_RVALUE_REFERENCE(_Tp)>\n    {};\n\ntemplate<class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_reference\n    : public integral_constant<bool, _LIBCUDACXX_IS_REFERENCE(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_lvalue_reference_v = _LIBCUDACXX_IS_LVALUE_REFERENCE(_Tp);\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_rvalue_reference_v = _LIBCUDACXX_IS_RVALUE_REFERENCE(_Tp);\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_reference_v = _LIBCUDACXX_IS_REFERENCE(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_lvalue_reference       : public false_type {};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_lvalue_reference<_Tp&> : public true_type {};\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_rvalue_reference        : public false_type {};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_rvalue_reference<_Tp&&> : public true_type {};\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_reference        : public false_type {};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_reference<_Tp&>  : public true_type {};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_reference<_Tp&&> : public true_type {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_lvalue_reference_v = is_lvalue_reference<_Tp>::value;\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_rvalue_reference_v = is_rvalue_reference<_Tp>::value;\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_reference_v = is_reference<_Tp>::value;\n#endif\n\n#endif // __has_builtin(__is_lvalue_reference) && etc...\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_REFERENCE_H\n", "__type_traits/is_reference_wrapper.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_REFERENCE_WRAPPER_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_REFERENCE_WRAPPER_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/remove_cv.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp> class _LIBCUDACXX_TEMPLATE_VIS reference_wrapper;\n\ntemplate <class _Tp> struct __is_reference_wrapper_impl : public false_type {};\ntemplate <class _Tp> struct __is_reference_wrapper_impl<reference_wrapper<_Tp> > : public true_type {};\ntemplate <class _Tp> struct __is_reference_wrapper\n    : public __is_reference_wrapper_impl<__remove_cv_t<_Tp> > {};\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_ENABLE_IF_H\n", "__type_traits/is_referenceable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_REFERENCEABLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_REFERENCEABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_same.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_REFERENCEABLE) && !defined(_LIBCUDACXX_USE_IS_REFERENCEABLE_FALLBACK)\n\ntemplate <class _Tp>\nstruct __libcpp_is_referenceable\n  : public integral_constant<bool, _LIBCUDACXX_IS_REFERENCEABLE(_Tp)>\n  {};\n\n#else\nstruct __libcpp_is_referenceable_impl {\n  template <class _Tp>\n  _LIBCUDACXX_INLINE_VISIBILITY static _Tp& __test(int);\n  template <class _Tp>\n  _LIBCUDACXX_INLINE_VISIBILITY static false_type __test(...);\n};\n\ntemplate <class _Tp>\nstruct __libcpp_is_referenceable\n    : integral_constant<bool, _IsNotSame<decltype(__libcpp_is_referenceable_impl::__test<_Tp>(0)), false_type>::value> {\n};\n#endif // defined(_LIBCUDACXX_IS_REFERENCEABLE) && !defined(_LIBCUDACXX_USE_IS_REFERENCEABLE_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_REFERENCEABLE_H\n", "__type_traits/is_same.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_SAME_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_SAME_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_SAME) && !defined(_LIBCUDACXX_USE_IS_SAME_FALLBACK)\n\ntemplate <class _Tp, class _Up>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_same : _BoolConstant<_LIBCUDACXX_IS_SAME(_Tp, _Up)> { };\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, class _Up>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_same_v = _LIBCUDACXX_IS_SAME(_Tp, _Up);\n#endif\n\n// _IsSame<T,U> has the same effect as is_same<T,U> but instantiates fewer types:\n// is_same<A,B> and is_same<C,D> are guaranteed to be different types, but\n// _IsSame<A,B> and _IsSame<C,D> are the same type (namely, false_type).\n// Neither GCC nor Clang can mangle the __is_same builtin, so _IsSame\n// mustn't be directly used anywhere that contributes to name-mangling\n// (such as in a dependent return type).\n\ntemplate <class _Tp, class _Up>\nusing _IsSame = _BoolConstant<_LIBCUDACXX_IS_SAME(_Tp, _Up)>;\n\ntemplate <class _Tp, class _Up>\nusing _IsNotSame = _BoolConstant<!_LIBCUDACXX_IS_SAME(_Tp, _Up)>;\n\n#else\n\ntemplate <class _Tp, class _Up> struct _LIBCUDACXX_TEMPLATE_VIS is_same           : public false_type {};\ntemplate <class _Tp>            struct _LIBCUDACXX_TEMPLATE_VIS is_same<_Tp, _Tp> : public true_type {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, class _Up>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_same_v = is_same<_Tp, _Up>::value;\n#endif\n\n// _IsSame<T,U> has the same effect as is_same<T,U> but instantiates fewer types:\n// is_same<A,B> and is_same<C,D> are guaranteed to be different types, but\n// _IsSame<A,B> and _IsSame<C,D> are the same type (namely, false_type).\n// Neither GCC nor Clang can mangle the __is_same builtin, so _IsSame\n// mustn't be directly used anywhere that contributes to name-mangling\n// (such as in a dependent return type).\n\ntemplate <class _Tp, class _Up>\nusing _IsSame = _BoolConstant<is_same<_Tp, _Up>::value>;\n\ntemplate <class _Tp, class _Up>\nusing _IsNotSame = _BoolConstant<!is_same<_Tp, _Up>::value>;\n\n#endif // defined(_LIBCUDACXX_IS_SAME) && !defined(_LIBCUDACXX_USE_IS_SAME_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_SAME_H\n", "__type_traits/is_scalar.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_SCALAR_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_SCALAR_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_arithmetic.h\"\n#include \"../__type_traits/is_enum.h\"\n#include \"../__type_traits/is_member_pointer.h\"\n#include \"../__type_traits/is_null_pointer.h\"\n#include \"../__type_traits/is_pointer.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_SCALAR) && !defined(_LIBCUDACXX_USE_IS_SCALAR_FALLBACK)\n\ntemplate<class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_scalar\n    : public integral_constant<bool, _LIBCUDACXX_IS_SCALAR(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_scalar_v = _LIBCUDACXX_IS_SCALAR(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct __is_block : false_type {};\n#if defined(_LIBCUDACXX_HAS_EXTENSION_BLOCKS)\ntemplate <class _Rp, class ..._Args> struct __is_block<_Rp (^)(_Args...)> : true_type {};\n#endif\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_scalar\n    : public integral_constant<bool, is_arithmetic<_Tp>::value     ||\n                                     is_member_pointer<_Tp>::value ||\n                                     is_pointer<_Tp>::value        ||\n                                     __is_nullptr_t<_Tp>::value    ||\n                                     __is_block<_Tp>::value        ||\n                                     is_enum<_Tp>::value           > {};\n\ntemplate <> struct _LIBCUDACXX_TEMPLATE_VIS is_scalar<nullptr_t> : public true_type {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_scalar_v = is_scalar<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_SCALAR) && !defined(_LIBCUDACXX_USE_IS_SCALAR_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_SCALAR_H\n", "__type_traits/is_scoped_enum.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_SCOPED_ENUM_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_SCOPED_ENUM_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_convertible.h\"\n#include \"../__type_traits/is_enum.h\"\n#include \"../__type_traits/underlying_type.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 20\ntemplate <class _Tp, bool = is_enum_v<_Tp> >\nstruct __is_scoped_enum_helper : false_type {};\n\ntemplate <class _Tp>\nstruct __is_scoped_enum_helper<_Tp, true>\n    : public bool_constant<!is_convertible_v<_Tp, underlying_type_t<_Tp> > > {};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_scoped_enum\n    : public __is_scoped_enum_helper<_Tp> {};\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_scoped_enum_v = is_scoped_enum<_Tp>::value;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_SCOPED_ENUM_H\n", "__type_traits/is_signed.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_SIGNED_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_SIGNED_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_arithmetic.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_SIGNED) && !defined(_LIBCUDACXX_USE_IS_SIGNED_FALLBACK)\n\ntemplate<class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_signed\n    : public integral_constant<bool, _LIBCUDACXX_IS_SIGNED(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_signed_v = _LIBCUDACXX_IS_SIGNED(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp, bool = is_integral<_Tp>::value>\nstruct __libcpp_is_signed_impl : public _BoolConstant<(_Tp(-1) < _Tp(0))> {};\n\ntemplate <class _Tp>\nstruct __libcpp_is_signed_impl<_Tp, false> : public true_type {};  // floating point\n\ntemplate <class _Tp, bool = is_arithmetic<_Tp>::value>\nstruct __libcpp_is_signed : public __libcpp_is_signed_impl<_Tp> {};\n\ntemplate <class _Tp> struct __libcpp_is_signed<_Tp, false> : public false_type {};\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_signed : public __libcpp_is_signed<_Tp> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_signed_v = is_signed<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_SIGNED) && !defined(_LIBCUDACXX_USE_IS_SIGNED_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_SIGNED_H\n", "__type_traits/is_signed_integer.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_SIGNED_INTEGER_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_SIGNED_INTEGER_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp> struct __libcpp_is_signed_integer : public false_type {};\ntemplate <> struct __libcpp_is_signed_integer<signed char>      : public true_type {};\ntemplate <> struct __libcpp_is_signed_integer<signed short>     : public true_type {};\ntemplate <> struct __libcpp_is_signed_integer<signed int>       : public true_type {};\ntemplate <> struct __libcpp_is_signed_integer<signed long>      : public true_type {};\ntemplate <> struct __libcpp_is_signed_integer<signed long long> : public true_type {};\n#ifndef _LIBCUDACXX_HAS_NO_INT128\ntemplate <> struct __libcpp_is_signed_integer<__int128_t>       : public true_type {};\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_SIGNED_INTEGER_H\n", "__type_traits/is_standard_layout.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_STANDARD_LAYOUT_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_STANDARD_LAYOUT_H\n\n#ifndef __cuda_std__\n#include <__config>\n#include \"../__type_traits/remove_all_extents.h\"\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_STANDARD_LAYOUT) && !defined(_LIBCUDACXX_USE_IS_STANDARD_LAYOUT_FALLBACK)\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_standard_layout\n    : public integral_constant<bool, _LIBCUDACXX_IS_STANDARD_LAYOUT(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_standard_layout_v = _LIBCUDACXX_IS_STANDARD_LAYOUT(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_standard_layout\n    : integral_constant<bool, is_scalar<__remove_all_extents_t<_Tp>>::value>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_standard_layout_v\n    = is_standard_layout<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_STANDARD_LAYOUT) && !defined(_LIBCUDACXX_USE_IS_STANDARD_LAYOUT_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_STANDARD_LAYOUT_H\n", "__type_traits/is_swappable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_SWAPPABLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_SWAPPABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_lvalue_reference.h\"\n#include \"../__type_traits/conditional.h\"\n#include \"../__type_traits/enable_if.h\"\n#include \"../__type_traits/is_move_assignable.h\"\n#include \"../__type_traits/is_move_constructible.h\"\n#include \"../__type_traits/is_nothrow_move_assignable.h\"\n#include \"../__type_traits/is_nothrow_move_constructible.h\"\n#include \"../__type_traits/is_referenceable.h\"\n#include \"../__type_traits/is_same.h\"\n#include \"../__type_traits/is_void.h\"\n#include \"../__type_traits/nat.h\"\n#include \"../__utility/declval.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp> struct __is_swappable;\ntemplate <class _Tp> struct __is_nothrow_swappable;\n\ntemplate <class _Tp>\nusing __swap_result_t = __enable_if_t<_LIBCUDACXX_TRAIT(is_move_constructible, _Tp)\n                                   && _LIBCUDACXX_TRAIT(is_move_assignable, _Tp)>;\n\ntemplate <class _Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n__swap_result_t<_Tp>\nswap(_Tp& __x, _Tp& __y) noexcept(_LIBCUDACXX_TRAIT(is_nothrow_move_constructible, _Tp)\n                               && _LIBCUDACXX_TRAIT(is_nothrow_move_assignable, _Tp));\n\ntemplate<class _Tp, size_t _Np>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n__enable_if_t<__is_swappable<_Tp>::value>\nswap(_Tp (&__a)[_Np], _Tp (&__b)[_Np]) noexcept(__is_nothrow_swappable<_Tp>::value);\n\nnamespace __detail\n{\n// ALL generic swap overloads MUST already have a declaration available at this point.\n\ntemplate <class _Tp, class _Up = _Tp,\n          bool _NotVoid = !_LIBCUDACXX_TRAIT(is_void, _Tp) && !_LIBCUDACXX_TRAIT(is_void, _Up)>\nstruct __swappable_with\n{\n    template <class _LHS, class _RHS>\n    _LIBCUDACXX_INLINE_VISIBILITY static decltype(swap(_CUDA_VSTD::declval<_LHS>(), _CUDA_VSTD::declval<_RHS>()))\n    __test_swap(int);\n    template <class, class>\n    _LIBCUDACXX_INLINE_VISIBILITY static __nat __test_swap(long);\n\n    // Extra parens are needed for the C++03 definition of decltype.\n    typedef decltype((__test_swap<_Tp, _Up>(0))) __swap1;\n    typedef decltype((__test_swap<_Up, _Tp>(0))) __swap2;\n\n    static const bool value = _IsNotSame<__swap1, __nat>::value\n                           && _IsNotSame<__swap2, __nat>::value;\n};\n\ntemplate <class _Tp, class _Up>\nstruct __swappable_with<_Tp, _Up,  false> : false_type {};\n\ntemplate <class _Tp, class _Up = _Tp, bool _Swappable = __swappable_with<_Tp, _Up>::value>\nstruct __nothrow_swappable_with {\n  static const bool value =\n      noexcept(swap(_CUDA_VSTD::declval<_Tp>(), _CUDA_VSTD::declval<_Up>()))\n  &&  noexcept(swap(_CUDA_VSTD::declval<_Up>(), _CUDA_VSTD::declval<_Tp>()));\n};\n\ntemplate <class _Tp, class _Up>\nstruct __nothrow_swappable_with<_Tp, _Up, false> : false_type {};\n\n} // namespace __detail\n\ntemplate <class _Tp>\nstruct __is_swappable\n    : public integral_constant<bool, __detail::__swappable_with<_Tp&>::value>\n{\n};\n\ntemplate <class _Tp>\nstruct __is_nothrow_swappable\n    : public integral_constant<bool, __detail::__nothrow_swappable_with<_Tp&>::value>\n{\n};\n\n#if _LIBCUDACXX_STD_VER > 11\n\ntemplate <class _Tp, class _Up>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_swappable_with\n    : public integral_constant<bool, __detail::__swappable_with<_Tp, _Up>::value>\n{\n};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_swappable\n    : public __conditional_t<\n        __libcpp_is_referenceable<_Tp>::value,\n        is_swappable_with<\n            __add_lvalue_reference_t<_Tp>,\n            __add_lvalue_reference_t<_Tp> >,\n        false_type\n    >\n{\n};\n\ntemplate <class _Tp, class _Up>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_swappable_with\n    : public integral_constant<bool, __detail::__nothrow_swappable_with<_Tp, _Up>::value>\n{\n};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_nothrow_swappable\n    : public __conditional_t<\n        __libcpp_is_referenceable<_Tp>::value,\n        is_nothrow_swappable_with<\n            __add_lvalue_reference_t<_Tp>,\n            __add_lvalue_reference_t<_Tp> >,\n        false_type\n    >\n{\n};\n\ntemplate <class _Tp, class _Up>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_swappable_with_v = is_swappable_with<_Tp, _Up>::value;\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_swappable_v = is_swappable<_Tp>::value;\n\ntemplate <class _Tp, class _Up>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_swappable_with_v = is_nothrow_swappable_with<_Tp, _Up>::value;\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_nothrow_swappable_v = is_nothrow_swappable<_Tp>::value;\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_SWAPPABLE_H\n", "__type_traits/is_trivial.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIAL_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIAL_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_trivially_copyable.h\"\n#include \"../__type_traits/is_trivially_default_constructible.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_TRIVIAL) && !defined(_LIBCUDACXX_USE_IS_TRIVIAL_FALLBACK)\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_trivial\n    : public integral_constant<bool, _LIBCUDACXX_IS_TRIVIAL(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivial_v = _LIBCUDACXX_IS_TRIVIAL(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_trivial\n    : public integral_constant<bool, is_trivially_copyable<_Tp>::value &&\n                                     is_trivially_default_constructible<_Tp>::value>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivial_v\n    = is_trivial<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_TRIVIAL) && !defined(_LIBCUDACXX_USE_IS_TRIVIAL_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIAL_H\n", "__type_traits/is_trivially_assignable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_ASSIGNABLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_ASSIGNABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_scalar.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_TRIVIALLY_ASSIGNABLE) && !defined(_LIBCUDACXX_USE_IS_TRIVIALLY_ASSIGNABLE_FALLBACK)\n\ntemplate <class _Tp, class _Arg>\nstruct is_trivially_assignable\n    : integral_constant<bool, _LIBCUDACXX_IS_TRIVIALLY_ASSIGNABLE(_Tp, _Arg)>\n{ };\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, class _Arg>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivially_assignable_v = _LIBCUDACXX_IS_TRIVIALLY_ASSIGNABLE(_Tp, _Arg);\n#endif\n\n#else\n\ntemplate <class _Tp, class _Arg>\nstruct is_trivially_assignable\n    : public false_type {};\n\ntemplate <class _Tp>\nstruct is_trivially_assignable<_Tp&, _Tp>\n    : integral_constant<bool, is_scalar<_Tp>::value> {};\n\ntemplate <class _Tp>\nstruct is_trivially_assignable<_Tp&, _Tp&>\n    : integral_constant<bool, is_scalar<_Tp>::value> {};\n\ntemplate <class _Tp>\nstruct is_trivially_assignable<_Tp&, const _Tp&>\n    : integral_constant<bool, is_scalar<_Tp>::value> {};\n\ntemplate <class _Tp>\nstruct is_trivially_assignable<_Tp&, _Tp&&>\n    : integral_constant<bool, is_scalar<_Tp>::value> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, class _Arg>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivially_assignable_v\n    = is_trivially_assignable<_Tp, _Arg>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_TRIVIALLY_ASSIGNABLE) && !defined(_LIBCUDACXX_USE_IS_TRIVIALLY_ASSIGNABLE_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_ASSIGNABLE_H\n", "__type_traits/is_trivially_constructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_CONSTRUCTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_CONSTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_scalar.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_TRIVIALLY_CONSTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_TRIVIALLY_CONSTRUCTIBLE_FALLBACK)\n\ntemplate <class _Tp, class... _Args>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_trivially_constructible\n    : public integral_constant<bool, _LIBCUDACXX_IS_TRIVIALLY_CONSTRUCTIBLE(_Tp, _Args...)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, class... _Args>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivially_constructible_v =\n    _LIBCUDACXX_IS_TRIVIALLY_CONSTRUCTIBLE(_Tp, _Args...);\n#endif\n\n#else\n\ntemplate <class _Tp, class... _Args>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_trivially_constructible\n    : false_type\n{\n};\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_trivially_constructible<_Tp>\n#if defined(_LIBCUDACXX_HAS_TRIVIAL_CONSTRUCTOR) && !defined(_LIBCUDACXX_USE_HAS_TRIVIAL_CONSTRUCTOR_FALLBACK)\n    : integral_constant<bool, _LIBCUDACXX_HAS_TRIVIAL_CONSTRUCTOR(_Tp)>\n#else\n    : integral_constant<bool, is_scalar<_Tp>::value>\n#endif // defined(_LIBCUDACXX_HAS_TRIVIAL_CONSTRUCTOR) && !defined(_LIBCUDACXX_USE_HAS_TRIVIAL_CONSTRUCTOR_FALLBACK)\n{\n};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_trivially_constructible<_Tp, _Tp&&>\n    : integral_constant<bool, is_scalar<_Tp>::value>\n{\n};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_trivially_constructible<_Tp, const _Tp&>\n    : integral_constant<bool, is_scalar<_Tp>::value>\n{\n};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_trivially_constructible<_Tp, _Tp&>\n    : integral_constant<bool, is_scalar<_Tp>::value>\n{\n};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp, class... _Args>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivially_constructible_v\n    = is_trivially_constructible<_Tp, _Args...>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_TRIVIALLY_CONSTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_TRIVIALLY_CONSTRUCTIBLE_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_CONSTRUCTIBLE_H\n", "__type_traits/is_trivially_copy_assignable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_COPY_ASSIGNABLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_COPY_ASSIGNABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_const.h\"\n#include \"../__type_traits/add_lvalue_reference.h\"\n#include \"../__type_traits/is_trivially_assignable.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_TRIVIALLY_ASSIGNABLE) && !defined(_LIBCUDACXX_USE_IS_TRIVIALLY_ASSIGNABLE_FALLBACK)\n\ntemplate <class _Tp>\nstruct is_trivially_copy_assignable\n    : public integral_constant<bool,\n        _LIBCUDACXX_IS_TRIVIALLY_ASSIGNABLE(__add_lvalue_reference_t<_Tp>,\n                                            __add_lvalue_reference_t<typename add_const<_Tp>::type>)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivially_copy_assignable_v =\n    _LIBCUDACXX_IS_TRIVIALLY_ASSIGNABLE(__add_lvalue_reference_t<_Tp>,\n                                        __add_lvalue_reference_t<typename add_const<_Tp>::type>);\n#endif\n\n#else\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_trivially_copy_assignable\n    : public is_trivially_assignable<__add_lvalue_reference_t<_Tp>,\n                                     __add_lvalue_reference_t<typename add_const<_Tp>::type>>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivially_copy_assignable_v = is_trivially_copy_assignable<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_TRIVIALLY_ASSIGNABLE) && !defined(_LIBCUDACXX_USE_IS_TRIVIALLY_ASSIGNABLE_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_COPY_ASSIGNABLE_H\n", "__type_traits/is_trivially_copy_constructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_COPY_CONSTRUCTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_COPY_CONSTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_const.h\"\n#include \"../__type_traits/add_lvalue_reference.h\"\n#include \"../__type_traits/is_trivially_constructible.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_TRIVIALLY_CONSTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_TRIVIALLY_CONSTRUCTIBLE_FALLBACK)\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_trivially_copy_constructible\n    : public integral_constant<bool,\n        _LIBCUDACXX_IS_TRIVIALLY_CONSTRUCTIBLE(_Tp, __add_lvalue_reference_t<typename add_const<_Tp>::type>)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivially_copy_constructible_v =\n    _LIBCUDACXX_IS_TRIVIALLY_CONSTRUCTIBLE(_Tp, __add_lvalue_reference_t<typename add_const<_Tp>::type>);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_trivially_copy_constructible\n    : public is_trivially_constructible<_Tp, __add_lvalue_reference_t<typename add_const<_Tp>::type>>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivially_copy_constructible_v = is_trivially_copy_constructible<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_TRIVIALLY_CONSTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_TRIVIALLY_CONSTRUCTIBLE_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_COPY_CONSTRUCTIBLE_H\n", "__type_traits/is_trivially_copyable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_COPYABLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_COPYABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_scalar.h\"\n#include \"../__type_traits/remove_all_extents.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_TRIVIALLY_COPYABLE) && !defined(_LIBCUDACXX_USE_IS_TRIVIALLY_COPYABLE_FALLBACK)\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_trivially_copyable\n    : public integral_constant<bool, _LIBCUDACXX_IS_TRIVIALLY_COPYABLE(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivially_copyable_v = _LIBCUDACXX_IS_TRIVIALLY_COPYABLE(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_trivially_copyable\n    : integral_constant<bool, is_scalar<__remove_all_extents_t<_Tp>::type>::value>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivially_copyable_v\n    = is_trivially_copyable<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_TRIVIALLY_COPYABLE) && !defined(_LIBCUDACXX_USE_IS_TRIVIALLY_COPYABLE_FALLBACK)\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_COPYABLE_H\n", "__type_traits/is_trivially_default_constructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_DEFAULT_CONSTRUCTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_DEFAULT_CONSTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/is_trivially_constructible.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_TRIVIALLY_CONSTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_TRIVIALLY_CONSTRUCTIBLE_FALLBACK)\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_trivially_default_constructible\n    : public integral_constant<bool, _LIBCUDACXX_IS_TRIVIALLY_CONSTRUCTIBLE(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivially_default_constructible_v = _LIBCUDACXX_IS_TRIVIALLY_CONSTRUCTIBLE(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_trivially_default_constructible\n    : public is_trivially_constructible<_Tp>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivially_default_constructible_v\n    = is_trivially_default_constructible<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_TRIVIALLY_CONSTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_TRIVIALLY_CONSTRUCTIBLE_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_DEFAULT_CONSTRUCTIBLE_H\n", "__type_traits/is_trivially_destructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_DESTRUCTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_DESTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_destructible.h\"\n#include \"../__type_traits/is_reference.h\"\n#include \"../__type_traits/is_scalar.h\"\n#include \"../__type_traits/remove_all_extents.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_TRIVIALLY_DESTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_TRIVIALLY_DESTRUCTIBLE_FALLBACK)\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_trivially_destructible\n    : public integral_constant<bool, _LIBCUDACXX_IS_TRIVIALLY_DESTRUCTIBLE(_Tp)> {};\n\n#elif defined(_LIBCUDACXX_HAS_TRIVIAL_DESTRUCTOR) && !defined(_LIBCUDACXX_USE_HAS_TRIVIAL_DESTRUCTOR_FALLBACK)\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_trivially_destructible\n    : public integral_constant<bool, is_destructible<_Tp>::value && _LIBCUDACXX_HAS_TRIVIAL_DESTRUCTOR(_Tp)> {};\n\n#else\n\ntemplate <class _Tp> struct __libcpp_trivial_destructor\n    : public integral_constant<bool, is_scalar<_Tp>::value ||\n                                     is_reference<_Tp>::value> {};\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_trivially_destructible\n    : public __libcpp_trivial_destructor<__remove_all_extents_t<_Tp>> {};\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_trivially_destructible<_Tp[]>\n    : public false_type {};\n\n#endif // defined(_LIBCUDACXX_HAS_TRIVIAL_DESTRUCTOR) && !defined(_LIBCUDACXX_USE_HAS_TRIVIAL_DESTRUCTOR_FALLBACK)\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivially_destructible_v = is_trivially_destructible<_Tp>::value;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_DESTRUCTIBLE_H\n", "__type_traits/is_trivially_move_assignable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_MOVE_ASSIGNABLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_MOVE_ASSIGNABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_lvalue_reference.h\"\n#include \"../__type_traits/add_rvalue_reference.h\"\n#include \"../__type_traits/is_trivially_assignable.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_TRIVIALLY_ASSIGNABLE) && !defined(_LIBCUDACXX_USE_IS_TRIVIALLY_ASSIGNABLE_FALLBACK)\n\ntemplate <class _Tp>\nstruct is_trivially_move_assignable\n    : public integral_constant<bool,\n        _LIBCUDACXX_IS_TRIVIALLY_ASSIGNABLE(__add_lvalue_reference_t<_Tp>, __add_rvalue_reference_t<_Tp>)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivially_move_assignable_v =\n    _LIBCUDACXX_IS_TRIVIALLY_ASSIGNABLE(__add_lvalue_reference_t<_Tp>, __add_rvalue_reference_t<_Tp>);\n#endif\n\n#else\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_trivially_move_assignable\n    : public is_trivially_assignable<__add_lvalue_reference_t<_Tp>, __add_rvalue_reference_t<_Tp>>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivially_move_assignable_v = is_trivially_move_assignable<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_TRIVIALLY_ASSIGNABLE) && !defined(_LIBCUDACXX_USE_IS_TRIVIALLY_ASSIGNABLE_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_MOVE_ASSIGNABLE_H\n", "__type_traits/is_trivially_move_constructible.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_MOVE_CONSTRUCTIBLE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_MOVE_CONSTRUCTIBLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_rvalue_reference.h\"\n#include \"../__type_traits/is_trivially_constructible.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_TRIVIALLY_CONSTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_TRIVIALLY_CONSTRUCTIBLE_FALLBACK)\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_trivially_move_constructible\n    : public integral_constant<bool,\n        _LIBCUDACXX_IS_TRIVIALLY_CONSTRUCTIBLE(_Tp, __add_rvalue_reference_t<_Tp>)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivially_move_constructible_v =\n    _LIBCUDACXX_IS_TRIVIALLY_CONSTRUCTIBLE(_Tp, __add_rvalue_reference_t<_Tp>);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_trivially_move_constructible\n    : public is_trivially_constructible<_Tp, __add_rvalue_reference_t<_Tp>>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_trivially_move_constructible_v = is_trivially_move_constructible<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_TRIVIALLY_CONSTRUCTIBLE) && !defined(_LIBCUDACXX_USE_IS_TRIVIALLY_CONSTRUCTIBLE_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_TRIVIALLY_MOVE_CONSTRUCTIBLE_H\n", "__type_traits/is_unbounded_array.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_UNBOUNDED_ARRAY_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_UNBOUNDED_ARRAY_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class>     struct _LIBCUDACXX_TEMPLATE_VIS __libcpp_is_unbounded_array        : false_type {};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS __libcpp_is_unbounded_array<_Tp[]> : true_type {};\n\n#if _LIBCUDACXX_STD_VER > 11\n\ntemplate <class>     struct _LIBCUDACXX_TEMPLATE_VIS is_unbounded_array        : false_type {};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_unbounded_array<_Tp[]> : true_type {};\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_unbounded_array_v  = is_unbounded_array<_Tp>::value;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_UNBOUNDED_ARRAY_H\n", "__type_traits/is_union.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_UNION_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_UNION_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/remove_cv.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_UNION) && !defined(_LIBCUDACXX_USE_IS_UNION_FALLBACK)\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_union\n    : public integral_constant<bool, _LIBCUDACXX_IS_UNION(_Tp)> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_union_v = _LIBCUDACXX_IS_UNION(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct __libcpp_union : public false_type {};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_union\n    : public __libcpp_union<__remove_cv_t<_Tp>> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_union_v = is_union<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_UNION) && !defined(_LIBCUDACXX_USE_IS_UNION_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_UNION_H\n", "__type_traits/is_unsigned.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_UNSIGNED_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_UNSIGNED_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_arithmetic.h\"\n#include \"../__type_traits/is_integral.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n// Before AppleClang 14, __is_unsigned returned true for enums with signed underlying type.\n#if defined(_LIBCUDACXX_IS_UNSIGNED) && !defined(_LIBCUDACXX_USE_IS_UNSIGNED_FALLBACK) && !(defined(_LIBCUDACXX_APPLE_CLANG_VER) && _LIBCUDACXX_APPLE_CLANG_VER < 1400)\n\ntemplate<class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_unsigned\n    : public integral_constant<bool, _LIBCUDACXX_IS_UNSIGNED(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_unsigned_v = _LIBCUDACXX_IS_UNSIGNED(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp, bool = is_integral<_Tp>::value>\nstruct __libcpp_is_unsigned_impl : public _BoolConstant<(_Tp(0) < _Tp(-1))> {};\n\ntemplate <class _Tp>\nstruct __libcpp_is_unsigned_impl<_Tp, false> : public false_type {};  // floating point\n\ntemplate <class _Tp, bool = is_arithmetic<_Tp>::value>\nstruct __libcpp_is_unsigned : public __libcpp_is_unsigned_impl<_Tp> {};\n\ntemplate <class _Tp> struct __libcpp_is_unsigned<_Tp, false> : public false_type {};\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_unsigned : public __libcpp_is_unsigned<_Tp> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_unsigned_v = is_unsigned<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_UNSIGNED) && !defined(_LIBCUDACXX_USE_IS_UNSIGNED_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_UNSIGNED_H\n", "__type_traits/is_unsigned_integer.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_UNSIGNED_INTEGER_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_UNSIGNED_INTEGER_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp> struct __libcpp_is_unsigned_integer : public false_type {};\ntemplate <> struct __libcpp_is_unsigned_integer<unsigned char>      : public true_type {};\ntemplate <> struct __libcpp_is_unsigned_integer<unsigned short>     : public true_type {};\ntemplate <> struct __libcpp_is_unsigned_integer<unsigned int>       : public true_type {};\ntemplate <> struct __libcpp_is_unsigned_integer<unsigned long>      : public true_type {};\ntemplate <> struct __libcpp_is_unsigned_integer<unsigned long long> : public true_type {};\n#ifndef _LIBCUDACXX_HAS_NO_INT128\ntemplate <> struct __libcpp_is_unsigned_integer<__uint128_t>        : public true_type {};\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_UNSIGNED_INTEGER_H\n", "__type_traits/is_valid_expansion.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_VALID_EXPANSION_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_VALID_EXPANSION_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <template <class...> class _Templ, class ..._Args, class = _Templ<_Args...> >\n_LIBCUDACXX_INLINE_VISIBILITY true_type __sfinae_test_impl(int);\ntemplate <template <class...> class, class ...>\n_LIBCUDACXX_INLINE_VISIBILITY false_type __sfinae_test_impl(...);\n\ntemplate <template <class ...> class _Templ, class ..._Args>\nusing _IsValidExpansion _LIBCUDACXX_NODEBUG_TYPE = decltype(_CUDA_VSTD::__sfinae_test_impl<_Templ, _Args...>(0));\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_VALID_EXPANSION_H\n", "__type_traits/is_void.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_VOID_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_VOID_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_same.h\"\n#include \"../__type_traits/remove_cvref.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_VOID) && !defined(_LIBCUDACXX_USE_IS_VOID_FALLBACK)\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_void\n    : integral_constant<bool, _LIBCUDACXX_IS_VOID(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_void_v = __is_void(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_void\n    : public is_same<__remove_cv_t<_Tp>, void> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_void_v = is_void<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_VOID) && !defined(_LIBCUDACXX_USE_IS_VOID_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_VOID_H\n", "__type_traits/is_volatile.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_IS_VOLATILE_H\n#define _LIBCUDACXX___TYPE_TRAITS_IS_VOLATILE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_IS_VOLATILE) && !defined(_LIBCUDACXX_USE_IS_VOLATILE_FALLBACK)\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS is_volatile :\n    : public integral_constant<bool, _LIBCUDACXX_IS_VOLATILE(_Tp)>\n    {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_volatile_v = _LIBCUDACXX_IS_VOLATILE(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_volatile               : public false_type {};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS is_volatile<_Tp volatile> : public true_type {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool is_volatile_v = is_volatile<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_IS_VOLATILE) && !defined(_LIBCUDACXX_USE_IS_VOLATILE_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_IS_VOLATILE_H\n", "__type_traits/lazy.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_LAZY_H\n#define _LIBCUDACXX___TYPE_TRAITS_LAZY_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <template <class...> class _Func, class ..._Args>\nstruct _Lazy : _Func<_Args...> {};\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_LAZY_H\n", "__type_traits/make_32_64_or_128_bit.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_MAKE_32_64_OR_128_BIT_H\n#define _LIBCUDACXX___TYPE_TRAITS_MAKE_32_64_OR_128_BIT_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/conditional.h\"\n#include \"../__type_traits/is_same.h\"\n#include \"../__type_traits/is_signed.h\"\n#include \"../__type_traits/is_unsigned.h\"\n#include \"../__type_traits/make_unsigned.h\"\n#include \"../cstdint\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n/// Helper to promote an integral to smallest 32, 64, or 128 bit representation.\n///\n/// The restriction is the same as the integral version of to_char.\ntemplate <class _Tp>\n#if _LIBCUDACXX_STD_VER > 17\n  requires (is_signed_v<_Tp> || is_unsigned_v<_Tp> || is_same_v<_Tp, char>)\n#endif\nusing __make_32_64_or_128_bit_t =\n  __copy_unsigned_t<_Tp,\n    __conditional_t<sizeof(_Tp) <= sizeof(int32_t),    int32_t,\n    __conditional_t<sizeof(_Tp) <= sizeof(int64_t),    int64_t,\n#ifndef _LIBCUDACXX_HAS_NO_INT128\n    __conditional_t<sizeof(_Tp) <= sizeof(__int128_t), __int128_t,\n    /* else */                                         void>\n#else\n    /* else */                                         void\n#endif\n    > >\n  >;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_MAKE_32_64_OR_128_BIT_H\n", "__type_traits/make_const_lvalue_ref.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_MAKE_CONST_LVALUE_REF_H\n#define _LIBCUDACXX___TYPE_TRAITS_MAKE_CONST_LVALUE_REF_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/remove_reference.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate<class _Tp>\nusing __make_const_lvalue_ref = const __libcpp_remove_reference_t<_Tp>&;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_MAKE_CONST_LVALUE_REF_H\n", "__type_traits/make_signed.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_MAKE_SIGNED_H\n#define _LIBCUDACXX___TYPE_TRAITS_MAKE_SIGNED_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/apply_cv.h\"\n#include \"../__type_traits/is_enum.h\"\n#include \"../__type_traits/is_integral.h\"\n#include \"../__type_traits/nat.h\"\n#include \"../__type_traits/remove_cv.h\"\n#include \"../__type_traits/type_list.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_MAKE_SIGNED) && !defined(_LIBCUDACXX_USE_MAKE_SIGNED_FALLBACK)\n\ntemplate <class _Tp>\nusing __make_signed_t = _LIBCUDACXX_MAKE_SIGNED(_Tp);\n\n#else\ntypedef\n    __type_list<signed char,\n    __type_list<signed short,\n    __type_list<signed int,\n    __type_list<signed long,\n    __type_list<signed long long,\n#  ifndef _LIBCUDACXX_HAS_NO_INT128\n    __type_list<__int128_t,\n#  endif\n    __nat\n#  ifndef _LIBCUDACXX_HAS_NO_INT128\n    >\n#  endif\n    > > > > > __signed_types;\n\ntemplate <class _Tp, bool = is_integral<_Tp>::value || is_enum<_Tp>::value>\nstruct __make_signed_impl {};\n\ntemplate <class _Tp>\nstruct __make_signed_impl<_Tp, true>\n{\n    typedef typename __find_first<__signed_types, sizeof(_Tp)>::type type;\n};\n\ntemplate <> struct __make_signed_impl<bool,               true> {};\ntemplate <> struct __make_signed_impl<  signed short,     true> {typedef short     type;};\ntemplate <> struct __make_signed_impl<unsigned short,     true> {typedef short     type;};\ntemplate <> struct __make_signed_impl<  signed int,       true> {typedef int       type;};\ntemplate <> struct __make_signed_impl<unsigned int,       true> {typedef int       type;};\ntemplate <> struct __make_signed_impl<  signed long,      true> {typedef long      type;};\ntemplate <> struct __make_signed_impl<unsigned long,      true> {typedef long      type;};\ntemplate <> struct __make_signed_impl<  signed long long, true> {typedef long long type;};\ntemplate <> struct __make_signed_impl<unsigned long long, true> {typedef long long type;};\n#  ifndef _LIBCUDACXX_HAS_NO_INT128\ntemplate <> struct __make_signed_impl<__int128_t,         true> {typedef __int128_t type;};\ntemplate <> struct __make_signed_impl<__uint128_t,        true> {typedef __int128_t type;};\n#  endif\n\ntemplate <class _Tp>\nusing __make_signed_t = typename __apply_cv<_Tp, typename __make_signed_impl<__remove_cv_t<_Tp> >::type>::type;\n\n#endif // defined(_LIBCUDACXX_MAKE_SIGNED) && !defined(_LIBCUDACXX_USE_MAKE_SIGNED_FALLBACK)\n\ntemplate <class _Tp>\nstruct make_signed {\n  using type _LIBCUDACXX_NODEBUG_TYPE = __make_signed_t<_Tp>;\n};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> using make_signed_t = __make_signed_t<_Tp>;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_MAKE_SIGNED_H\n", "__type_traits/make_unsigned.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_MAKE_UNSIGNED_H\n#define _LIBCUDACXX___TYPE_TRAITS_MAKE_UNSIGNED_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/apply_cv.h\"\n#include \"../__type_traits/conditional.h\"\n#include \"../__type_traits/is_enum.h\"\n#include \"../__type_traits/is_integral.h\"\n#include \"../__type_traits/is_unsigned.h\"\n#include \"../__type_traits/nat.h\"\n#include \"../__type_traits/remove_cv.h\"\n#include \"../__type_traits/type_list.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_MAKE_UNSIGNED) && !defined(_LIBCUDACXX_USE_MAKE_UNSIGNED_FALLBACK)\n\ntemplate <class _Tp>\nusing __make_unsigned_t = _LIBCUDACXX_MAKE_UNSIGNED(_Tp);\n\n#else\ntypedef\n    __type_list<unsigned char,\n    __type_list<unsigned short,\n    __type_list<unsigned int,\n    __type_list<unsigned long,\n    __type_list<unsigned long long,\n#  ifndef _LIBCUDACXX_HAS_NO_INT128\n    __type_list<__uint128_t,\n#  endif\n    __nat\n#  ifndef _LIBCUDACXX_HAS_NO_INT128\n    >\n#  endif\n    > > > > > __unsigned_types;\n\ntemplate <class _Tp, bool = is_integral<_Tp>::value || is_enum<_Tp>::value>\nstruct __make_unsigned_impl {};\n\ntemplate <class _Tp>\nstruct __make_unsigned_impl<_Tp, true>\n{\n    typedef typename __find_first<__unsigned_types, sizeof(_Tp)>::type type;\n};\n\ntemplate <> struct __make_unsigned_impl<bool,               true> {};\ntemplate <> struct __make_unsigned_impl<  signed short,     true> {typedef unsigned short     type;};\ntemplate <> struct __make_unsigned_impl<unsigned short,     true> {typedef unsigned short     type;};\ntemplate <> struct __make_unsigned_impl<  signed int,       true> {typedef unsigned int       type;};\ntemplate <> struct __make_unsigned_impl<unsigned int,       true> {typedef unsigned int       type;};\ntemplate <> struct __make_unsigned_impl<  signed long,      true> {typedef unsigned long      type;};\ntemplate <> struct __make_unsigned_impl<unsigned long,      true> {typedef unsigned long      type;};\ntemplate <> struct __make_unsigned_impl<  signed long long, true> {typedef unsigned long long type;};\ntemplate <> struct __make_unsigned_impl<unsigned long long, true> {typedef unsigned long long type;};\n#  ifndef _LIBCUDACXX_HAS_NO_INT128\ntemplate <> struct __make_unsigned_impl<__int128_t,         true> {typedef __uint128_t        type;};\ntemplate <> struct __make_unsigned_impl<__uint128_t,        true> {typedef __uint128_t        type;};\n#  endif\n\ntemplate <class _Tp>\nusing __make_unsigned_t = typename __apply_cv<_Tp, typename __make_unsigned_impl<__remove_cv_t<_Tp> >::type>::type;\n\n#endif // defined(_LIBCUDACXX_MAKE_UNSIGNED) && !defined(_LIBCUDACXX_USE_MAKE_UNSIGNED_FALLBACK)\n\ntemplate <class _Tp>\nstruct make_unsigned {\n  using type _LIBCUDACXX_NODEBUG_TYPE = __make_unsigned_t<_Tp>;\n};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> using make_unsigned_t = __make_unsigned_t<_Tp>;\n#endif\n\ntemplate <class _Tp>\n_LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY constexpr\n__make_unsigned_t<_Tp> __to_unsigned_like(_Tp __x) noexcept {\n    return static_cast<__make_unsigned_t<_Tp> >(__x);\n}\n\ntemplate <class _Tp, class _Up>\nusing __copy_unsigned_t = __conditional_t<is_unsigned<_Tp>::value, __make_unsigned_t<_Up>, _Up>;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_MAKE_UNSIGNED_H\n", "__type_traits/maybe_const.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_MAYBE_CONST_H\n#define _LIBCUDACXX___TYPE_TRAITS_MAYBE_CONST_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/conditional.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate<bool _Const, class _Tp>\nusing __maybe_const = __conditional_t<_Const, const _Tp, _Tp>;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_MAYBE_CONST_H\n", "__type_traits/nat.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_NAT_H\n#define _LIBCUDACXX___TYPE_TRAITS_NAT_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\nstruct __nat\n{\n    __nat() = delete;\n    __nat(const __nat&) = delete;\n    __nat& operator=(const __nat&) = delete;\n    ~__nat() = delete;\n};\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_NAT_H\n", "__type_traits/negation.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_NEGATION_H\n#define _LIBCUDACXX___TYPE_TRAITS_NEGATION_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Pred>\nstruct _Not : _BoolConstant<!_Pred::value> {};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp>\nstruct negation : _Not<_Tp> {};\ntemplate<class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr bool negation_v = !_Tp::value;\n#endif // _LIBCUDACXX_STD_VER > 14\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_NEGATION_H\n", "__type_traits/promote.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_PROMOTE_H\n#define _LIBCUDACXX___TYPE_TRAITS_PROMOTE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_same.h\"\n#include \"../__utility/declval.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp>\nstruct __numeric_type\n{\n   _LIBCUDACXX_INLINE_VISIBILITY static void __test(...);\n   _LIBCUDACXX_INLINE_VISIBILITY static float __test(float);\n   _LIBCUDACXX_INLINE_VISIBILITY static double __test(char);\n   _LIBCUDACXX_INLINE_VISIBILITY static double __test(int);\n   _LIBCUDACXX_INLINE_VISIBILITY static double __test(unsigned);\n   _LIBCUDACXX_INLINE_VISIBILITY static double __test(long);\n   _LIBCUDACXX_INLINE_VISIBILITY static double __test(unsigned long);\n   _LIBCUDACXX_INLINE_VISIBILITY static double __test(long long);\n   _LIBCUDACXX_INLINE_VISIBILITY static double __test(unsigned long long);\n   _LIBCUDACXX_INLINE_VISIBILITY static double __test(double);\n   _LIBCUDACXX_INLINE_VISIBILITY static long double __test(long double);\n\n   typedef decltype(__test(declval<_Tp>())) type;\n   static const bool value = _IsNotSame<type, void>::value;\n};\n\ntemplate <>\nstruct __numeric_type<void>\n{\n   static const bool value = true;\n};\n\ntemplate <class _A1, class _A2 = void, class _A3 = void,\n          bool = __numeric_type<_A1>::value &&\n                 __numeric_type<_A2>::value &&\n                 __numeric_type<_A3>::value>\nclass __promote_imp\n{\npublic:\n    static const bool value = false;\n};\n\ntemplate <class _A1, class _A2, class _A3>\nclass __promote_imp<_A1, _A2, _A3, true>\n{\nprivate:\n    typedef typename __promote_imp<_A1>::type __type1;\n    typedef typename __promote_imp<_A2>::type __type2;\n    typedef typename __promote_imp<_A3>::type __type3;\npublic:\n    typedef decltype(__type1() + __type2() + __type3()) type;\n    static const bool value = true;\n};\n\ntemplate <class _A1, class _A2>\nclass __promote_imp<_A1, _A2, void, true>\n{\nprivate:\n    typedef typename __promote_imp<_A1>::type __type1;\n    typedef typename __promote_imp<_A2>::type __type2;\npublic:\n    typedef decltype(__type1() + __type2()) type;\n    static const bool value = true;\n};\n\ntemplate <class _A1>\nclass __promote_imp<_A1, void, void, true>\n{\npublic:\n    typedef typename __numeric_type<_A1>::type type;\n    static const bool value = true;\n};\n\ntemplate <class _A1, class _A2 = void, class _A3 = void>\nclass __promote : public __promote_imp<_A1, _A2, _A3> {};\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_PROMOTE_H\n", "__type_traits/rank.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_RANK_H\n#define _LIBCUDACXX___TYPE_TRAITS_RANK_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/integral_constant.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_ARRAY_RANK) && !defined(_LIBCUDACXX_USE_ARRAY_RANK_FALLBACK) && 0\n\ntemplate <class _Tp>\nstruct rank : integral_constant<size_t, _LIBCUDACXX_ARRAY_RANK(_Tp)> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr size_t rank_v = _LIBCUDACXX_ARRAY_RANK(_Tp);\n#endif\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS rank\n    : public integral_constant<size_t, 0> {};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS rank<_Tp[]>\n    : public integral_constant<size_t, rank<_Tp>::value + 1> {};\ntemplate <class _Tp, size_t _Np> struct _LIBCUDACXX_TEMPLATE_VIS rank<_Tp[_Np]>\n    : public integral_constant<size_t, rank<_Tp>::value + 1> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr size_t rank_v = rank<_Tp>::value;\n#endif\n\n#endif // defined(_LIBCUDACXX_ARRAY_RANK) && !defined(_LIBCUDACXX_USE_ARRAY_RANK_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_RANK_H\n", "__type_traits/remove_all_extents.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_REMOVE_ALL_EXTENTS_H\n#define _LIBCUDACXX___TYPE_TRAITS_REMOVE_ALL_EXTENTS_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_REMOVE_ALL_EXTENTS) && !defined(_LIBCUDACXX_USE_REMOVE_ALL_EXTENTS_FALLBACK)\ntemplate <class _Tp>\nstruct remove_all_extents {\n  using type _LIBCUDACXX_NODEBUG_TYPE = _LIBCUDACXX_REMOVE_ALL_EXTENTS(_Tp);\n};\n\ntemplate <class _Tp>\nusing __remove_all_extents_t = _LIBCUDACXX_REMOVE_ALL_EXTENTS(_Tp);\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS remove_all_extents\n    {typedef _Tp type;};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS remove_all_extents<_Tp[]>\n    {typedef typename remove_all_extents<_Tp>::type type;};\ntemplate <class _Tp, size_t _Np> struct _LIBCUDACXX_TEMPLATE_VIS remove_all_extents<_Tp[_Np]>\n    {typedef typename remove_all_extents<_Tp>::type type;};\n\ntemplate <class _Tp>\nusing __remove_all_extents_t = typename remove_all_extents<_Tp>::type;\n\n#endif // defined(_LIBCUDACXX_REMOVE_ALL_EXTENTS) && !defined(_LIBCUDACXX_USE_REMOVE_ALL_EXTENTS_FALLBACK)\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> using remove_all_extents_t = __remove_all_extents_t<_Tp>;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_REMOVE_ALL_EXTENTS_H\n", "__type_traits/remove_const.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_REMOVE_CONST_H\n#define _LIBCUDACXX___TYPE_TRAITS_REMOVE_CONST_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_REMOVE_CONST) && !defined(_LIBCUDACXX_USE_REMOVE_CONST_FALLBACK)\ntemplate <class _Tp>\nstruct remove_const {\n  using type _LIBCUDACXX_NODEBUG_TYPE = _LIBCUDACXX_REMOVE_CONST(_Tp);\n};\n\ntemplate <class _Tp>\nusing __remove_const_t = _LIBCUDACXX_REMOVE_CONST(_Tp);\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS remove_const            {typedef _Tp type;};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS remove_const<const _Tp> {typedef _Tp type;};\n\ntemplate <class _Tp>\nusing __remove_const_t = typename remove_const<_Tp>::type;\n\n#endif // defined(_LIBCUDACXX_REMOVE_CONST) && !defined(_LIBCUDACXX_USE_REMOVE_CONST_FALLBACK)\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> using remove_const_t = __remove_const_t<_Tp>;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_REMOVE_CONST_H\n", "__type_traits/remove_const_ref.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_REMOVE_CONST_REF_H\n#define _LIBCUDACXX___TYPE_TRAITS_REMOVE_CONST_REF_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/remove_const.h\"\n#include \"../__type_traits/remove_reference.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp>\nusing __remove_const_ref_t = __remove_const_t<__libcpp_remove_reference_t<_Tp> >;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_REMOVE_CONST_REF_H\n", "__type_traits/remove_cv.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_REMOVE_CV_H\n#define _LIBCUDACXX___TYPE_TRAITS_REMOVE_CV_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/remove_const.h\"\n#include \"../__type_traits/remove_volatile.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_REMOVE_CV) && !defined(_LIBCUDACXX_USE_REMOVE_CV_FALLBACK)\ntemplate <class _Tp>\nstruct remove_cv {\n  using type _LIBCUDACXX_NODEBUG_TYPE = _LIBCUDACXX_REMOVE_CV(_Tp);\n};\n\ntemplate <class _Tp>\nusing __remove_cv_t = _LIBCUDACXX_REMOVE_CV(_Tp);\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS remove_cv\n{typedef __remove_volatile_t<__remove_const_t<_Tp> > type;};\n\ntemplate <class _Tp>\nusing __remove_cv_t = __remove_volatile_t<__remove_const_t<_Tp> >;\n\n#endif // defined(_LIBCUDACXX_REMOVE_CV) && !defined(_LIBCUDACXX_USE_REMOVE_CV_FALLBACK)\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> using remove_cv_t = __remove_cv_t<_Tp>;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_REMOVE_CV_H\n", "__type_traits/remove_cvref.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_REMOVE_CVREF_H\n#define _LIBCUDACXX___TYPE_TRAITS_REMOVE_CVREF_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/is_same.h\"\n#include \"../__type_traits/remove_cv.h\"\n#include \"../__type_traits/remove_reference.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_REMOVE_CVREF) && !defined(_LIBCUDACXX_USE_REMOVE_CVREF_FALLBACK)\n\ntemplate <class _Tp>\nusing __remove_cvref_t _LIBCUDACXX_NODEBUG_TYPE = _LIBCUDACXX_REMOVE_CVREF(_Tp);\n\n#else\n\ntemplate <class _Tp>\nusing __remove_cvref_t _LIBCUDACXX_NODEBUG_TYPE = __remove_cv_t<__libcpp_remove_reference_t<_Tp> >;\n\n#endif // defined(_LIBCUDACXX_REMOVE_CVREF) && !defined(_LIBCUDACXX_USE_REMOVE_CVREF_FALLBACK)\n\ntemplate <class _Tp, class _Up>\nstruct __is_same_uncvref : _IsSame<__remove_cvref_t<_Tp>, __remove_cvref_t<_Up> > {};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp>\nstruct remove_cvref {\n    using type _LIBCUDACXX_NODEBUG_TYPE = __remove_cvref_t<_Tp>;\n};\n\ntemplate <class _Tp> using remove_cvref_t = __remove_cvref_t<_Tp>;\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_REMOVE_CVREF_H\n", "__type_traits/remove_extent.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_REMOVE_EXTENT_H\n#define _LIBCUDACXX___TYPE_TRAITS_REMOVE_EXTENT_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_REMOVE_EXTENT) && !defined(_LIBCUDACXX_USE_REMOVE_EXTENT_FALLBACK)\ntemplate <class _Tp>\nstruct remove_extent {\n  using type _LIBCUDACXX_NODEBUG_TYPE = _LIBCUDACXX_REMOVE_EXTENT(_Tp);\n};\n\ntemplate <class _Tp>\nusing __remove_extent_t = _LIBCUDACXX_REMOVE_EXTENT(_Tp);\n\n#else\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS remove_extent\n    {typedef _Tp type;};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS remove_extent<_Tp[]>\n    {typedef _Tp type;};\ntemplate <class _Tp, size_t _Np> struct _LIBCUDACXX_TEMPLATE_VIS remove_extent<_Tp[_Np]>\n    {typedef _Tp type;};\n\ntemplate <class _Tp>\nusing __remove_extent_t = typename remove_extent<_Tp>::type;\n\n#endif // defined(_LIBCUDACXX_REMOVE_EXTENT) && !defined(_LIBCUDACXX_USE_REMOVE_EXTENT_FALLBACK)\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> using remove_extent_t = __remove_extent_t<_Tp>;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_REMOVE_EXTENT_H\n", "__type_traits/remove_pointer.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_REMOVE_POINTER_H\n#define _LIBCUDACXX___TYPE_TRAITS_REMOVE_POINTER_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_REMOVE_POINTER) && !defined(_LIBCUDACXX_USE_REMOVE_POINTER_FALLBACK)\ntemplate <class _Tp>\nstruct remove_pointer {\n  using type _LIBCUDACXX_NODEBUG_TYPE = _LIBCUDACXX_REMOVE_POINTER(_Tp);\n};\n\ntemplate <class _Tp>\nusing __remove_pointer_t = _LIBCUDACXX_REMOVE_POINTER(_Tp);\n\n#else\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS remove_pointer                      {typedef _LIBCUDACXX_NODEBUG_TYPE _Tp type;};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS remove_pointer<_Tp*>                {typedef _LIBCUDACXX_NODEBUG_TYPE _Tp type;};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS remove_pointer<_Tp* const>          {typedef _LIBCUDACXX_NODEBUG_TYPE _Tp type;};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS remove_pointer<_Tp* volatile>       {typedef _LIBCUDACXX_NODEBUG_TYPE _Tp type;};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS remove_pointer<_Tp* const volatile> {typedef _LIBCUDACXX_NODEBUG_TYPE _Tp type;};\n\ntemplate <class _Tp>\nusing __remove_pointer_t = typename remove_pointer<_Tp>::type;\n\n#endif // defined(_LIBCUDACXX_REMOVE_POINTER) && !defined(_LIBCUDACXX_USE_REMOVE_POINTER_FALLBACK)\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> using remove_pointer_t = __remove_pointer_t<_Tp>;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_REMOVE_POINTER_H\n", "__type_traits/remove_reference.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_REMOVE_REFERENCE_H\n#define _LIBCUDACXX___TYPE_TRAITS_REMOVE_REFERENCE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_REMOVE_REFERENCE_T) && !defined(_LIBCUDACXX_USE_REMOVE_REFERENCE_T_FALLBACK)\ntemplate <class _Tp>\nstruct remove_reference {\n  using type _LIBCUDACXX_NODEBUG_TYPE = _LIBCUDACXX_REMOVE_REFERENCE_T(_Tp);\n};\n\ntemplate <class _Tp>\nusing __libcpp_remove_reference_t = _LIBCUDACXX_REMOVE_REFERENCE_T(_Tp);\n\n#else\n\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS remove_reference        {typedef _LIBCUDACXX_NODEBUG_TYPE _Tp type;};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS remove_reference<_Tp&>  {typedef _LIBCUDACXX_NODEBUG_TYPE _Tp type;};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS remove_reference<_Tp&&> {typedef _LIBCUDACXX_NODEBUG_TYPE _Tp type;};\n\ntemplate <class _Tp>\nusing __libcpp_remove_reference_t = typename remove_reference<_Tp>::type;\n\n#endif // defined(_LIBCUDACXX_REMOVE_REFERENCE_T) && !defined(_LIBCUDACXX_USE_REMOVE_REFERENCE_T_FALLBACK)\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> using remove_reference_t = __libcpp_remove_reference_t<_Tp>;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_REMOVE_REFERENCE_H\n", "__type_traits/remove_volatile.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_REMOVE_VOLATILE_H\n#define _LIBCUDACXX___TYPE_TRAITS_REMOVE_VOLATILE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_REMOVE_VOLATILE) && !defined(_LIBCUDACXX_USE_REMOVE_VOLATILE_FALLBACK)\ntemplate <class _Tp>\nstruct remove_volatile {\n  using type _LIBCUDACXX_NODEBUG_TYPE = _LIBCUDACXX_REMOVE_VOLATILE(_Tp);\n};\n\ntemplate <class _Tp>\nusing __remove_volatile_t = _LIBCUDACXX_REMOVE_VOLATILE(_Tp);\n\n#else\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS remove_volatile               {typedef _Tp type;};\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS remove_volatile<volatile _Tp> {typedef _Tp type;};\n\ntemplate <class _Tp>\nusing __remove_volatile_t = typename remove_volatile<_Tp>::type;\n\n#endif // defined(_LIBCUDACXX_REMOVE_VOLATILE) && !defined(_LIBCUDACXX_USE_REMOVE_VOLATILE_FALLBACK)\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> using remove_volatile_t = __remove_volatile_t<_Tp>;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_REMOVE_VOLATILE_H\n", "__type_traits/result_of.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_RESULT_OF_H\n#define _LIBCUDACXX___TYPE_TRAITS_RESULT_OF_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__functional/invoke.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n// result_of\n\n#if _LIBCUDACXX_STD_VER <= 17 || defined(_LIBCUDACXX_ENABLE_CXX20_REMOVED_TYPE_TRAITS)\ntemplate <class _Callable> class _LIBCUDACXX_DEPRECATED_IN_CXX17 result_of;\n\ntemplate <class _Fp, class ..._Args>\nclass _LIBCUDACXX_TEMPLATE_VIS result_of<_Fp(_Args...)>\n    : public __invoke_of<_Fp, _Args...>\n{\n};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> using result_of_t _LIBCUDACXX_DEPRECATED_IN_CXX17 = typename result_of<_Tp>::type;\n#endif // _LIBCUDACXX_STD_VER > 11\n#endif // _LIBCUDACXX_STD_VER <= 17 || defined(_LIBCUDACXX_ENABLE_CXX20_REMOVED_TYPE_TRAITS)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_RESULT_OF_H\n", "__type_traits/type_identity.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_TYPE_IDENTITY_H\n#define _LIBCUDACXX___TYPE_TRAITS_TYPE_IDENTITY_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp>\nstruct __type_identity { typedef _Tp type; };\n\ntemplate <class _Tp>\nusing __type_identity_t _LIBCUDACXX_NODEBUG_TYPE = typename __type_identity<_Tp>::type;\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate<class _Tp> struct type_identity { typedef _Tp type; };\ntemplate<class _Tp> using type_identity_t = typename type_identity<_Tp>::type;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_TYPE_IDENTITY_H\n", "__type_traits/type_list.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_TYPE_LIST_H\n#define _LIBCUDACXX___TYPE_TRAITS_TYPE_LIST_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Hp, class _Tp>\nstruct __type_list\n{\n    typedef _Hp _Head;\n    typedef _Tp _Tail;\n};\n\ntemplate <class _TypeList, size_t _Size, bool = _Size <= sizeof(typename _TypeList::_Head)> struct __find_first;\n\ntemplate <class _Hp, class _Tp, size_t _Size>\nstruct __find_first<__type_list<_Hp, _Tp>, _Size, true>\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE _Hp type;\n};\n\ntemplate <class _Hp, class _Tp, size_t _Size>\nstruct __find_first<__type_list<_Hp, _Tp>, _Size, false>\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE typename __find_first<_Tp, _Size>::type type;\n};\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_TYPE_LIST_H\n", "__type_traits/underlying_type.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_UNDERLYING_TYPE_H\n#define _LIBCUDACXX___TYPE_TRAITS_UNDERLYING_TYPE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/is_enum.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_UNDERLYING_TYPE) && !defined(_LIBCUDACXX_USE_UNDERLYING_TYPE_FALLBACK)\n\ntemplate <class _Tp, bool = is_enum<_Tp>::value> struct __underlying_type_impl;\n\ntemplate <class _Tp>\nstruct __underlying_type_impl<_Tp, false> {};\n\ntemplate <class _Tp>\nstruct __underlying_type_impl<_Tp, true>\n{\n    typedef _LIBCUDACXX_UNDERLYING_TYPE(_Tp) type;\n};\n\ntemplate <class _Tp>\nstruct underlying_type : __underlying_type_impl<_Tp, is_enum<_Tp>::value> {};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp> using underlying_type_t = typename underlying_type<_Tp>::type;\n#endif\n\n#else\n\ntemplate <class _Tp, bool _Support = false>\nstruct underlying_type\n{\n    static_assert(_Support, \"The underyling_type trait requires compiler \"\n                            \"support. Either no such support exists or \"\n                            \"libc++ does not know how to use it.\");\n};\n\n#endif // defined(_LIBCUDACXX_UNDERLYING_TYPE) && !defined(_LIBCUDACXX_USE_UNDERLYING_TYPE_FALLBACK)\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_UNDERLYING_TYPE_H\n", "__type_traits/void_t.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___TYPE_TRAITS_VOID_T_H\n#define _LIBCUDACXX___TYPE_TRAITS_VOID_T_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class...> using void_t = void;\n#endif\n\ntemplate <class...>\nusing __void_t = void;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___TYPE_TRAITS_VOID_T_H\n", "__utility/as_const.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___UTILITY_AS_CONST_H\n#define _LIBCUDACXX___UTILITY_AS_CONST_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/add_const.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _Tp>\n_LIBCUDACXX_NODISCARD_EXT _LIBCUDACXX_INLINE_VISIBILITY constexpr add_const_t<_Tp>& as_const(_Tp& __t) noexcept { return __t; }\n\ntemplate <class _Tp>\nvoid as_const(const _Tp&&) = delete;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___UTILITY_AS_CONST_H\n", "__utility/auto_cast.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___UTILITY_AUTO_CAST_H\n#define _LIBCUDACXX___UTILITY_AUTO_CAST_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/decay.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n#define _LIBCUDACXX_AUTO_CAST(expr) static_cast<_CUDA_VSTD::decay_t<decltype((expr))>>(expr)\n\n#endif // _LIBCUDACXX___UTILITY_AUTO_CAST_H\n", "__utility/cmp.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___UTILITY_CMP_H\n#define _LIBCUDACXX___UTILITY_CMP_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__utility/forward.h\"\n#include \"../__utility/move.h\"\n#include \"../__type_traits/disjunction.h\"\n#include \"../__type_traits/is_integral.h\"\n#include \"../__type_traits/is_same.h\"\n#include \"../__type_traits/is_signed.h\"\n#include \"../__type_traits/make_unsigned.h\"\n#include \"../limits\"\n\n#ifndef __cuda_std__\n#include <__pragma_push>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 17\ntemplate<class _Tp, class... _Up>\nstruct _IsSameAsAny : _Or<_IsSame<_Tp, _Up>...> {};\n\ntemplate<class _Tp>\nconcept __is_safe_integral_cmp = is_integral_v<_Tp> &&\n                      !_IsSameAsAny<_Tp, bool, char, char16_t, char32_t\n#ifndef _LIBCUDACXX_NO_HAS_CHAR8_T\n                                    , char8_t\n#endif\n#ifndef _LIBCUDACXX_HAS_NO_WIDE_CHARACTERS\n                                    , wchar_t\n#endif\n                                    >::value;\n\ntemplate<__is_safe_integral_cmp _Tp, __is_safe_integral_cmp _Up>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr\nbool cmp_equal(_Tp __t, _Up __u) noexcept\n{\n  if constexpr (is_signed_v<_Tp> == is_signed_v<_Up>)\n    return __t == __u;\n  else if constexpr (is_signed_v<_Tp>)\n    return __t < 0 ? false : make_unsigned_t<_Tp>(__t) == __u;\n  else\n    return __u < 0 ? false : __t == make_unsigned_t<_Up>(__u);\n}\n\ntemplate<__is_safe_integral_cmp _Tp, __is_safe_integral_cmp _Up>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr\nbool cmp_not_equal(_Tp __t, _Up __u) noexcept\n{\n  return !_CUDA_VSTD::cmp_equal(__t, __u);\n}\n\ntemplate<__is_safe_integral_cmp _Tp, __is_safe_integral_cmp _Up>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr\nbool cmp_less(_Tp __t, _Up __u) noexcept\n{\n  if constexpr (is_signed_v<_Tp> == is_signed_v<_Up>)\n    return __t < __u;\n  else if constexpr (is_signed_v<_Tp>)\n    return __t < 0 ? true : make_unsigned_t<_Tp>(__t) < __u;\n  else\n    return __u < 0 ? false : __t < make_unsigned_t<_Up>(__u);\n}\n\ntemplate<__is_safe_integral_cmp _Tp, __is_safe_integral_cmp _Up>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr\nbool cmp_greater(_Tp __t, _Up __u) noexcept\n{\n  return _CUDA_VSTD::cmp_less(__u, __t);\n}\n\ntemplate<__is_safe_integral_cmp _Tp, __is_safe_integral_cmp _Up>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr\nbool cmp_less_equal(_Tp __t, _Up __u) noexcept\n{\n  return !_CUDA_VSTD::cmp_greater(__t, __u);\n}\n\ntemplate<__is_safe_integral_cmp _Tp, __is_safe_integral_cmp _Up>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr\nbool cmp_greater_equal(_Tp __t, _Up __u) noexcept\n{\n  return !_CUDA_VSTD::cmp_less(__t, __u);\n}\n\ntemplate<__is_safe_integral_cmp _Tp, __is_safe_integral_cmp _Up>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr\nbool in_range(_Up __u) noexcept\n{\n  return _CUDA_VSTD::cmp_less_equal(__u, numeric_limits<_Tp>::max()) &&\n         _CUDA_VSTD::cmp_greater_equal(__u, numeric_limits<_Tp>::min());\n}\n#endif // _LIBCUDACXX_STD_VER > 17\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#ifndef __cuda_std__\n#include <__pragma_pop>\n#endif // __cuda_std__\n\n#endif // _LIBCUDACXX___UTILITY_CMP_H\n", "__utility/convert_to_integral.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___UTILITY_CONVERT_TO_INTEGRAL_H\n#define _LIBCUDACXX___UTILITY_CONVERT_TO_INTEGRAL_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/enable_if.h\"\n#include \"../__type_traits/is_enum.h\"\n#include \"../__type_traits/is_floating_point.h\"\n#include \"../__type_traits/underlying_type.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ninline _LIBCUDACXX_INLINE_VISIBILITY constexpr\nint __convert_to_integral(int __val) { return __val; }\n\ninline _LIBCUDACXX_INLINE_VISIBILITY constexpr\nunsigned __convert_to_integral(unsigned __val) { return __val; }\n\ninline _LIBCUDACXX_INLINE_VISIBILITY constexpr\nlong __convert_to_integral(long __val) { return __val; }\n\ninline _LIBCUDACXX_INLINE_VISIBILITY constexpr\nunsigned long __convert_to_integral(unsigned long __val) { return __val; }\n\ninline _LIBCUDACXX_INLINE_VISIBILITY constexpr\nlong long __convert_to_integral(long long __val) { return __val; }\n\ninline _LIBCUDACXX_INLINE_VISIBILITY constexpr\nunsigned long long __convert_to_integral(unsigned long long __val) {return __val; }\n\ntemplate<typename _Fp>\ninline _LIBCUDACXX_INLINE_VISIBILITY constexpr\n__enable_if_t<is_floating_point<_Fp>::value, long long>\n __convert_to_integral(_Fp __val) { return __val; }\n\n#ifndef _LIBCUDACXX_HAS_NO_INT128\ninline _LIBCUDACXX_INLINE_VISIBILITY constexpr\n__int128_t __convert_to_integral(__int128_t __val) { return __val; }\n\ninline _LIBCUDACXX_INLINE_VISIBILITY constexpr\n__uint128_t __convert_to_integral(__uint128_t __val) { return __val; }\n#endif\n\ntemplate <class _Tp, bool = is_enum<_Tp>::value>\nstruct __sfinae_underlying_type\n{\n    typedef typename underlying_type<_Tp>::type type;\n    typedef decltype(((type)1) + 0) __promoted_type;\n};\n\ntemplate <class _Tp>\nstruct __sfinae_underlying_type<_Tp, false> {};\n\ntemplate <class _Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY constexpr\ntypename __sfinae_underlying_type<_Tp>::__promoted_type\n__convert_to_integral(_Tp __val) { return __val; }\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___UTILITY_CONVERT_TO_INTEGRAL_H\n", "__utility/declval.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___UTILITY_DECLVAL_H\n#define _LIBCUDACXX___UTILITY_DECLVAL_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n// Suppress deprecation notice for volatile-qualified return type resulting\n// from volatile-qualified types _Tp.\n_LIBCUDACXX_SUPPRESS_DEPRECATED_PUSH\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY _Tp&& __declval(int);\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY _Tp __declval(long);\n_LIBCUDACXX_SUPPRESS_DEPRECATED_POP\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY decltype(_CUDA_VSTD::__declval<_Tp>(0)) declval() noexcept;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___UTILITY_DECLVAL_H\n", "__utility/exchange.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___UTILITY_EXCHANGE_H\n#define _LIBCUDACXX___UTILITY_EXCHANGE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/is_nothrow_assignable.h\"\n#include \"../__type_traits/is_nothrow_move_constructible.h\"\n#include \"../__utility/forward.h\"\n#include \"../__utility/move.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate<class _T1, class _T2 = _T1>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n_T1 exchange(_T1& __obj, _T2&& __new_value)\n    noexcept(is_nothrow_move_constructible<_T1>::value && is_nothrow_assignable<_T1&, _T2>::value)\n{\n    _T1 __old_value = _CUDA_VSTD::move(__obj);\n    __obj = _CUDA_VSTD::forward<_T2>(__new_value);\n    return __old_value;\n}\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___UTILITY_EXCHANGE_H\n", "__utility/forward.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___UTILITY_FORWARD_H\n#define _LIBCUDACXX___UTILITY_FORWARD_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/is_reference.h\"\n#include \"../__type_traits/remove_reference.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp>\n_LIBCUDACXX_NODISCARD_EXT inline _LIBCUDACXX_INLINE_VISIBILITY constexpr _Tp&&\nforward(__libcpp_remove_reference_t<_Tp>& __t) noexcept {\n  return static_cast<_Tp&&>(__t);\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_NODISCARD_EXT inline _LIBCUDACXX_INLINE_VISIBILITY constexpr _Tp&&\nforward(__libcpp_remove_reference_t<_Tp>&& __t) noexcept {\n  static_assert(!is_lvalue_reference<_Tp>::value, \"cannot forward an rvalue as an lvalue\");\n  return static_cast<_Tp&&>(__t);\n}\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___UTILITY_FORWARD_H\n", "__utility/forward_like.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___UTILITY_FORWARD_LIKE_H\n#define _LIBCUDACXX___UTILITY_FORWARD_LIKE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/conditional.h\"\n#include \"../__type_traits/is_const.h\"\n#include \"../__type_traits/is_reference.h\"\n#include \"../__type_traits/remove_reference.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 20\n\ntemplate <class _Ap, class _Bp>\nusing _CopyConst = _If<is_const_v<_Ap>, const _Bp, _Bp>;\n\ntemplate <class _Ap, class _Bp>\nusing _OverrideRef = _If<is_rvalue_reference_v<_Ap>, remove_reference_t<_Bp>&&, _Bp&>;\n\ntemplate <class _Ap, class _Bp>\nusing _ForwardLike = _OverrideRef<_Ap&&, _CopyConst<remove_reference_t<_Ap>, remove_reference_t<_Bp>>>;\n\ntemplate <class _Tp, class _Up>\n_LIBCUDACXX_NODISCARD_ATTRIBUTE _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr auto forward_like(_Up&& __ux) noexcept -> _ForwardLike<_Tp, _Up> {\n  return static_cast<_ForwardLike<_Tp, _Up>>(__ux);\n}\n\n#endif // _LIBCUDACXX_STD_VER > 20\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___UTILITY_FORWARD_LIKE_H\n", "__utility/in_place.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___UTILITY_IN_PLACE_H\n#define _LIBCUDACXX___UTILITY_IN_PLACE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/is_reference.h\"\n#include \"../__type_traits/remove_reference.h\"\n#include \"../__type_traits/remove_cvref.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 11\n\nstruct _LIBCUDACXX_TYPE_VIS in_place_t {\n    explicit in_place_t() = default;\n};\n_LIBCUDACXX_CPO_ACCESSIBILITY in_place_t in_place{};\n\ntemplate <class _Tp>\nstruct _LIBCUDACXX_TEMPLATE_VIS in_place_type_t {\n    explicit in_place_type_t() = default;\n};\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr in_place_type_t<_Tp> in_place_type{};\n\ntemplate <size_t _Idx>\nstruct _LIBCUDACXX_TEMPLATE_VIS in_place_index_t {\n    explicit in_place_index_t() = default;\n};\ntemplate <size_t _Idx>\n_LIBCUDACXX_INLINE_VAR constexpr in_place_index_t<_Idx> in_place_index{};\n\ntemplate <class _Tp> struct __is_inplace_type_imp : false_type {};\ntemplate <class _Tp> struct __is_inplace_type_imp<in_place_type_t<_Tp>> : true_type {};\n\ntemplate <class _Tp>\nusing __is_inplace_type = __is_inplace_type_imp<__remove_cvref_t<_Tp>>;\n\ntemplate <class _Tp> struct __is_inplace_index_imp : false_type {};\ntemplate <size_t _Idx> struct __is_inplace_index_imp<in_place_index_t<_Idx>> : true_type {};\n\ntemplate <class _Tp>\nusing __is_inplace_index = __is_inplace_index_imp<__remove_cvref_t<_Tp>>;\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___UTILITY_IN_PLACE_H\n", "__utility/integer_sequence.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___UTILITY_INTEGER_SEQUENCE_H\n#define _LIBCUDACXX___UTILITY_INTEGER_SEQUENCE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/is_integral.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <size_t...> struct __tuple_indices {};\n\ntemplate <class _IdxType, _IdxType... _Values>\nstruct __integer_sequence {\n  template <template <class _OIdxType, _OIdxType...> class _ToIndexSeq, class _ToIndexType>\n  using __convert = _ToIndexSeq<_ToIndexType, _Values...>;\n\n  template <size_t _Sp>\n  using __to_tuple_indices = __tuple_indices<(_Values + _Sp)...>;\n};\n\n#if !__has_builtin(__make_integer_seq) || defined(_LIBCUDACXX_TESTING_FALLBACK_MAKE_INTEGER_SEQUENCE)\n\nnamespace __detail {\n\ntemplate<typename _Tp, size_t ..._Extra> struct __repeat;\ntemplate<typename _Tp, _Tp ..._Np, size_t ..._Extra> struct __repeat<__integer_sequence<_Tp, _Np...>, _Extra...> {\n  typedef _LIBCUDACXX_NODEBUG_TYPE __integer_sequence<_Tp,\n                           _Np...,\n                           sizeof...(_Np) + _Np...,\n                           2 * sizeof...(_Np) + _Np...,\n                           3 * sizeof...(_Np) + _Np...,\n                           4 * sizeof...(_Np) + _Np...,\n                           5 * sizeof...(_Np) + _Np...,\n                           6 * sizeof...(_Np) + _Np...,\n                           7 * sizeof...(_Np) + _Np...,\n                           _Extra...> type;\n};\n\ntemplate<size_t _Np> struct __parity;\ntemplate<size_t _Np> struct __make : __parity<_Np % 8>::template __pmake<_Np> {};\n\ntemplate<> struct __make<0> { typedef __integer_sequence<size_t> type; };\ntemplate<> struct __make<1> { typedef __integer_sequence<size_t, 0> type; };\ntemplate<> struct __make<2> { typedef __integer_sequence<size_t, 0, 1> type; };\ntemplate<> struct __make<3> { typedef __integer_sequence<size_t, 0, 1, 2> type; };\ntemplate<> struct __make<4> { typedef __integer_sequence<size_t, 0, 1, 2, 3> type; };\ntemplate<> struct __make<5> { typedef __integer_sequence<size_t, 0, 1, 2, 3, 4> type; };\ntemplate<> struct __make<6> { typedef __integer_sequence<size_t, 0, 1, 2, 3, 4, 5> type; };\ntemplate<> struct __make<7> { typedef __integer_sequence<size_t, 0, 1, 2, 3, 4, 5, 6> type; };\n\ntemplate<> struct __parity<0> { template<size_t _Np> struct __pmake : __repeat<typename __make<_Np / 8>::type> {}; };\ntemplate<> struct __parity<1> { template<size_t _Np> struct __pmake : __repeat<typename __make<_Np / 8>::type, _Np - 1> {}; };\ntemplate<> struct __parity<2> { template<size_t _Np> struct __pmake : __repeat<typename __make<_Np / 8>::type, _Np - 2, _Np - 1> {}; };\ntemplate<> struct __parity<3> { template<size_t _Np> struct __pmake : __repeat<typename __make<_Np / 8>::type, _Np - 3, _Np - 2, _Np - 1> {}; };\ntemplate<> struct __parity<4> { template<size_t _Np> struct __pmake : __repeat<typename __make<_Np / 8>::type, _Np - 4, _Np - 3, _Np - 2, _Np - 1> {}; };\ntemplate<> struct __parity<5> { template<size_t _Np> struct __pmake : __repeat<typename __make<_Np / 8>::type, _Np - 5, _Np - 4, _Np - 3, _Np - 2, _Np - 1> {}; };\ntemplate<> struct __parity<6> { template<size_t _Np> struct __pmake : __repeat<typename __make<_Np / 8>::type, _Np - 6, _Np - 5, _Np - 4, _Np - 3, _Np - 2, _Np - 1> {}; };\ntemplate<> struct __parity<7> { template<size_t _Np> struct __pmake : __repeat<typename __make<_Np / 8>::type, _Np - 7, _Np - 6, _Np - 5, _Np - 4, _Np - 3, _Np - 2, _Np - 1> {}; };\n\n} // namespace detail\n\n#endif  // !__has_builtin(__make_integer_seq) || defined(_LIBCUDACXX_TESTING_FALLBACK_MAKE_INTEGER_SEQUENCE)\n\n#if __has_builtin(__make_integer_seq)\ntemplate <size_t _Ep, size_t _Sp>\nusing __make_indices_imp =\n    typename __make_integer_seq<__integer_sequence, size_t, _Ep - _Sp>::template\n    __to_tuple_indices<_Sp>;\n#else\ntemplate <size_t _Ep, size_t _Sp>\nusing __make_indices_imp =\n    typename __detail::__make<_Ep - _Sp>::type::template __to_tuple_indices<_Sp>;\n\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\n\ntemplate<class _Tp, _Tp... _Ip>\nstruct _LIBCUDACXX_TEMPLATE_VIS integer_sequence\n{\n    typedef _Tp value_type;\n    static_assert( is_integral<_Tp>::value,\n                  \"std::integer_sequence can only be instantiated with an integral type\" );\n    static\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr\n    size_t\n    size() noexcept { return sizeof...(_Ip); }\n};\n\ntemplate<size_t... _Ip>\n    using index_sequence = integer_sequence<size_t, _Ip...>;\n\n#if __has_builtin(__make_integer_seq) && !defined(_LIBCUDACXX_TESTING_FALLBACK_MAKE_INTEGER_SEQUENCE)\n\ntemplate <class _Tp, _Tp _Ep>\nusing __make_integer_sequence _LIBCUDACXX_NODEBUG_TYPE = __make_integer_seq<integer_sequence, _Tp, _Ep>;\n\n#else\n\ntemplate<typename _Tp, _Tp _Np> using __make_integer_sequence_unchecked _LIBCUDACXX_NODEBUG_TYPE =\n  typename __detail::__make<_Np>::type::template __convert<integer_sequence, _Tp>;\n\ntemplate <class _Tp, _Tp _Ep>\nstruct __make_integer_sequence_checked\n{\n    static_assert(is_integral<_Tp>::value,\n                  \"std::make_integer_sequence can only be instantiated with an integral type\" );\n    static_assert(0 <= _Ep, \"std::make_integer_sequence must have a non-negative sequence length\");\n    // Workaround GCC bug by preventing bad installations when 0 <= _Ep\n    // https://gcc.gnu.org/bugzilla/show_bug.cgi?id=68929\n    typedef _LIBCUDACXX_NODEBUG_TYPE __make_integer_sequence_unchecked<_Tp, 0 <= _Ep ? _Ep : 0> type;\n};\n\ntemplate <class _Tp, _Tp _Ep>\nusing __make_integer_sequence _LIBCUDACXX_NODEBUG_TYPE = typename __make_integer_sequence_checked<_Tp, _Ep>::type;\n\n#endif\n\ntemplate<class _Tp, _Tp _Np>\n    using make_integer_sequence = __make_integer_sequence<_Tp, _Np>;\n\ntemplate<size_t _Np>\n    using make_index_sequence = make_integer_sequence<size_t, _Np>;\n\ntemplate<class... _Tp>\n    using index_sequence_for = make_index_sequence<sizeof...(_Tp)>;\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___UTILITY_INTEGER_SEQUENCE_H\n", "__utility/move.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___UTILITY_MOVE_H\n#define _LIBCUDACXX___UTILITY_MOVE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/conditional.h\"\n#include \"../__type_traits/is_copy_constructible.h\"\n#include \"../__type_traits/is_nothrow_move_constructible.h\"\n#include \"../__type_traits/remove_reference.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp>\n_LIBCUDACXX_NODISCARD_EXT inline _LIBCUDACXX_INLINE_VISIBILITY constexpr __libcpp_remove_reference_t<_Tp>&&\nmove(_Tp&& __t) noexcept {\n  typedef _LIBCUDACXX_NODEBUG_TYPE __libcpp_remove_reference_t<_Tp> _Up;\n  return static_cast<_Up&&>(__t);\n}\n\ntemplate <class _Tp>\nusing __move_if_noexcept_result_t =\n    __conditional_t<!is_nothrow_move_constructible<_Tp>::value && is_copy_constructible<_Tp>::value, const _Tp&, _Tp&&>;\n\ntemplate <class _Tp>\n_LIBCUDACXX_NODISCARD_EXT inline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 __move_if_noexcept_result_t<_Tp>\nmove_if_noexcept(_Tp& __x) noexcept {\n  return _CUDA_VSTD::move(__x);\n}\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___UTILITY_MOVE_H\n", "__utility/pair.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___UTILITY_PAIR_H\n#define _LIBCUDACXX___UTILITY_PAIR_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#ifndef _LIBCUDACXX_HAS_NO_SPACESHIP_OPERATOR\n#include \"../__compare/common_comparison_category.h\"\n#include \"../__compare/synth_three_way.h\"\n#endif // _LIBCUDACXX_HAS_NO_SPACESHIP_OPERATOR\n\n#include \"../__functional/unwrap_ref.h\"\n#include \"../__fwd/get.h\"\n#include \"../__fwd/tuple.h\"\n#include \"../__tuple_dir/sfinae_helpers.h\"\n#include \"../__tuple_dir/structured_bindings.h\"\n#include \"../__tuple_dir/tuple_element.h\"\n#include \"../__tuple_dir/tuple_indices.h\"\n#include \"../__tuple_dir/tuple_size.h\"\n#include \"../__type_traits/common_reference.h\"\n#include \"../__type_traits/conditional.h\"\n#include \"../__type_traits/decay.h\"\n#include \"../__type_traits/enable_if.h\"\n#include \"../__type_traits/integral_constant.h\"\n#include \"../__type_traits/is_assignable.h\"\n#include \"../__type_traits/is_constructible.h\"\n#include \"../__type_traits/is_convertible.h\"\n#include \"../__type_traits/is_copy_assignable.h\"\n#include \"../__type_traits/is_default_constructible.h\"\n#include \"../__type_traits/is_implicitly_default_constructible.h\"\n#include \"../__type_traits/is_move_assignable.h\"\n#include \"../__type_traits/is_nothrow_assignable.h\"\n#include \"../__type_traits/is_nothrow_constructible.h\"\n#include \"../__type_traits/is_nothrow_copy_assignable.h\"\n#include \"../__type_traits/is_nothrow_copy_constructible.h\"\n#include \"../__type_traits/is_nothrow_default_constructible.h\"\n#include \"../__type_traits/is_nothrow_move_assignable.h\"\n#include \"../__type_traits/is_nothrow_move_constructible.h\"\n#include \"../__type_traits/is_same.h\"\n#include \"../__type_traits/is_swappable.h\"\n#include \"../__type_traits/make_const_lvalue_ref.h\"\n#include \"../__utility/forward.h\"\n#include \"../__utility/move.h\"\n#include \"../__utility/piecewise_construct.h\"\n#include \"../cstddef\"\n\n// Provide compatability between `std::pair` and `cuda::std::pair`\n#if defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n#include <utility>\n#endif // defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n#if defined(_LIBCUDACXX_DEPRECATED_ABI_DISABLE_PAIR_TRIVIAL_COPY_CTOR)\ntemplate <class, class>\nstruct __non_trivially_copyable_base {\n  _LIBCUDACXX_INLINE_VISIBILITY constexpr\n  __non_trivially_copyable_base() noexcept {}\n  _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n  __non_trivially_copyable_base(__non_trivially_copyable_base const&) noexcept {}\n};\n#endif\n\ntemplate <class _T1, class _T2>\nstruct _LIBCUDACXX_TEMPLATE_VIS pair\n#if defined(_LIBCUDACXX_DEPRECATED_ABI_DISABLE_PAIR_TRIVIAL_COPY_CTOR)\n: private __non_trivially_copyable_base<_T1, _T2>\n#endif\n{\n    typedef _T1 first_type;\n    typedef _T2 second_type;\n\n    _T1 first;\n    _T2 second;\n\n    pair(pair const&) = default;\n    pair(pair&&) = default;\n\n    struct _CheckArgs {\n      struct __enable_implicit_default : public integral_constant<bool,\n                 __is_implicitly_default_constructible<_T1>::value\n              && __is_implicitly_default_constructible<_T2>::value>\n      {};\n\n      struct __enable_explicit_default : public integral_constant<bool,\n                 is_default_constructible<_T1>::value\n              && is_default_constructible<_T2>::value\n              && !__enable_implicit_default::value>\n      {};\n\n      template <class _U1, class _U2>\n      struct __is_pair_constructible : public integral_constant<bool,\n                 is_constructible<first_type, _U1>::value\n              && is_constructible<second_type, _U2>::value>\n      {};\n\n      template <class _U1, class _U2>\n      struct __is_implicit : public integral_constant<bool,\n                 is_convertible<_U1, first_type>::value\n              && is_convertible<_U2, second_type>::value>\n      {};\n\n      template <class _U1, class _U2>\n      struct __enable_explicit : public integral_constant<bool,\n                 __is_pair_constructible<_U1, _U2>::value\n             && !__is_implicit<_U1, _U2>::value>\n      {};\n\n      template <class _U1, class _U2>\n      struct __enable_implicit : public integral_constant<bool,\n                 __is_pair_constructible<_U1, _U2>::value\n             &&  __is_implicit<_U1, _U2>::value>\n      {};\n    };\n\n    template <bool _MaybeEnable>\n    using _CheckArgsDep _LIBCUDACXX_NODEBUG_TYPE = __conditional_t<\n      _MaybeEnable, _CheckArgs, __check_tuple_constructor_fail>;\n\n    struct _CheckTupleLikeConstructor {\n        template <class _Tuple>\n        struct __enable_implicit : public integral_constant<bool,\n                    __tuple_convertible<_Tuple, pair>::value>\n        {};\n        template <class _Tuple>\n        struct __enable_explicit : public integral_constant<bool,\n                    __tuple_constructible<_Tuple, pair>::value\n                && !__tuple_convertible<_Tuple, pair>::value>\n        {};\n        template <class _Tuple>\n        struct __enable_assign : public integral_constant<bool,\n                    __tuple_assignable<_Tuple, pair>::value>\n        {};\n    };\n\n    template <class _Tuple>\n    using _CheckTLC _LIBCUDACXX_NODEBUG_TYPE = __conditional_t<\n        __tuple_like_with_size<_Tuple, 2>::value\n            && !is_same<__decay_t<_Tuple>, pair>::value,\n        _CheckTupleLikeConstructor,\n        __check_tuple_constructor_fail\n    >;\n\n    template<bool _Dummy = true, __enable_if_t<\n            _CheckArgsDep<_Dummy>::__enable_explicit_default::value\n    >* = nullptr>\n    explicit _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    pair() noexcept(is_nothrow_default_constructible<first_type>::value &&\n                      is_nothrow_default_constructible<second_type>::value)\n        : first(), second() {}\n\n    template<bool _Dummy = true, __enable_if_t<\n            _CheckArgsDep<_Dummy>::__enable_implicit_default::value\n    >* = nullptr>\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    pair() noexcept(is_nothrow_default_constructible<first_type>::value &&\n                      is_nothrow_default_constructible<second_type>::value)\n        : first(), second() {}\n\n    template <bool _Dummy = true, __enable_if_t<\n             _CheckArgsDep<_Dummy>::template __enable_explicit<__make_const_lvalue_ref<_T1>, __make_const_lvalue_ref<_T2>>::value\n    >* = nullptr>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    explicit pair(_T1 const& __t1, _T2 const& __t2)\n        noexcept(is_nothrow_copy_constructible<first_type>::value &&\n                   is_nothrow_copy_constructible<second_type>::value)\n        : first(__t1), second(__t2) {}\n\n    template<bool _Dummy = true, __enable_if_t<\n            _CheckArgsDep<_Dummy>::template __enable_implicit<__make_const_lvalue_ref<_T1>, __make_const_lvalue_ref<_T2>>::value\n    >* = nullptr>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    pair(_T1 const& __t1, _T2 const& __t2)\n        noexcept(is_nothrow_copy_constructible<first_type>::value &&\n                   is_nothrow_copy_constructible<second_type>::value)\n        : first(__t1), second(__t2) {}\n\n    template <\n#if _LIBCUDACXX_STD_VER > 20 // http://wg21.link/P1951\n        class _U1 = _T1, class _U2 = _T2,\n#else\n        class _U1, class _U2,\n#endif\n        __enable_if_t<_CheckArgs::template __enable_explicit<_U1, _U2>::value>* = nullptr\n    >\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    explicit pair(_U1&& __u1, _U2&& __u2)\n        noexcept((is_nothrow_constructible<first_type, _U1>::value &&\n                    is_nothrow_constructible<second_type, _U2>::value))\n        : first(_CUDA_VSTD::forward<_U1>(__u1)), second(_CUDA_VSTD::forward<_U2>(__u2)) {}\n\n    template <\n#if _LIBCUDACXX_STD_VER > 20 // http://wg21.link/P1951\n        class _U1 = _T1, class _U2 = _T2,\n#else\n        class _U1, class _U2,\n#endif\n        __enable_if_t<_CheckArgs::template __enable_implicit<_U1, _U2>::value>* = nullptr\n    >\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    pair(_U1&& __u1, _U2&& __u2)\n        noexcept((is_nothrow_constructible<first_type, _U1>::value &&\n                    is_nothrow_constructible<second_type, _U2>::value))\n        : first(_CUDA_VSTD::forward<_U1>(__u1)), second(_CUDA_VSTD::forward<_U2>(__u2)) {}\n\n#if _LIBCUDACXX_STD_VER > 20\n    template<class _U1, class _U2, __enable_if_t<\n            _CheckArgs::template __is_pair_constructible<_U1&, _U2&>::value\n    >* = nullptr>\n    _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    explicit(!_CheckArgs::template __is_implicit<_U1&, _U2&>()) pair(pair<_U1, _U2>& __p)\n        noexcept((is_nothrow_constructible<first_type, _U1&>::value &&\n                  is_nothrow_constructible<second_type, _U2&>::value))\n        : first(__p.first), second(__p.second) {}\n\n#if defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n    template<class _U1, class _U2, __enable_if_t<\n            _CheckArgs::template __is_pair_constructible<_U1&, _U2&>::value\n    >* = nullptr>\n    _LIBCUDACXX_HOST _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    explicit(!_CheckArgs::template __is_implicit<_U1&, _U2&>()) pair(::std::pair<_U1, _U2>& __p)\n        noexcept((is_nothrow_constructible<first_type, _U1&>::value &&\n                  is_nothrow_constructible<second_type, _U2&>::value))\n        : first(__p.first), second(__p.second) {}\n#endif // defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n#endif // _LIBCUDACXX_STD_VER > 20\n\n    template<class _U1, class _U2, __enable_if_t<\n            _CheckArgs::template __enable_explicit<_U1 const&, _U2 const&>::value\n    >* = nullptr>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    explicit pair(pair<_U1, _U2> const& __p)\n        noexcept((is_nothrow_constructible<first_type, _U1 const&>::value &&\n                    is_nothrow_constructible<second_type, _U2 const&>::value))\n        : first(__p.first), second(__p.second) {}\n\n#if defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n    template<class _U1, class _U2, __enable_if_t<\n            _CheckArgs::template __enable_explicit<_U1 const&, _U2 const&>::value\n    >* = nullptr>\n    _LIBCUDACXX_HOST _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    explicit pair(::std::pair<_U1, _U2> const& __p)\n        noexcept((is_nothrow_constructible<first_type, _U1 const&>::value &&\n                    is_nothrow_constructible<second_type, _U2 const&>::value))\n        : first(__p.first), second(__p.second) {}\n#endif // defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n\n    template<class _U1, class _U2, __enable_if_t<\n            _CheckArgs::template __enable_implicit<_U1 const&, _U2 const&>::value\n    >* = nullptr>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    pair(pair<_U1, _U2> const& __p)\n        noexcept((is_nothrow_constructible<first_type, _U1 const&>::value &&\n                    is_nothrow_constructible<second_type, _U2 const&>::value))\n        : first(__p.first), second(__p.second) {}\n\n#if defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n    template<class _U1, class _U2, __enable_if_t<\n            _CheckArgs::template __enable_implicit<_U1 const&, _U2 const&>::value\n    >* = nullptr>\n    _LIBCUDACXX_HOST _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    pair(::std::pair<_U1, _U2> const& __p)\n        noexcept((is_nothrow_constructible<first_type, _U1 const&>::value &&\n                    is_nothrow_constructible<second_type, _U2 const&>::value))\n        : first(__p.first), second(__p.second) {}\n#endif // defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n\n    template<class _U1, class _U2, __enable_if_t<\n            _CheckArgs::template __enable_explicit<_U1, _U2>::value\n    >* = nullptr>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    explicit pair(pair<_U1, _U2>&&__p)\n        noexcept((is_nothrow_constructible<first_type, _U1&&>::value &&\n                    is_nothrow_constructible<second_type, _U2&&>::value))\n        : first(_CUDA_VSTD::forward<_U1>(__p.first)), second(_CUDA_VSTD::forward<_U2>(__p.second)) {}\n\n#if defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n    template<class _U1, class _U2, __enable_if_t<\n            _CheckArgs::template __enable_explicit<_U1, _U2>::value\n    >* = nullptr>\n    _LIBCUDACXX_HOST _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    explicit pair(::std::pair<_U1, _U2>&&__p)\n        noexcept((is_nothrow_constructible<first_type, _U1&&>::value &&\n                    is_nothrow_constructible<second_type, _U2&&>::value))\n        : first(_CUDA_VSTD::forward<_U1>(__p.first)), second(_CUDA_VSTD::forward<_U2>(__p.second)) {}\n#endif // defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n\n    template<class _U1, class _U2, __enable_if_t<\n            _CheckArgs::template __enable_implicit<_U1, _U2>::value\n    >* = nullptr>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    pair(pair<_U1, _U2>&& __p)\n        noexcept((is_nothrow_constructible<first_type, _U1&&>::value &&\n                    is_nothrow_constructible<second_type, _U2&&>::value))\n        : first(_CUDA_VSTD::forward<_U1>(__p.first)), second(_CUDA_VSTD::forward<_U2>(__p.second)) {}\n\n#if defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n    template<class _U1, class _U2, __enable_if_t<\n            _CheckArgs::template __enable_implicit<_U1, _U2>::value\n    >* = nullptr>\n    _LIBCUDACXX_HOST _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    pair(::std::pair<_U1, _U2>&& __p)\n        noexcept((is_nothrow_constructible<first_type, _U1&&>::value &&\n                    is_nothrow_constructible<second_type, _U2&&>::value))\n        : first(_CUDA_VSTD::forward<_U1>(__p.first)), second(_CUDA_VSTD::forward<_U2>(__p.second)) {}\n#endif // defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n\n#if _LIBCUDACXX_STD_VER > 20\n    template<class _U1, class _U2, __enable_if_t<\n            _CheckArgs::template __is_pair_constructible<const _U1&&, const _U2&&>::value\n    >* = nullptr>\n    _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    explicit(!_CheckArgs::template __is_implicit<const _U1&&, const _U2&&>::value)\n    pair(const pair<_U1, _U2>&& __p)\n        noexcept(is_nothrow_constructible<first_type, const _U1&&>::value &&\n                 is_nothrow_constructible<second_type, const _U2&&>::value)\n        : first(_CUDA_VSTD::move(__p.first)), second(_CUDA_VSTD::move(__p.second)) {}\n\n#if defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n    template<class _U1, class _U2, __enable_if_t<\n            _CheckArgs::template __is_pair_constructible<const _U1&&, const _U2&&>::value\n    >* = nullptr>\n    _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_HOST constexpr\n    explicit(!_CheckArgs::template __is_implicit<const _U1&&, const _U2&&>::value)\n    pair(const ::std::pair<_U1, _U2>&& __p)\n        noexcept(is_nothrow_constructible<first_type, const _U1&&>::value &&\n                 is_nothrow_constructible<second_type, const _U2&&>::value)\n        : first(_CUDA_VSTD::move(__p.first)), second(_CUDA_VSTD::move(__p.second)) {}\n#endif // defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n#endif // _LIBCUDACXX_STD_VER > 20\n\n    template<class _Tuple, __enable_if_t<\n            _CheckTLC<_Tuple>::template __enable_explicit<_Tuple>::value\n    >* = nullptr>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    explicit pair(_Tuple&& __p)\n        : first(_CUDA_VSTD::get<0>(_CUDA_VSTD::forward<_Tuple>(__p))),\n          second(_CUDA_VSTD::get<1>(_CUDA_VSTD::forward<_Tuple>(__p))) {}\n\n    template<class _Tuple, __enable_if_t<\n            _CheckTLC<_Tuple>::template __enable_implicit<_Tuple>::value\n    >* = nullptr>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    pair(_Tuple&& __p)\n        : first(_CUDA_VSTD::get<0>(_CUDA_VSTD::forward<_Tuple>(__p))),\n          second(_CUDA_VSTD::get<1>(_CUDA_VSTD::forward<_Tuple>(__p))) {}\n\n    template <class... _Args1, class... _Args2>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n    pair(piecewise_construct_t __pc,\n         tuple<_Args1...> __first_args, tuple<_Args2...> __second_args)\n        noexcept((is_nothrow_constructible<first_type, _Args1...>::value &&\n                    is_nothrow_constructible<second_type, _Args2...>::value))\n        : pair(__pc, __first_args, __second_args,\n                typename __make_tuple_indices<sizeof...(_Args1)>::type(),\n                typename __make_tuple_indices<sizeof...(_Args2) >::type()) {}\n\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n    pair& operator=(__conditional_t<\n                        is_copy_assignable<first_type>::value &&\n                        is_copy_assignable<second_type>::value,\n                    pair, __nat> const& __p)\n        noexcept(is_nothrow_copy_assignable<first_type>::value &&\n                   is_nothrow_copy_assignable<second_type>::value)\n    {\n        first = __p.first;\n        second = __p.second;\n        return *this;\n    }\n\n#if defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n    template<class _UT1 = _T1, __enable_if_t<is_copy_assignable<_UT1>::value &&\n                                             is_copy_assignable<_T2>::value, int> = 0>\n    _LIBCUDACXX_HOST _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n    pair& operator=(::std::pair<_T1, _T2> const& __p)\n        noexcept(is_nothrow_copy_assignable<first_type>::value &&\n                   is_nothrow_copy_assignable<second_type>::value)\n    {\n        first = __p.first;\n        second = __p.second;\n        return *this;\n    }\n#endif // defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n    pair& operator=(__conditional_t<\n                        is_move_assignable<first_type>::value &&\n                        is_move_assignable<second_type>::value,\n                    pair, __nat>&& __p)\n        noexcept(is_nothrow_move_assignable<first_type>::value &&\n                   is_nothrow_move_assignable<second_type>::value)\n    {\n        first = _CUDA_VSTD::forward<first_type>(__p.first);\n        second = _CUDA_VSTD::forward<second_type>(__p.second);\n        return *this;\n    }\n\n#if defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n    template<class _UT1 = _T1, __enable_if_t<is_move_assignable<_UT1>::value &&\n                                             is_move_assignable<_T2>::value, int> = 0>\n    _LIBCUDACXX_HOST _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n    pair& operator=(::std::pair<_T1, _T2>&& __p)\n        noexcept(is_nothrow_move_assignable<first_type>::value &&\n                   is_nothrow_move_assignable<second_type>::value)\n    {\n        first = _CUDA_VSTD::forward<first_type>(__p.first);\n        second = _CUDA_VSTD::forward<second_type>(__p.second);\n        return *this;\n    }\n#endif // defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n\n#if _LIBCUDACXX_STD_VER > 20\n    _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    const pair& operator=(pair const& __p) const\n      noexcept(is_nothrow_copy_assignable_v<const first_type> &&\n               is_nothrow_copy_assignable_v<const second_type>)\n      requires(is_copy_assignable_v<const first_type> &&\n               is_copy_assignable_v<const second_type>) {\n        first = __p.first;\n        second = __p.second;\n        return *this;\n    }\n\n#if defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n    _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_HOST constexpr\n    const pair& operator=(::std::pair<_T1, _T2> const& __p) const\n      noexcept(is_nothrow_copy_assignable_v<const first_type> &&\n               is_nothrow_copy_assignable_v<const second_type>)\n      requires(is_copy_assignable_v<const first_type> &&\n               is_copy_assignable_v<const second_type>) {\n        first = __p.first;\n        second = __p.second;\n        return *this;\n    }\n#endif // defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n\n    _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    const pair& operator=(pair&& __p) const\n      noexcept(is_nothrow_assignable_v<const first_type&, first_type> &&\n               is_nothrow_assignable_v<const second_type&, second_type>)\n      requires(is_assignable_v<const first_type&, first_type> &&\n               is_assignable_v<const second_type&, second_type>) {\n        first = _CUDA_VSTD::forward<first_type>(__p.first);\n        second = _CUDA_VSTD::forward<second_type>(__p.second);\n        return *this;\n    }\n\n#if defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n    _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_HOST constexpr\n    const pair& operator=(::std::pair<_T1, _T2>&& __p) const\n      noexcept(is_nothrow_assignable_v<const first_type&, first_type> &&\n               is_nothrow_assignable_v<const second_type&, second_type>)\n      requires(is_assignable_v<const first_type&, first_type> &&\n               is_assignable_v<const second_type&, second_type>) {\n        first = _CUDA_VSTD::forward<first_type>(__p.first);\n        second = _CUDA_VSTD::forward<second_type>(__p.second);\n        return *this;\n    }\n#endif // defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n\n    template<class _U1, class _U2>\n    _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    const pair& operator=(const pair<_U1, _U2>& __p) const\n      requires(is_assignable_v<const first_type&, const _U1&> &&\n               is_assignable_v<const second_type&, const _U2&>) {\n        first = __p.first;\n        second = __p.second;\n        return *this;\n    }\n\n#if defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n    template<class _U1, class _U2>\n    _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_HOST constexpr\n    const pair& operator=(const ::std::pair<_U1, _U2>& __p) const\n      requires(is_assignable_v<const first_type&, const _U1&> &&\n               is_assignable_v<const second_type&, const _U2&>) {\n        first = __p.first;\n        second = __p.second;\n        return *this;\n    }\n#endif // defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n\n    template<class _U1, class _U2>\n    _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    const pair& operator=(pair<_U1, _U2>&& __p) const\n      requires(is_assignable_v<const first_type&, _U1> &&\n               is_assignable_v<const second_type&, _U2>) {\n        first = _CUDA_VSTD::forward<_U1>(__p.first);\n        second = _CUDA_VSTD::forward<_U2>(__p.second);\n        return *this;\n    }\n\n#if defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n    template<class _U1, class _U2>\n    _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_HOST constexpr\n    const pair& operator=(::std::pair<_U1, _U2>&& __p) const\n      requires(is_assignable_v<const first_type&, _U1> &&\n               is_assignable_v<const second_type&, _U2>) {\n        first = _CUDA_VSTD::forward<_U1>(__p.first);\n        second = _CUDA_VSTD::forward<_U2>(__p.second);\n        return *this;\n    }\n#endif // defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n#endif // _LIBCUDACXX_STD_VER > 20\n\n    template <class _Tuple, __enable_if_t<\n            _CheckTLC<_Tuple>::template __enable_assign<_Tuple>::value\n     >* = nullptr>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n    pair& operator=(_Tuple&& __p) {\n        first = _CUDA_VSTD::get<0>(_CUDA_VSTD::forward<_Tuple>(__p));\n        second = _CUDA_VSTD::get<1>(_CUDA_VSTD::forward<_Tuple>(__p));\n        return *this;\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n    void swap(pair& __p) noexcept(__is_nothrow_swappable<first_type>::value &&\n                                    __is_nothrow_swappable<second_type>::value)\n    {\n        using _CUDA_VSTD::swap;\n        swap(first,  __p.first);\n        swap(second, __p.second);\n    }\n\n#if _LIBCUDACXX_STD_VER > 20\n    _LIBCUDACXX_HIDE_FROM_ABI constexpr\n    void swap(const pair& __p) const\n        noexcept(__is_nothrow_swappable<const first_type>::value &&\n                 __is_nothrow_swappable<const second_type>::value)\n    {\n        using _CUDA_VSTD::swap;\n        swap(first,  __p.first);\n        swap(second, __p.second);\n    }\n#endif // _LIBCUDACXX_STD_VER > 20\n\n#if defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n    _LIBCUDACXX_HOST _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    operator ::std::pair<_T1, _T2>() const { return { first, second }; }\n#endif // defined(__cuda_std__) && !defined(__CUDACC_RTC__)\n\nprivate:\n\n    template <class... _Args1, class... _Args2, size_t... _I1, size_t... _I2>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n    pair(piecewise_construct_t,\n         tuple<_Args1...>& __first_args, tuple<_Args2...>& __second_args,\n         __tuple_indices<_I1...>, __tuple_indices<_I2...>);\n};\n\n#if _LIBCUDACXX_STD_VER > 14 && !defined(_LIBCUDACXX_HAS_NO_DEDUCTION_GUIDES)\ntemplate<class _T1, class _T2>\npair(_T1, _T2) -> pair<_T1, _T2>;\n#endif // _LIBCUDACXX_STD_VER > 14 && !defined(_LIBCUDACXX_HAS_NO_DEDUCTION_GUIDES)\n\n// [pairs.spec], specialized algorithms\n\ntemplate <class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nbool\noperator==(const pair<_T1,_T2>& __x, const pair<_T1,_T2>& __y)\n{\n    return __x.first == __y.first && __x.second == __y.second;\n}\n\n#ifndef _LIBCUDACXX_HAS_NO_SPACESHIP_OPERATOR\n\ntemplate <class _T1, class _T2>\n_LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY constexpr\ncommon_comparison_category_t<\n        __synth_three_way_result<_T1>,\n        __synth_three_way_result<_T2> >\noperator<=>(const pair<_T1,_T2>& __x, const pair<_T1,_T2>& __y)\n{\n    if (auto __c = _CUDA_VSTD::__synth_three_way(__x.first, __y.first); __c != 0) {\n      return __c;\n    }\n    return _CUDA_VSTD::__synth_three_way(__x.second, __y.second);\n}\n\n#else // _LIBCUDACXX_HAS_NO_SPACESHIP_OPERATOR\n\ntemplate <class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nbool\noperator!=(const pair<_T1,_T2>& __x, const pair<_T1,_T2>& __y)\n{\n    return !(__x == __y);\n}\n\ntemplate <class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nbool\noperator< (const pair<_T1,_T2>& __x, const pair<_T1,_T2>& __y)\n{\n    return __x.first < __y.first || (!(__y.first < __x.first) && __x.second < __y.second);\n}\n\ntemplate <class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nbool\noperator> (const pair<_T1,_T2>& __x, const pair<_T1,_T2>& __y)\n{\n    return __y < __x;\n}\n\ntemplate <class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nbool\noperator>=(const pair<_T1,_T2>& __x, const pair<_T1,_T2>& __y)\n{\n    return !(__x < __y);\n}\n\ntemplate <class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nbool\noperator<=(const pair<_T1,_T2>& __x, const pair<_T1,_T2>& __y)\n{\n    return !(__y < __x);\n}\n\n#endif // _LIBCUDACXX_HAS_NO_SPACESHIP_OPERATOR\n\n#if _LIBCUDACXX_STD_VER > 17\ntemplate <class _T1, class _T2, class _U1, class _U2, template<class> class _TQual, template<class> class _UQual>\n    requires requires { typename pair<common_reference_t<_TQual<_T1>, _UQual<_U1>>,\n                                      common_reference_t<_TQual<_T2>, _UQual<_U2>>>; }\nstruct basic_common_reference<pair<_T1, _T2>, pair<_U1, _U2>, _TQual, _UQual> {\n    using type = pair<common_reference_t<_TQual<_T1>, _UQual<_U1>>,\n                      common_reference_t<_TQual<_T2>, _UQual<_U2>>>;\n};\n\ntemplate <class _T1, class _T2, class _U1, class _U2>\n    requires requires { typename pair<common_type_t<_T1, _U1>, common_type_t<_T2, _U2>>; }\nstruct common_type<pair<_T1, _T2>, pair<_U1, _U2>> {\n    using type = pair<common_type_t<_T1, _U1>, common_type_t<_T2, _U2>>;\n};\n#endif // _LIBCUDACXX_STD_VER > 17\n\ntemplate <class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n__enable_if_t\n<\n    __is_swappable<_T1>::value &&\n    __is_swappable<_T2>::value,\n    void\n>\nswap(pair<_T1, _T2>& __x, pair<_T1, _T2>& __y)\n                     noexcept((__is_nothrow_swappable<_T1>::value &&\n                                 __is_nothrow_swappable<_T2>::value))\n{\n    __x.swap(__y);\n}\n\n#if _LIBCUDACXX_STD_VER > 20\ntemplate <class _T1, class _T2>\n  requires (__is_swappable<const _T1>::value &&\n            __is_swappable<const _T2>::value)\n_LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_INLINE_VISIBILITY constexpr\nvoid swap(const pair<_T1, _T2>& __x, const pair<_T1, _T2>& __y)\n    noexcept(noexcept(__x.swap(__y)))\n{\n    __x.swap(__y);\n}\n#endif // _LIBCUDACXX_STD_VER > 20\n\ntemplate <class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\npair<typename __unwrap_ref_decay<_T1>::type, typename __unwrap_ref_decay<_T2>::type>\nmake_pair(_T1&& __t1, _T2&& __t2)\n{\n    return pair<typename __unwrap_ref_decay<_T1>::type, typename __unwrap_ref_decay<_T2>::type>\n               (_CUDA_VSTD::forward<_T1>(__t1), _CUDA_VSTD::forward<_T2>(__t2));\n}\n\ntemplate <class _T1, class _T2>\n  struct _LIBCUDACXX_TEMPLATE_VIS tuple_size<pair<_T1, _T2> >\n    : public integral_constant<size_t, 2> {};\n\ntemplate <size_t _Ip, class _T1, class _T2>\nstruct _LIBCUDACXX_TEMPLATE_VIS tuple_element<_Ip, pair<_T1, _T2> >\n{\n    static_assert(_Ip < 2, \"Index out of bounds in std::tuple_element<std::pair<T1, T2>>\");\n};\n\ntemplate <class _T1, class _T2>\nstruct _LIBCUDACXX_TEMPLATE_VIS tuple_element<0, pair<_T1, _T2> >\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE _T1 type;\n};\n\ntemplate <class _T1, class _T2>\nstruct _LIBCUDACXX_TEMPLATE_VIS tuple_element<1, pair<_T1, _T2> >\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE _T2 type;\n};\n\ntemplate <size_t _Ip> struct __get_pair;\n\ntemplate <>\nstruct __get_pair<0>\n{\n    template <class _T1, class _T2>\n    static\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    _T1&\n    get(pair<_T1, _T2>& __p) noexcept {return __p.first;}\n\n    template <class _T1, class _T2>\n    static\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    const _T1&\n    get(const pair<_T1, _T2>& __p) noexcept {return __p.first;}\n\n    template <class _T1, class _T2>\n    static\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    _T1&&\n    get(pair<_T1, _T2>&& __p) noexcept {return _CUDA_VSTD::forward<_T1>(__p.first);}\n\n    template <class _T1, class _T2>\n    static\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    const _T1&&\n    get(const pair<_T1, _T2>&& __p) noexcept {return _CUDA_VSTD::forward<const _T1>(__p.first);}\n};\n\ntemplate <>\nstruct __get_pair<1>\n{\n    template <class _T1, class _T2>\n    static\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    _T2&\n    get(pair<_T1, _T2>& __p) noexcept {return __p.second;}\n\n    template <class _T1, class _T2>\n    static\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    const _T2&\n    get(const pair<_T1, _T2>& __p) noexcept {return __p.second;}\n\n    template <class _T1, class _T2>\n    static\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    _T2&&\n    get(pair<_T1, _T2>&& __p) noexcept {return _CUDA_VSTD::forward<_T2>(__p.second);}\n\n    template <class _T1, class _T2>\n    static\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    const _T2&&\n    get(const pair<_T1, _T2>&& __p) noexcept {return _CUDA_VSTD::forward<const _T2>(__p.second);}\n};\n\ntemplate <size_t _Ip, class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\ntypename tuple_element<_Ip, pair<_T1, _T2> >::type&\nget(pair<_T1, _T2>& __p) noexcept\n{\n    return __get_pair<_Ip>::get(__p);\n}\n\ntemplate <size_t _Ip, class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nconst typename tuple_element<_Ip, pair<_T1, _T2> >::type&\nget(const pair<_T1, _T2>& __p) noexcept\n{\n    return __get_pair<_Ip>::get(__p);\n}\n\ntemplate <size_t _Ip, class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\ntypename tuple_element<_Ip, pair<_T1, _T2> >::type&&\nget(pair<_T1, _T2>&& __p) noexcept\n{\n    return __get_pair<_Ip>::get(_CUDA_VSTD::move(__p));\n}\n\ntemplate <size_t _Ip, class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nconst typename tuple_element<_Ip, pair<_T1, _T2> >::type&&\nget(const pair<_T1, _T2>&& __p) noexcept\n{\n    return __get_pair<_Ip>::get(_CUDA_VSTD::move(__p));\n}\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr _T1 & get(pair<_T1, _T2>& __p) noexcept\n{\n    return __get_pair<0>::get(__p);\n}\n\ntemplate <class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr _T1 const & get(pair<_T1, _T2> const& __p) noexcept\n{\n    return __get_pair<0>::get(__p);\n}\n\ntemplate <class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr _T1 && get(pair<_T1, _T2>&& __p) noexcept\n{\n    return __get_pair<0>::get(_CUDA_VSTD::move(__p));\n}\n\ntemplate <class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr _T1 const && get(pair<_T1, _T2> const&& __p) noexcept\n{\n    return __get_pair<0>::get(_CUDA_VSTD::move(__p));\n}\n\ntemplate <class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr _T1 & get(pair<_T2, _T1>& __p) noexcept\n{\n    return __get_pair<1>::get(__p);\n}\n\ntemplate <class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr _T1 const & get(pair<_T2, _T1> const& __p) noexcept\n{\n    return __get_pair<1>::get(__p);\n}\n\ntemplate <class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr _T1 && get(pair<_T2, _T1>&& __p) noexcept\n{\n    return __get_pair<1>::get(_CUDA_VSTD::move(__p));\n}\n\ntemplate <class _T1, class _T2>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr _T1 const && get(pair<_T2, _T1> const&& __p) noexcept\n{\n    return __get_pair<1>::get(_CUDA_VSTD::move(__p));\n}\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___UTILITY_PAIR_H\n", "__utility/piecewise_construct.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___UTILITY_PIECEWISE_CONSTRUCT_H\n#define _LIBCUDACXX___UTILITY_PIECEWISE_CONSTRUCT_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\nstruct _LIBCUDACXX_TEMPLATE_VIS piecewise_construct_t { explicit piecewise_construct_t() = default; };\n#if defined(_LIBCUDACXX_BUILDING_LIBRARY)\nextern _LIBCUDACXX_EXPORTED_FROM_ABI const piecewise_construct_t piecewise_construct;// = piecewise_construct_t();\n#else\n/* _LIBCUDACXX_INLINE_VAR */ constexpr piecewise_construct_t piecewise_construct = piecewise_construct_t();\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___UTILITY_PIECEWISE_CONSTRUCT_H\n", "__utility/priority_tag.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___UTILITY_PRIORITY_TAG_H\n#define _LIBCUDACXX___UTILITY_PRIORITY_TAG_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate<size_t _Ip> struct __priority_tag : __priority_tag<_Ip - 1> {};\ntemplate<> struct __priority_tag<0> {};\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___UTILITY_PRIORITY_TAG_H\n", "__utility/rel_ops.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___UTILITY_REL_OPS_H\n#define _LIBCUDACXX___UTILITY_REL_OPS_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__utility/forward.h\"\n#include \"../__utility/move.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\nnamespace rel_ops\n{\n\ntemplate<class _Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nbool\noperator!=(const _Tp& __x, const _Tp& __y)\n{\n    return !(__x == __y);\n}\n\ntemplate<class _Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nbool\noperator> (const _Tp& __x, const _Tp& __y)\n{\n    return __y < __x;\n}\n\ntemplate<class _Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nbool\noperator<=(const _Tp& __x, const _Tp& __y)\n{\n    return !(__y < __x);\n}\n\ntemplate<class _Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nbool\noperator>=(const _Tp& __x, const _Tp& __y)\n{\n    return !(__x < __y);\n}\n\n} // namespace rel_ops\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___UTILITY_REL_OPS_H\n", "__utility/swap.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___UTILITY_SWAP_H\n#define _LIBCUDACXX___UTILITY_SWAP_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/enable_if.h\"\n#include \"../__type_traits/is_move_assignable.h\"\n#include \"../__type_traits/is_move_constructible.h\"\n#include \"../__type_traits/is_nothrow_move_assignable.h\"\n#include \"../__type_traits/is_nothrow_move_constructible.h\"\n#include \"../__type_traits/is_swappable.h\"\n#include \"../__utility/move.h\"\n#include \"../cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n__swap_result_t<_Tp>\nswap(_Tp& __x, _Tp& __y) noexcept(_LIBCUDACXX_TRAIT(is_nothrow_move_constructible, _Tp)\n                               && _LIBCUDACXX_TRAIT(is_nothrow_move_assignable, _Tp)) {\n  _Tp __t(_CUDA_VSTD::move(__x));\n  __x = _CUDA_VSTD::move(__y);\n  __y = _CUDA_VSTD::move(__t);\n}\n\ntemplate <class _Tp, size_t _Np>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n__enable_if_t<__is_swappable<_Tp>::value>\nswap(_Tp (&__a)[_Np], _Tp (&__b)[_Np]) noexcept(__is_nothrow_swappable<_Tp>::value) {\n  for (size_t __i = 0; __i != _Np; ++__i) {\n    swap(__a[__i], __b[__i]);\n  }\n}\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___UTILITY_SWAP_H\n", "__utility/to_underlying.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___UTILITY_TO_UNDERLYING_H\n#define _LIBCUDACXX___UTILITY_TO_UNDERLYING_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../__type_traits/underlying_type.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr typename underlying_type<_Tp>::type\n__to_underlying(_Tp __val) noexcept {\n  return static_cast<typename underlying_type<_Tp>::type>(__val);\n}\n\n#if _LIBCUDACXX_STD_VER > 20\ntemplate <class _Tp>\n_LIBCUDACXX_NODISCARD_EXT _LIBCUDACXX_INLINE_VISIBILITY constexpr underlying_type_t<_Tp>\nto_underlying(_Tp __val) noexcept {\n  return _CUDA_VSTD::__to_underlying(__val);\n}\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // _LIBCUDACXX___UTILITY_TO_UNDERLYING_H\n", "__utility/unreachable.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___UTILITY_UNREACHABLE_H\n#define _LIBCUDACXX___UTILITY_UNREACHABLE_H\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"../cstdlib\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n_LIBCUDACXX_NORETURN _LIBCUDACXX_INLINE_VISIBILITY\ninline void __libcpp_unreachable()\n{\n  _LIBCUDACXX_UNREACHABLE();\n}\n\n#if _LIBCUDACXX_STD_VER > 20\n\n[[noreturn]] _LIBCUDACXX_INLINE_VISIBILITY\ninline void unreachable() { _LIBCUDACXX_UNREACHABLE(); }\n\n#endif // _LIBCUDACXX_STD_VER > 20\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif\n", "__verbose_abort": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX___VERBOSE_ABORT\n#define _LIBCUDACXX___VERBOSE_ABORT\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"__availability\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n// Provide a default implementation of __libcpp_verbose_abort if we know that neither the built\n// library nor the user is providing one. Otherwise, just declare it and use the one from the\n// built library or the one provided by the user.\n//\n// We can't provide a great implementation because it needs to be pretty much\n// dependency-free (this is included everywhere else in the library).\n#if defined(_LIBCUDACXX_HAS_NO_VERBOSE_ABORT_IN_LIBRARY) && !defined(_LIBCUDACXX_AVAILABILITY_CUSTOM_VERBOSE_ABORT_PROVIDED)\n\nextern \"C\" void abort();\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n_LIBCUDACXX_NORETURN _LIBCUDACXX_ATTRIBUTE_FORMAT(__printf__, 1, 2) _LIBCUDACXX_HIDE_FROM_ABI inline\nvoid __libcpp_verbose_abort(const char *, ...) {\n  ::abort();\n  __builtin_unreachable(); // never reached, but needed to tell the compiler that the function never returns\n}\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#else\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n_LIBCUDACXX_NORETURN _LIBCUDACXX_OVERRIDABLE_FUNC_VIS _LIBCUDACXX_ATTRIBUTE_FORMAT(__printf__, 1, 2)\nvoid __libcpp_verbose_abort(const char *__format, ...);\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif\n\n#endif // _LIBCUDACXX___VERBOSE_ABORT\n", "assert.h": "\n    #pragma once\n ", "atomic": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===--------------------------- atomic -----------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX_ATOMIC\n#define _LIBCUDACXX_ATOMIC\n\n/*\n    atomic synopsis\n\nnamespace std\n{\n\n// feature test macro\n\n#define __cpp_lib_atomic_is_always_lock_free // as specified by SG10\n\n // order and consistency\n\n enum memory_order: unspecified // enum class in C++20\n {\n    relaxed,\n    consume, // load-consume\n    acquire, // load-acquire\n    release, // store-release\n    acq_rel, // store-release load-acquire\n    seq_cst // store-release load-acquire\n };\n\n inline constexpr auto memory_order_relaxed = memory_order::relaxed;\n inline constexpr auto memory_order_consume = memory_order::consume;\n inline constexpr auto memory_order_acquire = memory_order::acquire;\n inline constexpr auto memory_order_release = memory_order::release;\n inline constexpr auto memory_order_acq_rel = memory_order::acq_rel;\n inline constexpr auto memory_order_seq_cst = memory_order::seq_cst;\n\ntemplate <class T> T kill_dependency(T y) noexcept;\n\n// lock-free property\n\n#define ATOMIC_BOOL_LOCK_FREE unspecified\n#define ATOMIC_CHAR_LOCK_FREE unspecified\n#define ATOMIC_CHAR16_T_LOCK_FREE unspecified\n#define ATOMIC_CHAR32_T_LOCK_FREE unspecified\n#define ATOMIC_WCHAR_T_LOCK_FREE unspecified\n#define ATOMIC_SHORT_LOCK_FREE unspecified\n#define ATOMIC_INT_LOCK_FREE unspecified\n#define ATOMIC_LONG_LOCK_FREE unspecified\n#define ATOMIC_LLONG_LOCK_FREE unspecified\n#define ATOMIC_POINTER_LOCK_FREE unspecified\n\n// flag type and operations\n\ntypedef struct atomic_flag\n{\n    bool test_and_set(memory_order m = memory_order_seq_cst) volatile noexcept;\n    bool test_and_set(memory_order m = memory_order_seq_cst) noexcept;\n    void clear(memory_order m = memory_order_seq_cst) volatile noexcept;\n    void clear(memory_order m = memory_order_seq_cst) noexcept;\n    atomic_flag()  noexcept = default;\n    atomic_flag(const atomic_flag&) = delete;\n    atomic_flag& operator=(const atomic_flag&) = delete;\n    atomic_flag& operator=(const atomic_flag&) volatile = delete;\n} atomic_flag;\n\nbool\n    atomic_flag_test_and_set(volatile atomic_flag* obj) noexcept;\n\nbool\n    atomic_flag_test_and_set(atomic_flag* obj) noexcept;\n\nbool\n    atomic_flag_test_and_set_explicit(volatile atomic_flag* obj,\n                                      memory_order m) noexcept;\n\nbool\n    atomic_flag_test_and_set_explicit(atomic_flag* obj, memory_order m) noexcept;\n\nvoid\n    atomic_flag_clear(volatile atomic_flag* obj) noexcept;\n\nvoid\n    atomic_flag_clear(atomic_flag* obj) noexcept;\n\nvoid\n    atomic_flag_clear_explicit(volatile atomic_flag* obj, memory_order m) noexcept;\n\nvoid\n    atomic_flag_clear_explicit(atomic_flag* obj, memory_order m) noexcept;\n\n#define ATOMIC_FLAG_INIT see below\n#define ATOMIC_VAR_INIT(value) see below\n\ntemplate <class T>\nstruct atomic\n{\n    static constexpr bool is_always_lock_free;\n    bool is_lock_free() const volatile noexcept;\n    bool is_lock_free() const noexcept;\n    void store(T desr, memory_order m = memory_order_seq_cst) volatile noexcept;\n    void store(T desr, memory_order m = memory_order_seq_cst) noexcept;\n    T load(memory_order m = memory_order_seq_cst) const volatile noexcept;\n    T load(memory_order m = memory_order_seq_cst) const noexcept;\n    operator T() const volatile noexcept;\n    operator T() const noexcept;\n    T exchange(T desr, memory_order m = memory_order_seq_cst) volatile noexcept;\n    T exchange(T desr, memory_order m = memory_order_seq_cst) noexcept;\n    bool compare_exchange_weak(T& expc, T desr,\n                               memory_order s, memory_order f) volatile noexcept;\n    bool compare_exchange_weak(T& expc, T desr, memory_order s, memory_order f) noexcept;\n    bool compare_exchange_strong(T& expc, T desr,\n                                 memory_order s, memory_order f) volatile noexcept;\n    bool compare_exchange_strong(T& expc, T desr,\n                                 memory_order s, memory_order f) noexcept;\n    bool compare_exchange_weak(T& expc, T desr,\n                               memory_order m = memory_order_seq_cst) volatile noexcept;\n    bool compare_exchange_weak(T& expc, T desr,\n                               memory_order m = memory_order_seq_cst) noexcept;\n    bool compare_exchange_strong(T& expc, T desr,\n                                memory_order m = memory_order_seq_cst) volatile noexcept;\n    bool compare_exchange_strong(T& expc, T desr,\n                                 memory_order m = memory_order_seq_cst) noexcept;\n\n    atomic() noexcept = default;\n    constexpr atomic(T desr) noexcept;\n    atomic(const atomic&) = delete;\n    atomic& operator=(const atomic&) = delete;\n    atomic& operator=(const atomic&) volatile = delete;\n    T operator=(T) volatile noexcept;\n    T operator=(T) noexcept;\n};\n\ntemplate <>\nstruct atomic<integral>\n{\n    static constexpr bool is_always_lock_free;\n    bool is_lock_free() const volatile noexcept;\n    bool is_lock_free() const noexcept;\n    void store(integral desr, memory_order m = memory_order_seq_cst) volatile noexcept;\n    void store(integral desr, memory_order m = memory_order_seq_cst) noexcept;\n    integral load(memory_order m = memory_order_seq_cst) const volatile noexcept;\n    integral load(memory_order m = memory_order_seq_cst) const noexcept;\n    operator integral() const volatile noexcept;\n    operator integral() const noexcept;\n    integral exchange(integral desr,\n                      memory_order m = memory_order_seq_cst) volatile noexcept;\n    integral exchange(integral desr, memory_order m = memory_order_seq_cst) noexcept;\n    bool compare_exchange_weak(integral& expc, integral desr,\n                               memory_order s, memory_order f) volatile noexcept;\n    bool compare_exchange_weak(integral& expc, integral desr,\n                               memory_order s, memory_order f) noexcept;\n    bool compare_exchange_strong(integral& expc, integral desr,\n                                 memory_order s, memory_order f) volatile noexcept;\n    bool compare_exchange_strong(integral& expc, integral desr,\n                                 memory_order s, memory_order f) noexcept;\n    bool compare_exchange_weak(integral& expc, integral desr,\n                               memory_order m = memory_order_seq_cst) volatile noexcept;\n    bool compare_exchange_weak(integral& expc, integral desr,\n                               memory_order m = memory_order_seq_cst) noexcept;\n    bool compare_exchange_strong(integral& expc, integral desr,\n                                memory_order m = memory_order_seq_cst) volatile noexcept;\n    bool compare_exchange_strong(integral& expc, integral desr,\n                                 memory_order m = memory_order_seq_cst) noexcept;\n\n    integral\n        fetch_add(integral op, memory_order m = memory_order_seq_cst) volatile noexcept;\n    integral fetch_add(integral op, memory_order m = memory_order_seq_cst) noexcept;\n    integral\n        fetch_sub(integral op, memory_order m = memory_order_seq_cst) volatile noexcept;\n    integral fetch_sub(integral op, memory_order m = memory_order_seq_cst) noexcept;\n    integral\n        fetch_and(integral op, memory_order m = memory_order_seq_cst) volatile noexcept;\n    integral fetch_and(integral op, memory_order m = memory_order_seq_cst) noexcept;\n    integral\n        fetch_or(integral op, memory_order m = memory_order_seq_cst) volatile noexcept;\n    integral fetch_or(integral op, memory_order m = memory_order_seq_cst) noexcept;\n    integral\n        fetch_xor(integral op, memory_order m = memory_order_seq_cst) volatile noexcept;\n    integral fetch_xor(integral op, memory_order m = memory_order_seq_cst) noexcept;\n\n    atomic() noexcept = default;\n    constexpr atomic(integral desr) noexcept;\n    atomic(const atomic&) = delete;\n    atomic& operator=(const atomic&) = delete;\n    atomic& operator=(const atomic&) volatile = delete;\n    integral operator=(integral desr) volatile noexcept;\n    integral operator=(integral desr) noexcept;\n\n    integral operator++(int) volatile noexcept;\n    integral operator++(int) noexcept;\n    integral operator--(int) volatile noexcept;\n    integral operator--(int) noexcept;\n    integral operator++() volatile noexcept;\n    integral operator++() noexcept;\n    integral operator--() volatile noexcept;\n    integral operator--() noexcept;\n    integral operator+=(integral op) volatile noexcept;\n    integral operator+=(integral op) noexcept;\n    integral operator-=(integral op) volatile noexcept;\n    integral operator-=(integral op) noexcept;\n    integral operator&=(integral op) volatile noexcept;\n    integral operator&=(integral op) noexcept;\n    integral operator|=(integral op) volatile noexcept;\n    integral operator|=(integral op) noexcept;\n    integral operator^=(integral op) volatile noexcept;\n    integral operator^=(integral op) noexcept;\n};\n\ntemplate <class T>\nstruct atomic<T*>\n{\n    static constexpr bool is_always_lock_free;\n    bool is_lock_free() const volatile noexcept;\n    bool is_lock_free() const noexcept;\n    void store(T* desr, memory_order m = memory_order_seq_cst) volatile noexcept;\n    void store(T* desr, memory_order m = memory_order_seq_cst) noexcept;\n    T* load(memory_order m = memory_order_seq_cst) const volatile noexcept;\n    T* load(memory_order m = memory_order_seq_cst) const noexcept;\n    operator T*() const volatile noexcept;\n    operator T*() const noexcept;\n    T* exchange(T* desr, memory_order m = memory_order_seq_cst) volatile noexcept;\n    T* exchange(T* desr, memory_order m = memory_order_seq_cst) noexcept;\n    bool compare_exchange_weak(T*& expc, T* desr,\n                               memory_order s, memory_order f) volatile noexcept;\n    bool compare_exchange_weak(T*& expc, T* desr,\n                               memory_order s, memory_order f) noexcept;\n    bool compare_exchange_strong(T*& expc, T* desr,\n                                 memory_order s, memory_order f) volatile noexcept;\n    bool compare_exchange_strong(T*& expc, T* desr,\n                                 memory_order s, memory_order f) noexcept;\n    bool compare_exchange_weak(T*& expc, T* desr,\n                               memory_order m = memory_order_seq_cst) volatile noexcept;\n    bool compare_exchange_weak(T*& expc, T* desr,\n                               memory_order m = memory_order_seq_cst) noexcept;\n    bool compare_exchange_strong(T*& expc, T* desr,\n                                memory_order m = memory_order_seq_cst) volatile noexcept;\n    bool compare_exchange_strong(T*& expc, T* desr,\n                                 memory_order m = memory_order_seq_cst) noexcept;\n    T* fetch_add(ptrdiff_t op, memory_order m = memory_order_seq_cst) volatile noexcept;\n    T* fetch_add(ptrdiff_t op, memory_order m = memory_order_seq_cst) noexcept;\n    T* fetch_sub(ptrdiff_t op, memory_order m = memory_order_seq_cst) volatile noexcept;\n    T* fetch_sub(ptrdiff_t op, memory_order m = memory_order_seq_cst) noexcept;\n\n    atomic() noexcept = default;\n    constexpr atomic(T* desr) noexcept;\n    atomic(const atomic&) = delete;\n    atomic& operator=(const atomic&) = delete;\n    atomic& operator=(const atomic&) volatile = delete;\n\n    T* operator=(T*) volatile noexcept;\n    T* operator=(T*) noexcept;\n    T* operator++(int) volatile noexcept;\n    T* operator++(int) noexcept;\n    T* operator--(int) volatile noexcept;\n    T* operator--(int) noexcept;\n    T* operator++() volatile noexcept;\n    T* operator++() noexcept;\n    T* operator--() volatile noexcept;\n    T* operator--() noexcept;\n    T* operator+=(ptrdiff_t op) volatile noexcept;\n    T* operator+=(ptrdiff_t op) noexcept;\n    T* operator-=(ptrdiff_t op) volatile noexcept;\n    T* operator-=(ptrdiff_t op) noexcept;\n};\n\n\ntemplate <class T>\n    bool\n    atomic_is_lock_free(const volatile atomic<T>* obj) noexcept;\n\ntemplate <class T>\n    bool\n    atomic_is_lock_free(const atomic<T>* obj) noexcept;\n\ntemplate <class T>\n    void\n    atomic_init(volatile atomic<T>* obj, T desr) noexcept;\n\ntemplate <class T>\n    void\n    atomic_init(atomic<T>* obj, T desr) noexcept;\n\ntemplate <class T>\n    void\n    atomic_store(volatile atomic<T>* obj, T desr) noexcept;\n\ntemplate <class T>\n    void\n    atomic_store(atomic<T>* obj, T desr) noexcept;\n\ntemplate <class T>\n    void\n    atomic_store_explicit(volatile atomic<T>* obj, T desr, memory_order m) noexcept;\n\ntemplate <class T>\n    void\n    atomic_store_explicit(atomic<T>* obj, T desr, memory_order m) noexcept;\n\ntemplate <class T>\n    T\n    atomic_load(const volatile atomic<T>* obj) noexcept;\n\ntemplate <class T>\n    T\n    atomic_load(const atomic<T>* obj) noexcept;\n\ntemplate <class T>\n    T\n    atomic_load_explicit(const volatile atomic<T>* obj, memory_order m) noexcept;\n\ntemplate <class T>\n    T\n    atomic_load_explicit(const atomic<T>* obj, memory_order m) noexcept;\n\ntemplate <class T>\n    T\n    atomic_exchange(volatile atomic<T>* obj, T desr) noexcept;\n\ntemplate <class T>\n    T\n    atomic_exchange(atomic<T>* obj, T desr) noexcept;\n\ntemplate <class T>\n    T\n    atomic_exchange_explicit(volatile atomic<T>* obj, T desr, memory_order m) noexcept;\n\ntemplate <class T>\n    T\n    atomic_exchange_explicit(atomic<T>* obj, T desr, memory_order m) noexcept;\n\ntemplate <class T>\n    bool\n    atomic_compare_exchange_weak(volatile atomic<T>* obj, T* expc, T desr) noexcept;\n\ntemplate <class T>\n    bool\n    atomic_compare_exchange_weak(atomic<T>* obj, T* expc, T desr) noexcept;\n\ntemplate <class T>\n    bool\n    atomic_compare_exchange_strong(volatile atomic<T>* obj, T* expc, T desr) noexcept;\n\ntemplate <class T>\n    bool\n    atomic_compare_exchange_strong(atomic<T>* obj, T* expc, T desr) noexcept;\n\ntemplate <class T>\n    bool\n    atomic_compare_exchange_weak_explicit(volatile atomic<T>* obj, T* expc,\n                                          T desr,\n                                          memory_order s, memory_order f) noexcept;\n\ntemplate <class T>\n    bool\n    atomic_compare_exchange_weak_explicit(atomic<T>* obj, T* expc, T desr,\n                                          memory_order s, memory_order f) noexcept;\n\ntemplate <class T>\n    bool\n    atomic_compare_exchange_strong_explicit(volatile atomic<T>* obj,\n                                            T* expc, T desr,\n                                            memory_order s, memory_order f) noexcept;\n\ntemplate <class T>\n    bool\n    atomic_compare_exchange_strong_explicit(atomic<T>* obj, T* expc,\n                                            T desr,\n                                            memory_order s, memory_order f) noexcept;\n\ntemplate <class Integral>\n    Integral\n    atomic_fetch_add(volatile atomic<Integral>* obj, Integral op) noexcept;\n\ntemplate <class Integral>\n    Integral\n    atomic_fetch_add(atomic<Integral>* obj, Integral op) noexcept;\n\ntemplate <class Integral>\n    Integral\n    atomic_fetch_add_explicit(volatile atomic<Integral>* obj, Integral op,\n                              memory_order m) noexcept;\ntemplate <class Integral>\n    Integral\n    atomic_fetch_add_explicit(atomic<Integral>* obj, Integral op,\n                              memory_order m) noexcept;\ntemplate <class Integral>\n    Integral\n    atomic_fetch_sub(volatile atomic<Integral>* obj, Integral op) noexcept;\n\ntemplate <class Integral>\n    Integral\n    atomic_fetch_sub(atomic<Integral>* obj, Integral op) noexcept;\n\ntemplate <class Integral>\n    Integral\n    atomic_fetch_sub_explicit(volatile atomic<Integral>* obj, Integral op,\n                              memory_order m) noexcept;\ntemplate <class Integral>\n    Integral\n    atomic_fetch_sub_explicit(atomic<Integral>* obj, Integral op,\n                              memory_order m) noexcept;\ntemplate <class Integral>\n    Integral\n    atomic_fetch_and(volatile atomic<Integral>* obj, Integral op) noexcept;\n\ntemplate <class Integral>\n    Integral\n    atomic_fetch_and(atomic<Integral>* obj, Integral op) noexcept;\n\ntemplate <class Integral>\n    Integral\n    atomic_fetch_and_explicit(volatile atomic<Integral>* obj, Integral op,\n                              memory_order m) noexcept;\ntemplate <class Integral>\n    Integral\n    atomic_fetch_and_explicit(atomic<Integral>* obj, Integral op,\n                              memory_order m) noexcept;\ntemplate <class Integral>\n    Integral\n    atomic_fetch_or(volatile atomic<Integral>* obj, Integral op) noexcept;\n\ntemplate <class Integral>\n    Integral\n    atomic_fetch_or(atomic<Integral>* obj, Integral op) noexcept;\n\ntemplate <class Integral>\n    Integral\n    atomic_fetch_or_explicit(volatile atomic<Integral>* obj, Integral op,\n                             memory_order m) noexcept;\ntemplate <class Integral>\n    Integral\n    atomic_fetch_or_explicit(atomic<Integral>* obj, Integral op,\n                             memory_order m) noexcept;\ntemplate <class Integral>\n    Integral\n    atomic_fetch_xor(volatile atomic<Integral>* obj, Integral op) noexcept;\n\ntemplate <class Integral>\n    Integral\n    atomic_fetch_xor(atomic<Integral>* obj, Integral op) noexcept;\n\ntemplate <class Integral>\n    Integral\n    atomic_fetch_xor_explicit(volatile atomic<Integral>* obj, Integral op,\n                              memory_order m) noexcept;\ntemplate <class Integral>\n    Integral\n    atomic_fetch_xor_explicit(atomic<Integral>* obj, Integral op,\n                              memory_order m) noexcept;\n\ntemplate <class T>\n    T*\n    atomic_fetch_add(volatile atomic<T*>* obj, ptrdiff_t op) noexcept;\n\ntemplate <class T>\n    T*\n    atomic_fetch_add(atomic<T*>* obj, ptrdiff_t op) noexcept;\n\ntemplate <class T>\n    T*\n    atomic_fetch_add_explicit(volatile atomic<T*>* obj, ptrdiff_t op,\n                              memory_order m) noexcept;\ntemplate <class T>\n    T*\n    atomic_fetch_add_explicit(atomic<T*>* obj, ptrdiff_t op, memory_order m) noexcept;\n\ntemplate <class T>\n    T*\n    atomic_fetch_sub(volatile atomic<T*>* obj, ptrdiff_t op) noexcept;\n\ntemplate <class T>\n    T*\n    atomic_fetch_sub(atomic<T*>* obj, ptrdiff_t op) noexcept;\n\ntemplate <class T>\n    T*\n    atomic_fetch_sub_explicit(volatile atomic<T*>* obj, ptrdiff_t op,\n                              memory_order m) noexcept;\ntemplate <class T>\n    T*\n    atomic_fetch_sub_explicit(atomic<T*>* obj, ptrdiff_t op, memory_order m) noexcept;\n\n// Atomics for standard typedef types\n\ntypedef atomic<bool>               atomic_bool;\ntypedef atomic<char>               atomic_char;\ntypedef atomic<signed char>        atomic_schar;\ntypedef atomic<unsigned char>      atomic_uchar;\ntypedef atomic<short>              atomic_short;\ntypedef atomic<unsigned short>     atomic_ushort;\ntypedef atomic<int>                atomic_int;\ntypedef atomic<unsigned int>       atomic_uint;\ntypedef atomic<long>               atomic_long;\ntypedef atomic<unsigned long>      atomic_ulong;\ntypedef atomic<long long>          atomic_llong;\ntypedef atomic<unsigned long long> atomic_ullong;\ntypedef atomic<char16_t>           atomic_char16_t;\ntypedef atomic<char32_t>           atomic_char32_t;\ntypedef atomic<wchar_t>            atomic_wchar_t;\n\ntypedef atomic<int_least8_t>   atomic_int_least8_t;\ntypedef atomic<uint_least8_t>  atomic_uint_least8_t;\ntypedef atomic<int_least16_t>  atomic_int_least16_t;\ntypedef atomic<uint_least16_t> atomic_uint_least16_t;\ntypedef atomic<int_least32_t>  atomic_int_least32_t;\ntypedef atomic<uint_least32_t> atomic_uint_least32_t;\ntypedef atomic<int_least64_t>  atomic_int_least64_t;\ntypedef atomic<uint_least64_t> atomic_uint_least64_t;\n\ntypedef atomic<int_fast8_t>   atomic_int_fast8_t;\ntypedef atomic<uint_fast8_t>  atomic_uint_fast8_t;\ntypedef atomic<int_fast16_t>  atomic_int_fast16_t;\ntypedef atomic<uint_fast16_t> atomic_uint_fast16_t;\ntypedef atomic<int_fast32_t>  atomic_int_fast32_t;\ntypedef atomic<uint_fast32_t> atomic_uint_fast32_t;\ntypedef atomic<int_fast64_t>  atomic_int_fast64_t;\ntypedef atomic<uint_fast64_t> atomic_uint_fast64_t;\n\ntypedef atomic<int8_t>   atomic_int8_t;\ntypedef atomic<uint8_t>  atomic_uint8_t;\ntypedef atomic<int16_t>  atomic_int16_t;\ntypedef atomic<uint16_t> atomic_uint16_t;\ntypedef atomic<int32_t>  atomic_int32_t;\ntypedef atomic<uint32_t> atomic_uint32_t;\ntypedef atomic<int64_t>  atomic_int64_t;\ntypedef atomic<uint64_t> atomic_uint64_t;\n\ntypedef atomic<intptr_t>  atomic_intptr_t;\ntypedef atomic<uintptr_t> atomic_uintptr_t;\ntypedef atomic<size_t>    atomic_size_t;\ntypedef atomic<ptrdiff_t> atomic_ptrdiff_t;\ntypedef atomic<intmax_t>  atomic_intmax_t;\ntypedef atomic<uintmax_t> atomic_uintmax_t;\n\n// fences\n\nvoid atomic_thread_fence(memory_order m) noexcept;\nvoid atomic_signal_fence(memory_order m) noexcept;\n\n}  // std\n\n*/\n\n#ifndef __cuda_std__\n#include <__config>\n#include <cstring>\n#endif // __cuda_std__\n\n#include \"__assert\" // all public C++ headers provide the assertion handler\n#include \"__debug\"\n#include \"__threading_support\"\n#include \"__type_traits/conditional.h\"\n#include \"__type_traits/enable_if.h\"\n#include \"__type_traits/is_assignable.h\"\n#include \"__type_traits/is_floating_point.h\"\n#include \"__type_traits/is_integral.h\"\n#include \"__type_traits/is_same.h\"\n#include \"__type_traits/is_trivially_copyable.h\"\n#include \"__type_traits/underlying_type.h\"\n#include \"__utility/forward.h\"\n#include \"cstddef\"\n#include \"cstdint\"\n#include \"type_traits\"\n#include \"version\"\n\n#ifndef __cuda_std__\n#include <__pragma_push>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n#ifdef _LIBCUDACXX_HAS_NO_THREADS\n# error <atomic> is not supported on this single threaded system\n#endif\n#ifdef _LIBCUDACXX_HAS_NO_ATOMIC_HEADER\n# error <atomic> is not implemented\n#endif\n#ifdef _LIBCUDACXX_UNSUPPORTED_THREAD_API\n# error \"<atomic> is not supported on this system\"\n#endif\n#ifdef kill_dependency\n# error C++ standard library is incompatible with <stdatomic.h>\n#endif\n\n#define _LIBCUDACXX_CHECK_STORE_MEMORY_ORDER(__m) \\\n  _LIBCUDACXX_DIAGNOSE_WARNING(__m == memory_order_consume || \\\n                           __m == memory_order_acquire || \\\n                           __m == memory_order_acq_rel,   \\\n                        \"memory order argument to atomic operation is invalid\")\n\n#define _LIBCUDACXX_CHECK_LOAD_MEMORY_ORDER(__m) \\\n  _LIBCUDACXX_DIAGNOSE_WARNING(__m == memory_order_release || \\\n                           __m == memory_order_acq_rel,   \\\n                        \"memory order argument to atomic operation is invalid\")\n\n#define _LIBCUDACXX_CHECK_EXCHANGE_MEMORY_ORDER(__m, __f) \\\n  _LIBCUDACXX_DIAGNOSE_WARNING(__f == memory_order_release || \\\n                           __f == memory_order_acq_rel,   \\\n                        \"memory order argument to atomic operation is invalid\")\n\n#if defined(_LIBCUDACXX_HAS_MSVC_ATOMIC_IMPL)\n#  include <intrin.h>\n#endif\n\n#if !defined(_LIBCUDACXX_COMPILER_NVRTC)\n#  include <string.h>\n#endif\n\n#if !defined(__CLANG_ATOMIC_BOOL_LOCK_FREE) && !defined(__GCC_ATOMIC_BOOL_LOCK_FREE)\n#define ATOMIC_BOOL_LOCK_FREE      2\n#define ATOMIC_CHAR_LOCK_FREE      2\n#define ATOMIC_CHAR16_T_LOCK_FREE  2\n#define ATOMIC_CHAR32_T_LOCK_FREE  2\n#define ATOMIC_WCHAR_T_LOCK_FREE   2\n#define ATOMIC_SHORT_LOCK_FREE     2\n#define ATOMIC_INT_LOCK_FREE       2\n#define ATOMIC_LONG_LOCK_FREE      2\n#define ATOMIC_LLONG_LOCK_FREE     2\n#define ATOMIC_POINTER_LOCK_FREE   2\n#endif //!defined(__CLANG_ATOMIC_BOOL_LOCK_FREE) && !defined(__GCC_ATOMIC_BOOL_LOCK_FREE)\n\n#ifndef __ATOMIC_RELAXED\n#define __ATOMIC_RELAXED 0\n#define __ATOMIC_CONSUME 1\n#define __ATOMIC_ACQUIRE 2\n#define __ATOMIC_RELEASE 3\n#define __ATOMIC_ACQ_REL 4\n#define __ATOMIC_SEQ_CST 5\n#endif //__ATOMIC_RELAXED\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n// Figure out what the underlying type for `memory_order` would be if it were\n// declared as an unscoped enum (accounting for -fshort-enums). Use this result\n// to pin the underlying type in C++20.\nenum __legacy_memory_order {\n    __mo_relaxed,\n    __mo_consume,\n    __mo_acquire,\n    __mo_release,\n    __mo_acq_rel,\n    __mo_seq_cst\n};\n\ntypedef underlying_type<__legacy_memory_order>::type __memory_order_underlying_t;\n\n#if _LIBCUDACXX_STD_VER > 17\n\nenum class memory_order : __memory_order_underlying_t {\n  relaxed = __mo_relaxed,\n  consume = __mo_consume,\n  acquire = __mo_acquire,\n  release = __mo_release,\n  acq_rel = __mo_acq_rel,\n  seq_cst = __mo_seq_cst\n};\n\ninline constexpr auto memory_order_relaxed = memory_order::relaxed;\ninline constexpr auto memory_order_consume = memory_order::consume;\ninline constexpr auto memory_order_acquire = memory_order::acquire;\ninline constexpr auto memory_order_release = memory_order::release;\ninline constexpr auto memory_order_acq_rel = memory_order::acq_rel;\ninline constexpr auto memory_order_seq_cst = memory_order::seq_cst;\n\n#else\n\ntypedef enum memory_order {\n  memory_order_relaxed = __mo_relaxed,\n  memory_order_consume = __mo_consume,\n  memory_order_acquire = __mo_acquire,\n  memory_order_release = __mo_release,\n  memory_order_acq_rel = __mo_acq_rel,\n  memory_order_seq_cst = __mo_seq_cst,\n} memory_order;\n\n#endif // _LIBCUDACXX_STD_VER > 17\n\ntemplate <typename _Tp> _LIBCUDACXX_INLINE_VISIBILITY\nbool __cxx_nonatomic_compare_equal(_Tp const& __lhs, _Tp const& __rhs) {\n#if defined(_LIBCUDACXX_COMPILER_NVCC) || defined(_LIBCUDACXX_COMPILER_NVRTC)\n    return __lhs == __rhs;\n#else\n    return memcmp(&__lhs, &__rhs, sizeof(_Tp)) == 0;\n#endif\n}\n\nstatic_assert((is_same<underlying_type<memory_order>::type, __memory_order_underlying_t>::value),\n  \"unexpected underlying type for std::memory_order\");\n\n#if defined(_LIBCUDACXX_HAS_GCC_ATOMIC_IMP) || \\\n    defined(_LIBCUDACXX_ATOMIC_ONLY_USE_BUILTINS)\n\n// [atomics.types.generic]p1 guarantees _Tp is trivially copyable. Because\n// the default operator= in an object is not volatile, a byte-by-byte copy\n// is required.\ntemplate <typename _Tp, typename _Tv> _LIBCUDACXX_INLINE_VISIBILITY\n__enable_if_t<is_assignable<_Tp&, _Tv>::value>\n__cxx_atomic_assign_volatile(_Tp& __a_value, _Tv const& __val) {\n  __a_value = __val;\n}\ntemplate <typename _Tp, typename _Tv> _LIBCUDACXX_INLINE_VISIBILITY\n__enable_if_t<is_assignable<_Tp&, _Tv>::value>\n__cxx_atomic_assign_volatile(_Tp volatile& __a_value, _Tv volatile const& __val) {\n  volatile char* __to = reinterpret_cast<volatile char*>(&__a_value);\n  volatile char* __end = __to + sizeof(_Tp);\n  volatile const char* __from = reinterpret_cast<volatile const char*>(&__val);\n  while (__to != __end)\n    *__to++ = *__from++;\n}\n\n#endif\n\n// Headers are wrapped like so: (cuda::std::|std::)detail\nnamespace __detail {\n#if defined(_LIBCUDACXX_HAS_CUDA_ATOMIC_EXT)\n#  include \"support/atomic/atomic_scopes.h\"\n#endif\n\n#if defined(_LIBCUDACXX_HAS_CUDA_ATOMIC_IMPL)\n#  include \"support/atomic/atomic_cuda.h\"\n#elif defined(_LIBCUDACXX_HAS_MSVC_ATOMIC_IMPL)\n#  include \"support/atomic/atomic_msvc.h\"\n#elif defined(_LIBCUDACXX_HAS_GCC_ATOMIC_IMP)\n#  include \"support/atomic/atomic_gcc.h\"\n#elif defined(_LIBCUDACXX_HAS_C_ATOMIC_IMP)\n// TODO: Maybe support C11 atomics?\n// #include \"support/atomic/atomic_c11.h\"\n#endif // _LIBCUDACXX_HAS_GCC_ATOMIC_IMP, _LIBCUDACXX_HAS_C_ATOMIC_IMP\n}\n\nusing __detail::__cxx_atomic_base_impl;\nusing __detail::__cxx_atomic_ref_base_impl;\nusing __detail::__cxx_atomic_thread_fence;\nusing __detail::__cxx_atomic_signal_fence;\nusing __detail::__cxx_atomic_load;\nusing __detail::__cxx_atomic_store;\nusing __detail::__cxx_atomic_exchange;\nusing __detail::__cxx_atomic_compare_exchange_weak;\nusing __detail::__cxx_atomic_compare_exchange_strong;\nusing __detail::__cxx_atomic_fetch_add;\nusing __detail::__cxx_atomic_fetch_sub;\nusing __detail::__cxx_atomic_fetch_or;\nusing __detail::__cxx_atomic_fetch_and;\nusing __detail::__cxx_atomic_fetch_xor;\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n_Tp kill_dependency(_Tp __y) noexcept\n{\n    return __y;\n}\n\n#if defined(__CLANG_ATOMIC_BOOL_LOCK_FREE)\n# define ATOMIC_BOOL_LOCK_FREE      __CLANG_ATOMIC_BOOL_LOCK_FREE\n# define ATOMIC_CHAR_LOCK_FREE      __CLANG_ATOMIC_CHAR_LOCK_FREE\n# define ATOMIC_CHAR16_T_LOCK_FREE  __CLANG_ATOMIC_CHAR16_T_LOCK_FREE\n# define ATOMIC_CHAR32_T_LOCK_FREE  __CLANG_ATOMIC_CHAR32_T_LOCK_FREE\n# define ATOMIC_WCHAR_T_LOCK_FREE   __CLANG_ATOMIC_WCHAR_T_LOCK_FREE\n# define ATOMIC_SHORT_LOCK_FREE     __CLANG_ATOMIC_SHORT_LOCK_FREE\n# define ATOMIC_INT_LOCK_FREE       __CLANG_ATOMIC_INT_LOCK_FREE\n# define ATOMIC_LONG_LOCK_FREE      __CLANG_ATOMIC_LONG_LOCK_FREE\n# define ATOMIC_LLONG_LOCK_FREE     __CLANG_ATOMIC_LLONG_LOCK_FREE\n# define ATOMIC_POINTER_LOCK_FREE   __CLANG_ATOMIC_POINTER_LOCK_FREE\n#elif defined(__GCC_ATOMIC_BOOL_LOCK_FREE)\n# define ATOMIC_BOOL_LOCK_FREE      __GCC_ATOMIC_BOOL_LOCK_FREE\n# define ATOMIC_CHAR_LOCK_FREE      __GCC_ATOMIC_CHAR_LOCK_FREE\n# define ATOMIC_CHAR16_T_LOCK_FREE  __GCC_ATOMIC_CHAR16_T_LOCK_FREE\n# define ATOMIC_CHAR32_T_LOCK_FREE  __GCC_ATOMIC_CHAR32_T_LOCK_FREE\n# define ATOMIC_WCHAR_T_LOCK_FREE   __GCC_ATOMIC_WCHAR_T_LOCK_FREE\n# define ATOMIC_SHORT_LOCK_FREE     __GCC_ATOMIC_SHORT_LOCK_FREE\n# define ATOMIC_INT_LOCK_FREE       __GCC_ATOMIC_INT_LOCK_FREE\n# define ATOMIC_LONG_LOCK_FREE      __GCC_ATOMIC_LONG_LOCK_FREE\n# define ATOMIC_LLONG_LOCK_FREE     __GCC_ATOMIC_LLONG_LOCK_FREE\n# define ATOMIC_POINTER_LOCK_FREE   __GCC_ATOMIC_POINTER_LOCK_FREE\n#endif\n\n#ifdef _LIBCUDACXX_ATOMIC_ONLY_USE_BUILTINS\n\ntemplate<typename _Tp, int _Sco>\nstruct __cxx_atomic_lock_impl {\n\n  _LIBCUDACXX_INLINE_VISIBILITY\n  __cxx_atomic_lock_impl() noexcept\n    : __a_value(), __a_lock(0) {}\n  _LIBCUDACXX_INLINE_VISIBILITY constexpr explicit\n  __cxx_atomic_lock_impl(_Tp value) noexcept\n    : __a_value(value), __a_lock(0) {}\n\n  _Tp __a_value;\n  mutable __cxx_atomic_base_impl<_LIBCUDACXX_ATOMIC_FLAG_TYPE, _Sco> __a_lock;\n\n  _LIBCUDACXX_INLINE_VISIBILITY void __lock() const volatile {\n    while(1 == __cxx_atomic_exchange(&__a_lock, _LIBCUDACXX_ATOMIC_FLAG_TYPE(true), memory_order_acquire))\n        /*spin*/;\n  }\n  _LIBCUDACXX_INLINE_VISIBILITY void __lock() const {\n    while(1 == __cxx_atomic_exchange(&__a_lock, _LIBCUDACXX_ATOMIC_FLAG_TYPE(true), memory_order_acquire))\n        /*spin*/;\n  }\n  _LIBCUDACXX_INLINE_VISIBILITY void __unlock() const volatile {\n    __cxx_atomic_store(&__a_lock, _LIBCUDACXX_ATOMIC_FLAG_TYPE(false), memory_order_release);\n  }\n  _LIBCUDACXX_INLINE_VISIBILITY void __unlock() const {\n    __cxx_atomic_store(&__a_lock, _LIBCUDACXX_ATOMIC_FLAG_TYPE(false), memory_order_release);\n  }\n  _LIBCUDACXX_INLINE_VISIBILITY _Tp __read() const volatile {\n    __lock();\n    _Tp __old;\n    __cxx_atomic_assign_volatile(__old, __a_value);\n    __unlock();\n    return __old;\n  }\n  _LIBCUDACXX_INLINE_VISIBILITY _Tp __read() const {\n    __lock();\n    _Tp __old = __a_value;\n    __unlock();\n    return __old;\n  }\n};\n\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY\nvoid __cxx_atomic_init(volatile __cxx_atomic_lock_impl<_Tp, _Sco>* __a,  _Tp __val) {\n  __cxx_atomic_assign_volatile(__a->__a_value, __val);\n}\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY\nvoid __cxx_atomic_init(__cxx_atomic_lock_impl<_Tp, _Sco>* __a,  _Tp __val) {\n  __a->__a_value = __val;\n}\n\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY\nvoid __cxx_atomic_store(volatile __cxx_atomic_lock_impl<_Tp, _Sco>* __a,  _Tp __val, memory_order) {\n  __a->__lock();\n  __cxx_atomic_assign_volatile(__a->__a_value, __val);\n  __a->__unlock();\n}\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY\nvoid __cxx_atomic_store(__cxx_atomic_lock_impl<_Tp, _Sco>* __a,  _Tp __val, memory_order) {\n  __a->__lock();\n  __a->__a_value = __val;\n  __a->__unlock();\n}\n\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY\n_Tp __cxx_atomic_load(const volatile __cxx_atomic_lock_impl<_Tp, _Sco>* __a, memory_order) {\n  return __a->__read();\n}\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY\n_Tp __cxx_atomic_load(const __cxx_atomic_lock_impl<_Tp, _Sco>* __a, memory_order) {\n  return __a->__read();\n}\n\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY\n_Tp __cxx_atomic_exchange(volatile __cxx_atomic_lock_impl<_Tp, _Sco>* __a, _Tp __value, memory_order) {\n  __a->__lock();\n  _Tp __old;\n  __cxx_atomic_assign_volatile(__old, __a->__a_value);\n  __cxx_atomic_assign_volatile(__a->__a_value, __value);\n  __a->__unlock();\n  return __old;\n}\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY\n_Tp __cxx_atomic_exchange(__cxx_atomic_lock_impl<_Tp, _Sco>* __a, _Tp __value, memory_order) {\n  __a->__lock();\n  _Tp __old = __a->__a_value;\n  __a->__a_value = __value;\n  __a->__unlock();\n  return __old;\n}\n\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY\nbool __cxx_atomic_compare_exchange_strong(volatile __cxx_atomic_lock_impl<_Tp, _Sco>* __a,\n                                          _Tp* __expected, _Tp __value, memory_order, memory_order) {\n  __a->__lock();\n  _Tp __temp;\n  __cxx_atomic_assign_volatile(__temp, __a->__a_value);\n  bool __ret = __temp == *__expected;\n  if(__ret)\n    __cxx_atomic_assign_volatile(__a->__a_value, __value);\n  else\n    __cxx_atomic_assign_volatile(*__expected, __a->__a_value);\n  __a->__unlock();\n  return __ret;\n}\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY\nbool __cxx_atomic_compare_exchange_strong(__cxx_atomic_lock_impl<_Tp, _Sco>* __a,\n                                          _Tp* __expected, _Tp __value, memory_order, memory_order) {\n  __a->__lock();\n  bool __ret = __a->__a_value == *__expected;\n  if(__ret)\n    __a->__a_value = __value;\n  else\n    *__expected = __a->__a_value;\n  __a->__unlock();\n  return __ret;\n}\n\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY\nbool __cxx_atomic_compare_exchange_weak(volatile __cxx_atomic_lock_impl<_Tp, _Sco>* __a,\n                                        _Tp* __expected, _Tp __value, memory_order, memory_order) {\n  __a->__lock();\n  _Tp __temp;\n  __cxx_atomic_assign_volatile(__temp, __a->__a_value);\n  bool __ret = __temp == *__expected;\n  if(__ret)\n    __cxx_atomic_assign_volatile(__a->__a_value, __value);\n  else\n    __cxx_atomic_assign_volatile(*__expected, __a->__a_value);\n  __a->__unlock();\n  return __ret;\n}\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY\nbool __cxx_atomic_compare_exchange_weak(__cxx_atomic_lock_impl<_Tp, _Sco>* __a,\n                                        _Tp* __expected, _Tp __value, memory_order, memory_order) {\n  __a->__lock();\n  bool __ret = __a->__a_value == *__expected;\n  if(__ret)\n    __a->__a_value = __value;\n  else\n    *__expected = __a->__a_value;\n  __a->__unlock();\n  return __ret;\n}\n\ntemplate <typename _Tp, typename _Td, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY\n_Tp __cxx_atomic_fetch_add(volatile __cxx_atomic_lock_impl<_Tp, _Sco>* __a,\n                           _Td __delta, memory_order) {\n  __a->__lock();\n  _Tp __old;\n  __cxx_atomic_assign_volatile(__old, __a->__a_value);\n  __cxx_atomic_assign_volatile(__a->__a_value, _Tp(__old + __delta));\n  __a->__unlock();\n  return __old;\n}\ntemplate <typename _Tp, typename _Td, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY\n_Tp __cxx_atomic_fetch_add(__cxx_atomic_lock_impl<_Tp, _Sco>* __a,\n                           _Td __delta, memory_order) {\n  __a->__lock();\n  _Tp __old = __a->__a_value;\n  __a->__a_value += __delta;\n  __a->__unlock();\n  return __old;\n}\n\ntemplate <typename _Tp, typename _Td, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY\n_Tp* __cxx_atomic_fetch_add(volatile __cxx_atomic_lock_impl<_Tp*, _Sco>* __a,\n                           ptrdiff_t __delta, memory_order) {\n  __a->__lock();\n  _Tp* __old;\n  __cxx_atomic_assign_volatile(__old, __a->__a_value);\n  __cxx_atomic_assign_volatile(__a->__a_value, __old + __delta);\n  __a->__unlock();\n  return __old;\n}\ntemplate <typename _Tp, typename _Td, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY\n_Tp* __cxx_atomic_fetch_add(__cxx_atomic_lock_impl<_Tp*, _Sco>* __a,\n                            ptrdiff_t __delta, memory_order) {\n  __a->__lock();\n  _Tp* __old = __a->__a_value;\n  __a->__a_value += __delta;\n  __a->__unlock();\n  return __old;\n}\n\ntemplate <typename _Tp, typename _Td, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY\n_Tp __cxx_atomic_fetch_sub(volatile __cxx_atomic_lock_impl<_Tp, _Sco>* __a,\n                           _Td __delta, memory_order) {\n  __a->__lock();\n  _Tp __old;\n  __cxx_atomic_assign_volatile(__old, __a->__a_value);\n  __cxx_atomic_assign_volatile(__a->__a_value, _Tp(__old - __delta));\n  __a->__unlock();\n  return __old;\n}\ntemplate <typename _Tp, typename _Td, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY\n_Tp __cxx_atomic_fetch_sub(__cxx_atomic_lock_impl<_Tp, _Sco>* __a,\n                           _Td __delta, memory_order) {\n  __a->__lock();\n  _Tp __old = __a->__a_value;\n  __a->__a_value -= __delta;\n  __a->__unlock();\n  return __old;\n}\n\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY\n_Tp __cxx_atomic_fetch_and(volatile __cxx_atomic_lock_impl<_Tp, _Sco>* __a,\n                           _Tp __pattern, memory_order) {\n  __a->__lock();\n  _Tp __old;\n  __cxx_atomic_assign_volatile(__old, __a->__a_value);\n  __cxx_atomic_assign_volatile(__a->__a_value, _Tp(__old & __pattern));\n  __a->__unlock();\n  return __old;\n}\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY\n_Tp __cxx_atomic_fetch_and(__cxx_atomic_lock_impl<_Tp, _Sco>* __a,\n                           _Tp __pattern, memory_order) {\n  __a->__lock();\n  _Tp __old = __a->__a_value;\n  __a->__a_value &= __pattern;\n  __a->__unlock();\n  return __old;\n}\n\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY\n_Tp __cxx_atomic_fetch_or(volatile __cxx_atomic_lock_impl<_Tp, _Sco>* __a,\n                          _Tp __pattern, memory_order) {\n  __a->__lock();\n  _Tp __old;\n  __cxx_atomic_assign_volatile(__old, __a->__a_value);\n  __cxx_atomic_assign_volatile(__a->__a_value, _Tp(__old | __pattern));\n  __a->__unlock();\n  return __old;\n}\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY\n_Tp __cxx_atomic_fetch_or(__cxx_atomic_lock_impl<_Tp, _Sco>* __a,\n                          _Tp __pattern, memory_order) {\n  __a->__lock();\n  _Tp __old = __a->__a_value;\n  __a->__a_value |= __pattern;\n  __a->__unlock();\n  return __old;\n}\n\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY\n_Tp __cxx_atomic_fetch_xor(volatile __cxx_atomic_lock_impl<_Tp, _Sco>* __a,\n                           _Tp __pattern, memory_order) {\n  __a->__lock();\n  _Tp __old;\n  __cxx_atomic_assign_volatile(__old, __a->__a_value);\n  __cxx_atomic_assign_volatile(__a->__a_value, _Tp(__old ^ __pattern));\n  __a->__unlock();\n  return __old;\n}\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY\n_Tp __cxx_atomic_fetch_xor(__cxx_atomic_lock_impl<_Tp, _Sco>* __a,\n                           _Tp __pattern, memory_order) {\n  __a->__lock();\n  _Tp __old = __a->__a_value;\n  __a->__a_value ^= __pattern;\n  __a->__unlock();\n  return __old;\n}\n\n#if defined(_LIBCUDACXX_ATOMIC_ALWAYS_LOCK_FREE)\n\ntemplate<typename _Tp> struct __cxx_is_always_lock_free {\n    enum { __value = _LIBCUDACXX_ATOMIC_ALWAYS_LOCK_FREE(sizeof(_Tp), 0) }; };\n\n#else\n\ntemplate<typename _Tp> struct __cxx_is_always_lock_free {\n    enum { __value = sizeof(_Tp) <= 8 }; };\n\n#endif // defined(_LIBCUDACXX_ATOMIC_ALWAYS_LOCK_FREE)\n\ntemplate <typename _Tp, int _Sco>\nstruct __cxx_atomic_impl_conditional {\n    using type = __conditional_t<__cxx_is_always_lock_free<_Tp>::__value,\n                                                __cxx_atomic_base_impl<_Tp, _Sco>,\n                                                __cxx_atomic_lock_impl<_Tp, _Sco> >;\n};\n\ntemplate <typename _Tp, int _Sco,\n          typename _Base = typename __cxx_atomic_impl_conditional<_Tp, _Sco>::type >\n#else\ntemplate <typename _Tp, int _Sco,\n          typename _Base = __cxx_atomic_base_impl<_Tp, _Sco> >\n#endif //_LIBCUDACXX_ATOMIC_ONLY_USE_BUILTINS\nstruct __cxx_atomic_impl : public _Base {\n  __cxx_atomic_impl() noexcept = default;\n  _LIBCUDACXX_INLINE_VISIBILITY constexpr explicit __cxx_atomic_impl(_Tp value) noexcept\n    : _Base(value) {}\n};\n\n\ntemplate<int _Sco, typename _Tp = int>\n_LIBCUDACXX_INLINE_VISIBILITY\n__cxx_atomic_impl<_Tp, _Sco>* __cxx_atomic_rebind(_Tp* __inst) {\n    static_assert(sizeof(__cxx_atomic_impl<_Tp, _Sco>) == sizeof(_Tp),\"\");\n    static_assert(alignof(__cxx_atomic_impl<_Tp, _Sco>) == alignof(_Tp),\"\");\n    return (__cxx_atomic_impl<_Tp, _Sco>*)__inst;\n}\n\ntemplate <typename _Tp, int _Sco>\nusing __cxx_atomic_ref_impl = __cxx_atomic_ref_base_impl<_Tp, _Sco>;\n\n#ifdef _LIBCUDACXX_HAS_NO_THREAD_CONTENTION_TABLE\n\ntemplate <class _Ty, class _Tp = __detail::__cxx_atomic_underlying_t<_Ty>, int _Sco = _Ty::__sco>\nstruct __cxx_atomic_poll_tester {\n    _Ty const volatile* __a;\n    _Tp __val;\n    memory_order __order;\n\n    _LIBCUDACXX_INLINE_VISIBILITY __cxx_atomic_poll_tester(_Ty const volatile* __a_, _Tp __val_, memory_order __order_)\n      : __a(__a_)\n      , __val(__val_)\n      , __order(__order_)\n    {}\n\n    _LIBCUDACXX_INLINE_VISIBILITY bool operator()() const {\n      return !(__cxx_atomic_load(__a, __order) == __val);\n    }\n};\n\ntemplate <class _Ty, class _Tp = __detail::__cxx_atomic_underlying_t<_Ty>, int _Sco = _Ty::__sco>\n_LIBCUDACXX_INLINE_VISIBILITY void __cxx_atomic_try_wait_slow_fallback(_Ty const volatile* __a, _Tp __val, memory_order __order) {\n    __libcpp_thread_poll_with_backoff(__cxx_atomic_poll_tester<_Ty>(__a, __val, __order));\n}\n\n#endif\n\n#ifdef _LIBCUDACXX_HAS_PLATFORM_WAIT\n\ntemplate <class _Tp, int _Sco, __enable_if_t<!__libcpp_platform_wait_uses_type<_Tp>::__value, int> = 1>\n_LIBCUDACXX_INLINE_VISIBILITY void __cxx_atomic_notify_all(__cxx_atomic_impl<_Tp, _Sco> const volatile* __a) {\n#ifndef _LIBCUDACXX_HAS_NO_THREAD_CONTENTION_TABLE\n    auto * const __c = __libcpp_contention_state(__a);\n    __cxx_atomic_fetch_add(__cxx_atomic_rebind<_Sco>(&__c->__version), (__libcpp_platform_wait_t)1, memory_order_relaxed);\n    __cxx_atomic_thread_fence(memory_order_seq_cst);\n    if (0 != __cxx_atomic_exchange(__cxx_atomic_rebind<_Sco>(&__c->__waiters), (ptrdiff_t)0, memory_order_relaxed))\n        __libcpp_platform_wake(&__c->__version, true);\n#endif\n}\ntemplate <class _Tp, int _Sco, __enable_if_t<!__libcpp_platform_wait_uses_type<_Tp>::__value, int> = 1>\n_LIBCUDACXX_INLINE_VISIBILITY void __cxx_atomic_notify_one(__cxx_atomic_impl<_Tp, _Sco> const volatile* __a) {\n    __cxx_atomic_notify_all(__a);\n}\ntemplate <class _Ty, class _Tp = __detail::__cxx_atomic_underlying_t<_Ty>, int _Sco = _Ty::__sco, __enable_if_t<!__libcpp_platform_wait_uses_type<_Tp>::__value, int> = 1>\n_LIBCUDACXX_INLINE_VISIBILITY void __cxx_atomic_try_wait_slow(_Ty const volatile* __a, _Tp const __val, memory_order __order) {\n#ifndef _LIBCUDACXX_HAS_NO_THREAD_CONTENTION_TABLE\n    auto * const __c = __libcpp_contention_state(__a);\n    __cxx_atomic_store(__cxx_atomic_rebind<_Sco>(&__c->__waiters), (ptrdiff_t)1, memory_order_relaxed);\n    __cxx_atomic_thread_fence(memory_order_seq_cst);\n    auto const __version = __cxx_atomic_load(__cxx_atomic_rebind<_Sco>(&__c->__version), memory_order_relaxed);\n    if (!__cxx_nonatomic_compare_equal(__cxx_atomic_load(__a, __order), __val))\n        return;\n    if(sizeof(__libcpp_platform_wait_t) < 8) {\n        constexpr timespec __timeout = { 2, 0 }; // Hedge on rare 'int version' aliasing.\n        __libcpp_platform_wait(&__c->__version, __version, &__timeout);\n    }\n    else\n        __libcpp_platform_wait(&__c->__version, __version, nullptr);\n#else\n    __cxx_atomic_try_wait_slow_fallback(__a, __val, __order);\n#endif // _LIBCUDACXX_HAS_NO_THREAD_CONTENTION_TABLE\n}\n\ntemplate <class _Tp, int _Sco, __enable_if_t<__libcpp_platform_wait_uses_type<_Tp>::__value, int> = 1>\n_LIBCUDACXX_INLINE_VISIBILITY void __cxx_atomic_try_wait_slow(__cxx_atomic_impl<_Tp, _Sco> const volatile* __a, _Tp __val, memory_order) {\n#ifndef _LIBCUDACXX_HAS_NO_THREAD_CONTENTION_TABLE\n    auto * const __c = __libcpp_contention_state(__a);\n    __cxx_atomic_fetch_add(__cxx_atomic_rebind<_Sco>(&__c->__waiters), (ptrdiff_t)1, memory_order_relaxed);\n    __cxx_atomic_thread_fence(memory_order_seq_cst);\n#endif\n    __libcpp_platform_wait((_Tp*)__a, __val, nullptr);\n#ifndef _LIBCUDACXX_HAS_NO_THREAD_CONTENTION_TABLE\n    __cxx_atomic_fetch_sub(__cxx_atomic_rebind<_Sco>(&__c->__waiters), (ptrdiff_t)1, memory_order_relaxed);\n#endif\n}\ntemplate <class _Tp, int _Sco, __enable_if_t<__libcpp_platform_wait_uses_type<_Tp>::__value, int> = 1>\n_LIBCUDACXX_INLINE_VISIBILITY void __cxx_atomic_notify_all(__cxx_atomic_impl<_Tp, _Sco> const volatile* __a) {\n#ifndef _LIBCUDACXX_HAS_NO_THREAD_CONTENTION_TABLE\n    auto * const __c = __libcpp_contention_state(__a);\n    __cxx_atomic_thread_fence(memory_order_seq_cst);\n    if (0 != __cxx_atomic_load(__cxx_atomic_rebind<_Sco>(&__c->__waiters), memory_order_relaxed))\n#endif\n        __libcpp_platform_wake((_Tp*)__a, true);\n}\ntemplate <class _Tp, int _Sco, __enable_if_t<__libcpp_platform_wait_uses_type<_Tp>::__value, int> = 1>\n_LIBCUDACXX_INLINE_VISIBILITY void __cxx_atomic_notify_one(__cxx_atomic_impl<_Tp, _Sco> const volatile* __a) {\n#ifndef _LIBCUDACXX_HAS_NO_THREAD_CONTENTION_TABLE\n    auto * const __c = __libcpp_contention_state(__a);\n    __cxx_atomic_thread_fence(memory_order_seq_cst);\n    if (0 != __cxx_atomic_load(__cxx_atomic_rebind<_Sco>(&__c->__waiters), memory_order_relaxed))\n#endif\n        __libcpp_platform_wake((_Tp*)__a, false);\n}\n\n#elif !defined(_LIBCUDACXX_HAS_NO_THREAD_CONTENTION_TABLE)\n\ntemplate <class _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY void __cxx_atomic_notify_all(__cxx_atomic_impl<_Tp, _Sco> const volatile* __a) {\n    auto * const __c = __libcpp_contention_state(__a);\n    __cxx_atomic_thread_fence(memory_order_seq_cst);\n    if(0 == __cxx_atomic_load(__cxx_atomic_rebind<_Sco>(&__c->__credit), memory_order_relaxed))\n        return;\n    if(0 != __cxx_atomic_exchange(__cxx_atomic_rebind<_Sco>(&__c->__credit), (ptrdiff_t)0, memory_order_relaxed)) {\n        __libcpp_mutex_lock(&__c->__mutex);\n        __libcpp_mutex_unlock(&__c->__mutex);\n        __libcpp_condvar_broadcast(&__c->__condvar);\n    }\n}\ntemplate <class _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY void __cxx_atomic_notify_one(__cxx_atomic_impl<_Tp, _Sco> const volatile* __a) {\n    __cxx_atomic_notify_all(__a);\n}\ntemplate <class _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY void __cxx_atomic_try_wait_slow(__cxx_atomic_impl<_Tp, _Sco> const volatile* __a, _Tp const __val, memory_order __order) {\n    auto * const __c = __libcpp_contention_state(__a);\n    __libcpp_mutex_lock(&__c->__mutex);\n    __cxx_atomic_store(__cxx_atomic_rebind<_Sco>(&__c->__credit), (ptrdiff_t)1, memory_order_relaxed);\n    __cxx_atomic_thread_fence(memory_order_seq_cst);\n    if (__cxx_nonatomic_compare_equal(__cxx_atomic_load(__a, __order), __val))\n        __libcpp_condvar_wait(&__c->__condvar, &__c->__mutex);\n    __libcpp_mutex_unlock(&__c->__mutex);\n}\n\n#else\n\ntemplate<typename T>\nstruct __atomic_wait_and_notify_supported\n#if defined(__CUDA_MINIMUM_ARCH__) && __CUDA_MINIMUM_ARCH__ < 700\n    : false_type\n#else\n    : true_type\n#endif\n{};\n\ntemplate <class _Ty, class _Tp = __detail::__cxx_atomic_underlying_t<_Ty>>\n_LIBCUDACXX_INLINE_VISIBILITY void __cxx_atomic_try_wait_slow(_Ty const volatile* __a, _Tp __val, memory_order __order) {\n    static_assert(__atomic_wait_and_notify_supported<_Tp>::value, \"atomic wait operations are unsupported on Pascal\");\n    __cxx_atomic_try_wait_slow_fallback(__a, __val, __order);\n}\n\ntemplate <class _Ty, class _Tp = __detail::__cxx_atomic_underlying_t<_Ty>>\n_LIBCUDACXX_INLINE_VISIBILITY void __cxx_atomic_notify_one(_Ty const volatile*) {\n    static_assert(__atomic_wait_and_notify_supported<_Tp>::value, \"atomic notify-one operations are unsupported on Pascal\");\n}\n\ntemplate <class _Ty, class _Tp = __detail::__cxx_atomic_underlying_t<_Ty>>\n_LIBCUDACXX_INLINE_VISIBILITY void __cxx_atomic_notify_all(_Ty const volatile*) {\n    static_assert(__atomic_wait_and_notify_supported<_Tp>::value, \"atomic notify-all operations are unsupported on Pascal\");\n}\n\n#endif // _LIBCUDACXX_HAS_PLATFORM_WAIT || !defined(_LIBCUDACXX_HAS_NO_THREAD_CONTENTION_TABLE)\n\ntemplate <class _Ty, class _Tp = __detail::__cxx_atomic_underlying_t<_Ty>>\n_LIBCUDACXX_INLINE_VISIBILITY void __cxx_atomic_wait(_Ty const volatile* __a, _Tp const __val, memory_order __order) {\n    for(int __i = 0; __i < _LIBCUDACXX_POLLING_COUNT; ++__i) {\n        if(!__cxx_nonatomic_compare_equal(__cxx_atomic_load(__a, __order), __val))\n            return;\n        if(__i < 12)\n            __libcpp_thread_yield_processor();\n        else\n            __libcpp_thread_yield();\n    }\n    while(__cxx_nonatomic_compare_equal(__cxx_atomic_load(__a, __order), __val))\n        __cxx_atomic_try_wait_slow(__a, __val, __order);\n}\n\ntemplate <class _Tp, typename _Storage>\nstruct __atomic_base_storage {\n    mutable _Storage __a_;\n\n    __atomic_base_storage() = default;\n    __atomic_base_storage(const __atomic_base_storage&) = default;\n    __atomic_base_storage(__atomic_base_storage&&) = default;\n\n    __atomic_base_storage& operator=(const __atomic_base_storage&) = default;\n    __atomic_base_storage& operator=(__atomic_base_storage&&) = default;\n\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    __atomic_base_storage(_Storage&& __a) noexcept : __a_(_CUDA_VSTD::forward<_Storage>(__a)) {}\n};\n\ntemplate <class _Tp, bool _Cq, typename _Storage>\nstruct __atomic_base_core : public __atomic_base_storage<_Tp, _Storage>{\n    __atomic_base_core() = default;\n    __atomic_base_core(const __atomic_base_core&) = delete;\n    __atomic_base_core(__atomic_base_core&&) = delete;\n\n    __atomic_base_core& operator=(const __atomic_base_core&) = delete;\n    __atomic_base_core& operator=(__atomic_base_core&&) = delete;\n\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    __atomic_base_core(_Storage&& __a) noexcept : __atomic_base_storage<_Tp, _Storage>(_CUDA_VSTD::forward<_Storage>(__a)) {}\n\n#if defined(_LIBCUDACXX_ATOMIC_ALWAYS_LOCK_FREE)\n    static constexpr bool is_always_lock_free = _LIBCUDACXX_ATOMIC_ALWAYS_LOCK_FREE(sizeof(_Tp), 0);\n#endif // defined(_LIBCUDACXX_ATOMIC_ALWAYS_LOCK_FREE)\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool is_lock_free() const volatile noexcept\n        {return _LIBCUDACXX_ATOMIC_IS_LOCK_FREE(sizeof(_Tp));}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool is_lock_free() const noexcept\n        {return static_cast<__atomic_base_core const volatile*>(this)->is_lock_free();}\n    _LIBCUDACXX_INLINE_VISIBILITY\n\n    void store(_Tp __d, memory_order __m = memory_order_seq_cst) volatile noexcept\n      _LIBCUDACXX_CHECK_STORE_MEMORY_ORDER(__m)\n        {__cxx_atomic_store(&this->__a_, __d, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    void store(_Tp __d, memory_order __m = memory_order_seq_cst) noexcept\n      _LIBCUDACXX_CHECK_STORE_MEMORY_ORDER(__m)\n        {__cxx_atomic_store(&this->__a_, __d, __m);}\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp load(memory_order __m = memory_order_seq_cst) const volatile noexcept\n      _LIBCUDACXX_CHECK_LOAD_MEMORY_ORDER(__m)\n        {return __cxx_atomic_load(&this->__a_, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp load(memory_order __m = memory_order_seq_cst) const noexcept\n      _LIBCUDACXX_CHECK_LOAD_MEMORY_ORDER(__m)\n        {return __cxx_atomic_load(&this->__a_, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    operator _Tp() const volatile noexcept {return load();}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    operator _Tp() const noexcept          {return load();}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp exchange(_Tp __d, memory_order __m = memory_order_seq_cst) volatile noexcept\n        {return __cxx_atomic_exchange(&this->__a_, __d, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp exchange(_Tp __d, memory_order __m = memory_order_seq_cst) noexcept\n        {return __cxx_atomic_exchange(&this->__a_, __d, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool compare_exchange_weak(_Tp& __e, _Tp __d,\n                               memory_order __s, memory_order __f) volatile noexcept\n      _LIBCUDACXX_CHECK_EXCHANGE_MEMORY_ORDER(__s, __f)\n        {return __cxx_atomic_compare_exchange_weak(&this->__a_, &__e, __d, __s, __f);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool compare_exchange_weak(_Tp& __e, _Tp __d,\n                               memory_order __s, memory_order __f) noexcept\n      _LIBCUDACXX_CHECK_EXCHANGE_MEMORY_ORDER(__s, __f)\n        {return __cxx_atomic_compare_exchange_weak(&this->__a_, &__e, __d, __s, __f);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool compare_exchange_strong(_Tp& __e, _Tp __d,\n                                 memory_order __s, memory_order __f) volatile noexcept\n      _LIBCUDACXX_CHECK_EXCHANGE_MEMORY_ORDER(__s, __f)\n        {return __cxx_atomic_compare_exchange_strong(&this->__a_, &__e, __d, __s, __f);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool compare_exchange_strong(_Tp& __e, _Tp __d,\n                                 memory_order __s, memory_order __f) noexcept\n      _LIBCUDACXX_CHECK_EXCHANGE_MEMORY_ORDER(__s, __f)\n        {return __cxx_atomic_compare_exchange_strong(&this->__a_, &__e, __d, __s, __f);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool compare_exchange_weak(_Tp& __e, _Tp __d,\n                              memory_order __m = memory_order_seq_cst) volatile noexcept {\n        if (memory_order_acq_rel == __m)\n            return __cxx_atomic_compare_exchange_weak(&this->__a_, &__e, __d, __m, memory_order_acquire);\n        else if (memory_order_release == __m)\n            return __cxx_atomic_compare_exchange_weak(&this->__a_, &__e, __d, __m, memory_order_relaxed);\n        else\n            return __cxx_atomic_compare_exchange_weak(&this->__a_, &__e, __d, __m, __m);\n    }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool compare_exchange_weak(_Tp& __e, _Tp __d,\n                               memory_order __m = memory_order_seq_cst) noexcept {\n        if(memory_order_acq_rel == __m)\n            return __cxx_atomic_compare_exchange_weak(&this->__a_, &__e, __d, __m, memory_order_acquire);\n        else if(memory_order_release == __m)\n            return __cxx_atomic_compare_exchange_weak(&this->__a_, &__e, __d, __m, memory_order_relaxed);\n        else\n            return __cxx_atomic_compare_exchange_weak(&this->__a_, &__e, __d, __m, __m);\n    }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool compare_exchange_strong(_Tp& __e, _Tp __d,\n                              memory_order __m = memory_order_seq_cst) volatile noexcept {\n        if (memory_order_acq_rel == __m)\n            return __cxx_atomic_compare_exchange_strong(&this->__a_, &__e, __d, __m, memory_order_acquire);\n        else if (memory_order_release == __m)\n            return __cxx_atomic_compare_exchange_strong(&this->__a_, &__e, __d, __m, memory_order_relaxed);\n        else\n            return __cxx_atomic_compare_exchange_strong(&this->__a_, &__e, __d, __m, __m);\n    }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool compare_exchange_strong(_Tp& __e, _Tp __d,\n                                 memory_order __m = memory_order_seq_cst) noexcept {\n        if (memory_order_acq_rel == __m)\n            return __cxx_atomic_compare_exchange_strong(&this->__a_, &__e, __d, __m, memory_order_acquire);\n        else if (memory_order_release == __m)\n            return __cxx_atomic_compare_exchange_strong(&this->__a_, &__e, __d, __m, memory_order_relaxed);\n        else\n            return __cxx_atomic_compare_exchange_strong(&this->__a_, &__e, __d, __m, __m);\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY void wait(_Tp __v, memory_order __m = memory_order_seq_cst) const volatile noexcept\n        {__cxx_atomic_wait(&this->__a_, __v, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY void wait(_Tp __v, memory_order __m = memory_order_seq_cst) const noexcept\n        {__cxx_atomic_wait(&this->__a_, __v, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY void notify_one() volatile noexcept\n        {__cxx_atomic_notify_one(&this->__a_);}\n    _LIBCUDACXX_INLINE_VISIBILITY void notify_one() noexcept\n        {__cxx_atomic_notify_one(&this->__a_);}\n    _LIBCUDACXX_INLINE_VISIBILITY void notify_all() volatile noexcept\n        {__cxx_atomic_notify_all(&this->__a_);}\n    _LIBCUDACXX_INLINE_VISIBILITY void notify_all() noexcept\n        {__cxx_atomic_notify_all(&this->__a_);}\n};\n\ntemplate <class _Tp, typename _Storage>\nstruct __atomic_base_core<_Tp, true, _Storage> : public __atomic_base_storage<_Tp, _Storage>{\n    __atomic_base_core() = default;\n    __atomic_base_core(const __atomic_base_core&) = default;\n    __atomic_base_core(__atomic_base_core&&) = default;\n\n    __atomic_base_core& operator=(const __atomic_base_core&) = default;\n    __atomic_base_core& operator=(__atomic_base_core&&) = default;\n\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    __atomic_base_core(_Storage&& __a) noexcept : __atomic_base_storage<_Tp, _Storage>(_CUDA_VSTD::forward<_Storage>(__a)) {}\n\n#if defined(_LIBCUDACXX_ATOMIC_ALWAYS_LOCK_FREE)\n    static constexpr bool is_always_lock_free = _LIBCUDACXX_ATOMIC_ALWAYS_LOCK_FREE(sizeof(_Tp), 0);\n#endif // defined(_LIBCUDACXX_ATOMIC_ALWAYS_LOCK_FREE)\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool is_lock_free() const volatile noexcept\n        {return _LIBCUDACXX_ATOMIC_IS_LOCK_FREE(sizeof(_Tp));}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool is_lock_free() const noexcept\n        {return static_cast<__atomic_base_core const volatile*>(this)->is_lock_free();}\n    _LIBCUDACXX_INLINE_VISIBILITY\n\n    void store(_Tp __d, memory_order __m = memory_order_seq_cst) const volatile noexcept\n      _LIBCUDACXX_CHECK_STORE_MEMORY_ORDER(__m)\n        {__cxx_atomic_store(&this->__a_, __d, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    void store(_Tp __d, memory_order __m = memory_order_seq_cst) const noexcept\n      _LIBCUDACXX_CHECK_STORE_MEMORY_ORDER(__m)\n        {__cxx_atomic_store(&this->__a_, __d, __m);}\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp load(memory_order __m = memory_order_seq_cst) const volatile noexcept\n      _LIBCUDACXX_CHECK_LOAD_MEMORY_ORDER(__m)\n        {return __cxx_atomic_load(&this->__a_, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp load(memory_order __m = memory_order_seq_cst) const noexcept\n      _LIBCUDACXX_CHECK_LOAD_MEMORY_ORDER(__m)\n        {return __cxx_atomic_load(&this->__a_, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    operator _Tp() const volatile noexcept {return load();}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    operator _Tp() const noexcept          {return load();}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp exchange(_Tp __d, memory_order __m = memory_order_seq_cst) const volatile noexcept\n        {return __cxx_atomic_exchange(&this->__a_, __d, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp exchange(_Tp __d, memory_order __m = memory_order_seq_cst) const noexcept\n        {return __cxx_atomic_exchange(&this->__a_, __d, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool compare_exchange_weak(_Tp& __e, _Tp __d,\n                               memory_order __s, memory_order __f) const volatile noexcept\n      _LIBCUDACXX_CHECK_EXCHANGE_MEMORY_ORDER(__s, __f)\n        {return __cxx_atomic_compare_exchange_weak(&this->__a_, &__e, __d, __s, __f);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool compare_exchange_weak(_Tp& __e, _Tp __d,\n                               memory_order __s, memory_order __f) const noexcept\n      _LIBCUDACXX_CHECK_EXCHANGE_MEMORY_ORDER(__s, __f)\n        {return __cxx_atomic_compare_exchange_weak(&this->__a_, &__e, __d, __s, __f);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool compare_exchange_strong(_Tp& __e, _Tp __d,\n                                 memory_order __s, memory_order __f) const volatile noexcept\n      _LIBCUDACXX_CHECK_EXCHANGE_MEMORY_ORDER(__s, __f)\n        {return __cxx_atomic_compare_exchange_strong(&this->__a_, &__e, __d, __s, __f);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool compare_exchange_strong(_Tp& __e, _Tp __d,\n                                 memory_order __s, memory_order __f) const noexcept\n      _LIBCUDACXX_CHECK_EXCHANGE_MEMORY_ORDER(__s, __f)\n        {return __cxx_atomic_compare_exchange_strong(&this->__a_, &__e, __d, __s, __f);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool compare_exchange_weak(_Tp& __e, _Tp __d,\n                              memory_order __m = memory_order_seq_cst) const volatile noexcept {\n        if (memory_order_acq_rel == __m)\n            return __cxx_atomic_compare_exchange_weak(&this->__a_, &__e, __d, __m, memory_order_acquire);\n        else if (memory_order_release == __m)\n            return __cxx_atomic_compare_exchange_weak(&this->__a_, &__e, __d, __m, memory_order_relaxed);\n        else\n            return __cxx_atomic_compare_exchange_weak(&this->__a_, &__e, __d, __m, __m);\n    }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool compare_exchange_weak(_Tp& __e, _Tp __d,\n                               memory_order __m = memory_order_seq_cst) const noexcept {\n        if(memory_order_acq_rel == __m)\n            return __cxx_atomic_compare_exchange_weak(&this->__a_, &__e, __d, __m, memory_order_acquire);\n        else if(memory_order_release == __m)\n            return __cxx_atomic_compare_exchange_weak(&this->__a_, &__e, __d, __m, memory_order_relaxed);\n        else\n            return __cxx_atomic_compare_exchange_weak(&this->__a_, &__e, __d, __m, __m);\n    }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool compare_exchange_strong(_Tp& __e, _Tp __d,\n                              memory_order __m = memory_order_seq_cst) const volatile noexcept {\n        if (memory_order_acq_rel == __m)\n            return __cxx_atomic_compare_exchange_strong(&this->__a_, &__e, __d, __m, memory_order_acquire);\n        else if (memory_order_release == __m)\n            return __cxx_atomic_compare_exchange_strong(&this->__a_, &__e, __d, __m, memory_order_relaxed);\n        else\n            return __cxx_atomic_compare_exchange_strong(&this->__a_, &__e, __d, __m, __m);\n    }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool compare_exchange_strong(_Tp& __e, _Tp __d,\n                                 memory_order __m = memory_order_seq_cst) const noexcept {\n        if (memory_order_acq_rel == __m)\n            return __cxx_atomic_compare_exchange_strong(&this->__a_, &__e, __d, __m, memory_order_acquire);\n        else if (memory_order_release == __m)\n            return __cxx_atomic_compare_exchange_strong(&this->__a_, &__e, __d, __m, memory_order_relaxed);\n        else\n            return __cxx_atomic_compare_exchange_strong(&this->__a_, &__e, __d, __m, __m);\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY void wait(_Tp __v, memory_order __m = memory_order_seq_cst) const volatile noexcept\n        {__cxx_atomic_wait(&this->__a_, __v, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY void wait(_Tp __v, memory_order __m = memory_order_seq_cst) const noexcept\n        {__cxx_atomic_wait(&this->__a_, __v, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY void notify_one() const volatile noexcept\n        {__cxx_atomic_notify_one(&this->__a_);}\n    _LIBCUDACXX_INLINE_VISIBILITY void notify_one() const noexcept\n        {__cxx_atomic_notify_one(&this->__a_);}\n    _LIBCUDACXX_INLINE_VISIBILITY void notify_all() const volatile noexcept\n        {__cxx_atomic_notify_all(&this->__a_);}\n    _LIBCUDACXX_INLINE_VISIBILITY void notify_all() const noexcept\n        {__cxx_atomic_notify_all(&this->__a_);}\n};\n\ntemplate <class _Tp, bool _Cq, typename _Storage>\nstruct __atomic_base_arithmetic : public __atomic_base_core<_Tp, _Cq, _Storage> {\n    __atomic_base_arithmetic() = default;\n    __atomic_base_arithmetic(const __atomic_base_arithmetic&) = delete;\n    __atomic_base_arithmetic(__atomic_base_arithmetic&&) = delete;\n\n    __atomic_base_arithmetic& operator=(const __atomic_base_arithmetic&) = delete;\n    __atomic_base_arithmetic& operator=(__atomic_base_arithmetic&&) = delete;\n\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    __atomic_base_arithmetic(_Storage&& __a) noexcept : __atomic_base_core<_Tp, _Cq, _Storage>(_CUDA_VSTD::forward<_Storage>(__a)) {}\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp fetch_add(_Tp __op, memory_order __m = memory_order_seq_cst) volatile noexcept\n        {return __cxx_atomic_fetch_add(&this->__a_, __op, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp fetch_add(_Tp __op, memory_order __m = memory_order_seq_cst) noexcept\n        {return __cxx_atomic_fetch_add(&this->__a_, __op, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp fetch_sub(_Tp __op, memory_order __m = memory_order_seq_cst) volatile noexcept\n        {return __cxx_atomic_fetch_sub(&this->__a_, __op, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp fetch_sub(_Tp __op, memory_order __m = memory_order_seq_cst) noexcept\n        {return __cxx_atomic_fetch_sub(&this->__a_, __op, __m);}\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator++(int) volatile noexcept      {return fetch_add(_Tp(1));}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator++(int) noexcept               {return fetch_add(_Tp(1));}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator--(int) volatile noexcept      {return fetch_sub(_Tp(1));}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator--(int) noexcept               {return fetch_sub(_Tp(1));}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator++() volatile noexcept         {return fetch_add(_Tp(1)) + _Tp(1);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator++() noexcept                  {return fetch_add(_Tp(1)) + _Tp(1);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator--() volatile noexcept         {return fetch_sub(_Tp(1)) - _Tp(1);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator--() noexcept                  {return fetch_sub(_Tp(1)) - _Tp(1);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator+=(_Tp __op) volatile noexcept {return fetch_add(__op) + __op;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator+=(_Tp __op) noexcept          {return fetch_add(__op) + __op;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator-=(_Tp __op) volatile noexcept {return fetch_sub(__op) - __op;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator-=(_Tp __op) noexcept          {return fetch_sub(__op) - __op;}\n};\n\ntemplate <class _Tp, typename _Storage>\nstruct __atomic_base_arithmetic<_Tp, true, _Storage> : public __atomic_base_core<_Tp, true, _Storage> {\n    __atomic_base_arithmetic() = default;\n    __atomic_base_arithmetic(const __atomic_base_arithmetic&) = default;\n    __atomic_base_arithmetic(__atomic_base_arithmetic&&) = default;\n\n    __atomic_base_arithmetic& operator=(const __atomic_base_arithmetic&) = default;\n    __atomic_base_arithmetic& operator=(__atomic_base_arithmetic&&) = default;\n\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    __atomic_base_arithmetic(_Storage&& __a) noexcept : __atomic_base_core<_Tp, true, _Storage>(_CUDA_VSTD::forward<_Storage>(__a)) {}\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp fetch_add(_Tp __op, memory_order __m = memory_order_seq_cst) const volatile noexcept\n        {return __cxx_atomic_fetch_add(&this->__a_, __op, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp fetch_add(_Tp __op, memory_order __m = memory_order_seq_cst) const noexcept\n        {return __cxx_atomic_fetch_add(&this->__a_, __op, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp fetch_sub(_Tp __op, memory_order __m = memory_order_seq_cst) const volatile noexcept\n        {return __cxx_atomic_fetch_sub(&this->__a_, __op, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp fetch_sub(_Tp __op, memory_order __m = memory_order_seq_cst) const noexcept\n        {return __cxx_atomic_fetch_sub(&this->__a_, __op, __m);}\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator++(int) const volatile noexcept      {return fetch_add(_Tp(1));}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator++(int) const noexcept               {return fetch_add(_Tp(1));}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator--(int) const volatile noexcept      {return fetch_sub(_Tp(1));}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator--(int) const noexcept               {return fetch_sub(_Tp(1));}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator++() const volatile noexcept         {return fetch_add(_Tp(1)) + _Tp(1);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator++() const noexcept                  {return fetch_add(_Tp(1)) + _Tp(1);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator--() const volatile noexcept         {return fetch_sub(_Tp(1)) - _Tp(1);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator--() const noexcept                  {return fetch_sub(_Tp(1)) - _Tp(1);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator+=(_Tp __op) const volatile noexcept {return fetch_add(__op) + __op;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator+=(_Tp __op) const noexcept          {return fetch_add(__op) + __op;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator-=(_Tp __op) const volatile noexcept {return fetch_sub(__op) - __op;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator-=(_Tp __op) const noexcept          {return fetch_sub(__op) - __op;}\n};\n\ntemplate <class _Tp, bool _Cq, typename _Storage>\nstruct __atomic_base_bitwise : public __atomic_base_arithmetic<_Tp, _Cq, _Storage> {\n    __atomic_base_bitwise() = default;\n    __atomic_base_bitwise(const __atomic_base_bitwise&) = delete;\n    __atomic_base_bitwise(__atomic_base_bitwise&&) = delete;\n\n    __atomic_base_bitwise& operator=(const __atomic_base_bitwise&) = delete;\n    __atomic_base_bitwise& operator=(__atomic_base_bitwise&&) = delete;\n\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    __atomic_base_bitwise(_Storage&& __a) noexcept : __atomic_base_arithmetic<_Tp, _Cq, _Storage>(_CUDA_VSTD::forward<_Storage>(__a)) {}\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp fetch_and(_Tp __op, memory_order __m = memory_order_seq_cst) volatile noexcept\n        {return __cxx_atomic_fetch_and(&this->__a_, __op, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp fetch_and(_Tp __op, memory_order __m = memory_order_seq_cst) noexcept\n        {return __cxx_atomic_fetch_and(&this->__a_, __op, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp fetch_or(_Tp __op, memory_order __m = memory_order_seq_cst) volatile noexcept\n        {return __cxx_atomic_fetch_or(&this->__a_, __op, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp fetch_or(_Tp __op, memory_order __m = memory_order_seq_cst) noexcept\n        {return __cxx_atomic_fetch_or(&this->__a_, __op, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp fetch_xor(_Tp __op, memory_order __m = memory_order_seq_cst) volatile noexcept\n        {return __cxx_atomic_fetch_xor(&this->__a_, __op, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp fetch_xor(_Tp __op, memory_order __m = memory_order_seq_cst) noexcept\n        {return __cxx_atomic_fetch_xor(&this->__a_, __op, __m);}\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator&=(_Tp __op) volatile noexcept {return fetch_and(__op) & __op;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator&=(_Tp __op) noexcept          {return fetch_and(__op) & __op;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator|=(_Tp __op) volatile noexcept {return fetch_or(__op) | __op;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator|=(_Tp __op) noexcept          {return fetch_or(__op) | __op;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator^=(_Tp __op) volatile noexcept {return fetch_xor(__op) ^ __op;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator^=(_Tp __op) noexcept          {return fetch_xor(__op) ^ __op;}\n};\n\ntemplate <class _Tp, typename _Storage>\nstruct __atomic_base_bitwise<_Tp, true, _Storage> : public __atomic_base_arithmetic<_Tp, true, _Storage> {\n    __atomic_base_bitwise() = default;\n    __atomic_base_bitwise(const __atomic_base_bitwise&) = default;\n    __atomic_base_bitwise(__atomic_base_bitwise&&) = default;\n\n    __atomic_base_bitwise& operator=(const __atomic_base_bitwise&) = default;\n    __atomic_base_bitwise& operator=(__atomic_base_bitwise&&) = default;\n\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    __atomic_base_bitwise(_Storage&& __a) noexcept : __atomic_base_arithmetic<_Tp, true, _Storage>(_CUDA_VSTD::forward<_Storage>(__a)) {}\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp fetch_and(_Tp __op, memory_order __m = memory_order_seq_cst) const volatile noexcept\n        {return __cxx_atomic_fetch_and(&this->__a_, __op, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp fetch_and(_Tp __op, memory_order __m = memory_order_seq_cst) const noexcept\n        {return __cxx_atomic_fetch_and(&this->__a_, __op, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp fetch_or(_Tp __op, memory_order __m = memory_order_seq_cst) const volatile noexcept\n        {return __cxx_atomic_fetch_or(&this->__a_, __op, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp fetch_or(_Tp __op, memory_order __m = memory_order_seq_cst) const noexcept\n        {return __cxx_atomic_fetch_or(&this->__a_, __op, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp fetch_xor(_Tp __op, memory_order __m = memory_order_seq_cst) const volatile noexcept\n        {return __cxx_atomic_fetch_xor(&this->__a_, __op, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp fetch_xor(_Tp __op, memory_order __m = memory_order_seq_cst) const noexcept\n        {return __cxx_atomic_fetch_xor(&this->__a_, __op, __m);}\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator&=(_Tp __op) const volatile noexcept {return fetch_and(__op) & __op;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator&=(_Tp __op) const noexcept          {return fetch_and(__op) & __op;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator|=(_Tp __op) const volatile noexcept {return fetch_or(__op) | __op;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator|=(_Tp __op) const noexcept          {return fetch_or(__op) | __op;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator^=(_Tp __op) const volatile noexcept {return fetch_xor(__op) ^ __op;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator^=(_Tp __op) const noexcept          {return fetch_xor(__op) ^ __op;}\n};\n\ntemplate <typename _Tp, bool _Cq, typename _Storage>\nusing __atomic_select_base = __conditional_t<is_floating_point<_Tp>::value,\n                                             __atomic_base_arithmetic<_Tp, _Cq, _Storage>,\n                                             __conditional_t<is_integral<_Tp>::value,\n                                                __atomic_base_bitwise<_Tp, _Cq, _Storage>,\n                                                __atomic_base_core<_Tp, _Cq, _Storage> >>;\n\ntemplate <typename _Tp, int _Sco = 0, typename _Base = __atomic_select_base<_Tp, false, __cxx_atomic_impl<_Tp, _Sco>>>\nstruct __atomic_base : public _Base {\n    __atomic_base() = default;\n    __atomic_base(const __atomic_base&) = delete;\n    __atomic_base(__atomic_base&&) = delete;\n\n    __atomic_base& operator=(const __atomic_base&) = delete;\n    __atomic_base& operator=(__atomic_base&&) = delete;\n\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    __atomic_base(const _Tp& __a) noexcept :\n        _Base(__cxx_atomic_impl<_Tp, _Sco>(__a)) {}\n};\n\ntemplate <typename _Tp, int _Sco = 0, typename _Base = __atomic_select_base<_Tp, true, __cxx_atomic_ref_impl<_Tp, _Sco>>>\nstruct __atomic_base_ref : public _Base {\n    __atomic_base_ref() = default;\n    __atomic_base_ref(const __atomic_base_ref&) = default;\n    __atomic_base_ref(__atomic_base_ref&&) = default;\n\n    __atomic_base_ref& operator=(const __atomic_base_ref&) = default;\n    __atomic_base_ref& operator=(__atomic_base_ref&&) = default;\n\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    __atomic_base_ref(_Tp& __a) noexcept :\n        _Base(__cxx_atomic_ref_impl<_Tp, _Sco>(__a)) {}\n};\n\n#if defined(_LIBCUDACXX_ATOMIC_ALWAYS_LOCK_FREE)\ntemplate <class _Tp, bool _Cq, typename _Storage>\nconstexpr bool __atomic_base_core<_Tp, _Cq, _Storage>::is_always_lock_free;\n#endif\n\n// atomic<T>\ntemplate <class _Tp>\nstruct atomic\n    : public __atomic_base<_Tp>\n{\n    typedef __atomic_base<_Tp> __base;\n    using value_type = _Tp;\n\n    atomic() noexcept = default;\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr atomic(_Tp __d) noexcept : __base(__d) {}\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator=(_Tp __d) volatile noexcept\n        {__base::store(__d); return __d;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator=(_Tp __d) noexcept\n        {__base::store(__d); return __d;}\n};\n\n// atomic<T*>\n\ntemplate <class _Tp>\nstruct atomic<_Tp*>\n    : public __atomic_base<_Tp*>\n{\n    typedef __atomic_base<_Tp*> __base;\n    using value_type = _Tp*;\n\n    atomic() noexcept = default;\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr atomic(_Tp* __d) noexcept : __base(__d) {}\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* operator=(_Tp* __d) volatile noexcept\n        {__base::store(__d); return __d;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* operator=(_Tp* __d) noexcept\n        {__base::store(__d); return __d;}\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* fetch_add(ptrdiff_t __op, memory_order __m = memory_order_seq_cst)\n                                                                        volatile noexcept\n        {return __cxx_atomic_fetch_add(&this->__a_, __op, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* fetch_add(ptrdiff_t __op, memory_order __m = memory_order_seq_cst)\n                                                                        noexcept\n        {return __cxx_atomic_fetch_add(&this->__a_, __op, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* fetch_sub(ptrdiff_t __op, memory_order __m = memory_order_seq_cst)\n                                                                        volatile noexcept\n        {return __cxx_atomic_fetch_sub(&this->__a_, __op, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* fetch_sub(ptrdiff_t __op, memory_order __m = memory_order_seq_cst)\n                                                                        noexcept\n        {return __cxx_atomic_fetch_sub(&this->__a_, __op, __m);}\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* operator++(int) volatile noexcept            {return fetch_add(1);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* operator++(int) noexcept                     {return fetch_add(1);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* operator--(int) volatile noexcept            {return fetch_sub(1);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* operator--(int) noexcept                     {return fetch_sub(1);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* operator++() volatile noexcept               {return fetch_add(1) + 1;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* operator++() noexcept                        {return fetch_add(1) + 1;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* operator--() volatile noexcept               {return fetch_sub(1) - 1;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* operator--() noexcept                        {return fetch_sub(1) - 1;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* operator+=(ptrdiff_t __op) volatile noexcept {return fetch_add(__op) + __op;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* operator+=(ptrdiff_t __op) noexcept          {return fetch_add(__op) + __op;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* operator-=(ptrdiff_t __op) volatile noexcept {return fetch_sub(__op) - __op;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* operator-=(ptrdiff_t __op) noexcept          {return fetch_sub(__op) - __op;}\n};\n\n// atomic_ref<T>\n\ntemplate <class _Tp>\n struct atomic_ref\n    : public __atomic_base_ref<_Tp>\n{\n    typedef __atomic_base_ref<_Tp> __base;\n    using value_type = _Tp;\n\n    static constexpr size_t required_alignment = sizeof(_Tp);\n\n    static constexpr bool is_always_lock_free = sizeof(_Tp) <= 8;\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    explicit atomic_ref(_Tp& __ref) : __base(__ref) {}\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator=(_Tp __v) const noexcept {__base::store(__v); return __v;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp operator=(_Tp __v) const volatile noexcept {__base::store(__v); return __v;}\n};\n\n// atomic_ref<T*>\n\ntemplate <class _Tp>\n struct atomic_ref<_Tp*>\n    : public __atomic_base_ref<_Tp*>\n{\n    typedef __atomic_base_ref<_Tp*> __base;\n    using value_type = _Tp*;\n\n    static constexpr size_t required_alignment = sizeof(_Tp*);\n\n    static constexpr bool is_always_lock_free = sizeof(_Tp*) <= 8;\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    explicit atomic_ref(_Tp*& __ref) : __base(__ref) {}\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* operator=(_Tp* __v) const noexcept {__base::store(__v); return __v;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* operator=(_Tp* __v) const volatile noexcept {__base::store(__v); return __v;}\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* fetch_add(ptrdiff_t __op, memory_order __m = memory_order_seq_cst)\n                                                                        const volatile noexcept\n        {return __cxx_atomic_fetch_add(&this->__a_, __op, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* fetch_add(ptrdiff_t __op, memory_order __m = memory_order_seq_cst)\n                                                                        const noexcept\n        {return __cxx_atomic_fetch_add(&this->__a_, __op, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* fetch_sub(ptrdiff_t __op, memory_order __m = memory_order_seq_cst)\n                                                                        const volatile noexcept\n        {return __cxx_atomic_fetch_sub(&this->__a_, __op, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* fetch_sub(ptrdiff_t __op, memory_order __m = memory_order_seq_cst)\n                                                                        const noexcept\n        {return __cxx_atomic_fetch_sub(&this->__a_, __op, __m);}\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* operator++(int) const volatile noexcept            {return fetch_add(1);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* operator++(int) const noexcept                     {return fetch_add(1);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* operator--(int) const volatile noexcept            {return fetch_sub(1);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* operator--(int) const noexcept                     {return fetch_sub(1);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* operator++() const volatile noexcept               {return fetch_add(1) + 1;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* operator++() const noexcept                        {return fetch_add(1) + 1;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* operator--() const volatile noexcept               {return fetch_sub(1) - 1;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* operator--() const noexcept                        {return fetch_sub(1) - 1;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* operator+=(ptrdiff_t __op) const volatile noexcept {return fetch_add(__op) + __op;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* operator+=(ptrdiff_t __op) const noexcept          {return fetch_add(__op) + __op;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* operator-=(ptrdiff_t __op) const volatile noexcept {return fetch_sub(__op) - __op;}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _Tp* operator-=(ptrdiff_t __op) const noexcept          {return fetch_sub(__op) - __op;}\n};\n\n// atomic_is_lock_free\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\nbool\natomic_is_lock_free(const volatile atomic<_Tp>* __o) noexcept\n{\n    return __o->is_lock_free();\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\nbool\natomic_is_lock_free(const atomic<_Tp>* __o) noexcept\n{\n    return __o->is_lock_free();\n}\n\n// atomic_init\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\nvoid\natomic_init(volatile atomic<_Tp>* __o, _Tp __d) noexcept\n{\n    __cxx_atomic_init(&__o->__a_, __d);\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\nvoid\natomic_init(atomic<_Tp>* __o, _Tp __d) noexcept\n{\n    __cxx_atomic_init(&__o->__a_, __d);\n}\n\n// atomic_store\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\nvoid\natomic_store(volatile atomic<_Tp>* __o, _Tp __d) noexcept\n{\n    __o->store(__d);\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\nvoid\natomic_store(atomic<_Tp>* __o, _Tp __d) noexcept\n{\n    __o->store(__d);\n}\n\n// atomic_store_explicit\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\nvoid\natomic_store_explicit(volatile atomic<_Tp>* __o, _Tp __d, memory_order __m) noexcept\n  _LIBCUDACXX_CHECK_STORE_MEMORY_ORDER(__m)\n{\n    __o->store(__d, __m);\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\nvoid\natomic_store_explicit(atomic<_Tp>* __o, _Tp __d, memory_order __m) noexcept\n  _LIBCUDACXX_CHECK_STORE_MEMORY_ORDER(__m)\n{\n    __o->store(__d, __m);\n}\n\n// atomic_load\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n_Tp\natomic_load(const volatile atomic<_Tp>* __o) noexcept\n{\n    return __o->load();\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n_Tp\natomic_load(const atomic<_Tp>* __o) noexcept\n{\n    return __o->load();\n}\n\n// atomic_load_explicit\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n_Tp\natomic_load_explicit(const volatile atomic<_Tp>* __o, memory_order __m) noexcept\n  _LIBCUDACXX_CHECK_LOAD_MEMORY_ORDER(__m)\n{\n    return __o->load(__m);\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n_Tp\natomic_load_explicit(const atomic<_Tp>* __o, memory_order __m) noexcept\n  _LIBCUDACXX_CHECK_LOAD_MEMORY_ORDER(__m)\n{\n    return __o->load(__m);\n}\n\n// atomic_exchange\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n_Tp\natomic_exchange(volatile atomic<_Tp>* __o, _Tp __d) noexcept\n{\n    return __o->exchange(__d);\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n_Tp\natomic_exchange(atomic<_Tp>* __o, _Tp __d) noexcept\n{\n    return __o->exchange(__d);\n}\n\n// atomic_exchange_explicit\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n_Tp\natomic_exchange_explicit(volatile atomic<_Tp>* __o, _Tp __d, memory_order __m) noexcept\n{\n    return __o->exchange(__d, __m);\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n_Tp\natomic_exchange_explicit(atomic<_Tp>* __o, _Tp __d, memory_order __m) noexcept\n{\n    return __o->exchange(__d, __m);\n}\n\n// atomic_compare_exchange_weak\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\nbool\natomic_compare_exchange_weak(volatile atomic<_Tp>* __o, _Tp* __e, _Tp __d) noexcept\n{\n    return __o->compare_exchange_weak(*__e, __d);\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\nbool\natomic_compare_exchange_weak(atomic<_Tp>* __o, _Tp* __e, _Tp __d) noexcept\n{\n    return __o->compare_exchange_weak(*__e, __d);\n}\n\n// atomic_compare_exchange_strong\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\nbool\natomic_compare_exchange_strong(volatile atomic<_Tp>* __o, _Tp* __e, _Tp __d) noexcept\n{\n    return __o->compare_exchange_strong(*__e, __d);\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\nbool\natomic_compare_exchange_strong(atomic<_Tp>* __o, _Tp* __e, _Tp __d) noexcept\n{\n    return __o->compare_exchange_strong(*__e, __d);\n}\n\n// atomic_compare_exchange_weak_explicit\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\nbool\natomic_compare_exchange_weak_explicit(volatile atomic<_Tp>* __o, _Tp* __e,\n                                      _Tp __d,\n                                      memory_order __s, memory_order __f) noexcept\n  _LIBCUDACXX_CHECK_EXCHANGE_MEMORY_ORDER(__s, __f)\n{\n    return __o->compare_exchange_weak(*__e, __d, __s, __f);\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\nbool\natomic_compare_exchange_weak_explicit(atomic<_Tp>* __o, _Tp* __e, _Tp __d,\n                                      memory_order __s, memory_order __f) noexcept\n  _LIBCUDACXX_CHECK_EXCHANGE_MEMORY_ORDER(__s, __f)\n{\n    return __o->compare_exchange_weak(*__e, __d, __s, __f);\n}\n\n// atomic_compare_exchange_strong_explicit\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\nbool\natomic_compare_exchange_strong_explicit(volatile atomic<_Tp>* __o,\n                                        _Tp* __e, _Tp __d,\n                                        memory_order __s, memory_order __f) noexcept\n  _LIBCUDACXX_CHECK_EXCHANGE_MEMORY_ORDER(__s, __f)\n{\n    return __o->compare_exchange_strong(*__e, __d, __s, __f);\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\nbool\natomic_compare_exchange_strong_explicit(atomic<_Tp>* __o, _Tp* __e,\n                                        _Tp __d,\n                                        memory_order __s, memory_order __f) noexcept\n  _LIBCUDACXX_CHECK_EXCHANGE_MEMORY_ORDER(__s, __f)\n{\n    return __o->compare_exchange_strong(*__e, __d, __s, __f);\n}\n\n// atomic_wait\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\nvoid atomic_wait(const volatile atomic<_Tp>* __o,\n                    typename atomic<_Tp>::value_type __v) noexcept\n{\n    return __o->wait(__v);\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\nvoid atomic_wait(const atomic<_Tp>* __o,\n                    typename atomic<_Tp>::value_type __v) noexcept\n{\n    return __o->wait(__v);\n}\n\n// atomic_wait_explicit\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\nvoid atomic_wait_explicit(const volatile atomic<_Tp>* __o,\n                            typename atomic<_Tp>::value_type __v,\n                            memory_order __m) noexcept\n  _LIBCUDACXX_CHECK_LOAD_MEMORY_ORDER(__m)\n{\n    return __o->wait(__v, __m);\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\nvoid atomic_wait_explicit(const atomic<_Tp>* __o,\n                            typename atomic<_Tp>::value_type __v,\n                            memory_order __m) noexcept\n  _LIBCUDACXX_CHECK_LOAD_MEMORY_ORDER(__m)\n{\n    return __o->wait(__v, __m);\n}\n\n// atomic_notify_one\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\nvoid atomic_notify_one(volatile atomic<_Tp>* __o) noexcept\n{\n    __o->notify_one();\n}\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\nvoid atomic_notify_one(atomic<_Tp>* __o) noexcept\n{\n    __o->notify_one();\n}\n\n// atomic_notify_one\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\nvoid atomic_notify_all(volatile atomic<_Tp>* __o) noexcept\n{\n    __o->notify_all();\n}\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\nvoid atomic_notify_all(atomic<_Tp>* __o) noexcept\n{\n    __o->notify_all();\n}\n\n// atomic_fetch_add\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n__enable_if_t\n<\n    (is_integral<_Tp>::value && !is_same<_Tp, bool>::value) || is_floating_point<_Tp>::value,\n    _Tp\n>\natomic_fetch_add(volatile atomic<_Tp>* __o, _Tp __op) noexcept\n{\n    return __o->fetch_add(__op);\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n__enable_if_t\n<\n    (is_integral<_Tp>::value && !is_same<_Tp, bool>::value) || is_floating_point<_Tp>::value,\n    _Tp\n>\natomic_fetch_add(atomic<_Tp>* __o, _Tp __op) noexcept\n{\n    return __o->fetch_add(__op);\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n_Tp*\natomic_fetch_add(volatile atomic<_Tp*>* __o, ptrdiff_t __op) noexcept\n{\n    return __o->fetch_add(__op);\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n_Tp*\natomic_fetch_add(atomic<_Tp*>* __o, ptrdiff_t __op) noexcept\n{\n    return __o->fetch_add(__op);\n}\n\n// atomic_fetch_add_explicit\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n__enable_if_t\n<\n    (is_integral<_Tp>::value && !is_same<_Tp, bool>::value) || is_floating_point<_Tp>::value,\n    _Tp\n>\natomic_fetch_add_explicit(volatile atomic<_Tp>* __o, _Tp __op, memory_order __m) noexcept\n{\n    return __o->fetch_add(__op, __m);\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n__enable_if_t\n<\n    (is_integral<_Tp>::value && !is_same<_Tp, bool>::value) || is_floating_point<_Tp>::value,\n    _Tp\n>\natomic_fetch_add_explicit(atomic<_Tp>* __o, _Tp __op, memory_order __m) noexcept\n{\n    return __o->fetch_add(__op, __m);\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n_Tp*\natomic_fetch_add_explicit(volatile atomic<_Tp*>* __o, ptrdiff_t __op,\n                          memory_order __m) noexcept\n{\n    return __o->fetch_add(__op, __m);\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n_Tp*\natomic_fetch_add_explicit(atomic<_Tp*>* __o, ptrdiff_t __op, memory_order __m) noexcept\n{\n    return __o->fetch_add(__op, __m);\n}\n\n// atomic_fetch_sub\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n__enable_if_t\n<\n    (is_integral<_Tp>::value && !is_same<_Tp, bool>::value) || is_floating_point<_Tp>::value,\n    _Tp\n>\natomic_fetch_sub(volatile atomic<_Tp>* __o, _Tp __op) noexcept\n{\n    return __o->fetch_sub(__op);\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n__enable_if_t\n<\n    (is_integral<_Tp>::value && !is_same<_Tp, bool>::value) || is_floating_point<_Tp>::value,\n    _Tp\n>\natomic_fetch_sub(atomic<_Tp>* __o, _Tp __op) noexcept\n{\n    return __o->fetch_sub(__op);\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n_Tp*\natomic_fetch_sub(volatile atomic<_Tp*>* __o, ptrdiff_t __op) noexcept\n{\n    return __o->fetch_sub(__op);\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n_Tp*\natomic_fetch_sub(atomic<_Tp*>* __o, ptrdiff_t __op) noexcept\n{\n    return __o->fetch_sub(__op);\n}\n\n// atomic_fetch_sub_explicit\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n__enable_if_t\n<\n    (is_integral<_Tp>::value && !is_same<_Tp, bool>::value) || is_floating_point<_Tp>::value,\n    _Tp\n>\natomic_fetch_sub_explicit(volatile atomic<_Tp>* __o, _Tp __op, memory_order __m) noexcept\n{\n    return __o->fetch_sub(__op, __m);\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n__enable_if_t\n<\n    (is_integral<_Tp>::value && !is_same<_Tp, bool>::value) || is_floating_point<_Tp>::value,\n    _Tp\n>\natomic_fetch_sub_explicit(atomic<_Tp>* __o, _Tp __op, memory_order __m) noexcept\n{\n    return __o->fetch_sub(__op, __m);\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n_Tp*\natomic_fetch_sub_explicit(volatile atomic<_Tp*>* __o, ptrdiff_t __op,\n                          memory_order __m) noexcept\n{\n    return __o->fetch_sub(__op, __m);\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n_Tp*\natomic_fetch_sub_explicit(atomic<_Tp*>* __o, ptrdiff_t __op, memory_order __m) noexcept\n{\n    return __o->fetch_sub(__op, __m);\n}\n\n// atomic_fetch_and\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n__enable_if_t\n<\n    is_integral<_Tp>::value && !is_same<_Tp, bool>::value,\n    _Tp\n>\natomic_fetch_and(volatile atomic<_Tp>* __o, _Tp __op) noexcept\n{\n    return __o->fetch_and(__op);\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n__enable_if_t\n<\n    is_integral<_Tp>::value && !is_same<_Tp, bool>::value,\n    _Tp\n>\natomic_fetch_and(atomic<_Tp>* __o, _Tp __op) noexcept\n{\n    return __o->fetch_and(__op);\n}\n\n// atomic_fetch_and_explicit\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n__enable_if_t\n<\n    is_integral<_Tp>::value && !is_same<_Tp, bool>::value,\n    _Tp\n>\natomic_fetch_and_explicit(volatile atomic<_Tp>* __o, _Tp __op, memory_order __m) noexcept\n{\n    return __o->fetch_and(__op, __m);\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n__enable_if_t\n<\n    is_integral<_Tp>::value && !is_same<_Tp, bool>::value,\n    _Tp\n>\natomic_fetch_and_explicit(atomic<_Tp>* __o, _Tp __op, memory_order __m) noexcept\n{\n    return __o->fetch_and(__op, __m);\n}\n\n// atomic_fetch_or\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n__enable_if_t\n<\n    is_integral<_Tp>::value && !is_same<_Tp, bool>::value,\n    _Tp\n>\natomic_fetch_or(volatile atomic<_Tp>* __o, _Tp __op) noexcept\n{\n    return __o->fetch_or(__op);\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n__enable_if_t\n<\n    is_integral<_Tp>::value && !is_same<_Tp, bool>::value,\n    _Tp\n>\natomic_fetch_or(atomic<_Tp>* __o, _Tp __op) noexcept\n{\n    return __o->fetch_or(__op);\n}\n\n// atomic_fetch_or_explicit\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n__enable_if_t\n<\n    is_integral<_Tp>::value && !is_same<_Tp, bool>::value,\n    _Tp\n>\natomic_fetch_or_explicit(volatile atomic<_Tp>* __o, _Tp __op, memory_order __m) noexcept\n{\n    return __o->fetch_or(__op, __m);\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n__enable_if_t\n<\n    is_integral<_Tp>::value && !is_same<_Tp, bool>::value,\n    _Tp\n>\natomic_fetch_or_explicit(atomic<_Tp>* __o, _Tp __op, memory_order __m) noexcept\n{\n    return __o->fetch_or(__op, __m);\n}\n\n// atomic_fetch_xor\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n__enable_if_t\n<\n    is_integral<_Tp>::value && !is_same<_Tp, bool>::value,\n    _Tp\n>\natomic_fetch_xor(volatile atomic<_Tp>* __o, _Tp __op) noexcept\n{\n    return __o->fetch_xor(__op);\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n__enable_if_t\n<\n    is_integral<_Tp>::value && !is_same<_Tp, bool>::value,\n    _Tp\n>\natomic_fetch_xor(atomic<_Tp>* __o, _Tp __op) noexcept\n{\n    return __o->fetch_xor(__op);\n}\n\n// atomic_fetch_xor_explicit\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n__enable_if_t\n<\n    is_integral<_Tp>::value && !is_same<_Tp, bool>::value,\n    _Tp\n>\natomic_fetch_xor_explicit(volatile atomic<_Tp>* __o, _Tp __op, memory_order __m) noexcept\n{\n    return __o->fetch_xor(__op, __m);\n}\n\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\n__enable_if_t\n<\n    is_integral<_Tp>::value && !is_same<_Tp, bool>::value,\n    _Tp\n>\natomic_fetch_xor_explicit(atomic<_Tp>* __o, _Tp __op, memory_order __m) noexcept\n{\n    return __o->fetch_xor(__op, __m);\n}\n\n// flag type and operations\n\ntypedef struct atomic_flag\n{\n    __cxx_atomic_impl<_LIBCUDACXX_ATOMIC_FLAG_TYPE, 0> __a_;\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool test(memory_order __m = memory_order_seq_cst) const volatile noexcept\n        {return _LIBCUDACXX_ATOMIC_FLAG_TYPE(true)==__cxx_atomic_load(&__a_, __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool test(memory_order __m = memory_order_seq_cst) const noexcept\n        {return _LIBCUDACXX_ATOMIC_FLAG_TYPE(true)==__cxx_atomic_load(&__a_, __m);}\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool test_and_set(memory_order __m = memory_order_seq_cst) volatile noexcept\n        {return __cxx_atomic_exchange(&__a_, _LIBCUDACXX_ATOMIC_FLAG_TYPE(true), __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool test_and_set(memory_order __m = memory_order_seq_cst) noexcept\n        {return __cxx_atomic_exchange(&__a_, _LIBCUDACXX_ATOMIC_FLAG_TYPE(true), __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    void clear(memory_order __m = memory_order_seq_cst) volatile noexcept\n        {__cxx_atomic_store(&__a_, _LIBCUDACXX_ATOMIC_FLAG_TYPE(false), __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    void clear(memory_order __m = memory_order_seq_cst) noexcept\n        {__cxx_atomic_store(&__a_, _LIBCUDACXX_ATOMIC_FLAG_TYPE(false), __m);}\n\n#if !defined(__CUDA_MINIMUM_ARCH__) || __CUDA_MINIMUM_ARCH__ >= 700\n    _LIBCUDACXX_INLINE_VISIBILITY\n    void wait(bool __v, memory_order __m = memory_order_seq_cst) const volatile noexcept\n        {__cxx_atomic_wait(&__a_, _LIBCUDACXX_ATOMIC_FLAG_TYPE(__v), __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    void wait(bool __v, memory_order __m = memory_order_seq_cst) const noexcept\n        {__cxx_atomic_wait(&__a_, _LIBCUDACXX_ATOMIC_FLAG_TYPE(__v), __m);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    void notify_one() volatile noexcept\n        {__cxx_atomic_notify_one(&__a_);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    void notify_one() noexcept\n        {__cxx_atomic_notify_one(&__a_);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    void notify_all() volatile noexcept\n        {__cxx_atomic_notify_all(&__a_);}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    void notify_all() noexcept\n        {__cxx_atomic_notify_all(&__a_);}\n#endif\n\n    atomic_flag() noexcept = default;\n\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    atomic_flag(bool __b) noexcept : __a_(__b) {} // EXTENSION\n\n    atomic_flag(const atomic_flag&) = delete;\n    atomic_flag& operator=(const atomic_flag&) = delete;\n    atomic_flag& operator=(const atomic_flag&) volatile = delete;\n} atomic_flag;\n\n\ninline _LIBCUDACXX_INLINE_VISIBILITY\nbool\natomic_flag_test(const volatile atomic_flag* __o) noexcept\n{\n    return __o->test();\n}\n\ninline _LIBCUDACXX_INLINE_VISIBILITY\nbool\natomic_flag_test(const atomic_flag* __o) noexcept\n{\n    return __o->test();\n}\n\ninline _LIBCUDACXX_INLINE_VISIBILITY\nbool\natomic_flag_test_explicit(const volatile atomic_flag* __o, memory_order __m) noexcept\n{\n    return __o->test(__m);\n}\n\ninline _LIBCUDACXX_INLINE_VISIBILITY\nbool\natomic_flag_test_explicit(const atomic_flag* __o, memory_order __m) noexcept\n{\n    return __o->test(__m);\n}\n\ninline _LIBCUDACXX_INLINE_VISIBILITY\nbool\natomic_flag_test_and_set(volatile atomic_flag* __o) noexcept\n{\n    return __o->test_and_set();\n}\n\ninline _LIBCUDACXX_INLINE_VISIBILITY\nbool\natomic_flag_test_and_set(atomic_flag* __o) noexcept\n{\n    return __o->test_and_set();\n}\n\ninline _LIBCUDACXX_INLINE_VISIBILITY\nbool\natomic_flag_test_and_set_explicit(volatile atomic_flag* __o, memory_order __m) noexcept\n{\n    return __o->test_and_set(__m);\n}\n\ninline _LIBCUDACXX_INLINE_VISIBILITY\nbool\natomic_flag_test_and_set_explicit(atomic_flag* __o, memory_order __m) noexcept\n{\n    return __o->test_and_set(__m);\n}\n\ninline _LIBCUDACXX_INLINE_VISIBILITY\nvoid\natomic_flag_clear(volatile atomic_flag* __o) noexcept\n{\n    __o->clear();\n}\n\ninline _LIBCUDACXX_INLINE_VISIBILITY\nvoid\natomic_flag_clear(atomic_flag* __o) noexcept\n{\n    __o->clear();\n}\n\ninline _LIBCUDACXX_INLINE_VISIBILITY\nvoid\natomic_flag_clear_explicit(volatile atomic_flag* __o, memory_order __m) noexcept\n{\n    __o->clear(__m);\n}\n\ninline _LIBCUDACXX_INLINE_VISIBILITY\nvoid\natomic_flag_clear_explicit(atomic_flag* __o, memory_order __m) noexcept\n{\n    __o->clear(__m);\n}\n\n#if !defined(__CUDA_MINIMUM_ARCH__) || __CUDA_MINIMUM_ARCH__ >= 700\n\ninline _LIBCUDACXX_INLINE_VISIBILITY\nvoid\natomic_flag_wait(const volatile atomic_flag* __o, bool __v) noexcept\n{\n    __o->wait(__v);\n}\n\ninline _LIBCUDACXX_INLINE_VISIBILITY\nvoid\natomic_flag_wait(const atomic_flag* __o, bool __v) noexcept\n{\n    __o->wait(__v);\n}\n\ninline _LIBCUDACXX_INLINE_VISIBILITY\nvoid\natomic_flag_wait_explicit(const volatile atomic_flag* __o,\n                          bool __v, memory_order __m) noexcept\n{\n    __o->wait(__v, __m);\n}\n\ninline _LIBCUDACXX_INLINE_VISIBILITY\nvoid\natomic_flag_wait_explicit(const atomic_flag* __o,\n                          bool __v, memory_order __m) noexcept\n{\n    __o->wait(__v, __m);\n}\n\ninline _LIBCUDACXX_INLINE_VISIBILITY\nvoid\natomic_flag_notify_one(volatile atomic_flag* __o) noexcept\n{\n    __o->notify_one();\n}\n\ninline _LIBCUDACXX_INLINE_VISIBILITY\nvoid\natomic_flag_notify_one(atomic_flag* __o) noexcept\n{\n    __o->notify_one();\n}\n\ninline _LIBCUDACXX_INLINE_VISIBILITY\nvoid\natomic_flag_notify_all(volatile atomic_flag* __o) noexcept\n{\n    __o->notify_all();\n}\n\ninline _LIBCUDACXX_INLINE_VISIBILITY\nvoid\natomic_flag_notify_all(atomic_flag* __o) noexcept\n{\n    __o->notify_all();\n}\n\n#endif\n\n// fences\n\ninline _LIBCUDACXX_INLINE_VISIBILITY\nvoid\natomic_thread_fence(memory_order __m) noexcept\n{\n    __cxx_atomic_thread_fence(__m);\n}\n\ninline _LIBCUDACXX_INLINE_VISIBILITY\nvoid\natomic_signal_fence(memory_order __m) noexcept\n{\n    __cxx_atomic_signal_fence(__m);\n}\n\n// Atomics for standard typedef types\n\ntypedef atomic<bool>               atomic_bool;\ntypedef atomic<char>               atomic_char;\ntypedef atomic<signed char>        atomic_schar;\ntypedef atomic<unsigned char>      atomic_uchar;\ntypedef atomic<short>              atomic_short;\ntypedef atomic<unsigned short>     atomic_ushort;\ntypedef atomic<int>                atomic_int;\ntypedef atomic<unsigned int>       atomic_uint;\ntypedef atomic<long>               atomic_long;\ntypedef atomic<unsigned long>      atomic_ulong;\ntypedef atomic<long long>          atomic_llong;\ntypedef atomic<unsigned long long> atomic_ullong;\ntypedef atomic<char16_t>           atomic_char16_t;\ntypedef atomic<char32_t>           atomic_char32_t;\ntypedef atomic<wchar_t>            atomic_wchar_t;\n\ntypedef atomic<int_least8_t>   atomic_int_least8_t;\ntypedef atomic<uint_least8_t>  atomic_uint_least8_t;\ntypedef atomic<int_least16_t>  atomic_int_least16_t;\ntypedef atomic<uint_least16_t> atomic_uint_least16_t;\ntypedef atomic<int_least32_t>  atomic_int_least32_t;\ntypedef atomic<uint_least32_t> atomic_uint_least32_t;\ntypedef atomic<int_least64_t>  atomic_int_least64_t;\ntypedef atomic<uint_least64_t> atomic_uint_least64_t;\n\ntypedef atomic<int_fast8_t>   atomic_int_fast8_t;\ntypedef atomic<uint_fast8_t>  atomic_uint_fast8_t;\ntypedef atomic<int_fast16_t>  atomic_int_fast16_t;\ntypedef atomic<uint_fast16_t> atomic_uint_fast16_t;\ntypedef atomic<int_fast32_t>  atomic_int_fast32_t;\ntypedef atomic<uint_fast32_t> atomic_uint_fast32_t;\ntypedef atomic<int_fast64_t>  atomic_int_fast64_t;\ntypedef atomic<uint_fast64_t> atomic_uint_fast64_t;\n\ntypedef atomic< int8_t>  atomic_int8_t;\ntypedef atomic<uint8_t>  atomic_uint8_t;\ntypedef atomic< int16_t> atomic_int16_t;\ntypedef atomic<uint16_t> atomic_uint16_t;\ntypedef atomic< int32_t> atomic_int32_t;\ntypedef atomic<uint32_t> atomic_uint32_t;\ntypedef atomic< int64_t> atomic_int64_t;\ntypedef atomic<uint64_t> atomic_uint64_t;\n\ntypedef atomic<intptr_t>  atomic_intptr_t;\ntypedef atomic<uintptr_t> atomic_uintptr_t;\ntypedef atomic<size_t>    atomic_size_t;\ntypedef atomic<ptrdiff_t> atomic_ptrdiff_t;\ntypedef atomic<intmax_t>  atomic_intmax_t;\ntypedef atomic<uintmax_t> atomic_uintmax_t;\n\nstatic_assert(ATOMIC_INT_LOCK_FREE, \"This library assumes atomic<int> is lock-free.\");\n\ntypedef atomic<int>       atomic_signed_lock_free;\ntypedef atomic<unsigned>  atomic_unsigned_lock_free;\n\n#define ATOMIC_FLAG_INIT {false}\n#define ATOMIC_VAR_INIT(__v) {__v}\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#ifndef __cuda_std__\n#include <__pragma_pop>\n#else\n#include \"__cuda/atomic.h\"\n#endif // __cuda_std__\n\n#endif  // _LIBCUDACXX_ATOMIC\n", "atomic_cuda_derived.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of libcu++, the C++ Standard Library for your entire system,\n// under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\ntemplate<class _Type, class _Scope, typename _CUDA_VSTD::enable_if<sizeof(_Type) <= 2, int>::type = 0>\nbool _LIBCUDACXX_DEVICE __atomic_compare_exchange_cuda(_Type volatile *__ptr, _Type *__expected, const _Type *__desired, bool, int __success_memorder, int __failure_memorder, _Scope __s) {\n\n    auto const __aligned = (uint32_t*)((intptr_t)__ptr & ~(sizeof(uint32_t) - 1));\n    auto const __offset = uint32_t((intptr_t)__ptr & (sizeof(uint32_t) - 1)) * 8;\n    auto const __mask = ((1 << sizeof(_Type)*8) - 1) << __offset;\n\n    uint32_t __old = *__expected << __offset;\n    uint32_t __old_value;\n    while (1) {\n        __old_value = (__old & __mask) >> __offset;\n        if (__old_value != *__expected)\n            break;\n        uint32_t const __attempt = (__old & ~__mask) | (*__desired << __offset);\n        if (__atomic_compare_exchange_cuda(__aligned, &__old, &__attempt, true, __success_memorder, __failure_memorder, __s))\n            return true;\n    }\n    *__expected = __old_value;\n    return false;\n}\n\ntemplate<class _Type, class _Scope, typename _CUDA_VSTD::enable_if<sizeof(_Type)<=2, int>::type = 0>\nvoid _LIBCUDACXX_DEVICE __atomic_exchange_cuda(_Type volatile *__ptr, _Type *__val, _Type *__ret, int __memorder, _Scope __s) {\n\n    _Type __expected = __atomic_load_n_cuda(__ptr, __ATOMIC_RELAXED, __s);\n    while(!__atomic_compare_exchange_cuda(__ptr, &__expected, __val, true, __memorder, __memorder, __s))\n        ;\n    *__ret = __expected;\n}\n\ntemplate<class _Type, class _Delta, class _Scope, typename _CUDA_VSTD::enable_if<sizeof(_Type)<=2, int>::type = 0>\n_Type _LIBCUDACXX_DEVICE __atomic_fetch_add_cuda(_Type volatile *__ptr, _Delta __val, int __memorder, _Scope __s) {\n\n    _Type __expected = __atomic_load_n_cuda(__ptr, __ATOMIC_RELAXED, __s);\n    _Type __desired = __expected + __val;\n    while(!__atomic_compare_exchange_cuda(__ptr, &__expected, &__desired, true, __memorder, __memorder, __s))\n        __desired = __expected + __val;\n    return __expected;\n}\n\ntemplate<class _Type, class _Delta, class _Scope, typename _CUDA_VSTD::enable_if<sizeof(_Type)<=2 || _CUDA_VSTD::is_floating_point<_Type>::value, int>::type = 0>\n_Type _LIBCUDACXX_HOST_DEVICE __atomic_fetch_max_cuda(_Type volatile *__ptr, _Delta __val, int __memorder, _Scope __s) {\n    _Type __expected = __atomic_load_n_cuda(__ptr, __ATOMIC_RELAXED, __s);\n    _Type __desired = __expected > __val ? __expected : __val;\n\n    while(__desired == __val &&\n            !__atomic_compare_exchange_cuda(__ptr, &__expected, &__desired, true, __memorder, __memorder, __s)) {\n        __desired = __expected > __val ? __expected : __val;\n    }\n\n    return __expected;\n}\n\ntemplate<class _Type, class _Delta, class _Scope, typename _CUDA_VSTD::enable_if<sizeof(_Type)<=2 || _CUDA_VSTD::is_floating_point<_Type>::value, int>::type = 0>\n_Type _LIBCUDACXX_HOST_DEVICE __atomic_fetch_min_cuda(_Type volatile *__ptr, _Delta __val, int __memorder, _Scope __s) {\n    _Type __expected = __atomic_load_n_cuda(__ptr, __ATOMIC_RELAXED, __s);\n    _Type __desired = __expected < __val ? __expected : __val;\n\n    while(__desired == __val &&\n            !__atomic_compare_exchange_cuda(__ptr, &__expected, &__desired, true, __memorder, __memorder, __s)) {\n        __desired = __expected < __val ? __expected : __val;\n    }\n\n    return __expected;\n}\n\ntemplate<class _Type, class _Delta, class _Scope, typename _CUDA_VSTD::enable_if<sizeof(_Type)<=2, int>::type = 0>\n_Type _LIBCUDACXX_DEVICE __atomic_fetch_sub_cuda(_Type volatile *__ptr, _Delta __val, int __memorder, _Scope __s) {\n\n    _Type __expected = __atomic_load_n_cuda(__ptr, __ATOMIC_RELAXED, __s);\n    _Type __desired = __expected - __val;\n    while(!__atomic_compare_exchange_cuda(__ptr, &__expected, &__desired, true, __memorder, __memorder, __s))\n        __desired = __expected - __val;\n    return __expected;\n}\n\ntemplate<class _Type, class _Delta, class _Scope, typename _CUDA_VSTD::enable_if<sizeof(_Type)<=2, int>::type = 0>\n_Type _LIBCUDACXX_DEVICE __atomic_fetch_and_cuda(_Type volatile *__ptr, _Delta __val, int __memorder, _Scope __s) {\n\n    _Type __expected = __atomic_load_n_cuda(__ptr, __ATOMIC_RELAXED, __s);\n    _Type __desired = __expected & __val;\n    while(!__atomic_compare_exchange_cuda(__ptr, &__expected, &__desired, true, __memorder, __memorder, __s))\n        __desired = __expected & __val;\n    return __expected;\n}\n\ntemplate<class _Type, class _Delta, class _Scope, typename _CUDA_VSTD::enable_if<sizeof(_Type)<=2, int>::type = 0>\n_Type _LIBCUDACXX_DEVICE __atomic_fetch_xor_cuda(_Type volatile *__ptr, _Delta __val, int __memorder, _Scope __s) {\n\n    _Type __expected = __atomic_load_n_cuda(__ptr, __ATOMIC_RELAXED, __s);\n    _Type __desired = __expected ^ __val;\n    while(!__atomic_compare_exchange_cuda(__ptr, &__expected, &__desired, true, __memorder, __memorder, __s))\n        __desired = __expected ^ __val;\n    return __expected;\n}\n\ntemplate<class _Type, class _Delta, class _Scope, typename _CUDA_VSTD::enable_if<sizeof(_Type)<=2, int>::type = 0>\n_Type _LIBCUDACXX_DEVICE __atomic_fetch_or_cuda(_Type volatile *__ptr, _Delta __val, int __memorder, _Scope __s) {\n\n    _Type __expected = __atomic_load_n_cuda(__ptr, __ATOMIC_RELAXED, __s);\n    _Type __desired = __expected | __val;\n    while(!__atomic_compare_exchange_cuda(__ptr, &__expected, &__desired, true, __memorder, __memorder, __s))\n        __desired = __expected | __val;\n    return __expected;\n}\n\ntemplate<class _Type, class _Scope>\n_Type _LIBCUDACXX_DEVICE __atomic_load_n_cuda(const _Type volatile *__ptr, int __memorder, _Scope __s) {\n    _Type __ret;\n    __atomic_load_cuda(__ptr, &__ret, __memorder, __s);\n    return __ret;\n}\n\ntemplate<class _Type, class _Scope>\nvoid _LIBCUDACXX_DEVICE __atomic_store_n_cuda(_Type volatile *__ptr, _Type __val, int __memorder, _Scope __s) {\n    __atomic_store_cuda(__ptr, &__val, __memorder, __s);\n}\n\ntemplate<class _Type, class _Scope>\nbool _LIBCUDACXX_DEVICE __atomic_compare_exchange_n_cuda(_Type volatile *__ptr, _Type *__expected, _Type __desired, bool __weak, int __success_memorder, int __failure_memorder, _Scope __s) {\n    return __atomic_compare_exchange_cuda(__ptr, __expected, &__desired, __weak, __success_memorder, __failure_memorder, __s);\n}\n\ntemplate<class _Type, class _Scope>\n_Type _LIBCUDACXX_DEVICE __atomic_exchange_n_cuda(_Type volatile *__ptr, _Type __val, int __memorder, _Scope __s) {\n    _Type __ret;\n    __atomic_exchange_cuda(__ptr, &__val, &__ret, __memorder, __s);\n    return __ret;\n}\n\nstatic inline _LIBCUDACXX_DEVICE void __atomic_signal_fence_cuda(int) {\n    asm volatile(\"\":::\"memory\");\n}\n\n", "atomic_cuda_generated.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of libcu++, the C++ Standard Library for your entire system,\n// under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n\nstatic inline _LIBCUDACXX_DEVICE void __cuda_membar_block() { asm volatile(\"membar.cta;\":::\"memory\"); }\nstatic inline _LIBCUDACXX_DEVICE void __cuda_fence_acq_rel_block() { asm volatile(\"fence.acq_rel.cta;\":::\"memory\"); }\nstatic inline _LIBCUDACXX_DEVICE void __cuda_fence_sc_block() { asm volatile(\"fence.sc.cta;\":::\"memory\"); }\nstatic inline _LIBCUDACXX_DEVICE void __atomic_thread_fence_cuda(int __memorder, __thread_scope_block_tag) {\n  NV_DISPATCH_TARGET(\n    NV_PROVIDES_SM_70, (\n      switch (__memorder) {\n        case __ATOMIC_SEQ_CST: __cuda_fence_sc_block(); break;\n        case __ATOMIC_CONSUME:\n        case __ATOMIC_ACQUIRE:\n        case __ATOMIC_ACQ_REL:\n        case __ATOMIC_RELEASE: __cuda_fence_acq_rel_block(); break;\n        case __ATOMIC_RELAXED: break;\n        default: assert(0);\n      }\n    ),\n    NV_IS_DEVICE, (\n      switch (__memorder) {\n        case __ATOMIC_SEQ_CST:\n        case __ATOMIC_CONSUME:\n        case __ATOMIC_ACQUIRE:\n        case __ATOMIC_ACQ_REL:\n        case __ATOMIC_RELEASE: __cuda_membar_block(); break;\n        case __ATOMIC_RELAXED: break;\n        default: assert(0);\n      }\n    )\n  )\n}\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_load_acquire_32_block(_CUDA_A __ptr, _CUDA_B& __dst) {asm volatile(\"ld.acquire.cta.b32 %0,[%1];\" : \"=r\"(__dst) : \"l\"(__ptr) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_load_relaxed_32_block(_CUDA_A __ptr, _CUDA_B& __dst) {asm volatile(\"ld.relaxed.cta.b32 %0,[%1];\" : \"=r\"(__dst) : \"l\"(__ptr) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_load_volatile_32_block(_CUDA_A __ptr, _CUDA_B& __dst) {asm volatile(\"ld.volatile.b32 %0,[%1];\" : \"=r\"(__dst) : \"l\"(__ptr) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4, int> = 0>\n_LIBCUDACXX_DEVICE void __atomic_load_cuda(const volatile _Type *__ptr, _Type *__ret, int __memorder, __thread_scope_block_tag) {\n    uint32_t __tmp = 0;\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_load_acquire_32_block(__ptr, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_load_relaxed_32_block(__ptr, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_membar_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_load_volatile_32_block(__ptr, __tmp); __cuda_membar_block(); break;\n          case __ATOMIC_RELAXED: __cuda_load_volatile_32_block(__ptr, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(__ret, &__tmp, 4);\n}\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_load_acquire_64_block(_CUDA_A __ptr, _CUDA_B& __dst) {asm volatile(\"ld.acquire.cta.b64 %0,[%1];\" : \"=l\"(__dst) : \"l\"(__ptr) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_load_relaxed_64_block(_CUDA_A __ptr, _CUDA_B& __dst) {asm volatile(\"ld.relaxed.cta.b64 %0,[%1];\" : \"=l\"(__dst) : \"l\"(__ptr) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_load_volatile_64_block(_CUDA_A __ptr, _CUDA_B& __dst) {asm volatile(\"ld.volatile.b64 %0,[%1];\" : \"=l\"(__dst) : \"l\"(__ptr) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8, int> = 0>\n_LIBCUDACXX_DEVICE void __atomic_load_cuda(const volatile _Type *__ptr, _Type *__ret, int __memorder, __thread_scope_block_tag) {\n    uint64_t __tmp = 0;\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_load_acquire_64_block(__ptr, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_load_relaxed_64_block(__ptr, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_membar_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_load_volatile_64_block(__ptr, __tmp); __cuda_membar_block(); break;\n          case __ATOMIC_RELAXED: __cuda_load_volatile_64_block(__ptr, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(__ret, &__tmp, 8);\n}\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_store_relaxed_32_block(_CUDA_A __ptr, _CUDA_B __src) { asm volatile(\"st.relaxed.cta.b32 [%0], %1;\" :: \"l\"(__ptr),\"r\"(__src) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_store_release_32_block(_CUDA_A __ptr, _CUDA_B __src) { asm volatile(\"st.release.cta.b32 [%0], %1;\" :: \"l\"(__ptr),\"r\"(__src) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_store_volatile_32_block(_CUDA_A __ptr, _CUDA_B __src) { asm volatile(\"st.volatile.b32 [%0], %1;\" :: \"l\"(__ptr),\"r\"(__src) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4, int> = 0>\n_LIBCUDACXX_DEVICE void __atomic_store_cuda(volatile _Type *__ptr, _Type *__val, int __memorder, __thread_scope_block_tag) {\n    uint32_t __tmp = 0;\n    memcpy(&__tmp, __val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_RELEASE: __cuda_store_release_32_block(__ptr, __tmp); break;\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();\n          case __ATOMIC_RELAXED: __cuda_store_relaxed_32_block(__ptr, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_RELEASE:\n          case __ATOMIC_SEQ_CST: __cuda_membar_block();\n          case __ATOMIC_RELAXED: __cuda_store_volatile_32_block(__ptr, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n}\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_store_relaxed_64_block(_CUDA_A __ptr, _CUDA_B __src) { asm volatile(\"st.relaxed.cta.b64 [%0], %1;\" :: \"l\"(__ptr),\"l\"(__src) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_store_release_64_block(_CUDA_A __ptr, _CUDA_B __src) { asm volatile(\"st.release.cta.b64 [%0], %1;\" :: \"l\"(__ptr),\"l\"(__src) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_store_volatile_64_block(_CUDA_A __ptr, _CUDA_B __src) { asm volatile(\"st.volatile.b64 [%0], %1;\" :: \"l\"(__ptr),\"l\"(__src) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8, int> = 0>\n_LIBCUDACXX_DEVICE void __atomic_store_cuda(volatile _Type *__ptr, _Type *__val, int __memorder, __thread_scope_block_tag) {\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, __val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_RELEASE: __cuda_store_release_64_block(__ptr, __tmp); break;\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();\n          case __ATOMIC_RELAXED: __cuda_store_relaxed_64_block(__ptr, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_RELEASE:\n          case __ATOMIC_SEQ_CST: __cuda_membar_block();\n          case __ATOMIC_RELAXED: __cuda_store_volatile_64_block(__ptr, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline _LIBCUDACXX_DEVICE void __cuda_compare_exchange_acq_rel_32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __cmp, _CUDA_D __op) { asm volatile(\"atom.cas.acq_rel.cta.b32 %0,[%1],%2,%3;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__cmp),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline _LIBCUDACXX_DEVICE void __cuda_compare_exchange_acquire_32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __cmp, _CUDA_D __op) { asm volatile(\"atom.cas.acquire.cta.b32 %0,[%1],%2,%3;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__cmp),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline _LIBCUDACXX_DEVICE void __cuda_compare_exchange_relaxed_32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __cmp, _CUDA_D __op) { asm volatile(\"atom.cas.relaxed.cta.b32 %0,[%1],%2,%3;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__cmp),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline _LIBCUDACXX_DEVICE void __cuda_compare_exchange_release_32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __cmp, _CUDA_D __op) { asm volatile(\"atom.cas.release.cta.b32 %0,[%1],%2,%3;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__cmp),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline _LIBCUDACXX_DEVICE void __cuda_compare_exchange_volatile_32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __cmp, _CUDA_D __op) { asm volatile(\"atom.cas.cta.b32 %0,[%1],%2,%3;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__cmp),\"r\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4, int> = 0>\n_LIBCUDACXX_DEVICE bool __atomic_compare_exchange_cuda(volatile _Type *__ptr, _Type *__expected, const _Type *__desired, bool, int __success_memorder, int __failure_memorder, __thread_scope_block_tag) {\n    uint32_t __tmp = 0, __old = 0, __old_tmp;\n    memcpy(&__tmp, __desired, 4);\n    memcpy(&__old, __expected, 4);\n    __old_tmp = __old;\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__stronger_order_cuda(__success_memorder, __failure_memorder)) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_compare_exchange_acquire_32_block(__ptr, __old, __old_tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_compare_exchange_acq_rel_32_block(__ptr, __old, __old_tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_compare_exchange_release_32_block(__ptr, __old, __old_tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_compare_exchange_relaxed_32_block(__ptr, __old, __old_tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__stronger_order_cuda(__success_memorder, __failure_memorder)) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_compare_exchange_volatile_32_block(__ptr, __old, __old_tmp, __tmp); __cuda_membar_block(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_compare_exchange_volatile_32_block(__ptr, __old, __old_tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_compare_exchange_volatile_32_block(__ptr, __old, __old_tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    bool const __ret = __old == __old_tmp;\n    memcpy(__expected, &__old, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_exchange_acq_rel_32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.exch.acq_rel.cta.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_exchange_acquire_32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.exch.acquire.cta.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_exchange_relaxed_32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.exch.relaxed.cta.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_exchange_release_32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.exch.release.cta.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_exchange_volatile_32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.exch.cta.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4, int> = 0>\n_LIBCUDACXX_DEVICE void __atomic_exchange_cuda(volatile _Type *__ptr, _Type *__val, _Type *__ret, int __memorder, __thread_scope_block_tag) {\n    uint32_t __tmp = 0;\n    memcpy(&__tmp, __val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_exchange_acquire_32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_exchange_acq_rel_32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_exchange_release_32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_exchange_relaxed_32_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_exchange_volatile_32_block(__ptr, __tmp, __tmp); __cuda_membar_block(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_exchange_volatile_32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_exchange_volatile_32_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(__ret, &__tmp, 4);\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_and_acq_rel_32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.and.acq_rel.cta.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_and_acquire_32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.and.acquire.cta.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_and_relaxed_32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.and.relaxed.cta.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_and_release_32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.and.release.cta.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_and_volatile_32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.and.cta.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_and_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_block_tag) {\n    _Type __ret;\n    uint32_t __tmp = 0;\n    memcpy(&__tmp, &__val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_and_acquire_32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_and_acq_rel_32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_and_release_32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_and_relaxed_32_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_and_volatile_32_block(__ptr, __tmp, __tmp); __cuda_membar_block(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_and_volatile_32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_and_volatile_32_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_or_acq_rel_32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.or.acq_rel.cta.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_or_acquire_32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.or.acquire.cta.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_or_relaxed_32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.or.relaxed.cta.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_or_release_32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.or.release.cta.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_or_volatile_32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.or.cta.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_or_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_block_tag) {\n    _Type __ret;\n    uint32_t __tmp = 0;\n    memcpy(&__tmp, &__val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_or_acquire_32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_or_acq_rel_32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_or_release_32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_or_relaxed_32_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_or_volatile_32_block(__ptr, __tmp, __tmp); __cuda_membar_block(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_or_volatile_32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_or_volatile_32_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_xor_acq_rel_32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.xor.acq_rel.cta.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_xor_acquire_32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.xor.acquire.cta.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_xor_relaxed_32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.xor.relaxed.cta.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_xor_release_32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.xor.release.cta.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_xor_volatile_32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.xor.cta.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_xor_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_block_tag) {\n    _Type __ret;\n    uint32_t __tmp = 0;\n    memcpy(&__tmp, &__val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_xor_acquire_32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_xor_acq_rel_32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_xor_release_32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_xor_relaxed_32_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_xor_volatile_32_block(__ptr, __tmp, __tmp); __cuda_membar_block(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_xor_volatile_32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_xor_volatile_32_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_acq_rel_f32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.acq_rel.cta.f32 %0,[%1],%2;\" : \"=f\"(__dst) : \"l\"(__ptr),\"f\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_acquire_f32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.acquire.cta.f32 %0,[%1],%2;\" : \"=f\"(__dst) : \"l\"(__ptr),\"f\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_relaxed_f32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.relaxed.cta.f32 %0,[%1],%2;\" : \"=f\"(__dst) : \"l\"(__ptr),\"f\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_release_f32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.release.cta.f32 %0,[%1],%2;\" : \"=f\"(__dst) : \"l\"(__ptr),\"f\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_volatile_f32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.cta.f32 %0,[%1],%2;\" : \"=f\"(__dst) : \"l\"(__ptr),\"f\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4 && _CUDA_VSTD::is_floating_point<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_add_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_block_tag) {\n    _Type __ret;\n    float __tmp = 0;\n    memcpy(&__tmp, &__val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_acquire_f32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_add_acq_rel_f32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_add_release_f32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_relaxed_f32_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_volatile_f32_block(__ptr, __tmp, __tmp); __cuda_membar_block(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_add_volatile_f32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_volatile_f32_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_acq_rel_u32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.acq_rel.cta.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_acquire_u32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.acquire.cta.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_relaxed_u32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.relaxed.cta.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_release_u32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.release.cta.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_volatile_u32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.cta.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4 && _CUDA_VSTD::is_integral<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_add_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_block_tag) {\n    _Type __ret;\n    uint32_t __tmp = 0;\n    memcpy(&__tmp, &__val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_acquire_u32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_add_acq_rel_u32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_add_release_u32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_relaxed_u32_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_volatile_u32_block(__ptr, __tmp, __tmp); __cuda_membar_block(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_add_volatile_u32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_volatile_u32_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_acq_rel_s32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.acq_rel.cta.s32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_acquire_s32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.acquire.cta.s32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_relaxed_s32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.relaxed.cta.s32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_release_s32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.release.cta.s32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_volatile_s32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.cta.s32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4 && _CUDA_VSTD::is_integral<_Type>::value && _CUDA_VSTD::is_signed<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_max_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_block_tag) {\n    _Type __ret;\n    uint32_t __tmp = 0;\n    memcpy(&__tmp, &__val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_max_acquire_s32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_max_acq_rel_s32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_max_release_s32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_max_relaxed_s32_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_max_volatile_s32_block(__ptr, __tmp, __tmp); __cuda_membar_block(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_max_volatile_s32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_max_volatile_s32_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_acq_rel_u32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.acq_rel.cta.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_acquire_u32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.acquire.cta.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_relaxed_u32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.relaxed.cta.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_release_u32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.release.cta.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_volatile_u32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.cta.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4 && _CUDA_VSTD::is_integral<_Type>::value && _CUDA_VSTD::is_unsigned<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_max_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_block_tag) {\n    _Type __ret;\n    uint32_t __tmp = 0;\n    memcpy(&__tmp, &__val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_max_acquire_u32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_max_acq_rel_u32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_max_release_u32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_max_relaxed_u32_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_max_volatile_u32_block(__ptr, __tmp, __tmp); __cuda_membar_block(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_max_volatile_u32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_max_volatile_u32_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_acq_rel_s32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.acq_rel.cta.s32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_acquire_s32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.acquire.cta.s32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_relaxed_s32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.relaxed.cta.s32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_release_s32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.release.cta.s32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_volatile_s32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.cta.s32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4 && _CUDA_VSTD::is_integral<_Type>::value && _CUDA_VSTD::is_signed<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_min_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_block_tag) {\n    _Type __ret;\n    uint32_t __tmp = 0;\n    memcpy(&__tmp, &__val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_min_acquire_s32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_min_acq_rel_s32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_min_release_s32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_min_relaxed_s32_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_min_volatile_s32_block(__ptr, __tmp, __tmp); __cuda_membar_block(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_min_volatile_s32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_min_volatile_s32_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_acq_rel_u32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.acq_rel.cta.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_acquire_u32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.acquire.cta.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_relaxed_u32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.relaxed.cta.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_release_u32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.release.cta.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_volatile_u32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.cta.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4 && _CUDA_VSTD::is_integral<_Type>::value && _CUDA_VSTD::is_unsigned<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_min_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_block_tag) {\n    _Type __ret;\n    uint32_t __tmp = 0;\n    memcpy(&__tmp, &__val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_min_acquire_u32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_min_acq_rel_u32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_min_release_u32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_min_relaxed_u32_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_min_volatile_u32_block(__ptr, __tmp, __tmp); __cuda_membar_block(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_min_volatile_u32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_min_volatile_u32_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_acq_rel_f32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.acq_rel.cta.f32 %0,[%1],%2;\" : \"=f\"(__dst) : \"l\"(__ptr),\"f\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_acquire_f32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.acquire.cta.f32 %0,[%1],%2;\" : \"=f\"(__dst) : \"l\"(__ptr),\"f\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_relaxed_f32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.relaxed.cta.f32 %0,[%1],%2;\" : \"=f\"(__dst) : \"l\"(__ptr),\"f\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_release_f32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.release.cta.f32 %0,[%1],%2;\" : \"=f\"(__dst) : \"l\"(__ptr),\"f\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_volatile_f32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.cta.f32 %0,[%1],%2;\" : \"=f\"(__dst) : \"l\"(__ptr),\"f\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4 && _CUDA_VSTD::is_floating_point<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_sub_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_block_tag) {\n    _Type __ret;\n    float __tmp = 0;\n    memcpy(&__tmp, &__val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_sub_acquire_f32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_sub_acq_rel_f32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_sub_release_f32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_sub_relaxed_f32_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_sub_volatile_f32_block(__ptr, __tmp, __tmp); __cuda_membar_block(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_sub_volatile_f32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_sub_volatile_f32_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_acq_rel_u32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.acq_rel.cta.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_acquire_u32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.acquire.cta.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_relaxed_u32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.relaxed.cta.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_release_u32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.release.cta.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_volatile_u32_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.cta.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4 && _CUDA_VSTD::is_integral<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_sub_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_block_tag) {\n    _Type __ret;\n    uint32_t __tmp = 0;\n    memcpy(&__tmp, &__val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_sub_acquire_u32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_sub_acq_rel_u32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_sub_release_u32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_sub_relaxed_u32_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_sub_volatile_u32_block(__ptr, __tmp, __tmp); __cuda_membar_block(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_sub_volatile_u32_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_sub_volatile_u32_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline _LIBCUDACXX_DEVICE void __cuda_compare_exchange_acq_rel_64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __cmp, _CUDA_D __op) { asm volatile(\"atom.cas.acq_rel.cta.b64 %0,[%1],%2,%3;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__cmp),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline _LIBCUDACXX_DEVICE void __cuda_compare_exchange_acquire_64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __cmp, _CUDA_D __op) { asm volatile(\"atom.cas.acquire.cta.b64 %0,[%1],%2,%3;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__cmp),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline _LIBCUDACXX_DEVICE void __cuda_compare_exchange_relaxed_64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __cmp, _CUDA_D __op) { asm volatile(\"atom.cas.relaxed.cta.b64 %0,[%1],%2,%3;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__cmp),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline _LIBCUDACXX_DEVICE void __cuda_compare_exchange_release_64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __cmp, _CUDA_D __op) { asm volatile(\"atom.cas.release.cta.b64 %0,[%1],%2,%3;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__cmp),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline _LIBCUDACXX_DEVICE void __cuda_compare_exchange_volatile_64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __cmp, _CUDA_D __op) { asm volatile(\"atom.cas.cta.b64 %0,[%1],%2,%3;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__cmp),\"l\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8, int> = 0>\n_LIBCUDACXX_DEVICE bool __atomic_compare_exchange_cuda(volatile _Type *__ptr, _Type *__expected, const _Type *__desired, bool, int __success_memorder, int __failure_memorder, __thread_scope_block_tag) {\n    uint64_t __tmp = 0, __old = 0, __old_tmp;\n    memcpy(&__tmp, __desired, 8);\n    memcpy(&__old, __expected, 8);\n    __old_tmp = __old;\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__stronger_order_cuda(__success_memorder, __failure_memorder)) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_compare_exchange_acquire_64_block(__ptr, __old, __old_tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_compare_exchange_acq_rel_64_block(__ptr, __old, __old_tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_compare_exchange_release_64_block(__ptr, __old, __old_tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_compare_exchange_relaxed_64_block(__ptr, __old, __old_tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__stronger_order_cuda(__success_memorder, __failure_memorder)) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_compare_exchange_volatile_64_block(__ptr, __old, __old_tmp, __tmp); __cuda_membar_block(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_compare_exchange_volatile_64_block(__ptr, __old, __old_tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_compare_exchange_volatile_64_block(__ptr, __old, __old_tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    bool const __ret = __old == __old_tmp;\n    memcpy(__expected, &__old, 8);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_exchange_acq_rel_64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.exch.acq_rel.cta.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_exchange_acquire_64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.exch.acquire.cta.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_exchange_relaxed_64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.exch.relaxed.cta.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_exchange_release_64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.exch.release.cta.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_exchange_volatile_64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.exch.cta.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8, int> = 0>\n_LIBCUDACXX_DEVICE void __atomic_exchange_cuda(volatile _Type *__ptr, _Type *__val, _Type *__ret, int __memorder, __thread_scope_block_tag) {\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, __val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_exchange_acquire_64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_exchange_acq_rel_64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_exchange_release_64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_exchange_relaxed_64_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_exchange_volatile_64_block(__ptr, __tmp, __tmp); __cuda_membar_block(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_exchange_volatile_64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_exchange_volatile_64_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(__ret, &__tmp, 8);\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_and_acq_rel_64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.and.acq_rel.cta.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_and_acquire_64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.and.acquire.cta.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_and_relaxed_64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.and.relaxed.cta.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_and_release_64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.and.release.cta.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_and_volatile_64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.and.cta.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_and_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_block_tag) {\n    _Type __ret;\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_and_acquire_64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_and_acq_rel_64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_and_release_64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_and_relaxed_64_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_and_volatile_64_block(__ptr, __tmp, __tmp); __cuda_membar_block(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_and_volatile_64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_and_volatile_64_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_or_acq_rel_64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.or.acq_rel.cta.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_or_acquire_64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.or.acquire.cta.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_or_relaxed_64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.or.relaxed.cta.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_or_release_64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.or.release.cta.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_or_volatile_64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.or.cta.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_or_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_block_tag) {\n    _Type __ret;\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_or_acquire_64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_or_acq_rel_64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_or_release_64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_or_relaxed_64_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_or_volatile_64_block(__ptr, __tmp, __tmp); __cuda_membar_block(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_or_volatile_64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_or_volatile_64_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_xor_acq_rel_64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.xor.acq_rel.cta.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_xor_acquire_64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.xor.acquire.cta.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_xor_relaxed_64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.xor.relaxed.cta.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_xor_release_64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.xor.release.cta.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_xor_volatile_64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.xor.cta.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_xor_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_block_tag) {\n    _Type __ret;\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_xor_acquire_64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_xor_acq_rel_64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_xor_release_64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_xor_relaxed_64_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_xor_volatile_64_block(__ptr, __tmp, __tmp); __cuda_membar_block(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_xor_volatile_64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_xor_volatile_64_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_acq_rel_f64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.acq_rel.cta.f64 %0,[%1],%2;\" : \"=d\"(__dst) : \"l\"(__ptr),\"d\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_acquire_f64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.acquire.cta.f64 %0,[%1],%2;\" : \"=d\"(__dst) : \"l\"(__ptr),\"d\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_relaxed_f64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.relaxed.cta.f64 %0,[%1],%2;\" : \"=d\"(__dst) : \"l\"(__ptr),\"d\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_release_f64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.release.cta.f64 %0,[%1],%2;\" : \"=d\"(__dst) : \"l\"(__ptr),\"d\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_volatile_f64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.cta.f64 %0,[%1],%2;\" : \"=d\"(__dst) : \"l\"(__ptr),\"d\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8 && _CUDA_VSTD::is_floating_point<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_add_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_block_tag) {\n    _Type __ret;\n    double __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_acquire_f64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_add_acq_rel_f64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_add_release_f64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_relaxed_f64_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_volatile_f64_block(__ptr, __tmp, __tmp); __cuda_membar_block(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_add_volatile_f64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_volatile_f64_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_acq_rel_u64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.acq_rel.cta.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_acquire_u64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.acquire.cta.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_relaxed_u64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.relaxed.cta.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_release_u64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.release.cta.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_volatile_u64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.cta.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8 && _CUDA_VSTD::is_integral<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_add_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_block_tag) {\n    _Type __ret;\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_acquire_u64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_add_acq_rel_u64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_add_release_u64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_relaxed_u64_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_volatile_u64_block(__ptr, __tmp, __tmp); __cuda_membar_block(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_add_volatile_u64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_volatile_u64_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_acq_rel_s64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.acq_rel.cta.s64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_acquire_s64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.acquire.cta.s64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_relaxed_s64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.relaxed.cta.s64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_release_s64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.release.cta.s64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_volatile_s64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.cta.s64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8 && _CUDA_VSTD::is_integral<_Type>::value && _CUDA_VSTD::is_signed<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_max_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_block_tag) {\n    _Type __ret;\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_max_acquire_s64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_max_acq_rel_s64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_max_release_s64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_max_relaxed_s64_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_max_volatile_s64_block(__ptr, __tmp, __tmp); __cuda_membar_block(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_max_volatile_s64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_max_volatile_s64_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_acq_rel_u64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.acq_rel.cta.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_acquire_u64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.acquire.cta.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_relaxed_u64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.relaxed.cta.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_release_u64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.release.cta.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_volatile_u64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.cta.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8 && _CUDA_VSTD::is_integral<_Type>::value && _CUDA_VSTD::is_unsigned<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_max_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_block_tag) {\n    _Type __ret;\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_max_acquire_u64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_max_acq_rel_u64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_max_release_u64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_max_relaxed_u64_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_max_volatile_u64_block(__ptr, __tmp, __tmp); __cuda_membar_block(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_max_volatile_u64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_max_volatile_u64_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_acq_rel_s64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.acq_rel.cta.s64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_acquire_s64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.acquire.cta.s64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_relaxed_s64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.relaxed.cta.s64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_release_s64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.release.cta.s64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_volatile_s64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.cta.s64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8 && _CUDA_VSTD::is_integral<_Type>::value && _CUDA_VSTD::is_signed<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_min_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_block_tag) {\n    _Type __ret;\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_min_acquire_s64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_min_acq_rel_s64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_min_release_s64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_min_relaxed_s64_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_min_volatile_s64_block(__ptr, __tmp, __tmp); __cuda_membar_block(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_min_volatile_s64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_min_volatile_s64_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_acq_rel_u64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.acq_rel.cta.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_acquire_u64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.acquire.cta.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_relaxed_u64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.relaxed.cta.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_release_u64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.release.cta.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_volatile_u64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.cta.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8 && _CUDA_VSTD::is_integral<_Type>::value && _CUDA_VSTD::is_unsigned<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_min_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_block_tag) {\n    _Type __ret;\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_min_acquire_u64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_min_acq_rel_u64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_min_release_u64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_min_relaxed_u64_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_min_volatile_u64_block(__ptr, __tmp, __tmp); __cuda_membar_block(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_min_volatile_u64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_min_volatile_u64_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_acq_rel_f64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.acq_rel.cta.f64 %0,[%1],%2;\" : \"=d\"(__dst) : \"l\"(__ptr),\"d\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_acquire_f64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.acquire.cta.f64 %0,[%1],%2;\" : \"=d\"(__dst) : \"l\"(__ptr),\"d\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_relaxed_f64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.relaxed.cta.f64 %0,[%1],%2;\" : \"=d\"(__dst) : \"l\"(__ptr),\"d\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_release_f64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.release.cta.f64 %0,[%1],%2;\" : \"=d\"(__dst) : \"l\"(__ptr),\"d\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_volatile_f64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.cta.f64 %0,[%1],%2;\" : \"=d\"(__dst) : \"l\"(__ptr),\"d\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8 && _CUDA_VSTD::is_floating_point<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_sub_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_block_tag) {\n    _Type __ret;\n    double __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_sub_acquire_f64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_sub_acq_rel_f64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_sub_release_f64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_sub_relaxed_f64_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_sub_volatile_f64_block(__ptr, __tmp, __tmp); __cuda_membar_block(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_sub_volatile_f64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_sub_volatile_f64_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_acq_rel_u64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.acq_rel.cta.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_acquire_u64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.acquire.cta.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_relaxed_u64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.relaxed.cta.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_release_u64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.release.cta.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_volatile_u64_block(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.cta.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8 && _CUDA_VSTD::is_integral<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_sub_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_block_tag) {\n    _Type __ret;\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_sub_acquire_u64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_sub_acq_rel_u64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_sub_release_u64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_sub_relaxed_u64_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_sub_volatile_u64_block(__ptr, __tmp, __tmp); __cuda_membar_block(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_sub_volatile_u64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_sub_volatile_u64_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _Type>\n_LIBCUDACXX_DEVICE _Type* __atomic_fetch_add_cuda(_Type *volatile *__ptr, ptrdiff_t __val, int __memorder, __thread_scope_block_tag) {\n    _Type* __ret;\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    __tmp *= sizeof(_Type);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_acquire_u64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_add_acq_rel_u64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_add_release_u64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_relaxed_u64_block(__ptr, __tmp, __tmp); break;\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_volatile_u64_block(__ptr, __tmp, __tmp); __cuda_membar_block(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_add_volatile_u64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_volatile_u64_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _Type>\n_LIBCUDACXX_DEVICE _Type* __atomic_fetch_sub_cuda(_Type *volatile *__ptr, ptrdiff_t __val, int __memorder, __thread_scope_block_tag) {\n    _Type* __ret;\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    __tmp = -__tmp;\n    __tmp *= sizeof(_Type);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_acquire_u64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_add_acq_rel_u64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_add_release_u64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_relaxed_u64_block(__ptr, __tmp, __tmp); break;\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_block();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_volatile_u64_block(__ptr, __tmp, __tmp); __cuda_membar_block(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_add_volatile_u64_block(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_volatile_u64_block(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\nstatic inline _LIBCUDACXX_DEVICE void __cuda_membar_device() { asm volatile(\"membar.gl;\":::\"memory\"); }\nstatic inline _LIBCUDACXX_DEVICE void __cuda_fence_acq_rel_device() { asm volatile(\"fence.acq_rel.gpu;\":::\"memory\"); }\nstatic inline _LIBCUDACXX_DEVICE void __cuda_fence_sc_device() { asm volatile(\"fence.sc.gpu;\":::\"memory\"); }\nstatic inline _LIBCUDACXX_DEVICE void __atomic_thread_fence_cuda(int __memorder, __thread_scope_device_tag) {\n  NV_DISPATCH_TARGET(\n    NV_PROVIDES_SM_70, (\n      switch (__memorder) {\n        case __ATOMIC_SEQ_CST: __cuda_fence_sc_device(); break;\n        case __ATOMIC_CONSUME:\n        case __ATOMIC_ACQUIRE:\n        case __ATOMIC_ACQ_REL:\n        case __ATOMIC_RELEASE: __cuda_fence_acq_rel_device(); break;\n        case __ATOMIC_RELAXED: break;\n        default: assert(0);\n      }\n    ),\n    NV_IS_DEVICE, (\n      switch (__memorder) {\n        case __ATOMIC_SEQ_CST:\n        case __ATOMIC_CONSUME:\n        case __ATOMIC_ACQUIRE:\n        case __ATOMIC_ACQ_REL:\n        case __ATOMIC_RELEASE: __cuda_membar_device(); break;\n        case __ATOMIC_RELAXED: break;\n        default: assert(0);\n      }\n    )\n  )\n}\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_load_acquire_32_device(_CUDA_A __ptr, _CUDA_B& __dst) {asm volatile(\"ld.acquire.gpu.b32 %0,[%1];\" : \"=r\"(__dst) : \"l\"(__ptr) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_load_relaxed_32_device(_CUDA_A __ptr, _CUDA_B& __dst) {asm volatile(\"ld.relaxed.gpu.b32 %0,[%1];\" : \"=r\"(__dst) : \"l\"(__ptr) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_load_volatile_32_device(_CUDA_A __ptr, _CUDA_B& __dst) {asm volatile(\"ld.volatile.b32 %0,[%1];\" : \"=r\"(__dst) : \"l\"(__ptr) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4, int> = 0>\n_LIBCUDACXX_DEVICE void __atomic_load_cuda(const volatile _Type *__ptr, _Type *__ret, int __memorder, __thread_scope_device_tag) {\n    uint32_t __tmp = 0;\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_load_acquire_32_device(__ptr, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_load_relaxed_32_device(__ptr, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_membar_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_load_volatile_32_device(__ptr, __tmp); __cuda_membar_device(); break;\n          case __ATOMIC_RELAXED: __cuda_load_volatile_32_device(__ptr, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(__ret, &__tmp, 4);\n}\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_load_acquire_64_device(_CUDA_A __ptr, _CUDA_B& __dst) {asm volatile(\"ld.acquire.gpu.b64 %0,[%1];\" : \"=l\"(__dst) : \"l\"(__ptr) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_load_relaxed_64_device(_CUDA_A __ptr, _CUDA_B& __dst) {asm volatile(\"ld.relaxed.gpu.b64 %0,[%1];\" : \"=l\"(__dst) : \"l\"(__ptr) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_load_volatile_64_device(_CUDA_A __ptr, _CUDA_B& __dst) {asm volatile(\"ld.volatile.b64 %0,[%1];\" : \"=l\"(__dst) : \"l\"(__ptr) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8, int> = 0>\n_LIBCUDACXX_DEVICE void __atomic_load_cuda(const volatile _Type *__ptr, _Type *__ret, int __memorder, __thread_scope_device_tag) {\n    uint64_t __tmp = 0;\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_load_acquire_64_device(__ptr, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_load_relaxed_64_device(__ptr, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_membar_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_load_volatile_64_device(__ptr, __tmp); __cuda_membar_device(); break;\n          case __ATOMIC_RELAXED: __cuda_load_volatile_64_device(__ptr, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(__ret, &__tmp, 8);\n}\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_store_relaxed_32_device(_CUDA_A __ptr, _CUDA_B __src) { asm volatile(\"st.relaxed.gpu.b32 [%0], %1;\" :: \"l\"(__ptr),\"r\"(__src) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_store_release_32_device(_CUDA_A __ptr, _CUDA_B __src) { asm volatile(\"st.release.gpu.b32 [%0], %1;\" :: \"l\"(__ptr),\"r\"(__src) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_store_volatile_32_device(_CUDA_A __ptr, _CUDA_B __src) { asm volatile(\"st.volatile.b32 [%0], %1;\" :: \"l\"(__ptr),\"r\"(__src) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4, int> = 0>\n_LIBCUDACXX_DEVICE void __atomic_store_cuda(volatile _Type *__ptr, _Type *__val, int __memorder, __thread_scope_device_tag) {\n    uint32_t __tmp = 0;\n    memcpy(&__tmp, __val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_RELEASE: __cuda_store_release_32_device(__ptr, __tmp); break;\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();\n          case __ATOMIC_RELAXED: __cuda_store_relaxed_32_device(__ptr, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_RELEASE:\n          case __ATOMIC_SEQ_CST: __cuda_membar_device();\n          case __ATOMIC_RELAXED: __cuda_store_volatile_32_device(__ptr, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n}\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_store_relaxed_64_device(_CUDA_A __ptr, _CUDA_B __src) { asm volatile(\"st.relaxed.gpu.b64 [%0], %1;\" :: \"l\"(__ptr),\"l\"(__src) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_store_release_64_device(_CUDA_A __ptr, _CUDA_B __src) { asm volatile(\"st.release.gpu.b64 [%0], %1;\" :: \"l\"(__ptr),\"l\"(__src) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_store_volatile_64_device(_CUDA_A __ptr, _CUDA_B __src) { asm volatile(\"st.volatile.b64 [%0], %1;\" :: \"l\"(__ptr),\"l\"(__src) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8, int> = 0>\n_LIBCUDACXX_DEVICE void __atomic_store_cuda(volatile _Type *__ptr, _Type *__val, int __memorder, __thread_scope_device_tag) {\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, __val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_RELEASE: __cuda_store_release_64_device(__ptr, __tmp); break;\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();\n          case __ATOMIC_RELAXED: __cuda_store_relaxed_64_device(__ptr, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_RELEASE:\n          case __ATOMIC_SEQ_CST: __cuda_membar_device();\n          case __ATOMIC_RELAXED: __cuda_store_volatile_64_device(__ptr, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline _LIBCUDACXX_DEVICE void __cuda_compare_exchange_acq_rel_32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __cmp, _CUDA_D __op) { asm volatile(\"atom.cas.acq_rel.gpu.b32 %0,[%1],%2,%3;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__cmp),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline _LIBCUDACXX_DEVICE void __cuda_compare_exchange_acquire_32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __cmp, _CUDA_D __op) { asm volatile(\"atom.cas.acquire.gpu.b32 %0,[%1],%2,%3;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__cmp),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline _LIBCUDACXX_DEVICE void __cuda_compare_exchange_relaxed_32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __cmp, _CUDA_D __op) { asm volatile(\"atom.cas.relaxed.gpu.b32 %0,[%1],%2,%3;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__cmp),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline _LIBCUDACXX_DEVICE void __cuda_compare_exchange_release_32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __cmp, _CUDA_D __op) { asm volatile(\"atom.cas.release.gpu.b32 %0,[%1],%2,%3;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__cmp),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline _LIBCUDACXX_DEVICE void __cuda_compare_exchange_volatile_32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __cmp, _CUDA_D __op) { asm volatile(\"atom.cas.gpu.b32 %0,[%1],%2,%3;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__cmp),\"r\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4, int> = 0>\n_LIBCUDACXX_DEVICE bool __atomic_compare_exchange_cuda(volatile _Type *__ptr, _Type *__expected, const _Type *__desired, bool, int __success_memorder, int __failure_memorder, __thread_scope_device_tag) {\n    uint32_t __tmp = 0, __old = 0, __old_tmp;\n    memcpy(&__tmp, __desired, 4);\n    memcpy(&__old, __expected, 4);\n    __old_tmp = __old;\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__stronger_order_cuda(__success_memorder, __failure_memorder)) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_compare_exchange_acquire_32_device(__ptr, __old, __old_tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_compare_exchange_acq_rel_32_device(__ptr, __old, __old_tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_compare_exchange_release_32_device(__ptr, __old, __old_tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_compare_exchange_relaxed_32_device(__ptr, __old, __old_tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__stronger_order_cuda(__success_memorder, __failure_memorder)) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_compare_exchange_volatile_32_device(__ptr, __old, __old_tmp, __tmp); __cuda_membar_device(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_compare_exchange_volatile_32_device(__ptr, __old, __old_tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_compare_exchange_volatile_32_device(__ptr, __old, __old_tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    bool const __ret = __old == __old_tmp;\n    memcpy(__expected, &__old, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_exchange_acq_rel_32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.exch.acq_rel.gpu.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_exchange_acquire_32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.exch.acquire.gpu.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_exchange_relaxed_32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.exch.relaxed.gpu.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_exchange_release_32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.exch.release.gpu.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_exchange_volatile_32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.exch.gpu.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4, int> = 0>\n_LIBCUDACXX_DEVICE void __atomic_exchange_cuda(volatile _Type *__ptr, _Type *__val, _Type *__ret, int __memorder, __thread_scope_device_tag) {\n    uint32_t __tmp = 0;\n    memcpy(&__tmp, __val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_exchange_acquire_32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_exchange_acq_rel_32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_exchange_release_32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_exchange_relaxed_32_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_exchange_volatile_32_device(__ptr, __tmp, __tmp); __cuda_membar_device(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_exchange_volatile_32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_exchange_volatile_32_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(__ret, &__tmp, 4);\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_and_acq_rel_32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.and.acq_rel.gpu.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_and_acquire_32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.and.acquire.gpu.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_and_relaxed_32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.and.relaxed.gpu.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_and_release_32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.and.release.gpu.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_and_volatile_32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.and.gpu.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_and_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_device_tag) {\n    _Type __ret;\n    uint32_t __tmp = 0;\n    memcpy(&__tmp, &__val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_and_acquire_32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_and_acq_rel_32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_and_release_32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_and_relaxed_32_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_and_volatile_32_device(__ptr, __tmp, __tmp); __cuda_membar_device(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_and_volatile_32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_and_volatile_32_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_or_acq_rel_32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.or.acq_rel.gpu.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_or_acquire_32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.or.acquire.gpu.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_or_relaxed_32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.or.relaxed.gpu.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_or_release_32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.or.release.gpu.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_or_volatile_32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.or.gpu.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_or_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_device_tag) {\n    _Type __ret;\n    uint32_t __tmp = 0;\n    memcpy(&__tmp, &__val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_or_acquire_32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_or_acq_rel_32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_or_release_32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_or_relaxed_32_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_or_volatile_32_device(__ptr, __tmp, __tmp); __cuda_membar_device(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_or_volatile_32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_or_volatile_32_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_xor_acq_rel_32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.xor.acq_rel.gpu.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_xor_acquire_32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.xor.acquire.gpu.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_xor_relaxed_32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.xor.relaxed.gpu.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_xor_release_32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.xor.release.gpu.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_xor_volatile_32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.xor.gpu.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_xor_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_device_tag) {\n    _Type __ret;\n    uint32_t __tmp = 0;\n    memcpy(&__tmp, &__val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_xor_acquire_32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_xor_acq_rel_32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_xor_release_32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_xor_relaxed_32_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_xor_volatile_32_device(__ptr, __tmp, __tmp); __cuda_membar_device(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_xor_volatile_32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_xor_volatile_32_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_acq_rel_f32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.acq_rel.gpu.f32 %0,[%1],%2;\" : \"=f\"(__dst) : \"l\"(__ptr),\"f\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_acquire_f32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.acquire.gpu.f32 %0,[%1],%2;\" : \"=f\"(__dst) : \"l\"(__ptr),\"f\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_relaxed_f32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.relaxed.gpu.f32 %0,[%1],%2;\" : \"=f\"(__dst) : \"l\"(__ptr),\"f\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_release_f32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.release.gpu.f32 %0,[%1],%2;\" : \"=f\"(__dst) : \"l\"(__ptr),\"f\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_volatile_f32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.gpu.f32 %0,[%1],%2;\" : \"=f\"(__dst) : \"l\"(__ptr),\"f\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4 && _CUDA_VSTD::is_floating_point<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_add_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_device_tag) {\n    _Type __ret;\n    float __tmp = 0;\n    memcpy(&__tmp, &__val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_acquire_f32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_add_acq_rel_f32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_add_release_f32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_relaxed_f32_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_volatile_f32_device(__ptr, __tmp, __tmp); __cuda_membar_device(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_add_volatile_f32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_volatile_f32_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_acq_rel_u32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.acq_rel.gpu.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_acquire_u32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.acquire.gpu.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_relaxed_u32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.relaxed.gpu.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_release_u32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.release.gpu.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_volatile_u32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.gpu.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4 && _CUDA_VSTD::is_integral<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_add_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_device_tag) {\n    _Type __ret;\n    uint32_t __tmp = 0;\n    memcpy(&__tmp, &__val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_acquire_u32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_add_acq_rel_u32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_add_release_u32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_relaxed_u32_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_volatile_u32_device(__ptr, __tmp, __tmp); __cuda_membar_device(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_add_volatile_u32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_volatile_u32_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_acq_rel_s32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.acq_rel.gpu.s32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_acquire_s32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.acquire.gpu.s32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_relaxed_s32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.relaxed.gpu.s32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_release_s32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.release.gpu.s32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_volatile_s32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.gpu.s32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4 && _CUDA_VSTD::is_integral<_Type>::value && _CUDA_VSTD::is_signed<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_max_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_device_tag) {\n    _Type __ret;\n    uint32_t __tmp = 0;\n    memcpy(&__tmp, &__val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_max_acquire_s32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_max_acq_rel_s32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_max_release_s32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_max_relaxed_s32_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_max_volatile_s32_device(__ptr, __tmp, __tmp); __cuda_membar_device(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_max_volatile_s32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_max_volatile_s32_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_acq_rel_u32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.acq_rel.gpu.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_acquire_u32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.acquire.gpu.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_relaxed_u32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.relaxed.gpu.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_release_u32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.release.gpu.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_volatile_u32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.gpu.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4 && _CUDA_VSTD::is_integral<_Type>::value && _CUDA_VSTD::is_unsigned<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_max_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_device_tag) {\n    _Type __ret;\n    uint32_t __tmp = 0;\n    memcpy(&__tmp, &__val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_max_acquire_u32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_max_acq_rel_u32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_max_release_u32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_max_relaxed_u32_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_max_volatile_u32_device(__ptr, __tmp, __tmp); __cuda_membar_device(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_max_volatile_u32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_max_volatile_u32_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_acq_rel_s32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.acq_rel.gpu.s32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_acquire_s32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.acquire.gpu.s32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_relaxed_s32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.relaxed.gpu.s32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_release_s32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.release.gpu.s32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_volatile_s32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.gpu.s32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4 && _CUDA_VSTD::is_integral<_Type>::value && _CUDA_VSTD::is_signed<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_min_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_device_tag) {\n    _Type __ret;\n    uint32_t __tmp = 0;\n    memcpy(&__tmp, &__val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_min_acquire_s32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_min_acq_rel_s32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_min_release_s32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_min_relaxed_s32_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_min_volatile_s32_device(__ptr, __tmp, __tmp); __cuda_membar_device(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_min_volatile_s32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_min_volatile_s32_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_acq_rel_u32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.acq_rel.gpu.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_acquire_u32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.acquire.gpu.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_relaxed_u32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.relaxed.gpu.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_release_u32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.release.gpu.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_volatile_u32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.gpu.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4 && _CUDA_VSTD::is_integral<_Type>::value && _CUDA_VSTD::is_unsigned<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_min_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_device_tag) {\n    _Type __ret;\n    uint32_t __tmp = 0;\n    memcpy(&__tmp, &__val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_min_acquire_u32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_min_acq_rel_u32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_min_release_u32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_min_relaxed_u32_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_min_volatile_u32_device(__ptr, __tmp, __tmp); __cuda_membar_device(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_min_volatile_u32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_min_volatile_u32_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_acq_rel_f32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.acq_rel.gpu.f32 %0,[%1],%2;\" : \"=f\"(__dst) : \"l\"(__ptr),\"f\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_acquire_f32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.acquire.gpu.f32 %0,[%1],%2;\" : \"=f\"(__dst) : \"l\"(__ptr),\"f\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_relaxed_f32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.relaxed.gpu.f32 %0,[%1],%2;\" : \"=f\"(__dst) : \"l\"(__ptr),\"f\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_release_f32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.release.gpu.f32 %0,[%1],%2;\" : \"=f\"(__dst) : \"l\"(__ptr),\"f\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_volatile_f32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.gpu.f32 %0,[%1],%2;\" : \"=f\"(__dst) : \"l\"(__ptr),\"f\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4 && _CUDA_VSTD::is_floating_point<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_sub_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_device_tag) {\n    _Type __ret;\n    float __tmp = 0;\n    memcpy(&__tmp, &__val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_sub_acquire_f32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_sub_acq_rel_f32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_sub_release_f32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_sub_relaxed_f32_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_sub_volatile_f32_device(__ptr, __tmp, __tmp); __cuda_membar_device(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_sub_volatile_f32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_sub_volatile_f32_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_acq_rel_u32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.acq_rel.gpu.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_acquire_u32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.acquire.gpu.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_relaxed_u32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.relaxed.gpu.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_release_u32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.release.gpu.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_volatile_u32_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.gpu.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4 && _CUDA_VSTD::is_integral<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_sub_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_device_tag) {\n    _Type __ret;\n    uint32_t __tmp = 0;\n    memcpy(&__tmp, &__val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_sub_acquire_u32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_sub_acq_rel_u32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_sub_release_u32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_sub_relaxed_u32_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_sub_volatile_u32_device(__ptr, __tmp, __tmp); __cuda_membar_device(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_sub_volatile_u32_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_sub_volatile_u32_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline _LIBCUDACXX_DEVICE void __cuda_compare_exchange_acq_rel_64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __cmp, _CUDA_D __op) { asm volatile(\"atom.cas.acq_rel.gpu.b64 %0,[%1],%2,%3;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__cmp),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline _LIBCUDACXX_DEVICE void __cuda_compare_exchange_acquire_64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __cmp, _CUDA_D __op) { asm volatile(\"atom.cas.acquire.gpu.b64 %0,[%1],%2,%3;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__cmp),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline _LIBCUDACXX_DEVICE void __cuda_compare_exchange_relaxed_64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __cmp, _CUDA_D __op) { asm volatile(\"atom.cas.relaxed.gpu.b64 %0,[%1],%2,%3;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__cmp),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline _LIBCUDACXX_DEVICE void __cuda_compare_exchange_release_64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __cmp, _CUDA_D __op) { asm volatile(\"atom.cas.release.gpu.b64 %0,[%1],%2,%3;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__cmp),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline _LIBCUDACXX_DEVICE void __cuda_compare_exchange_volatile_64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __cmp, _CUDA_D __op) { asm volatile(\"atom.cas.gpu.b64 %0,[%1],%2,%3;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__cmp),\"l\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8, int> = 0>\n_LIBCUDACXX_DEVICE bool __atomic_compare_exchange_cuda(volatile _Type *__ptr, _Type *__expected, const _Type *__desired, bool, int __success_memorder, int __failure_memorder, __thread_scope_device_tag) {\n    uint64_t __tmp = 0, __old = 0, __old_tmp;\n    memcpy(&__tmp, __desired, 8);\n    memcpy(&__old, __expected, 8);\n    __old_tmp = __old;\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__stronger_order_cuda(__success_memorder, __failure_memorder)) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_compare_exchange_acquire_64_device(__ptr, __old, __old_tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_compare_exchange_acq_rel_64_device(__ptr, __old, __old_tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_compare_exchange_release_64_device(__ptr, __old, __old_tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_compare_exchange_relaxed_64_device(__ptr, __old, __old_tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__stronger_order_cuda(__success_memorder, __failure_memorder)) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_compare_exchange_volatile_64_device(__ptr, __old, __old_tmp, __tmp); __cuda_membar_device(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_compare_exchange_volatile_64_device(__ptr, __old, __old_tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_compare_exchange_volatile_64_device(__ptr, __old, __old_tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    bool const __ret = __old == __old_tmp;\n    memcpy(__expected, &__old, 8);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_exchange_acq_rel_64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.exch.acq_rel.gpu.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_exchange_acquire_64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.exch.acquire.gpu.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_exchange_relaxed_64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.exch.relaxed.gpu.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_exchange_release_64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.exch.release.gpu.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_exchange_volatile_64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.exch.gpu.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8, int> = 0>\n_LIBCUDACXX_DEVICE void __atomic_exchange_cuda(volatile _Type *__ptr, _Type *__val, _Type *__ret, int __memorder, __thread_scope_device_tag) {\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, __val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_exchange_acquire_64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_exchange_acq_rel_64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_exchange_release_64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_exchange_relaxed_64_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_exchange_volatile_64_device(__ptr, __tmp, __tmp); __cuda_membar_device(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_exchange_volatile_64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_exchange_volatile_64_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(__ret, &__tmp, 8);\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_and_acq_rel_64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.and.acq_rel.gpu.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_and_acquire_64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.and.acquire.gpu.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_and_relaxed_64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.and.relaxed.gpu.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_and_release_64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.and.release.gpu.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_and_volatile_64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.and.gpu.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_and_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_device_tag) {\n    _Type __ret;\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_and_acquire_64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_and_acq_rel_64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_and_release_64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_and_relaxed_64_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_and_volatile_64_device(__ptr, __tmp, __tmp); __cuda_membar_device(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_and_volatile_64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_and_volatile_64_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_or_acq_rel_64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.or.acq_rel.gpu.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_or_acquire_64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.or.acquire.gpu.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_or_relaxed_64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.or.relaxed.gpu.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_or_release_64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.or.release.gpu.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_or_volatile_64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.or.gpu.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_or_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_device_tag) {\n    _Type __ret;\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_or_acquire_64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_or_acq_rel_64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_or_release_64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_or_relaxed_64_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_or_volatile_64_device(__ptr, __tmp, __tmp); __cuda_membar_device(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_or_volatile_64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_or_volatile_64_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_xor_acq_rel_64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.xor.acq_rel.gpu.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_xor_acquire_64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.xor.acquire.gpu.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_xor_relaxed_64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.xor.relaxed.gpu.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_xor_release_64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.xor.release.gpu.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_xor_volatile_64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.xor.gpu.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_xor_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_device_tag) {\n    _Type __ret;\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_xor_acquire_64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_xor_acq_rel_64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_xor_release_64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_xor_relaxed_64_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_xor_volatile_64_device(__ptr, __tmp, __tmp); __cuda_membar_device(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_xor_volatile_64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_xor_volatile_64_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_acq_rel_f64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.acq_rel.gpu.f64 %0,[%1],%2;\" : \"=d\"(__dst) : \"l\"(__ptr),\"d\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_acquire_f64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.acquire.gpu.f64 %0,[%1],%2;\" : \"=d\"(__dst) : \"l\"(__ptr),\"d\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_relaxed_f64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.relaxed.gpu.f64 %0,[%1],%2;\" : \"=d\"(__dst) : \"l\"(__ptr),\"d\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_release_f64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.release.gpu.f64 %0,[%1],%2;\" : \"=d\"(__dst) : \"l\"(__ptr),\"d\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_volatile_f64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.gpu.f64 %0,[%1],%2;\" : \"=d\"(__dst) : \"l\"(__ptr),\"d\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8 && _CUDA_VSTD::is_floating_point<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_add_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_device_tag) {\n    _Type __ret;\n    double __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_acquire_f64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_add_acq_rel_f64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_add_release_f64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_relaxed_f64_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_volatile_f64_device(__ptr, __tmp, __tmp); __cuda_membar_device(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_add_volatile_f64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_volatile_f64_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_acq_rel_u64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.acq_rel.gpu.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_acquire_u64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.acquire.gpu.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_relaxed_u64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.relaxed.gpu.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_release_u64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.release.gpu.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_volatile_u64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.gpu.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8 && _CUDA_VSTD::is_integral<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_add_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_device_tag) {\n    _Type __ret;\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_acquire_u64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_add_acq_rel_u64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_add_release_u64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_relaxed_u64_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_volatile_u64_device(__ptr, __tmp, __tmp); __cuda_membar_device(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_add_volatile_u64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_volatile_u64_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_acq_rel_s64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.acq_rel.gpu.s64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_acquire_s64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.acquire.gpu.s64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_relaxed_s64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.relaxed.gpu.s64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_release_s64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.release.gpu.s64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_volatile_s64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.gpu.s64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8 && _CUDA_VSTD::is_integral<_Type>::value && _CUDA_VSTD::is_signed<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_max_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_device_tag) {\n    _Type __ret;\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_max_acquire_s64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_max_acq_rel_s64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_max_release_s64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_max_relaxed_s64_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_max_volatile_s64_device(__ptr, __tmp, __tmp); __cuda_membar_device(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_max_volatile_s64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_max_volatile_s64_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_acq_rel_u64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.acq_rel.gpu.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_acquire_u64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.acquire.gpu.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_relaxed_u64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.relaxed.gpu.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_release_u64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.release.gpu.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_volatile_u64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.gpu.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8 && _CUDA_VSTD::is_integral<_Type>::value && _CUDA_VSTD::is_unsigned<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_max_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_device_tag) {\n    _Type __ret;\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_max_acquire_u64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_max_acq_rel_u64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_max_release_u64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_max_relaxed_u64_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_max_volatile_u64_device(__ptr, __tmp, __tmp); __cuda_membar_device(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_max_volatile_u64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_max_volatile_u64_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_acq_rel_s64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.acq_rel.gpu.s64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_acquire_s64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.acquire.gpu.s64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_relaxed_s64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.relaxed.gpu.s64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_release_s64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.release.gpu.s64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_volatile_s64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.gpu.s64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8 && _CUDA_VSTD::is_integral<_Type>::value && _CUDA_VSTD::is_signed<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_min_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_device_tag) {\n    _Type __ret;\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_min_acquire_s64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_min_acq_rel_s64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_min_release_s64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_min_relaxed_s64_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_min_volatile_s64_device(__ptr, __tmp, __tmp); __cuda_membar_device(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_min_volatile_s64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_min_volatile_s64_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_acq_rel_u64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.acq_rel.gpu.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_acquire_u64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.acquire.gpu.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_relaxed_u64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.relaxed.gpu.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_release_u64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.release.gpu.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_volatile_u64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.gpu.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8 && _CUDA_VSTD::is_integral<_Type>::value && _CUDA_VSTD::is_unsigned<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_min_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_device_tag) {\n    _Type __ret;\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_min_acquire_u64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_min_acq_rel_u64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_min_release_u64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_min_relaxed_u64_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_min_volatile_u64_device(__ptr, __tmp, __tmp); __cuda_membar_device(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_min_volatile_u64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_min_volatile_u64_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_acq_rel_f64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.acq_rel.gpu.f64 %0,[%1],%2;\" : \"=d\"(__dst) : \"l\"(__ptr),\"d\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_acquire_f64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.acquire.gpu.f64 %0,[%1],%2;\" : \"=d\"(__dst) : \"l\"(__ptr),\"d\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_relaxed_f64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.relaxed.gpu.f64 %0,[%1],%2;\" : \"=d\"(__dst) : \"l\"(__ptr),\"d\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_release_f64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.release.gpu.f64 %0,[%1],%2;\" : \"=d\"(__dst) : \"l\"(__ptr),\"d\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_volatile_f64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.gpu.f64 %0,[%1],%2;\" : \"=d\"(__dst) : \"l\"(__ptr),\"d\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8 && _CUDA_VSTD::is_floating_point<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_sub_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_device_tag) {\n    _Type __ret;\n    double __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_sub_acquire_f64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_sub_acq_rel_f64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_sub_release_f64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_sub_relaxed_f64_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_sub_volatile_f64_device(__ptr, __tmp, __tmp); __cuda_membar_device(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_sub_volatile_f64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_sub_volatile_f64_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_acq_rel_u64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.acq_rel.gpu.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_acquire_u64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.acquire.gpu.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_relaxed_u64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.relaxed.gpu.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_release_u64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.release.gpu.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_volatile_u64_device(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.gpu.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8 && _CUDA_VSTD::is_integral<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_sub_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_device_tag) {\n    _Type __ret;\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_sub_acquire_u64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_sub_acq_rel_u64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_sub_release_u64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_sub_relaxed_u64_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_sub_volatile_u64_device(__ptr, __tmp, __tmp); __cuda_membar_device(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_sub_volatile_u64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_sub_volatile_u64_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _Type>\n_LIBCUDACXX_DEVICE _Type* __atomic_fetch_add_cuda(_Type *volatile *__ptr, ptrdiff_t __val, int __memorder, __thread_scope_device_tag) {\n    _Type* __ret;\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    __tmp *= sizeof(_Type);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_acquire_u64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_add_acq_rel_u64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_add_release_u64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_relaxed_u64_device(__ptr, __tmp, __tmp); break;\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_volatile_u64_device(__ptr, __tmp, __tmp); __cuda_membar_device(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_add_volatile_u64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_volatile_u64_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _Type>\n_LIBCUDACXX_DEVICE _Type* __atomic_fetch_sub_cuda(_Type *volatile *__ptr, ptrdiff_t __val, int __memorder, __thread_scope_device_tag) {\n    _Type* __ret;\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    __tmp = -__tmp;\n    __tmp *= sizeof(_Type);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_acquire_u64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_add_acq_rel_u64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_add_release_u64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_relaxed_u64_device(__ptr, __tmp, __tmp); break;\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_device();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_volatile_u64_device(__ptr, __tmp, __tmp); __cuda_membar_device(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_add_volatile_u64_device(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_volatile_u64_device(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\nstatic inline _LIBCUDACXX_DEVICE void __cuda_membar_system() { asm volatile(\"membar.sys;\":::\"memory\"); }\nstatic inline _LIBCUDACXX_DEVICE void __cuda_fence_acq_rel_system() { asm volatile(\"fence.acq_rel.sys;\":::\"memory\"); }\nstatic inline _LIBCUDACXX_DEVICE void __cuda_fence_sc_system() { asm volatile(\"fence.sc.sys;\":::\"memory\"); }\nstatic inline _LIBCUDACXX_DEVICE void __atomic_thread_fence_cuda(int __memorder, __thread_scope_system_tag) {\n  NV_DISPATCH_TARGET(\n    NV_PROVIDES_SM_70, (\n      switch (__memorder) {\n        case __ATOMIC_SEQ_CST: __cuda_fence_sc_system(); break;\n        case __ATOMIC_CONSUME:\n        case __ATOMIC_ACQUIRE:\n        case __ATOMIC_ACQ_REL:\n        case __ATOMIC_RELEASE: __cuda_fence_acq_rel_system(); break;\n        case __ATOMIC_RELAXED: break;\n        default: assert(0);\n      }\n    ),\n    NV_IS_DEVICE, (\n      switch (__memorder) {\n        case __ATOMIC_SEQ_CST:\n        case __ATOMIC_CONSUME:\n        case __ATOMIC_ACQUIRE:\n        case __ATOMIC_ACQ_REL:\n        case __ATOMIC_RELEASE: __cuda_membar_system(); break;\n        case __ATOMIC_RELAXED: break;\n        default: assert(0);\n      }\n    )\n  )\n}\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_load_acquire_32_system(_CUDA_A __ptr, _CUDA_B& __dst) {asm volatile(\"ld.acquire.sys.b32 %0,[%1];\" : \"=r\"(__dst) : \"l\"(__ptr) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_load_relaxed_32_system(_CUDA_A __ptr, _CUDA_B& __dst) {asm volatile(\"ld.relaxed.sys.b32 %0,[%1];\" : \"=r\"(__dst) : \"l\"(__ptr) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_load_volatile_32_system(_CUDA_A __ptr, _CUDA_B& __dst) {asm volatile(\"ld.volatile.b32 %0,[%1];\" : \"=r\"(__dst) : \"l\"(__ptr) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4, int> = 0>\n_LIBCUDACXX_DEVICE void __atomic_load_cuda(const volatile _Type *__ptr, _Type *__ret, int __memorder, __thread_scope_system_tag) {\n    uint32_t __tmp = 0;\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_load_acquire_32_system(__ptr, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_load_relaxed_32_system(__ptr, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_membar_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_load_volatile_32_system(__ptr, __tmp); __cuda_membar_system(); break;\n          case __ATOMIC_RELAXED: __cuda_load_volatile_32_system(__ptr, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(__ret, &__tmp, 4);\n}\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_load_acquire_64_system(_CUDA_A __ptr, _CUDA_B& __dst) {asm volatile(\"ld.acquire.sys.b64 %0,[%1];\" : \"=l\"(__dst) : \"l\"(__ptr) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_load_relaxed_64_system(_CUDA_A __ptr, _CUDA_B& __dst) {asm volatile(\"ld.relaxed.sys.b64 %0,[%1];\" : \"=l\"(__dst) : \"l\"(__ptr) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_load_volatile_64_system(_CUDA_A __ptr, _CUDA_B& __dst) {asm volatile(\"ld.volatile.b64 %0,[%1];\" : \"=l\"(__dst) : \"l\"(__ptr) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8, int> = 0>\n_LIBCUDACXX_DEVICE void __atomic_load_cuda(const volatile _Type *__ptr, _Type *__ret, int __memorder, __thread_scope_system_tag) {\n    uint64_t __tmp = 0;\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_load_acquire_64_system(__ptr, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_load_relaxed_64_system(__ptr, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_membar_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_load_volatile_64_system(__ptr, __tmp); __cuda_membar_system(); break;\n          case __ATOMIC_RELAXED: __cuda_load_volatile_64_system(__ptr, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(__ret, &__tmp, 8);\n}\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_store_relaxed_32_system(_CUDA_A __ptr, _CUDA_B __src) { asm volatile(\"st.relaxed.sys.b32 [%0], %1;\" :: \"l\"(__ptr),\"r\"(__src) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_store_release_32_system(_CUDA_A __ptr, _CUDA_B __src) { asm volatile(\"st.release.sys.b32 [%0], %1;\" :: \"l\"(__ptr),\"r\"(__src) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_store_volatile_32_system(_CUDA_A __ptr, _CUDA_B __src) { asm volatile(\"st.volatile.b32 [%0], %1;\" :: \"l\"(__ptr),\"r\"(__src) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4, int> = 0>\n_LIBCUDACXX_DEVICE void __atomic_store_cuda(volatile _Type *__ptr, _Type *__val, int __memorder, __thread_scope_system_tag) {\n    uint32_t __tmp = 0;\n    memcpy(&__tmp, __val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_RELEASE: __cuda_store_release_32_system(__ptr, __tmp); break;\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();\n          case __ATOMIC_RELAXED: __cuda_store_relaxed_32_system(__ptr, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_RELEASE:\n          case __ATOMIC_SEQ_CST: __cuda_membar_system();\n          case __ATOMIC_RELAXED: __cuda_store_volatile_32_system(__ptr, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n}\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_store_relaxed_64_system(_CUDA_A __ptr, _CUDA_B __src) { asm volatile(\"st.relaxed.sys.b64 [%0], %1;\" :: \"l\"(__ptr),\"l\"(__src) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_store_release_64_system(_CUDA_A __ptr, _CUDA_B __src) { asm volatile(\"st.release.sys.b64 [%0], %1;\" :: \"l\"(__ptr),\"l\"(__src) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B> static inline _LIBCUDACXX_DEVICE void __cuda_store_volatile_64_system(_CUDA_A __ptr, _CUDA_B __src) { asm volatile(\"st.volatile.b64 [%0], %1;\" :: \"l\"(__ptr),\"l\"(__src) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8, int> = 0>\n_LIBCUDACXX_DEVICE void __atomic_store_cuda(volatile _Type *__ptr, _Type *__val, int __memorder, __thread_scope_system_tag) {\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, __val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_RELEASE: __cuda_store_release_64_system(__ptr, __tmp); break;\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();\n          case __ATOMIC_RELAXED: __cuda_store_relaxed_64_system(__ptr, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_RELEASE:\n          case __ATOMIC_SEQ_CST: __cuda_membar_system();\n          case __ATOMIC_RELAXED: __cuda_store_volatile_64_system(__ptr, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline _LIBCUDACXX_DEVICE void __cuda_compare_exchange_acq_rel_32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __cmp, _CUDA_D __op) { asm volatile(\"atom.cas.acq_rel.sys.b32 %0,[%1],%2,%3;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__cmp),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline _LIBCUDACXX_DEVICE void __cuda_compare_exchange_acquire_32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __cmp, _CUDA_D __op) { asm volatile(\"atom.cas.acquire.sys.b32 %0,[%1],%2,%3;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__cmp),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline _LIBCUDACXX_DEVICE void __cuda_compare_exchange_relaxed_32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __cmp, _CUDA_D __op) { asm volatile(\"atom.cas.relaxed.sys.b32 %0,[%1],%2,%3;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__cmp),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline _LIBCUDACXX_DEVICE void __cuda_compare_exchange_release_32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __cmp, _CUDA_D __op) { asm volatile(\"atom.cas.release.sys.b32 %0,[%1],%2,%3;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__cmp),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline _LIBCUDACXX_DEVICE void __cuda_compare_exchange_volatile_32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __cmp, _CUDA_D __op) { asm volatile(\"atom.cas.sys.b32 %0,[%1],%2,%3;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__cmp),\"r\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4, int> = 0>\n_LIBCUDACXX_DEVICE bool __atomic_compare_exchange_cuda(volatile _Type *__ptr, _Type *__expected, const _Type *__desired, bool, int __success_memorder, int __failure_memorder, __thread_scope_system_tag) {\n    uint32_t __tmp = 0, __old = 0, __old_tmp;\n    memcpy(&__tmp, __desired, 4);\n    memcpy(&__old, __expected, 4);\n    __old_tmp = __old;\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__stronger_order_cuda(__success_memorder, __failure_memorder)) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_compare_exchange_acquire_32_system(__ptr, __old, __old_tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_compare_exchange_acq_rel_32_system(__ptr, __old, __old_tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_compare_exchange_release_32_system(__ptr, __old, __old_tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_compare_exchange_relaxed_32_system(__ptr, __old, __old_tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__stronger_order_cuda(__success_memorder, __failure_memorder)) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_compare_exchange_volatile_32_system(__ptr, __old, __old_tmp, __tmp); __cuda_membar_system(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_compare_exchange_volatile_32_system(__ptr, __old, __old_tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_compare_exchange_volatile_32_system(__ptr, __old, __old_tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    bool const __ret = __old == __old_tmp;\n    memcpy(__expected, &__old, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_exchange_acq_rel_32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.exch.acq_rel.sys.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_exchange_acquire_32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.exch.acquire.sys.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_exchange_relaxed_32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.exch.relaxed.sys.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_exchange_release_32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.exch.release.sys.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_exchange_volatile_32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.exch.sys.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4, int> = 0>\n_LIBCUDACXX_DEVICE void __atomic_exchange_cuda(volatile _Type *__ptr, _Type *__val, _Type *__ret, int __memorder, __thread_scope_system_tag) {\n    uint32_t __tmp = 0;\n    memcpy(&__tmp, __val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_exchange_acquire_32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_exchange_acq_rel_32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_exchange_release_32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_exchange_relaxed_32_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_exchange_volatile_32_system(__ptr, __tmp, __tmp); __cuda_membar_system(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_exchange_volatile_32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_exchange_volatile_32_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(__ret, &__tmp, 4);\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_and_acq_rel_32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.and.acq_rel.sys.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_and_acquire_32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.and.acquire.sys.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_and_relaxed_32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.and.relaxed.sys.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_and_release_32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.and.release.sys.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_and_volatile_32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.and.sys.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_and_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_system_tag) {\n    _Type __ret;\n    uint32_t __tmp = 0;\n    memcpy(&__tmp, &__val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_and_acquire_32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_and_acq_rel_32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_and_release_32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_and_relaxed_32_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_and_volatile_32_system(__ptr, __tmp, __tmp); __cuda_membar_system(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_and_volatile_32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_and_volatile_32_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_or_acq_rel_32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.or.acq_rel.sys.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_or_acquire_32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.or.acquire.sys.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_or_relaxed_32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.or.relaxed.sys.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_or_release_32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.or.release.sys.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_or_volatile_32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.or.sys.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_or_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_system_tag) {\n    _Type __ret;\n    uint32_t __tmp = 0;\n    memcpy(&__tmp, &__val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_or_acquire_32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_or_acq_rel_32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_or_release_32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_or_relaxed_32_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_or_volatile_32_system(__ptr, __tmp, __tmp); __cuda_membar_system(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_or_volatile_32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_or_volatile_32_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_xor_acq_rel_32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.xor.acq_rel.sys.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_xor_acquire_32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.xor.acquire.sys.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_xor_relaxed_32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.xor.relaxed.sys.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_xor_release_32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.xor.release.sys.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_xor_volatile_32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.xor.sys.b32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_xor_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_system_tag) {\n    _Type __ret;\n    uint32_t __tmp = 0;\n    memcpy(&__tmp, &__val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_xor_acquire_32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_xor_acq_rel_32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_xor_release_32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_xor_relaxed_32_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_xor_volatile_32_system(__ptr, __tmp, __tmp); __cuda_membar_system(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_xor_volatile_32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_xor_volatile_32_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_acq_rel_f32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.acq_rel.sys.f32 %0,[%1],%2;\" : \"=f\"(__dst) : \"l\"(__ptr),\"f\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_acquire_f32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.acquire.sys.f32 %0,[%1],%2;\" : \"=f\"(__dst) : \"l\"(__ptr),\"f\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_relaxed_f32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.relaxed.sys.f32 %0,[%1],%2;\" : \"=f\"(__dst) : \"l\"(__ptr),\"f\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_release_f32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.release.sys.f32 %0,[%1],%2;\" : \"=f\"(__dst) : \"l\"(__ptr),\"f\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_volatile_f32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.sys.f32 %0,[%1],%2;\" : \"=f\"(__dst) : \"l\"(__ptr),\"f\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4 && _CUDA_VSTD::is_floating_point<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_add_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_system_tag) {\n    _Type __ret;\n    float __tmp = 0;\n    memcpy(&__tmp, &__val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_acquire_f32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_add_acq_rel_f32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_add_release_f32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_relaxed_f32_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_volatile_f32_system(__ptr, __tmp, __tmp); __cuda_membar_system(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_add_volatile_f32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_volatile_f32_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_acq_rel_u32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.acq_rel.sys.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_acquire_u32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.acquire.sys.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_relaxed_u32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.relaxed.sys.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_release_u32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.release.sys.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_volatile_u32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.sys.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4 && _CUDA_VSTD::is_integral<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_add_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_system_tag) {\n    _Type __ret;\n    uint32_t __tmp = 0;\n    memcpy(&__tmp, &__val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_acquire_u32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_add_acq_rel_u32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_add_release_u32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_relaxed_u32_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_volatile_u32_system(__ptr, __tmp, __tmp); __cuda_membar_system(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_add_volatile_u32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_volatile_u32_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_acq_rel_s32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.acq_rel.sys.s32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_acquire_s32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.acquire.sys.s32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_relaxed_s32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.relaxed.sys.s32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_release_s32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.release.sys.s32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_volatile_s32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.sys.s32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4 && _CUDA_VSTD::is_integral<_Type>::value && _CUDA_VSTD::is_signed<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_max_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_system_tag) {\n    _Type __ret;\n    uint32_t __tmp = 0;\n    memcpy(&__tmp, &__val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_max_acquire_s32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_max_acq_rel_s32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_max_release_s32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_max_relaxed_s32_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_max_volatile_s32_system(__ptr, __tmp, __tmp); __cuda_membar_system(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_max_volatile_s32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_max_volatile_s32_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_acq_rel_u32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.acq_rel.sys.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_acquire_u32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.acquire.sys.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_relaxed_u32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.relaxed.sys.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_release_u32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.release.sys.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_volatile_u32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.sys.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4 && _CUDA_VSTD::is_integral<_Type>::value && _CUDA_VSTD::is_unsigned<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_max_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_system_tag) {\n    _Type __ret;\n    uint32_t __tmp = 0;\n    memcpy(&__tmp, &__val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_max_acquire_u32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_max_acq_rel_u32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_max_release_u32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_max_relaxed_u32_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_max_volatile_u32_system(__ptr, __tmp, __tmp); __cuda_membar_system(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_max_volatile_u32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_max_volatile_u32_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_acq_rel_s32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.acq_rel.sys.s32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_acquire_s32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.acquire.sys.s32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_relaxed_s32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.relaxed.sys.s32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_release_s32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.release.sys.s32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_volatile_s32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.sys.s32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4 && _CUDA_VSTD::is_integral<_Type>::value && _CUDA_VSTD::is_signed<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_min_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_system_tag) {\n    _Type __ret;\n    uint32_t __tmp = 0;\n    memcpy(&__tmp, &__val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_min_acquire_s32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_min_acq_rel_s32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_min_release_s32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_min_relaxed_s32_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_min_volatile_s32_system(__ptr, __tmp, __tmp); __cuda_membar_system(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_min_volatile_s32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_min_volatile_s32_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_acq_rel_u32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.acq_rel.sys.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_acquire_u32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.acquire.sys.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_relaxed_u32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.relaxed.sys.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_release_u32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.release.sys.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_volatile_u32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.sys.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4 && _CUDA_VSTD::is_integral<_Type>::value && _CUDA_VSTD::is_unsigned<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_min_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_system_tag) {\n    _Type __ret;\n    uint32_t __tmp = 0;\n    memcpy(&__tmp, &__val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_min_acquire_u32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_min_acq_rel_u32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_min_release_u32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_min_relaxed_u32_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_min_volatile_u32_system(__ptr, __tmp, __tmp); __cuda_membar_system(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_min_volatile_u32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_min_volatile_u32_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_acq_rel_f32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.acq_rel.sys.f32 %0,[%1],%2;\" : \"=f\"(__dst) : \"l\"(__ptr),\"f\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_acquire_f32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.acquire.sys.f32 %0,[%1],%2;\" : \"=f\"(__dst) : \"l\"(__ptr),\"f\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_relaxed_f32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.relaxed.sys.f32 %0,[%1],%2;\" : \"=f\"(__dst) : \"l\"(__ptr),\"f\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_release_f32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.release.sys.f32 %0,[%1],%2;\" : \"=f\"(__dst) : \"l\"(__ptr),\"f\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_volatile_f32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.sys.f32 %0,[%1],%2;\" : \"=f\"(__dst) : \"l\"(__ptr),\"f\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4 && _CUDA_VSTD::is_floating_point<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_sub_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_system_tag) {\n    _Type __ret;\n    float __tmp = 0;\n    memcpy(&__tmp, &__val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_sub_acquire_f32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_sub_acq_rel_f32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_sub_release_f32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_sub_relaxed_f32_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_sub_volatile_f32_system(__ptr, __tmp, __tmp); __cuda_membar_system(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_sub_volatile_f32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_sub_volatile_f32_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_acq_rel_u32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.acq_rel.sys.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_acquire_u32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.acquire.sys.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_relaxed_u32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.relaxed.sys.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_release_u32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.release.sys.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_volatile_u32_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.sys.u32 %0,[%1],%2;\" : \"=r\"(__dst) : \"l\"(__ptr),\"r\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==4 && _CUDA_VSTD::is_integral<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_sub_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_system_tag) {\n    _Type __ret;\n    uint32_t __tmp = 0;\n    memcpy(&__tmp, &__val, 4);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_sub_acquire_u32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_sub_acq_rel_u32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_sub_release_u32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_sub_relaxed_u32_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_sub_volatile_u32_system(__ptr, __tmp, __tmp); __cuda_membar_system(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_sub_volatile_u32_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_sub_volatile_u32_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 4);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline _LIBCUDACXX_DEVICE void __cuda_compare_exchange_acq_rel_64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __cmp, _CUDA_D __op) { asm volatile(\"atom.cas.acq_rel.sys.b64 %0,[%1],%2,%3;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__cmp),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline _LIBCUDACXX_DEVICE void __cuda_compare_exchange_acquire_64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __cmp, _CUDA_D __op) { asm volatile(\"atom.cas.acquire.sys.b64 %0,[%1],%2,%3;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__cmp),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline _LIBCUDACXX_DEVICE void __cuda_compare_exchange_relaxed_64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __cmp, _CUDA_D __op) { asm volatile(\"atom.cas.relaxed.sys.b64 %0,[%1],%2,%3;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__cmp),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline _LIBCUDACXX_DEVICE void __cuda_compare_exchange_release_64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __cmp, _CUDA_D __op) { asm volatile(\"atom.cas.release.sys.b64 %0,[%1],%2,%3;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__cmp),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline _LIBCUDACXX_DEVICE void __cuda_compare_exchange_volatile_64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __cmp, _CUDA_D __op) { asm volatile(\"atom.cas.sys.b64 %0,[%1],%2,%3;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__cmp),\"l\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8, int> = 0>\n_LIBCUDACXX_DEVICE bool __atomic_compare_exchange_cuda(volatile _Type *__ptr, _Type *__expected, const _Type *__desired, bool, int __success_memorder, int __failure_memorder, __thread_scope_system_tag) {\n    uint64_t __tmp = 0, __old = 0, __old_tmp;\n    memcpy(&__tmp, __desired, 8);\n    memcpy(&__old, __expected, 8);\n    __old_tmp = __old;\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__stronger_order_cuda(__success_memorder, __failure_memorder)) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_compare_exchange_acquire_64_system(__ptr, __old, __old_tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_compare_exchange_acq_rel_64_system(__ptr, __old, __old_tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_compare_exchange_release_64_system(__ptr, __old, __old_tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_compare_exchange_relaxed_64_system(__ptr, __old, __old_tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__stronger_order_cuda(__success_memorder, __failure_memorder)) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_compare_exchange_volatile_64_system(__ptr, __old, __old_tmp, __tmp); __cuda_membar_system(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_compare_exchange_volatile_64_system(__ptr, __old, __old_tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_compare_exchange_volatile_64_system(__ptr, __old, __old_tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    bool const __ret = __old == __old_tmp;\n    memcpy(__expected, &__old, 8);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_exchange_acq_rel_64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.exch.acq_rel.sys.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_exchange_acquire_64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.exch.acquire.sys.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_exchange_relaxed_64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.exch.relaxed.sys.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_exchange_release_64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.exch.release.sys.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_exchange_volatile_64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.exch.sys.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8, int> = 0>\n_LIBCUDACXX_DEVICE void __atomic_exchange_cuda(volatile _Type *__ptr, _Type *__val, _Type *__ret, int __memorder, __thread_scope_system_tag) {\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, __val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_exchange_acquire_64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_exchange_acq_rel_64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_exchange_release_64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_exchange_relaxed_64_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_exchange_volatile_64_system(__ptr, __tmp, __tmp); __cuda_membar_system(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_exchange_volatile_64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_exchange_volatile_64_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(__ret, &__tmp, 8);\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_and_acq_rel_64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.and.acq_rel.sys.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_and_acquire_64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.and.acquire.sys.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_and_relaxed_64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.and.relaxed.sys.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_and_release_64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.and.release.sys.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_and_volatile_64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.and.sys.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_and_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_system_tag) {\n    _Type __ret;\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_and_acquire_64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_and_acq_rel_64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_and_release_64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_and_relaxed_64_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_and_volatile_64_system(__ptr, __tmp, __tmp); __cuda_membar_system(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_and_volatile_64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_and_volatile_64_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_or_acq_rel_64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.or.acq_rel.sys.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_or_acquire_64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.or.acquire.sys.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_or_relaxed_64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.or.relaxed.sys.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_or_release_64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.or.release.sys.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_or_volatile_64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.or.sys.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_or_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_system_tag) {\n    _Type __ret;\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_or_acquire_64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_or_acq_rel_64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_or_release_64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_or_relaxed_64_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_or_volatile_64_system(__ptr, __tmp, __tmp); __cuda_membar_system(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_or_volatile_64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_or_volatile_64_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_xor_acq_rel_64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.xor.acq_rel.sys.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_xor_acquire_64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.xor.acquire.sys.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_xor_relaxed_64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.xor.relaxed.sys.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_xor_release_64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.xor.release.sys.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_xor_volatile_64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.xor.sys.b64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_xor_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_system_tag) {\n    _Type __ret;\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_xor_acquire_64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_xor_acq_rel_64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_xor_release_64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_xor_relaxed_64_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_xor_volatile_64_system(__ptr, __tmp, __tmp); __cuda_membar_system(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_xor_volatile_64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_xor_volatile_64_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_acq_rel_f64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.acq_rel.sys.f64 %0,[%1],%2;\" : \"=d\"(__dst) : \"l\"(__ptr),\"d\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_acquire_f64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.acquire.sys.f64 %0,[%1],%2;\" : \"=d\"(__dst) : \"l\"(__ptr),\"d\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_relaxed_f64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.relaxed.sys.f64 %0,[%1],%2;\" : \"=d\"(__dst) : \"l\"(__ptr),\"d\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_release_f64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.release.sys.f64 %0,[%1],%2;\" : \"=d\"(__dst) : \"l\"(__ptr),\"d\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_volatile_f64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.sys.f64 %0,[%1],%2;\" : \"=d\"(__dst) : \"l\"(__ptr),\"d\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8 && _CUDA_VSTD::is_floating_point<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_add_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_system_tag) {\n    _Type __ret;\n    double __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_acquire_f64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_add_acq_rel_f64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_add_release_f64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_relaxed_f64_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_volatile_f64_system(__ptr, __tmp, __tmp); __cuda_membar_system(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_add_volatile_f64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_volatile_f64_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_acq_rel_u64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.acq_rel.sys.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_acquire_u64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.acquire.sys.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_relaxed_u64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.relaxed.sys.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_release_u64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.release.sys.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_add_volatile_u64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.add.sys.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8 && _CUDA_VSTD::is_integral<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_add_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_system_tag) {\n    _Type __ret;\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_acquire_u64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_add_acq_rel_u64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_add_release_u64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_relaxed_u64_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_volatile_u64_system(__ptr, __tmp, __tmp); __cuda_membar_system(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_add_volatile_u64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_volatile_u64_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_acq_rel_s64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.acq_rel.sys.s64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_acquire_s64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.acquire.sys.s64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_relaxed_s64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.relaxed.sys.s64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_release_s64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.release.sys.s64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_volatile_s64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.sys.s64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8 && _CUDA_VSTD::is_integral<_Type>::value && _CUDA_VSTD::is_signed<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_max_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_system_tag) {\n    _Type __ret;\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_max_acquire_s64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_max_acq_rel_s64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_max_release_s64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_max_relaxed_s64_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_max_volatile_s64_system(__ptr, __tmp, __tmp); __cuda_membar_system(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_max_volatile_s64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_max_volatile_s64_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_acq_rel_u64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.acq_rel.sys.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_acquire_u64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.acquire.sys.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_relaxed_u64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.relaxed.sys.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_release_u64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.release.sys.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_max_volatile_u64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.max.sys.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8 && _CUDA_VSTD::is_integral<_Type>::value && _CUDA_VSTD::is_unsigned<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_max_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_system_tag) {\n    _Type __ret;\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_max_acquire_u64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_max_acq_rel_u64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_max_release_u64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_max_relaxed_u64_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_max_volatile_u64_system(__ptr, __tmp, __tmp); __cuda_membar_system(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_max_volatile_u64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_max_volatile_u64_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_acq_rel_s64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.acq_rel.sys.s64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_acquire_s64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.acquire.sys.s64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_relaxed_s64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.relaxed.sys.s64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_release_s64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.release.sys.s64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_volatile_s64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.sys.s64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8 && _CUDA_VSTD::is_integral<_Type>::value && _CUDA_VSTD::is_signed<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_min_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_system_tag) {\n    _Type __ret;\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_min_acquire_s64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_min_acq_rel_s64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_min_release_s64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_min_relaxed_s64_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_min_volatile_s64_system(__ptr, __tmp, __tmp); __cuda_membar_system(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_min_volatile_s64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_min_volatile_s64_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_acq_rel_u64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.acq_rel.sys.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_acquire_u64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.acquire.sys.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_relaxed_u64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.relaxed.sys.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_release_u64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.release.sys.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_min_volatile_u64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { asm volatile(\"atom.min.sys.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8 && _CUDA_VSTD::is_integral<_Type>::value && _CUDA_VSTD::is_unsigned<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_min_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_system_tag) {\n    _Type __ret;\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_min_acquire_u64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_min_acq_rel_u64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_min_release_u64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_min_relaxed_u64_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_min_volatile_u64_system(__ptr, __tmp, __tmp); __cuda_membar_system(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_min_volatile_u64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_min_volatile_u64_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_acq_rel_f64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.acq_rel.sys.f64 %0,[%1],%2;\" : \"=d\"(__dst) : \"l\"(__ptr),\"d\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_acquire_f64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.acquire.sys.f64 %0,[%1],%2;\" : \"=d\"(__dst) : \"l\"(__ptr),\"d\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_relaxed_f64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.relaxed.sys.f64 %0,[%1],%2;\" : \"=d\"(__dst) : \"l\"(__ptr),\"d\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_release_f64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.release.sys.f64 %0,[%1],%2;\" : \"=d\"(__dst) : \"l\"(__ptr),\"d\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_volatile_f64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.sys.f64 %0,[%1],%2;\" : \"=d\"(__dst) : \"l\"(__ptr),\"d\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8 && _CUDA_VSTD::is_floating_point<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_sub_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_system_tag) {\n    _Type __ret;\n    double __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_sub_acquire_f64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_sub_acq_rel_f64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_sub_release_f64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_sub_relaxed_f64_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_sub_volatile_f64_system(__ptr, __tmp, __tmp); __cuda_membar_system(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_sub_volatile_f64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_sub_volatile_f64_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_acq_rel_u64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.acq_rel.sys.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_acquire_u64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.acquire.sys.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_relaxed_u64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.relaxed.sys.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_release_u64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.release.sys.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline _LIBCUDACXX_DEVICE void __cuda_fetch_sub_volatile_u64_system(_CUDA_A __ptr, _CUDA_B& __dst, _CUDA_C __op) { __op = -__op;\nasm volatile(\"atom.add.sys.u64 %0,[%1],%2;\" : \"=l\"(__dst) : \"l\"(__ptr),\"l\"(__op) : \"memory\"); }\ntemplate<class _Type, _CUDA_VSTD::__enable_if_t<sizeof(_Type)==8 && _CUDA_VSTD::is_integral<_Type>::value, int> = 0>\n_LIBCUDACXX_DEVICE _Type __atomic_fetch_sub_cuda(volatile _Type *__ptr, _Type __val, int __memorder, __thread_scope_system_tag) {\n    _Type __ret;\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_sub_acquire_u64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_sub_acq_rel_u64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_sub_release_u64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_sub_relaxed_u64_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_sub_volatile_u64_system(__ptr, __tmp, __tmp); __cuda_membar_system(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_sub_volatile_u64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_sub_volatile_u64_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _Type>\n_LIBCUDACXX_DEVICE _Type* __atomic_fetch_add_cuda(_Type *volatile *__ptr, ptrdiff_t __val, int __memorder, __thread_scope_system_tag) {\n    _Type* __ret;\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    __tmp *= sizeof(_Type);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_acquire_u64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_add_acq_rel_u64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_add_release_u64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_relaxed_u64_system(__ptr, __tmp, __tmp); break;\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_volatile_u64_system(__ptr, __tmp, __tmp); __cuda_membar_system(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_add_volatile_u64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_volatile_u64_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\ntemplate<class _Type>\n_LIBCUDACXX_DEVICE _Type* __atomic_fetch_sub_cuda(_Type *volatile *__ptr, ptrdiff_t __val, int __memorder, __thread_scope_system_tag) {\n    _Type* __ret;\n    uint64_t __tmp = 0;\n    memcpy(&__tmp, &__val, 8);\n    __tmp = -__tmp;\n    __tmp *= sizeof(_Type);\n    NV_DISPATCH_TARGET(\n      NV_PROVIDES_SM_70, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_acquire_u64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_ACQ_REL: __cuda_fetch_add_acq_rel_u64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELEASE: __cuda_fetch_add_release_u64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_relaxed_u64_system(__ptr, __tmp, __tmp); break;\n        }\n      ),\n      NV_IS_DEVICE, (\n        switch (__memorder) {\n          case __ATOMIC_SEQ_CST:\n          case __ATOMIC_ACQ_REL: __cuda_membar_system();\n          case __ATOMIC_CONSUME:\n          case __ATOMIC_ACQUIRE: __cuda_fetch_add_volatile_u64_system(__ptr, __tmp, __tmp); __cuda_membar_system(); break;\n          case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_add_volatile_u64_system(__ptr, __tmp, __tmp); break;\n          case __ATOMIC_RELAXED: __cuda_fetch_add_volatile_u64_system(__ptr, __tmp, __tmp); break;\n          default: assert(0);\n        }\n      )\n    )\n    memcpy(&__ret, &__tmp, 8);\n    return __ret;\n}\n", "atomic_nvrtc.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of libcu++, the C++ Standard Library for your entire system,\n// under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX_ATOMIC_NVRTC_H\n#define _LIBCUDACXX_ATOMIC_NVRTC_H\n\n#include \"cxx_atomic.h\"\n\n#endif // _LIBCUDACXX_ATOMIC_NVRTC_H\n", "cfloat": "#ifndef _JITIFY_INCLUDE_GUARD_246D65269667AE14\n#define _JITIFY_INCLUDE_GUARD_246D65269667AE14\n#define cudaDeviceSynchronize() cudaSuccess\n\n#define FLT_RADIX       2\n#define FLT_MANT_DIG    24\n#define DBL_MANT_DIG    53\n#define FLT_DIG         6\n#define DBL_DIG         15\n#define FLT_MIN_EXP     -125\n#define DBL_MIN_EXP     -1021\n#define FLT_MIN_10_EXP  -37\n#define DBL_MIN_10_EXP  -307\n#define FLT_MAX_EXP     128\n#define DBL_MAX_EXP     1024\n#define FLT_MAX_10_EXP  38\n#define DBL_MAX_10_EXP  308\n#define FLT_MAX         3.4028234e38f\n#define DBL_MAX         1.7976931348623157e308\n#define FLT_EPSILON     1.19209289e-7f\n#define DBL_EPSILON     2.220440492503130e-16\n#define FLT_MIN         1.1754943e-38f\n#define DBL_MIN         2.2250738585072013e-308\n#define FLT_ROUNDS      1\n#if defined __cplusplus && __cplusplus >= 201103L\n#define FLT_EVAL_METHOD 0\n#define DECIMAL_DIG     21\n#endif\n\n#endif // _JITIFY_INCLUDE_GUARD_246D65269667AE14\n", "chrono": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===---------------------------- chrono ----------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX_CHRONO\n#define _LIBCUDACXX_CHRONO\n\n/*\n    chrono synopsis\n\nnamespace std\n{\nnamespace chrono\n{\n\ntemplate <class ToDuration, class Rep, class Period>\nconstexpr\nToDuration\nduration_cast(const duration<Rep, Period>& fd);\n\ntemplate <class Rep> struct treat_as_floating_point : is_floating_point<Rep> {};\n\ntemplate <class Rep> inline constexpr bool treat_as_floating_point_v\n    = treat_as_floating_point<Rep>::value;                       // C++17\n\ntemplate <class Rep>\nstruct duration_values\n{\npublic:\n    static constexpr Rep zero(); // noexcept in C++20\n    static constexpr Rep max();  // noexcept in C++20\n    static constexpr Rep min();  // noexcept in C++20\n};\n\n// duration\n\ntemplate <class Rep, class Period = ratio<1>>\nclass duration\n{\n    static_assert(!__is_duration<Rep>::value, \"A duration representation can not be a duration\");\n    static_assert(__is_ratio<Period>::value, \"Second template parameter of duration must be a std::ratio\");\n    static_assert(Period::num > 0, \"duration period must be positive\");\npublic:\n    typedef Rep rep;\n    typedef typename _Period::type period;\n\n    constexpr duration() = default;\n    template <class Rep2>\n        constexpr explicit duration(const Rep2& r,\n            typename enable_if\n            <\n               is_convertible<Rep2, rep>::value &&\n               (treat_as_floating_point<rep>::value ||\n               !treat_as_floating_point<rep>::value && !treat_as_floating_point<Rep2>::value)\n            >::type* = 0);\n\n    // conversions\n    template <class Rep2, class Period2>\n        constexpr duration(const duration<Rep2, Period2>& d,\n            typename enable_if\n            <\n                treat_as_floating_point<rep>::value ||\n                ratio_divide<Period2, period>::type::den == 1\n            >::type* = 0);\n\n    // observer\n\n    constexpr rep count() const;\n\n    // arithmetic\n\n    constexpr common_type<duration>::type  operator+() const;\n    constexpr common_type<duration>::type  operator-() const;\n    constexpr duration& operator++();    // constexpr in C++17\n    constexpr duration  operator++(int); // constexpr in C++17\n    constexpr duration& operator--();    // constexpr in C++17\n    constexpr duration  operator--(int); // constexpr in C++17\n\n    constexpr duration& operator+=(const duration& d);  // constexpr in C++17\n    constexpr duration& operator-=(const duration& d);  // constexpr in C++17\n\n    duration& operator*=(const rep& rhs);       // constexpr in C++17\n    duration& operator/=(const rep& rhs);       // constexpr in C++17\n    duration& operator%=(const rep& rhs);       // constexpr in C++17\n    duration& operator%=(const duration& rhs);  // constexpr in C++17\n\n    // special values\n\n    static constexpr duration zero(); // noexcept in C++20\n    static constexpr duration min();  // noexcept in C++20\n    static constexpr duration max();  // noexcept in C++20\n};\n\ntypedef duration<long long,         nano> nanoseconds;\ntypedef duration<long long,        micro> microseconds;\ntypedef duration<long long,        milli> milliseconds;\ntypedef duration<long long              > seconds;\ntypedef duration<     long, ratio<  60> > minutes;\ntypedef duration<     long, ratio<3600> > hours;\n\ntemplate <class Clock, class Duration = typename Clock::duration>\nclass time_point\n{\npublic:\n    typedef Clock                     clock;\n    typedef Duration                  duration;\n    typedef typename duration::rep    rep;\n    typedef typename duration::period period;\nprivate:\n    duration d_;  // exposition only\n\npublic:\n    time_point();  // has value \"epoch\" // constexpr in C++14\n    explicit time_point(const duration& d);  // same as time_point() + d // constexpr in C++14\n\n    // conversions\n    template <class Duration2>\n       time_point(const time_point<clock, Duration2>& t); // constexpr in C++14\n\n    // observer\n\n    duration time_since_epoch() const; // constexpr in C++14\n\n    // arithmetic\n\n    time_point& operator+=(const duration& d); // constexpr in C++17\n    time_point& operator-=(const duration& d); // constexpr in C++17\n\n    // special values\n\n    static constexpr time_point min();  // noexcept in C++20\n    static constexpr time_point max();  // noexcept in C++20\n};\n\n} // chrono\n\n// common_type traits\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\n  struct common_type<chrono::duration<Rep1, Period1>, chrono::duration<Rep2, Period2>>;\n\ntemplate <class Clock, class Duration1, class Duration2>\n  struct common_type<chrono::time_point<Clock, Duration1>, chrono::time_point<Clock, Duration2>>;\n\nnamespace chrono {\n\n\ntemplate<class T> struct is_clock;  // C++20\ntemplate<class T> inline constexpr bool is_clock_v = is_clock<T>::value;   // C++20\n\n\n// duration arithmetic\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\n  constexpr\n  typename common_type<duration<Rep1, Period1>, duration<Rep2, Period2>>::type\n  operator+(const duration<Rep1, Period1>& lhs, const duration<Rep2, Period2>& rhs);\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\n  constexpr\n  typename common_type<duration<Rep1, Period1>, duration<Rep2, Period2>>::type\n  operator-(const duration<Rep1, Period1>& lhs, const duration<Rep2, Period2>& rhs);\ntemplate <class Rep1, class Period, class Rep2>\n  constexpr\n  duration<typename common_type<Rep1, Rep2>::type, Period>\n  operator*(const duration<Rep1, Period>& d, const Rep2& s);\ntemplate <class Rep1, class Period, class Rep2>\n  constexpr\n  duration<typename common_type<Rep1, Rep2>::type, Period>\n  operator*(const Rep1& s, const duration<Rep2, Period>& d);\ntemplate <class Rep1, class Period, class Rep2>\n  constexpr\n  duration<typename common_type<Rep1, Rep2>::type, Period>\n  operator/(const duration<Rep1, Period>& d, const Rep2& s);\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\n  constexpr\n  typename common_type<Rep1, Rep2>::type\n  operator/(const duration<Rep1, Period1>& lhs, const duration<Rep2, Period2>& rhs);\n\n// duration comparisons\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\n   constexpr\n   bool operator==(const duration<Rep1, Period1>& lhs, const duration<Rep2, Period2>& rhs);\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\n   constexpr\n   bool operator!=(const duration<Rep1, Period1>& lhs, const duration<Rep2, Period2>& rhs);\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\n   constexpr\n   bool operator< (const duration<Rep1, Period1>& lhs, const duration<Rep2, Period2>& rhs);\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\n   constexpr\n   bool operator<=(const duration<Rep1, Period1>& lhs, const duration<Rep2, Period2>& rhs);\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\n   constexpr\n   bool operator> (const duration<Rep1, Period1>& lhs, const duration<Rep2, Period2>& rhs);\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\n   constexpr\n   bool operator>=(const duration<Rep1, Period1>& lhs, const duration<Rep2, Period2>& rhs);\n\n// duration_cast\ntemplate <class ToDuration, class Rep, class Period>\n  ToDuration duration_cast(const duration<Rep, Period>& d);\n\ntemplate <class ToDuration, class Rep, class Period>\n    constexpr ToDuration floor(const duration<Rep, Period>& d);    // C++17\ntemplate <class ToDuration, class Rep, class Period>\n    constexpr ToDuration ceil(const duration<Rep, Period>& d);     // C++17\ntemplate <class ToDuration, class Rep, class Period>\n    constexpr ToDuration round(const duration<Rep, Period>& d);    // C++17\n\n// duration I/O is elsewhere\n\n// time_point arithmetic (all constexpr in C++14)\ntemplate <class Clock, class Duration1, class Rep2, class Period2>\n  time_point<Clock, typename common_type<Duration1, duration<Rep2, Period2>>::type>\n  operator+(const time_point<Clock, Duration1>& lhs, const duration<Rep2, Period2>& rhs);\ntemplate <class Rep1, class Period1, class Clock, class Duration2>\n  time_point<Clock, typename common_type<duration<Rep1, Period1>, Duration2>::type>\n  operator+(const duration<Rep1, Period1>& lhs, const time_point<Clock, Duration2>& rhs);\ntemplate <class Clock, class Duration1, class Rep2, class Period2>\n  time_point<Clock, typename common_type<Duration1, duration<Rep2, Period2>>::type>\n  operator-(const time_point<Clock, Duration1>& lhs, const duration<Rep2, Period2>& rhs);\ntemplate <class Clock, class Duration1, class Duration2>\n  typename common_type<Duration1, Duration2>::type\n  operator-(const time_point<Clock, Duration1>& lhs, const time_point<Clock, Duration2>& rhs);\n\n// time_point comparisons (all constexpr in C++14)\ntemplate <class Clock, class Duration1, class Duration2>\n   bool operator==(const time_point<Clock, Duration1>& lhs, const time_point<Clock, Duration2>& rhs);\ntemplate <class Clock, class Duration1, class Duration2>\n   bool operator!=(const time_point<Clock, Duration1>& lhs, const time_point<Clock, Duration2>& rhs);\ntemplate <class Clock, class Duration1, class Duration2>\n   bool operator< (const time_point<Clock, Duration1>& lhs, const time_point<Clock, Duration2>& rhs);\ntemplate <class Clock, class Duration1, class Duration2>\n   bool operator<=(const time_point<Clock, Duration1>& lhs, const time_point<Clock, Duration2>& rhs);\ntemplate <class Clock, class Duration1, class Duration2>\n   bool operator> (const time_point<Clock, Duration1>& lhs, const time_point<Clock, Duration2>& rhs);\ntemplate <class Clock, class Duration1, class Duration2>\n   bool operator>=(const time_point<Clock, Duration1>& lhs, const time_point<Clock, Duration2>& rhs);\n\n// time_point_cast (constexpr in C++14)\n\ntemplate <class ToDuration, class Clock, class Duration>\n  time_point<Clock, ToDuration> time_point_cast(const time_point<Clock, Duration>& t);\n\ntemplate <class ToDuration, class Clock, class Duration>\n    constexpr time_point<Clock, ToDuration>\n    floor(const time_point<Clock, Duration>& tp);                  // C++17\n\ntemplate <class ToDuration, class Clock, class Duration>\n    constexpr time_point<Clock, ToDuration>\n    ceil(const time_point<Clock, Duration>& tp);                   // C++17\n\ntemplate <class ToDuration, class Clock, class Duration>\n    constexpr time_point<Clock, ToDuration>\n    round(const time_point<Clock, Duration>& tp);                  // C++17\n\ntemplate <class Rep, class Period>\n    constexpr duration<Rep, Period> abs(duration<Rep, Period> d);  // C++17\n\n// Clocks\n\nclass system_clock\n{\npublic:\n    typedef microseconds                     duration;\n    typedef duration::rep                    rep;\n    typedef duration::period                 period;\n    typedef chrono::time_point<system_clock> time_point;\n    static const bool is_steady =            false; // constexpr in C++14\n\n    static time_point now() noexcept;\n    static time_t     to_time_t  (const time_point& __t) noexcept;\n    static time_point from_time_t(time_t __t) noexcept;\n};\n\ntemplate <class Duration>\n  using sys_time  = time_point<system_clock, Duration>; // C++20\nusing sys_seconds = sys_time<seconds>;                  // C++20\nusing sys_days    = sys_time<days>;                     // C++20\n\nclass utc_clock;                                        // C++20\n\ntemplate <class Duration>\n  using utc_time  = time_point<utc_clock, Duration>;    // C++20\nusing utc_seconds = utc_time<seconds>;                  // C++20\n\nclass tai_clock;                                        // C++20\n\ntemplate <class Duration>\n  using tai_time  = time_point<tai_clock, Duration>;    // C++20\nusing tai_seconds = tai_time<seconds>;                  // C++20\n\nclass file_clock;                                       // C++20\n\ntemplate<class Duration>\n  using file_time = time_point<file_clock, Duration>;   // C++20\n\nclass steady_clock\n{\npublic:\n    typedef nanoseconds                                   duration;\n    typedef duration::rep                                 rep;\n    typedef duration::period                              period;\n    typedef chrono::time_point<steady_clock, duration>    time_point;\n    static const bool is_steady =                         true; // constexpr in C++14\n\n    static time_point now() noexcept;\n};\n\ntypedef steady_clock high_resolution_clock;\n\n// 25.7.8, local time           // C++20\nstruct local_t {};\ntemplate<class Duration>\n  using local_time  = time_point<local_t, Duration>;\nusing local_seconds = local_time<seconds>;\nusing local_days    = local_time<days>;\n\n// 25.7.9, time_point conversions template<class DestClock, class SourceClock>    // C++20\nstruct clock_time_conversion;\n\ntemplate<class DestClock, class SourceClock, class Duration>\n  auto clock_cast(const time_point<SourceClock, Duration>& t);\n\n// 25.8.2, class last_spec    // C++20\nstruct last_spec;\n\n// 25.8.3, class day          // C++20\n\nclass day;\nconstexpr bool operator==(const day& x, const day& y) noexcept;\nconstexpr bool operator!=(const day& x, const day& y) noexcept;\nconstexpr bool operator< (const day& x, const day& y) noexcept;\nconstexpr bool operator> (const day& x, const day& y) noexcept;\nconstexpr bool operator<=(const day& x, const day& y) noexcept;\nconstexpr bool operator>=(const day& x, const day& y) noexcept;\nconstexpr day  operator+(const day&  x, const days& y) noexcept;\nconstexpr day  operator+(const days& x, const day&  y) noexcept;\nconstexpr day  operator-(const day&  x, const days& y) noexcept;\nconstexpr days operator-(const day&  x, const day&  y) noexcept;\n\n// 25.8.4, class month    // C++20\nclass month;\nconstexpr bool operator==(const month& x, const month& y) noexcept;\nconstexpr bool operator!=(const month& x, const month& y) noexcept;\nconstexpr bool operator< (const month& x, const month& y) noexcept;\nconstexpr bool operator> (const month& x, const month& y) noexcept;\nconstexpr bool operator<=(const month& x, const month& y) noexcept;\nconstexpr bool operator>=(const month& x, const month& y) noexcept;\nconstexpr month  operator+(const month&  x, const months& y) noexcept;\nconstexpr month  operator+(const months& x,  const month& y) noexcept;\nconstexpr month  operator-(const month&  x, const months& y) noexcept;\nconstexpr months operator-(const month&  x,  const month& y) noexcept;\n\n// 25.8.5, class year    // C++20\nclass year;\nconstexpr bool operator==(const year& x, const year& y) noexcept;\nconstexpr bool operator!=(const year& x, const year& y) noexcept;\nconstexpr bool operator< (const year& x, const year& y) noexcept;\nconstexpr bool operator> (const year& x, const year& y) noexcept;\nconstexpr bool operator<=(const year& x, const year& y) noexcept;\nconstexpr bool operator>=(const year& x, const year& y) noexcept;\nconstexpr year  operator+(const year&  x, const years& y) noexcept;\nconstexpr year  operator+(const years& x, const year&  y) noexcept;\nconstexpr year  operator-(const year&  x, const years& y) noexcept;\nconstexpr years operator-(const year&  x, const year&  y) noexcept;\n\n// 25.8.6, class weekday    // C++20\nclass weekday;\n\nconstexpr bool operator==(const weekday& x, const weekday& y) noexcept;\nconstexpr bool operator!=(const weekday& x, const weekday& y) noexcept;\nconstexpr weekday operator+(const weekday& x, const days&    y) noexcept;\nconstexpr weekday operator+(const days&    x, const weekday& y) noexcept;\nconstexpr weekday operator-(const weekday& x, const days&    y) noexcept;\nconstexpr days    operator-(const weekday& x, const weekday& y) noexcept;\n\n// 25.8.7, class weekday_indexed    // C++20\n\nclass weekday_indexed;\nconstexpr bool operator==(const weekday_indexed& x, const weekday_indexed& y) noexcept;\nconstexpr bool operator!=(const weekday_indexed& x, const weekday_indexed& y) noexcept;\n\n// 25.8.8, class weekday_last    // C++20\nclass weekday_last;\n\nconstexpr bool operator==(const weekday_last& x, const weekday_last& y) noexcept;\nconstexpr bool operator!=(const weekday_last& x, const weekday_last& y) noexcept;\n\n// 25.8.9, class month_day    // C++20\nclass month_day;\n\nconstexpr bool operator==(const month_day& x, const month_day& y) noexcept;\nconstexpr bool operator!=(const month_day& x, const month_day& y) noexcept;\nconstexpr bool operator< (const month_day& x, const month_day& y) noexcept;\nconstexpr bool operator> (const month_day& x, const month_day& y) noexcept;\nconstexpr bool operator<=(const month_day& x, const month_day& y) noexcept;\nconstexpr bool operator>=(const month_day& x, const month_day& y) noexcept;\n\n\n// 25.8.10, class month_day_last    // C++20\nclass month_day_last;\n\nconstexpr bool operator==(const month_day_last& x, const month_day_last& y) noexcept;\nconstexpr bool operator!=(const month_day_last& x, const month_day_last& y) noexcept;\nconstexpr bool operator< (const month_day_last& x, const month_day_last& y) noexcept;\nconstexpr bool operator> (const month_day_last& x, const month_day_last& y) noexcept;\nconstexpr bool operator<=(const month_day_last& x, const month_day_last& y) noexcept;\nconstexpr bool operator>=(const month_day_last& x, const month_day_last& y) noexcept;\n\n// 25.8.11, class month_weekday    // C++20\nclass month_weekday;\n\nconstexpr bool operator==(const month_weekday& x, const month_weekday& y) noexcept;\nconstexpr bool operator!=(const month_weekday& x, const month_weekday& y) noexcept;\n\n// 25.8.12, class month_weekday_last    // C++20\nclass month_weekday_last;\n\nconstexpr bool operator==(const month_weekday_last& x, const month_weekday_last& y) noexcept;\nconstexpr bool operator!=(const month_weekday_last& x, const month_weekday_last& y) noexcept;\n\n\n// 25.8.13, class year_month    // C++20\nclass year_month;\n\nconstexpr bool operator==(const year_month& x, const year_month& y) noexcept;\nconstexpr bool operator!=(const year_month& x, const year_month& y) noexcept;\nconstexpr bool operator< (const year_month& x, const year_month& y) noexcept;\nconstexpr bool operator> (const year_month& x, const year_month& y) noexcept;\nconstexpr bool operator<=(const year_month& x, const year_month& y) noexcept;\nconstexpr bool operator>=(const year_month& x, const year_month& y) noexcept;\n\nconstexpr year_month operator+(const year_month& ym, const months& dm) noexcept;\nconstexpr year_month operator+(const months& dm, const year_month& ym) noexcept;\nconstexpr year_month operator-(const year_month& ym, const months& dm) noexcept;\nconstexpr months operator-(const year_month& x, const year_month& y) noexcept;\nconstexpr year_month operator+(const year_month& ym, const years& dy) noexcept;\nconstexpr year_month operator+(const years& dy, const year_month& ym) noexcept;\nconstexpr year_month operator-(const year_month& ym, const years& dy) noexcept;\n\n// 25.8.14, class year_month_day class    // C++20\nyear_month_day;\n\nconstexpr bool operator==(const year_month_day& x, const year_month_day& y) noexcept;\nconstexpr bool operator!=(const year_month_day& x, const year_month_day& y) noexcept;\nconstexpr bool operator< (const year_month_day& x, const year_month_day& y) noexcept;\nconstexpr bool operator> (const year_month_day& x, const year_month_day& y) noexcept;\nconstexpr bool operator<=(const year_month_day& x, const year_month_day& y) noexcept;\nconstexpr bool operator>=(const year_month_day& x, const year_month_day& y) noexcept;\n\nconstexpr year_month_day operator+(const year_month_day& ymd, const months& dm) noexcept;\nconstexpr year_month_day operator+(const months& dm, const year_month_day& ymd) noexcept;\nconstexpr year_month_day operator+(const year_month_day& ymd, const years& dy) noexcept;\nconstexpr year_month_day operator+(const years& dy, const year_month_day& ymd) noexcept;\nconstexpr year_month_day operator-(const year_month_day& ymd, const months& dm) noexcept;\nconstexpr year_month_day operator-(const year_month_day& ymd, const years& dy) noexcept;\n\n\n// 25.8.15, class year_month_day_last    // C++20\nclass year_month_day_last;\n\nconstexpr bool operator==(const year_month_day_last& x,\n                          const year_month_day_last& y) noexcept;\nconstexpr bool operator!=(const year_month_day_last& x,\n                          const year_month_day_last& y) noexcept;\nconstexpr bool operator< (const year_month_day_last& x,\n                          const year_month_day_last& y) noexcept;\nconstexpr bool operator> (const year_month_day_last& x,\n                          const year_month_day_last& y) noexcept;\nconstexpr bool operator<=(const year_month_day_last& x,\n                          const year_month_day_last& y) noexcept;\nconstexpr bool operator>=(const year_month_day_last& x,\n                          const year_month_day_last& y) noexcept;\n\nconstexpr year_month_day_last\n  operator+(const year_month_day_last& ymdl, const months& dm) noexcept;\nconstexpr year_month_day_last\n  operator+(const months& dm, const year_month_day_last& ymdl) noexcept;\nconstexpr year_month_day_last\n  operator+(const year_month_day_last& ymdl, const years& dy) noexcept;\nconstexpr year_month_day_last\n  operator+(const years& dy, const year_month_day_last& ymdl) noexcept;\nconstexpr year_month_day_last\n  operator-(const year_month_day_last& ymdl, const months& dm) noexcept;\nconstexpr year_month_day_last\n  operator-(const year_month_day_last& ymdl, const years& dy) noexcept;\n\n// 25.8.16, class year_month_weekday    // C++20\nclass year_month_weekday;\n\nconstexpr bool operator==(const year_month_weekday& x,\n                          const year_month_weekday& y) noexcept;\nconstexpr bool operator!=(const year_month_weekday& x,\n                          const year_month_weekday& y) noexcept;\n\nconstexpr year_month_weekday\n  operator+(const year_month_weekday& ymwd, const months& dm) noexcept;\nconstexpr year_month_weekday\n  operator+(const months& dm, const year_month_weekday& ymwd) noexcept;\nconstexpr year_month_weekday\n  operator+(const year_month_weekday& ymwd, const years& dy) noexcept;\nconstexpr year_month_weekday\n  operator+(const years& dy, const year_month_weekday& ymwd) noexcept;\nconstexpr year_month_weekday\n  operator-(const year_month_weekday& ymwd, const months& dm) noexcept;\nconstexpr year_month_weekday\n  operator-(const year_month_weekday& ymwd, const years& dy) noexcept;\n\n// 25.8.17, class year_month_weekday_last    // C++20\nclass year_month_weekday_last;\n\nconstexpr bool operator==(const year_month_weekday_last& x,\n                          const year_month_weekday_last& y) noexcept;\nconstexpr bool operator!=(const year_month_weekday_last& x,\n                          const year_month_weekday_last& y) noexcept;\nconstexpr year_month_weekday_last\n  operator+(const year_month_weekday_last& ymwdl, const months& dm) noexcept;\nconstexpr year_month_weekday_last\n  operator+(const months& dm, const year_month_weekday_last& ymwdl) noexcept;\nconstexpr year_month_weekday_last\n  operator+(const year_month_weekday_last& ymwdl, const years& dy) noexcept;\nconstexpr year_month_weekday_last\n  operator+(const years& dy, const year_month_weekday_last& ymwdl) noexcept;\nconstexpr year_month_weekday_last\n  operator-(const year_month_weekday_last& ymwdl, const months& dm) noexcept;\nconstexpr year_month_weekday_last\n  operator-(const year_month_weekday_last& ymwdl, const years& dy) noexcept;\n\n// 25.8.18, civil calendar conventional syntax operators    // C++20\nconstexpr year_month\n  operator/(const year& y, const month& m) noexcept;\nconstexpr year_month\n  operator/(const year& y, int m) noexcept;\nconstexpr month_day\n  operator/(const month& m, const day& d) noexcept;\nconstexpr month_day\n  operator/(const month& m, int d) noexcept;\nconstexpr month_day\n  operator/(int m, const day& d) noexcept;\nconstexpr month_day\n  operator/(const day& d, const month& m) noexcept;\nconstexpr month_day\n  operator/(const day& d, int m) noexcept;\nconstexpr month_day_last\n  operator/(const month& m, last_spec) noexcept;\nconstexpr month_day_last\n  operator/(int m, last_spec) noexcept;\nconstexpr month_day_last\n  operator/(last_spec, const month& m) noexcept;\nconstexpr month_day_last\n  operator/(last_spec, int m) noexcept;\nconstexpr month_weekday\n  operator/(const month& m, const weekday_indexed& wdi) noexcept;\nconstexpr month_weekday\n  operator/(int m, const weekday_indexed& wdi) noexcept;\nconstexpr month_weekday\n  operator/(const weekday_indexed& wdi, const month& m) noexcept;\nconstexpr month_weekday\n  operator/(const weekday_indexed& wdi, int m) noexcept;\nconstexpr month_weekday_last\n  operator/(const month& m, const weekday_last& wdl) noexcept;\nconstexpr month_weekday_last\n  operator/(int m, const weekday_last& wdl) noexcept;\nconstexpr month_weekday_last\n  operator/(const weekday_last& wdl, const month& m) noexcept;\nconstexpr month_weekday_last\n  operator/(const weekday_last& wdl, int m) noexcept;\nconstexpr year_month_day\n  operator/(const year_month& ym, const day& d) noexcept;\nconstexpr year_month_day\n  operator/(const year_month& ym, int d) noexcept;\nconstexpr year_month_day\n  operator/(const year& y, const month_day& md) noexcept;\nconstexpr year_month_day\n  operator/(int y, const month_day& md) noexcept;\nconstexpr year_month_day\n  operator/(const month_day& md, const year& y) noexcept;\nconstexpr year_month_day\n  operator/(const month_day& md, int y) noexcept;\nconstexpr year_month_day_last\n  operator/(const year_month& ym, last_spec) noexcept;\nconstexpr year_month_day_last\n  operator/(const year& y, const month_day_last& mdl) noexcept;\nconstexpr year_month_day_last\n  operator/(int y, const month_day_last& mdl) noexcept;\nconstexpr year_month_day_last\n  operator/(const month_day_last& mdl, const year& y) noexcept;\nconstexpr year_month_day_last\n  operator/(const month_day_last& mdl, int y) noexcept;\nconstexpr year_month_weekday\n  operator/(const year_month& ym, const weekday_indexed& wdi) noexcept;\nconstexpr year_month_weekday\n  operator/(const year& y, const month_weekday& mwd) noexcept;\nconstexpr year_month_weekday\n  operator/(int y, const month_weekday& mwd) noexcept;\nconstexpr year_month_weekday\n  operator/(const month_weekday& mwd, const year& y) noexcept;\nconstexpr year_month_weekday\n  operator/(const month_weekday& mwd, int y) noexcept;\nconstexpr year_month_weekday_last\n  operator/(const year_month& ym, const weekday_last& wdl) noexcept;\nconstexpr year_month_weekday_last\n  operator/(const year& y, const month_weekday_last& mwdl) noexcept;\nconstexpr year_month_weekday_last\n  operator/(int y, const month_weekday_last& mwdl) noexcept;\nconstexpr year_month_weekday_last\n  operator/(const month_weekday_last& mwdl, const year& y) noexcept;\nconstexpr year_month_weekday_last\n  operator/(const month_weekday_last& mwdl, int y) noexcept;\n\n// 26.9, class template hh_mm_ss\ntemplate <class Duration>\nclass hh_mm_ss\n{\n    bool            is_neg; // exposition only\n    chrono::hours   h;      // exposition only\n    chrono::minutes m;      // exposition only\n    chrono::seconds s;      // exposition only\n    precision       ss;     // exposition only\n\npublic:\n    static unsigned constexpr fractional_width = see below;\n    using precision                            = see below;\n\n    constexpr hh_mm_ss() noexcept : hh_mm_ss{Duration::zero()} {}\n    constexpr explicit hh_mm_ss(Duration d) noexcept;\n\n    constexpr bool is_negative() const noexcept;\n    constexpr chrono::hours hours() const noexcept;\n    constexpr chrono::minutes minutes() const noexcept;\n    constexpr chrono::seconds seconds() const noexcept;\n    constexpr precision subseconds() const noexcept;\n\n    constexpr explicit operator  precision()   const noexcept;\n    constexpr          precision to_duration() const noexcept;\n};\n\ntemplate <class charT, class traits, class Duration>\n  basic_ostream<charT, traits>&\n    operator<<(basic_ostream<charT, traits>& os, hh_mm_ss<Duration> const& hms);\n\n// 26.10, 12/24 hour functions\nconstexpr bool is_am(hours const& h) noexcept;\nconstexpr bool is_pm(hours const& h) noexcept;\nconstexpr hours make12(const hours& h) noexcept;\nconstexpr hours make24(const hours& h, bool is_pm) noexcept;\n\n\n// 25.10.2, time zone database     // C++20\nstruct tzdb;\nclass tzdb_list;\n\n// 25.10.2.3, time zone database access    // C++20\nconst tzdb& get_tzdb();\ntzdb_list& get_tzdb_list();\nconst time_zone* locate_zone(string_view tz_name);\nconst time_zone* current_zone();\n\n// 25.10.2.4, remote time zone database support    // C++20\nconst tzdb& reload_tzdb();\nstring remote_version();\n\n// 25.10.3, exception classes    // C++20\nclass nonexistent_local_time;\nclass ambiguous_local_time;\n\n// 25.10.4, information classes    // C++20\nstruct sys_info;\nstruct local_info;\n\n// 25.10.5, class time_zone    // C++20\nenum class choose {earliest, latest};\nclass time_zone;\nbool operator==(const time_zone& x, const time_zone& y) noexcept;\nbool operator!=(const time_zone& x, const time_zone& y) noexcept;\nbool operator<(const time_zone& x, const time_zone& y) noexcept;\nbool operator>(const time_zone& x, const time_zone& y) noexcept;\nbool operator<=(const time_zone& x, const time_zone& y) noexcept;\nbool operator>=(const time_zone& x, const time_zone& y) noexcept;\n\n// 25.10.6, class template zoned_traits    // C++20\ntemplate<class T> struct zoned_traits;\n\n// 25.10.7, class template zoned_time    // C++20\ntemplate<class Duration, class TimeZonePtr = const time_zone*> class zoned_time;\nusing zoned_seconds = zoned_time<seconds>;\n\ntemplate<class Duration1, class Duration2, class TimeZonePtr>\n  bool operator==(const zoned_time<Duration1, TimeZonePtr>& x,\n                  const zoned_time<Duration2, TimeZonePtr>& y);\ntemplate<class Duration1, class Duration2, class TimeZonePtr>\n  bool operator!=(const zoned_time<Duration1, TimeZonePtr>& x,\n                  const zoned_time<Duration2, TimeZonePtr>& y);\n\n// 25.10.8, leap second support    // C++20\nclass leap;\n\nbool operator==(const leap& x, const leap& y);\nbool operator!=(const leap& x, const leap& y);\nbool operator< (const leap& x, const leap& y);\nbool operator> (const leap& x, const leap& y);\nbool operator<=(const leap& x, const leap& y);\nbool operator>=(const leap& x, const leap& y);\ntemplate<class Duration>\n  bool operator==(const leap& x, const sys_time<Duration>& y);\ntemplate<class Duration>\n  bool operator==(const sys_time<Duration>& x, const leap& y);\ntemplate<class Duration>\n  bool operator!=(const leap& x, const sys_time<Duration>& y);\ntemplate<class Duration>\n  bool operator!=(const sys_time<Duration>& x, const leap& y);\ntemplate<class Duration>\n  bool operator< (const leap& x, const sys_time<Duration>& y);\ntemplate<class Duration>\n  bool operator< (const sys_time<Duration>& x, const leap& y);\ntemplate<class Duration>\n  bool operator> (const leap& x, const sys_time<Duration>& y);\ntemplate<class Duration>\n  bool operator> (const sys_time<Duration>& x, const leap& y);\ntemplate<class Duration>\n  bool operator<=(const leap& x, const sys_time<Duration>& y);\ntemplate<class Duration>\n  bool operator<=(const sys_time<Duration>& x, const leap& y);\ntemplate<class Duration>\n  bool operator>=(const leap& x, const sys_time<Duration>& y);\ntemplate<class Duration>\n  bool operator>=(const sys_time<Duration>& x, const leap& y);\n\n// 25.10.9, class link    // C++20\nclass link;\nbool operator==(const link& x, const link& y);\nbool operator!=(const link& x, const link& y);\nbool operator< (const link& x, const link& y);\nbool operator> (const link& x, const link& y);\nbool operator<=(const link& x, const link& y);\nbool operator>=(const link& x, const link& y);\n\n// 25.11, formatting    // C++20\ntemplate<class charT, class Streamable>\n  basic_string<charT>\n    format(const charT* fmt, const Streamable& s);\n\ntemplate<class charT, class Streamable>\n  basic_string<charT>\n    format(const locale& loc, const charT* fmt, const Streamable& s);\n\ntemplate<class charT, class traits, class Alloc, class Streamable>\n  basic_string<charT, traits, Alloc>\n    format(const basic_string<charT, traits, Alloc>& fmt, const Streamable& s);\n\ntemplate<class charT, class traits, class Alloc, class Streamable>\n  basic_string<charT, traits, Alloc>\n    format(const locale& loc, const basic_string<charT, traits, Alloc>& fmt,\n           const Streamable& s);\n\n// 25.12, parsing    // C++20\ntemplate<class charT, class traits, class Alloc, class Parsable>\nunspecified\n    parse(const basic_string<charT, traits, Alloc>& format, Parsable& tp);\n\ntemplate<class charT, class traits, class Alloc, class Parsable>\nunspecified\n    parse(const basic_string<charT, traits, Alloc>& format, Parsable& tp,\n          basic_string<charT, traits, Alloc>& abbrev);\n\ntemplate<class charT, class traits, class Alloc, class Parsable>\nunspecified\n    parse(const basic_string<charT, traits, Alloc>& format, Parsable& tp,\n          minutes& offset);\n\ntemplate<class charT, class traits, class Alloc, class Parsable>\nunspecified\n    parse(const basic_string<charT, traits, Alloc>& format, Parsable& tp,\n          basic_string<charT, traits, Alloc>& abbrev, minutes& offset);\n\n// calendrical constants\ninline constexpr last_spec                              last{};       // C++20\ninline constexpr chrono::weekday                        Sunday{0};    // C++20\ninline constexpr chrono::weekday                        Monday{1};    // C++20\ninline constexpr chrono::weekday                        Tuesday{2};   // C++20\ninline constexpr chrono::weekday                        Wednesday{3}; // C++20\ninline constexpr chrono::weekday                        Thursday{4};  // C++20\ninline constexpr chrono::weekday                        Friday{5};    // C++20\ninline constexpr chrono::weekday                        Saturday{6};  // C++20\n\ninline constexpr chrono::month                          January{1};   // C++20\ninline constexpr chrono::month                          February{2};  // C++20\ninline constexpr chrono::month                          March{3};     // C++20\ninline constexpr chrono::month                          April{4};     // C++20\ninline constexpr chrono::month                          May{5};       // C++20\ninline constexpr chrono::month                          June{6};      // C++20\ninline constexpr chrono::month                          July{7};      // C++20\ninline constexpr chrono::month                          August{8};    // C++20\ninline constexpr chrono::month                          September{9}; // C++20\ninline constexpr chrono::month                          October{10};  // C++20\ninline constexpr chrono::month                          November{11}; // C++20\ninline constexpr chrono::month                          December{12}; // C++20\n}  // chrono\n\ninline namespace literals {\n  inline namespace chrono_literals {\nconstexpr chrono::hours                                 operator \"\"h(unsigned long long); // C++14\nconstexpr chrono::duration<unspecified , ratio<3600,1>> operator \"\"h(long double); // C++14\nconstexpr chrono::minutes                               operator \"\"min(unsigned long long); // C++14\nconstexpr chrono::duration<unspecified , ratio<60,1>>   operator \"\"min(long double); // C++14\nconstexpr chrono::seconds                               operator \"\"s(unsigned long long); // C++14\nconstexpr chrono::duration<unspecified >                operator \"\"s(long double); // C++14\nconstexpr chrono::milliseconds                          operator \"\"ms(unsigned long long); // C++14\nconstexpr chrono::duration<unspecified , milli>         operator \"\"ms(long double); // C++14\nconstexpr chrono::microseconds                          operator \"\"us(unsigned long long); // C++14\nconstexpr chrono::duration<unspecified , micro>         operator \"\"us(long double); // C++14\nconstexpr chrono::nanoseconds                           operator \"\"ns(unsigned long long); // C++14\nconstexpr chrono::duration<unspecified , nano>          operator \"\"ns(long double); // C++14\nconstexpr chrono::day                                   operator \"\"d(unsigned long long d) noexcept; // C++20\nconstexpr chrono::year                                  operator \"\"y(unsigned long long y) noexcept; // C++20\n}  // chrono_literals\n}  // literals\n\n}  // std\n*/\n\n#ifndef __cuda_std__\n#include <__config>\n#else\n#ifndef _LIBCUDACXX_COMPILER_NVRTC\n#include <chrono>\n#endif // _LIBCUDACXX_COMPILER_NVRTC\n#endif // __cuda_std__\n\n#include \"__assert\" // all public C++ headers provide the assertion handler\n#include \"__type_traits/common_type.h\"\n#include \"__type_traits/enable_if.h\"\n#include \"__type_traits/integral_constant.h\"\n#include \"__type_traits/is_convertible.h\"\n#include \"__type_traits/is_floating_point.h\"\n#include \"ctime\"\n#include \"limits\"\n#include \"ratio\"\n\n// standard-mandated includes\n// TODO: Fix CPOs in split H/D compilation or inform users of what may happen\n// #include \"concepts\"\n#include \"version\"\n\n#ifndef __cuda_std__\n#include <__pragma_push>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n// Silence NVCC warnings `long double` arising from chrono floating pointer\n// user-defined literals which are defined in terms of `long double`.\n\n// FIXME: There is currently no way to disable this diagnostic in a fine-grained\n// fashion; if you include this header, the diagnostic will be suppressed\n// throughout the translation unit. The alternative is loosing (conforming)\n// chrono user-defined literals; this seems like the lesser of two evils, so...\n_LIBCUDACXX_NV_DIAG_SUPPRESS(cuda_demote_unsupported_floating_point)\n\n_LIBCUDACXX_BEGIN_NAMESPACE_FILESYSTEM\nstruct _FilesystemClock;\n_LIBCUDACXX_END_NAMESPACE_FILESYSTEM\n\n# if _LIBCUDACXX_CUDA_ABI_VERSION > 3\n#  define _LIBCUDACXX_CHRONO_LITERAL_INTERNAL_T double\n# else\n#  define _LIBCUDACXX_CHRONO_LITERAL_INTERNAL_T long double\n# endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\nnamespace chrono\n{\n\ntemplate <class _Rep, class _Period = ratio<1> > class _LIBCUDACXX_TEMPLATE_VIS duration;\n\ntemplate <class _Tp>\nstruct __is_duration : false_type {};\n\ntemplate <class _Rep, class _Period>\nstruct __is_duration<duration<_Rep, _Period> > : true_type  {};\n\ntemplate <class _Rep, class _Period>\nstruct __is_duration<const duration<_Rep, _Period> > : true_type  {};\n\ntemplate <class _Rep, class _Period>\nstruct __is_duration<volatile duration<_Rep, _Period> > : true_type  {};\n\ntemplate <class _Rep, class _Period>\nstruct __is_duration<const volatile duration<_Rep, _Period> > : true_type  {};\n\n} // chrono\n\ntemplate <class _Rep1, class _Period1, class _Rep2, class _Period2>\nstruct _LIBCUDACXX_TEMPLATE_VIS common_type<chrono::duration<_Rep1, _Period1>,\n                                         chrono::duration<_Rep2, _Period2> >\n{\n    typedef chrono::duration<typename common_type<_Rep1, _Rep2>::type,\n                             typename __ratio_gcd<_Period1, _Period2>::type> type;\n};\n\nnamespace chrono {\n\n// duration_cast\n\ntemplate <class _FromDuration, class _ToDuration,\n          class _Period = typename ratio_divide<typename _FromDuration::period, typename _ToDuration::period>::type,\n          bool = _Period::num == 1,\n          bool = _Period::den == 1>\nstruct __duration_cast;\n\ntemplate <class _FromDuration, class _ToDuration, class _Period>\nstruct __duration_cast<_FromDuration, _ToDuration, _Period, true, true>\n{\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    _ToDuration operator()(const _FromDuration& __fd) const\n    {\n        return _ToDuration(static_cast<typename _ToDuration::rep>(__fd.count()));\n    }\n};\n\ntemplate <class _FromDuration, class _ToDuration, class _Period>\nstruct __duration_cast<_FromDuration, _ToDuration, _Period, true, false>\n{\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    _ToDuration operator()(const _FromDuration& __fd) const\n    {\n        typedef typename common_type<typename _ToDuration::rep, typename _FromDuration::rep, intmax_t>::type _Ct;\n        return _ToDuration(static_cast<typename _ToDuration::rep>(\n                           static_cast<_Ct>(__fd.count()) / static_cast<_Ct>(_Period::den)));\n    }\n};\n\ntemplate <class _FromDuration, class _ToDuration, class _Period>\nstruct __duration_cast<_FromDuration, _ToDuration, _Period, false, true>\n{\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    _ToDuration operator()(const _FromDuration& __fd) const\n    {\n        typedef typename common_type<typename _ToDuration::rep, typename _FromDuration::rep, intmax_t>::type _Ct;\n        return _ToDuration(static_cast<typename _ToDuration::rep>(\n                           static_cast<_Ct>(__fd.count()) * static_cast<_Ct>(_Period::num)));\n    }\n};\n\ntemplate <class _FromDuration, class _ToDuration, class _Period>\nstruct __duration_cast<_FromDuration, _ToDuration, _Period, false, false>\n{\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    _ToDuration operator()(const _FromDuration& __fd) const\n    {\n        typedef typename common_type<typename _ToDuration::rep, typename _FromDuration::rep, intmax_t>::type _Ct;\n        return _ToDuration(static_cast<typename _ToDuration::rep>(\n                           static_cast<_Ct>(__fd.count()) * static_cast<_Ct>(_Period::num)\n                                                          / static_cast<_Ct>(_Period::den)));\n    }\n};\n\ntemplate <class _ToDuration, class _Rep, class _Period>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr\n__enable_if_t\n<\n    __is_duration<_ToDuration>::value,\n    _ToDuration\n>\nduration_cast(const duration<_Rep, _Period>& __fd)\n{\n    return __duration_cast<duration<_Rep, _Period>, _ToDuration>()(__fd);\n}\n\ntemplate <class _Rep>\nstruct _LIBCUDACXX_TEMPLATE_VIS treat_as_floating_point : is_floating_point<_Rep> {};\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _Rep>\n_LIBCUDACXX_INLINE_VAR constexpr bool treat_as_floating_point_v\n    = treat_as_floating_point<_Rep>::value;\n#endif // _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\n\ntemplate <class _Rep>\nstruct _LIBCUDACXX_TEMPLATE_VIS duration_values\n{\npublic:\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr _Rep zero() noexcept {return _Rep(0);}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr _Rep max()  noexcept {return numeric_limits<_Rep>::max();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr _Rep min()  noexcept {return numeric_limits<_Rep>::lowest();}\n};\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _ToDuration, class _Rep, class _Period>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n__enable_if_t\n<\n    __is_duration<_ToDuration>::value,\n    _ToDuration\n>\nfloor(const duration<_Rep, _Period>& __d)\n{\n    _ToDuration __t = duration_cast<_ToDuration>(__d);\n    if (__t > __d)\n        __t = __t - _ToDuration{1};\n    return __t;\n}\n\ntemplate <class _ToDuration, class _Rep, class _Period>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n__enable_if_t\n<\n    __is_duration<_ToDuration>::value,\n    _ToDuration\n>\nceil(const duration<_Rep, _Period>& __d)\n{\n    _ToDuration __t = duration_cast<_ToDuration>(__d);\n    if (__t < __d)\n        __t = __t + _ToDuration{1};\n    return __t;\n}\n\ntemplate <class _ToDuration, class _Rep, class _Period>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n__enable_if_t\n<\n    __is_duration<_ToDuration>::value,\n    _ToDuration\n>\nround(const duration<_Rep, _Period>& __d)\n{\n    _ToDuration __lower = floor<_ToDuration>(__d);\n    _ToDuration __upper = __lower + _ToDuration{1};\n    auto __lowerDiff = __d - __lower;\n    auto __upperDiff = __upper - __d;\n    if (__lowerDiff < __upperDiff)\n        return __lower;\n    if (__lowerDiff > __upperDiff)\n        return __upper;\n    return __lower.count() & 1 ? __upper : __lower;\n}\n#endif // _LIBCUDACXX_STD_VER > 11\n\n// duration\n\ntemplate <class _Rep, class _Period>\nclass _LIBCUDACXX_TEMPLATE_VIS duration\n{\n    static_assert(!__is_duration<_Rep>::value, \"A duration representation can not be a duration\");\n    static_assert(__is_ratio<_Period>::value, \"Second template parameter of duration must be a std::ratio\");\n    static_assert(_Period::num > 0, \"duration period must be positive\");\n\n    template <class _R1, class _R2>\n    struct __no_overflow\n    {\n    private:\n        static const intmax_t __gcd_n1_n2 = __static_gcd<_R1::num, _R2::num>::value;\n        static const intmax_t __gcd_d1_d2 = __static_gcd<_R1::den, _R2::den>::value;\n        static const intmax_t __n1 = _R1::num / __gcd_n1_n2;\n        static const intmax_t __d1 = _R1::den / __gcd_d1_d2;\n        static const intmax_t __n2 = _R2::num / __gcd_n1_n2;\n        static const intmax_t __d2 = _R2::den / __gcd_d1_d2;\n        static const intmax_t max = -((intmax_t(1) << (sizeof(intmax_t) * CHAR_BIT - 1)) + 1);\n\n        template <intmax_t _Xp, intmax_t _Yp, bool __overflow>\n        struct __mul    // __overflow == false\n        {\n            static const intmax_t value = _Xp * _Yp;\n        };\n\n        template <intmax_t _Xp, intmax_t _Yp>\n        struct __mul<_Xp, _Yp, true>\n        {\n            static const intmax_t value = 1;\n        };\n\n    public:\n        static const bool value = (__n1 <= max / __d2) && (__n2 <= max / __d1);\n        typedef ratio<__mul<__n1, __d2, !value>::value,\n                      __mul<__n2, __d1, !value>::value> type;\n    };\n\npublic:\n    typedef _Rep rep;\n    typedef typename _Period::type period;\nprivate:\n    rep __rep_;\npublic:\n\n    constexpr duration() = default;\n\n    template <class _Rep2>\n        _LIBCUDACXX_INLINE_VISIBILITY constexpr\n        explicit duration(const _Rep2& __r,\n            __enable_if_t\n            <\n               is_convertible<_Rep2, rep>::value &&\n               (treat_as_floating_point<rep>::value ||\n               !treat_as_floating_point<_Rep2>::value)\n            >* = 0)\n                : __rep_(static_cast<rep>(__r)) {}\n\n    // conversions\n    template <class _Rep2, class _Period2>\n        _LIBCUDACXX_INLINE_VISIBILITY constexpr\n        duration(const duration<_Rep2, _Period2>& __d,\n            __enable_if_t\n            <\n                __no_overflow<_Period2, period>::value && (\n                treat_as_floating_point<rep>::value ||\n                (__no_overflow<_Period2, period>::type::den == 1 &&\n                 !treat_as_floating_point<_Rep2>::value))\n            >* = 0)\n                : __rep_(_CUDA_VSTD::chrono::duration_cast<duration>(__d).count()) {}\n\n    // observer\n\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr rep count() const {return __rep_;}\n\n    // arithmetic\n\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr typename common_type<duration>::type operator+() const {return typename common_type<duration>::type(*this);}\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr typename common_type<duration>::type operator-() const {return typename common_type<duration>::type(-__rep_);}\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX14 duration& operator++()      {++__rep_; return *this;}\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX14 duration  operator++(int)   {return duration(__rep_++);}\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX14 duration& operator--()      {--__rep_; return *this;}\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX14 duration  operator--(int)   {return duration(__rep_--);}\n\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX14 duration& operator+=(const duration& __d) {__rep_ += __d.count(); return *this;}\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX14 duration& operator-=(const duration& __d) {__rep_ -= __d.count(); return *this;}\n\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX14 duration& operator*=(const rep& rhs) {__rep_ *= rhs; return *this;}\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX14 duration& operator/=(const rep& rhs) {__rep_ /= rhs; return *this;}\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX14 duration& operator%=(const rep& rhs) {__rep_ %= rhs; return *this;}\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX14 duration& operator%=(const duration& rhs) {__rep_ %= rhs.count(); return *this;}\n\n    // special values\n\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr duration zero() noexcept {return duration(duration_values<rep>::zero());}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr duration min()  noexcept {return duration(duration_values<rep>::min());}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr duration max()  noexcept {return duration(duration_values<rep>::max());}\n};\n\ntypedef duration<long long,         nano> nanoseconds;\ntypedef duration<long long,        micro> microseconds;\ntypedef duration<long long,        milli> milliseconds;\ntypedef duration<long long              > seconds;\ntypedef duration<     long, ratio<  60> > minutes;\ntypedef duration<     long, ratio<3600> > hours;\n#if _LIBCUDACXX_STD_VER > 11\ntypedef duration<     int, ratio_multiply<ratio<24>, hours::period>>         days;\ntypedef duration<     int, ratio_multiply<ratio<7>,   days::period>>         weeks;\ntypedef duration<     int, ratio_multiply<ratio<146097, 400>, days::period>> years;\ntypedef duration<     int, ratio_divide<years::period, ratio<12>>>           months;\n#endif // _LIBCUDACXX_STD_VER > 11\n// Duration ==\n\ntemplate <class _LhsDuration, class _RhsDuration>\nstruct __duration_eq\n{\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    bool operator()(const _LhsDuration& __lhs, const _RhsDuration& __rhs) const\n        {\n            typedef typename common_type<_LhsDuration, _RhsDuration>::type _Ct;\n            return _Ct(__lhs).count() == _Ct(__rhs).count();\n        }\n};\n\ntemplate <class _LhsDuration>\nstruct __duration_eq<_LhsDuration, _LhsDuration>\n{\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    bool operator()(const _LhsDuration& __lhs, const _LhsDuration& __rhs) const\n        {return __lhs.count() == __rhs.count();}\n};\n\ntemplate <class _Rep1, class _Period1, class _Rep2, class _Period2>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr bool\noperator==(const duration<_Rep1, _Period1>& __lhs, const duration<_Rep2, _Period2>& __rhs)\n{\n    return __duration_eq<duration<_Rep1, _Period1>, duration<_Rep2, _Period2> >()(__lhs, __rhs);\n}\n\n// Duration !=\n\ntemplate <class _Rep1, class _Period1, class _Rep2, class _Period2>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr bool\noperator!=(const duration<_Rep1, _Period1>& __lhs, const duration<_Rep2, _Period2>& __rhs)\n{\n    return !(__lhs == __rhs);\n}\n\n// Duration <\n\ntemplate <class _LhsDuration, class _RhsDuration>\nstruct __duration_lt\n{\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    bool operator()(const _LhsDuration& __lhs, const _RhsDuration& __rhs) const\n        {\n            typedef typename common_type<_LhsDuration, _RhsDuration>::type _Ct;\n            return _Ct(__lhs).count() < _Ct(__rhs).count();\n        }\n};\n\ntemplate <class _LhsDuration>\nstruct __duration_lt<_LhsDuration, _LhsDuration>\n{\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    bool operator()(const _LhsDuration& __lhs, const _LhsDuration& __rhs) const\n        {return __lhs.count() < __rhs.count();}\n};\n\ntemplate <class _Rep1, class _Period1, class _Rep2, class _Period2>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr bool\noperator< (const duration<_Rep1, _Period1>& __lhs, const duration<_Rep2, _Period2>& __rhs)\n{\n    return __duration_lt<duration<_Rep1, _Period1>, duration<_Rep2, _Period2> >()(__lhs, __rhs);\n}\n\n// Duration >\n\ntemplate <class _Rep1, class _Period1, class _Rep2, class _Period2>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr bool\noperator> (const duration<_Rep1, _Period1>& __lhs, const duration<_Rep2, _Period2>& __rhs)\n{\n    return __rhs < __lhs;\n}\n\n// Duration <=\n\ntemplate <class _Rep1, class _Period1, class _Rep2, class _Period2>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr bool\noperator<=(const duration<_Rep1, _Period1>& __lhs, const duration<_Rep2, _Period2>& __rhs)\n{\n    return !(__rhs < __lhs);\n}\n\n// Duration >=\n\ntemplate <class _Rep1, class _Period1, class _Rep2, class _Period2>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr bool\noperator>=(const duration<_Rep1, _Period1>& __lhs, const duration<_Rep2, _Period2>& __rhs)\n{\n    return !(__lhs < __rhs);\n}\n\n// Duration +\n\ntemplate <class _Rep1, class _Period1, class _Rep2, class _Period2>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr\ntypename common_type<duration<_Rep1, _Period1>, duration<_Rep2, _Period2> >::type\noperator+(const duration<_Rep1, _Period1>& __lhs, const duration<_Rep2, _Period2>& __rhs)\n{\n    typedef typename common_type<duration<_Rep1, _Period1>, duration<_Rep2, _Period2> >::type _Cd;\n    return _Cd(_Cd(__lhs).count() + _Cd(__rhs).count());\n}\n\n// Duration -\n\ntemplate <class _Rep1, class _Period1, class _Rep2, class _Period2>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr\ntypename common_type<duration<_Rep1, _Period1>, duration<_Rep2, _Period2> >::type\noperator-(const duration<_Rep1, _Period1>& __lhs, const duration<_Rep2, _Period2>& __rhs)\n{\n    typedef typename common_type<duration<_Rep1, _Period1>, duration<_Rep2, _Period2> >::type _Cd;\n    return _Cd(_Cd(__lhs).count() - _Cd(__rhs).count());\n}\n\n// Duration *\n\ntemplate <class _Rep1, class _Period, class _Rep2>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr\n__enable_if_t\n<\n    is_convertible<_Rep2, typename common_type<_Rep1, _Rep2>::type>::value,\n    duration<typename common_type<_Rep1, _Rep2>::type, _Period>\n>\noperator*(const duration<_Rep1, _Period>& __d, const _Rep2& __s)\n{\n    typedef typename common_type<_Rep1, _Rep2>::type _Cr;\n    typedef duration<_Cr, _Period> _Cd;\n    return _Cd(_Cd(__d).count() * static_cast<_Cr>(__s));\n}\n\ntemplate <class _Rep1, class _Period, class _Rep2>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr\n__enable_if_t\n<\n    is_convertible<_Rep1, typename common_type<_Rep1, _Rep2>::type>::value,\n    duration<typename common_type<_Rep1, _Rep2>::type, _Period>\n>\noperator*(const _Rep1& __s, const duration<_Rep2, _Period>& __d)\n{\n    return __d * __s;\n}\n\n// Duration /\n\ntemplate <class _Rep1, class _Period, class _Rep2>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr\n__enable_if_t\n<\n    !__is_duration<_Rep2>::value &&\n      is_convertible<_Rep2, typename common_type<_Rep1, _Rep2>::type>::value,\n    duration<typename common_type<_Rep1, _Rep2>::type, _Period>\n>\noperator/(const duration<_Rep1, _Period>& __d, const _Rep2& __s)\n{\n    typedef typename common_type<_Rep1, _Rep2>::type _Cr;\n    typedef duration<_Cr, _Period> _Cd;\n    return _Cd(_Cd(__d).count() / static_cast<_Cr>(__s));\n}\n\ntemplate <class _Rep1, class _Period1, class _Rep2, class _Period2>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr\ntypename common_type<_Rep1, _Rep2>::type\noperator/(const duration<_Rep1, _Period1>& __lhs, const duration<_Rep2, _Period2>& __rhs)\n{\n    typedef typename common_type<duration<_Rep1, _Period1>, duration<_Rep2, _Period2> >::type _Ct;\n    return _Ct(__lhs).count() / _Ct(__rhs).count();\n}\n\n// Duration %\n\ntemplate <class _Rep1, class _Period, class _Rep2>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr\n__enable_if_t\n<\n    !__is_duration<_Rep2>::value &&\n      is_convertible<_Rep2, typename common_type<_Rep1, _Rep2>::type>::value,\n    duration<typename common_type<_Rep1, _Rep2>::type, _Period>\n>\noperator%(const duration<_Rep1, _Period>& __d, const _Rep2& __s)\n{\n    typedef typename common_type<_Rep1, _Rep2>::type _Cr;\n    typedef duration<_Cr, _Period> _Cd;\n    return _Cd(_Cd(__d).count() % static_cast<_Cr>(__s));\n}\n\ntemplate <class _Rep1, class _Period1, class _Rep2, class _Period2>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr\ntypename common_type<duration<_Rep1, _Period1>, duration<_Rep2, _Period2> >::type\noperator%(const duration<_Rep1, _Period1>& __lhs, const duration<_Rep2, _Period2>& __rhs)\n{\n    typedef typename common_type<_Rep1, _Rep2>::type _Cr;\n    typedef typename common_type<duration<_Rep1, _Period1>, duration<_Rep2, _Period2> >::type _Cd;\n    return _Cd(static_cast<_Cr>(_Cd(__lhs).count()) % static_cast<_Cr>(_Cd(__rhs).count()));\n}\n\n//////////////////////////////////////////////////////////\n///////////////////// time_point /////////////////////////\n//////////////////////////////////////////////////////////\n\ntemplate <class _Clock, class _Duration = typename _Clock::duration>\nclass _LIBCUDACXX_TEMPLATE_VIS time_point\n{\n    static_assert(__is_duration<_Duration>::value,\n                  \"Second template parameter of time_point must be a std::chrono::duration\");\npublic:\n    typedef _Clock                    clock;\n    typedef _Duration                 duration;\n    typedef typename duration::rep    rep;\n    typedef typename duration::period period;\nprivate:\n    duration __d_;\n\npublic:\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 time_point() : __d_(duration::zero()) {}\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 explicit time_point(const duration& __d) : __d_(__d) {}\n\n    // conversions\n    template <class _Duration2>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    time_point(const time_point<clock, _Duration2>& t,\n        __enable_if_t\n        <\n            is_convertible<_Duration2, duration>::value\n        >* = 0)\n            : __d_(t.time_since_epoch()) {}\n\n    // observer\n\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 duration time_since_epoch() const {return __d_;}\n\n    // arithmetic\n\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX14 time_point& operator+=(const duration& __d) {__d_ += __d; return *this;}\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX14 time_point& operator-=(const duration& __d) {__d_ -= __d; return *this;}\n\n    // special values\n\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr time_point min() noexcept {return time_point(duration::min());}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr time_point max() noexcept {return time_point(duration::max());}\n};\n\n} // chrono\n\ntemplate <class _Clock, class _Duration1, class _Duration2>\nstruct _LIBCUDACXX_TEMPLATE_VIS common_type<chrono::time_point<_Clock, _Duration1>,\n                                         chrono::time_point<_Clock, _Duration2> >\n{\n    typedef chrono::time_point<_Clock, typename common_type<_Duration1, _Duration2>::type> type;\n};\n\nnamespace chrono {\n\ntemplate <class _ToDuration, class _Clock, class _Duration>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\ntime_point<_Clock, _ToDuration>\ntime_point_cast(const time_point<_Clock, _Duration>& __t)\n{\n    return time_point<_Clock, _ToDuration>(_CUDA_VSTD::chrono::duration_cast<_ToDuration>(__t.time_since_epoch()));\n}\n\n#if _LIBCUDACXX_STD_VER > 11\ntemplate <class _ToDuration, class _Clock, class _Duration>\ninline _LIBCUDACXX_INLINE_VISIBILITY constexpr\n__enable_if_t\n<\n    __is_duration<_ToDuration>::value,\n    time_point<_Clock, _ToDuration>\n>\nfloor(const time_point<_Clock, _Duration>& __t)\n{\n    return time_point<_Clock, _ToDuration>{floor<_ToDuration>(__t.time_since_epoch())};\n}\n\ntemplate <class _ToDuration, class _Clock, class _Duration>\ninline _LIBCUDACXX_INLINE_VISIBILITY constexpr\n__enable_if_t\n<\n    __is_duration<_ToDuration>::value,\n    time_point<_Clock, _ToDuration>\n>\nceil(const time_point<_Clock, _Duration>& __t)\n{\n    return time_point<_Clock, _ToDuration>{ceil<_ToDuration>(__t.time_since_epoch())};\n}\n\ntemplate <class _ToDuration, class _Clock, class _Duration>\ninline _LIBCUDACXX_INLINE_VISIBILITY constexpr\n__enable_if_t\n<\n    __is_duration<_ToDuration>::value,\n    time_point<_Clock, _ToDuration>\n>\nround(const time_point<_Clock, _Duration>& __t)\n{\n    return time_point<_Clock, _ToDuration>{round<_ToDuration>(__t.time_since_epoch())};\n}\n\ntemplate <class _Rep, class _Period>\ninline _LIBCUDACXX_INLINE_VISIBILITY constexpr\n__enable_if_t\n<\n    numeric_limits<_Rep>::is_signed,\n    duration<_Rep, _Period>\n>\nabs(duration<_Rep, _Period> __d)\n{\n    return __d >= __d.zero() ? +__d : -__d;\n}\n#endif // _LIBCUDACXX_STD_VER > 11\n\n// time_point ==\n\ntemplate <class _Clock, class _Duration1, class _Duration2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nbool\noperator==(const time_point<_Clock, _Duration1>& __lhs, const time_point<_Clock, _Duration2>& __rhs)\n{\n    return __lhs.time_since_epoch() == __rhs.time_since_epoch();\n}\n\n// time_point !=\n\ntemplate <class _Clock, class _Duration1, class _Duration2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nbool\noperator!=(const time_point<_Clock, _Duration1>& __lhs, const time_point<_Clock, _Duration2>& __rhs)\n{\n    return !(__lhs == __rhs);\n}\n\n// time_point <\n\ntemplate <class _Clock, class _Duration1, class _Duration2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nbool\noperator<(const time_point<_Clock, _Duration1>& __lhs, const time_point<_Clock, _Duration2>& __rhs)\n{\n    return __lhs.time_since_epoch() < __rhs.time_since_epoch();\n}\n\n// time_point >\n\ntemplate <class _Clock, class _Duration1, class _Duration2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nbool\noperator>(const time_point<_Clock, _Duration1>& __lhs, const time_point<_Clock, _Duration2>& __rhs)\n{\n    return __rhs < __lhs;\n}\n\n// time_point <=\n\ntemplate <class _Clock, class _Duration1, class _Duration2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nbool\noperator<=(const time_point<_Clock, _Duration1>& __lhs, const time_point<_Clock, _Duration2>& __rhs)\n{\n    return !(__rhs < __lhs);\n}\n\n// time_point >=\n\ntemplate <class _Clock, class _Duration1, class _Duration2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nbool\noperator>=(const time_point<_Clock, _Duration1>& __lhs, const time_point<_Clock, _Duration2>& __rhs)\n{\n    return !(__lhs < __rhs);\n}\n\n// time_point operator+(time_point x, duration y);\n\ntemplate <class _Clock, class _Duration1, class _Rep2, class _Period2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\ntime_point<_Clock, typename common_type<_Duration1, duration<_Rep2, _Period2> >::type>\noperator+(const time_point<_Clock, _Duration1>& __lhs, const duration<_Rep2, _Period2>& __rhs)\n{\n    typedef time_point<_Clock, typename common_type<_Duration1, duration<_Rep2, _Period2> >::type> _Tr;\n    return _Tr (__lhs.time_since_epoch() + __rhs);\n}\n\n// time_point operator+(duration x, time_point y);\n\ntemplate <class _Rep1, class _Period1, class _Clock, class _Duration2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\ntime_point<_Clock, typename common_type<duration<_Rep1, _Period1>, _Duration2>::type>\noperator+(const duration<_Rep1, _Period1>& __lhs, const time_point<_Clock, _Duration2>& __rhs)\n{\n    return __rhs + __lhs;\n}\n\n// time_point operator-(time_point x, duration y);\n\ntemplate <class _Clock, class _Duration1, class _Rep2, class _Period2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\ntime_point<_Clock, typename common_type<_Duration1, duration<_Rep2, _Period2> >::type>\noperator-(const time_point<_Clock, _Duration1>& __lhs, const duration<_Rep2, _Period2>& __rhs)\n{\n    typedef time_point<_Clock, typename common_type<_Duration1, duration<_Rep2, _Period2> >::type> _Ret;\n    return _Ret(__lhs.time_since_epoch() -__rhs);\n}\n\n// duration operator-(time_point x, time_point y);\n\ntemplate <class _Clock, class _Duration1, class _Duration2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\ntypename common_type<_Duration1, _Duration2>::type\noperator-(const time_point<_Clock, _Duration1>& __lhs, const time_point<_Clock, _Duration2>& __rhs)\n{\n    return __lhs.time_since_epoch() - __rhs.time_since_epoch();\n}\n\n//////////////////////////////////////////////////////////\n/////////////////////// clocks ///////////////////////////\n//////////////////////////////////////////////////////////\nclass _LIBCUDACXX_TYPE_VIS system_clock\n{\npublic:\n    typedef _LIBCUDACXX_SYS_CLOCK_DURATION   duration;\n    typedef duration::rep                    rep;\n    typedef duration::period                 period;\n    typedef chrono::time_point<system_clock> time_point;\n    static _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 const bool is_steady = false;\n\n    _LIBCUDACXX_HOST_DEVICE\n    static time_point now() noexcept;\n    _LIBCUDACXX_HOST_DEVICE\n    static time_t     to_time_t  (const time_point& __t) noexcept;\n    _LIBCUDACXX_HOST_DEVICE\n    static time_point from_time_t(time_t __t) noexcept;\n};\n\n#ifndef _LIBCUDACXX_HAS_NO_MONOTONIC_CLOCK\nclass _LIBCUDACXX_TYPE_VIS steady_clock\n{\npublic:\n    typedef nanoseconds                                   duration;\n    typedef duration::rep                                 rep;\n    typedef duration::period                              period;\n    typedef chrono::time_point<steady_clock, duration>    time_point;\n    static _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 const bool is_steady = true;\n\n    static time_point now() noexcept;\n};\n\ntypedef steady_clock high_resolution_clock;\n#else\ntypedef system_clock high_resolution_clock;\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11\n\n// [time.clock.file], type file_clock\nusing file_clock = _CUDA_VSTD_FS::_FilesystemClock;\n\ntemplate<class _Duration>\nusing file_time = time_point<file_clock, _Duration>;\n\n\ntemplate <class _Duration>\nusing sys_time    = time_point<system_clock, _Duration>;\nusing sys_seconds = sys_time<seconds>;\nusing sys_days    = sys_time<days>;\n\nstruct local_t {};\ntemplate<class Duration>\nusing local_time  = time_point<local_t, Duration>;\nusing local_seconds = local_time<seconds>;\nusing local_days    = local_time<days>;\n\nstruct last_spec { explicit last_spec() = default; };\n\nclass day {\nprivate:\n    unsigned char __d;\npublic:\n    day() = default;\n    _LIBCUDACXX_INLINE_VISIBILITY\n    explicit inline constexpr day(unsigned __val) noexcept : __d(static_cast<unsigned char>(__val)) {}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr day& operator++()    noexcept { ++__d; return *this; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr day  operator++(int) noexcept { day __tmp = *this; ++(*this); return __tmp; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr day& operator--()    noexcept { --__d; return *this; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr day  operator--(int) noexcept { day __tmp = *this; --(*this); return __tmp; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n           constexpr day& operator+=(const days& __dd) noexcept;\n    _LIBCUDACXX_INLINE_VISIBILITY\n           constexpr day& operator-=(const days& __dd) noexcept;\n    _LIBCUDACXX_INLINE_VISIBILITY\n    explicit inline constexpr operator unsigned() const noexcept { return __d; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr bool ok() const noexcept { return __d >= 1 && __d <= 31; }\n  };\n\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator==(const day& __lhs, const day& __rhs) noexcept\n{ return static_cast<unsigned>(__lhs) == static_cast<unsigned>(__rhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator!=(const day& __lhs, const day& __rhs) noexcept\n{ return !(__lhs == __rhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator< (const day& __lhs, const day& __rhs) noexcept\n{ return static_cast<unsigned>(__lhs) <  static_cast<unsigned>(__rhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator> (const day& __lhs, const day& __rhs) noexcept\n{ return __rhs < __lhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator<=(const day& __lhs, const day& __rhs) noexcept\n{ return !(__rhs < __lhs);}\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator>=(const day& __lhs, const day& __rhs) noexcept\n{ return !(__lhs < __rhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nday operator+ (const day& __lhs, const days& __rhs) noexcept\n{ return day(static_cast<unsigned>(__lhs) + static_cast<unsigned>(__rhs.count())); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nday operator+ (const days& __lhs, const day& __rhs) noexcept\n{ return __rhs + __lhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nday operator- (const day& __lhs, const days& __rhs) noexcept\n{ return __lhs + -__rhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\ndays operator-(const day& __lhs, const day& __rhs) noexcept\n{ return days(static_cast<int>(static_cast<unsigned>(__lhs)) -\n              static_cast<int>(static_cast<unsigned>(__rhs))); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr day& day::operator+=(const days& __dd) noexcept\n{ *this = *this + __dd; return *this; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr day& day::operator-=(const days& __dd) noexcept\n{ *this = *this - __dd; return *this; }\n\n\nclass month {\nprivate:\n    unsigned char __m;\npublic:\n    month() = default;\n    _LIBCUDACXX_INLINE_VISIBILITY\n    explicit inline constexpr month(unsigned __val) noexcept : __m(static_cast<unsigned char>(__val)) {}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr month& operator++()    noexcept { ++__m; return *this; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr month  operator++(int) noexcept { month __tmp = *this; ++(*this); return __tmp; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr month& operator--()    noexcept { --__m; return *this; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr month  operator--(int) noexcept { month __tmp = *this; --(*this); return __tmp; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n           constexpr month& operator+=(const months& __m1) noexcept;\n    _LIBCUDACXX_INLINE_VISIBILITY\n           constexpr month& operator-=(const months& __m1) noexcept;\n    _LIBCUDACXX_INLINE_VISIBILITY\n    explicit inline constexpr operator unsigned() const noexcept { return __m; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr bool ok() const noexcept { return __m >= 1 && __m <= 12; }\n};\n\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator==(const month& __lhs, const month& __rhs) noexcept\n{ return static_cast<unsigned>(__lhs) == static_cast<unsigned>(__rhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator!=(const month& __lhs, const month& __rhs) noexcept\n{ return !(__lhs == __rhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator< (const month& __lhs, const month& __rhs) noexcept\n{ return static_cast<unsigned>(__lhs)  < static_cast<unsigned>(__rhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator> (const month& __lhs, const month& __rhs) noexcept\n{ return __rhs < __lhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator<=(const month& __lhs, const month& __rhs) noexcept\n{ return !(__rhs < __lhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator>=(const month& __lhs, const month& __rhs) noexcept\n{ return !(__lhs < __rhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nmonth operator+ (const month& __lhs, const months& __rhs) noexcept\n{\n    auto const __mu = static_cast<long long>(static_cast<unsigned>(__lhs)) + (__rhs.count() - 1);\n    auto const __yr = (__mu >= 0 ? __mu : __mu - 11) / 12;\n    return month{static_cast<unsigned>(__mu - __yr * 12 + 1)};\n}\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nmonth operator+ (const months& __lhs, const month& __rhs) noexcept\n{ return __rhs + __lhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nmonth operator- (const month& __lhs, const months& __rhs) noexcept\n{ return __lhs + -__rhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nmonths operator-(const month& __lhs, const month& __rhs) noexcept\n{\n    auto const __dm = static_cast<unsigned>(__lhs) - static_cast<unsigned>(__rhs);\n    return months(__dm <= 11 ? __dm : __dm + 12);\n}\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr month& month::operator+=(const months& __dm) noexcept\n{ *this = *this + __dm; return *this; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr month& month::operator-=(const months& __dm) noexcept\n{ *this = *this - __dm; return *this; }\n\n\nclass year {\nprivate:\n    short __y;\npublic:\n    year() = default;\n    _LIBCUDACXX_INLINE_VISIBILITY\n    explicit inline constexpr year(int __val) noexcept : __y(static_cast<short>(__val)) {}\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr year& operator++()    noexcept { ++__y; return *this; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr year  operator++(int) noexcept { year __tmp = *this; ++(*this); return __tmp; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr year& operator--()    noexcept { --__y; return *this; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr year  operator--(int) noexcept { year __tmp = *this; --(*this); return __tmp; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n           constexpr year& operator+=(const years& __dy) noexcept;\n    _LIBCUDACXX_INLINE_VISIBILITY\n           constexpr year& operator-=(const years& __dy) noexcept;\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr year operator+() const noexcept { return *this; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr year operator-() const noexcept { return year{-__y}; }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr bool is_leap() const noexcept { return __y % 4 == 0 && (__y % 100 != 0 || __y % 400 == 0); }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    explicit inline constexpr operator int() const noexcept { return __y; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n           constexpr bool ok() const noexcept;\n    _LIBCUDACXX_INLINE_VISIBILITY\n    static inline constexpr year min() noexcept { return year{-32767}; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    static inline constexpr year max() noexcept { return year{ 32767}; }\n};\n\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator==(const year& __lhs, const year& __rhs) noexcept\n{ return static_cast<int>(__lhs) == static_cast<int>(__rhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator!=(const year& __lhs, const year& __rhs) noexcept\n{ return !(__lhs == __rhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator< (const year& __lhs, const year& __rhs) noexcept\n{ return static_cast<int>(__lhs)  < static_cast<int>(__rhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator> (const year& __lhs, const year& __rhs) noexcept\n{ return __rhs < __lhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator<=(const year& __lhs, const year& __rhs) noexcept\n{ return !(__rhs < __lhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator>=(const year& __lhs, const year& __rhs) noexcept\n{ return !(__lhs < __rhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear operator+ (const year& __lhs, const years& __rhs) noexcept\n{ return year(static_cast<int>(__lhs) + __rhs.count()); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear operator+ (const years& __lhs, const year& __rhs) noexcept\n{ return __rhs + __lhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear operator- (const year& __lhs, const years& __rhs) noexcept\n{ return __lhs + -__rhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyears operator-(const year& __lhs, const year& __rhs) noexcept\n{ return years{static_cast<int>(__lhs) - static_cast<int>(__rhs)}; }\n\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr year& year::operator+=(const years& __dy) noexcept\n{ *this = *this + __dy; return *this; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr year& year::operator-=(const years& __dy) noexcept\n{ *this = *this - __dy; return *this; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr bool year::ok() const noexcept\n{ return static_cast<int>(min()) <= __y && __y <= static_cast<int>(max()); }\n\nclass weekday_indexed;\nclass weekday_last;\n\nclass weekday {\nprivate:\n    unsigned char __wd;\npublic:\n  weekday() = default;\n  _LIBCUDACXX_INLINE_VISIBILITY\n  inline explicit constexpr weekday(unsigned __val) noexcept : __wd(static_cast<unsigned char>(__val == 7 ? 0 : __val)) {}\n  _LIBCUDACXX_INLINE_VISIBILITY\n  inline constexpr          weekday(const sys_days& __sysd) noexcept\n          : __wd(__weekday_from_days(__sysd.time_since_epoch().count())) {}\n  _LIBCUDACXX_INLINE_VISIBILITY\n  inline explicit constexpr weekday(const local_days& __locd) noexcept\n          : __wd(__weekday_from_days(__locd.time_since_epoch().count())) {}\n\n  _LIBCUDACXX_INLINE_VISIBILITY\n  inline constexpr weekday& operator++()    noexcept { __wd = (__wd == 6 ? 0 : __wd + 1); return *this; }\n  _LIBCUDACXX_INLINE_VISIBILITY\n  inline constexpr weekday  operator++(int) noexcept { weekday __tmp = *this; ++(*this); return __tmp; }\n  _LIBCUDACXX_INLINE_VISIBILITY\n  inline constexpr weekday& operator--()    noexcept { __wd = (__wd == 0 ? 6 : __wd - 1); return *this; }\n  _LIBCUDACXX_INLINE_VISIBILITY\n  inline constexpr weekday  operator--(int) noexcept { weekday __tmp = *this; --(*this); return __tmp; }\n  _LIBCUDACXX_INLINE_VISIBILITY\n         constexpr weekday& operator+=(const days& __dd) noexcept;\n  _LIBCUDACXX_INLINE_VISIBILITY\n         constexpr weekday& operator-=(const days& __dd) noexcept;\n  _LIBCUDACXX_INLINE_VISIBILITY\n  inline constexpr unsigned c_encoding()   const noexcept { return __wd; }\n  _LIBCUDACXX_INLINE_VISIBILITY\n  inline constexpr unsigned iso_encoding() const noexcept { return __wd == 0u ? 7 : __wd; }\n  _LIBCUDACXX_INLINE_VISIBILITY\n  inline constexpr bool ok() const noexcept { return __wd <= 6; }\n  _LIBCUDACXX_INLINE_VISIBILITY\n         constexpr weekday_indexed operator[](unsigned __index) const noexcept;\n  _LIBCUDACXX_INLINE_VISIBILITY\n         constexpr weekday_last    operator[](last_spec) const noexcept;\n\n  _LIBCUDACXX_INLINE_VISIBILITY\n  static constexpr unsigned char __weekday_from_days(int __days) noexcept;\n};\n\n\n// https://howardhinnant.github.io/date_algorithms.html#weekday_from_days\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nunsigned char weekday::__weekday_from_days(int __days) noexcept\n{\n    return static_cast<unsigned char>(\n              static_cast<unsigned>(__days >= -4 ? (__days+4) % 7 : (__days+5) % 7 + 6)\n           );\n}\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator==(const weekday& __lhs, const weekday& __rhs) noexcept\n{ return __lhs.c_encoding() == __rhs.c_encoding(); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator!=(const weekday& __lhs, const weekday& __rhs) noexcept\n{ return !(__lhs == __rhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator< (const weekday& __lhs, const weekday& __rhs) noexcept\n{ return __lhs.c_encoding() < __rhs.c_encoding(); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator> (const weekday& __lhs, const weekday& __rhs) noexcept\n{ return __rhs < __lhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator<=(const weekday& __lhs, const weekday& __rhs) noexcept\n{ return !(__rhs < __lhs);}\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator>=(const weekday& __lhs, const weekday& __rhs) noexcept\n{ return !(__lhs < __rhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\nconstexpr weekday operator+(const weekday& __lhs, const days& __rhs) noexcept\n{\n    auto const __mu = static_cast<long long>(__lhs.c_encoding()) + __rhs.count();\n    auto const __yr = (__mu >= 0 ? __mu : __mu - 6) / 7;\n    return weekday{static_cast<unsigned>(__mu - __yr * 7)};\n}\n\n_LIBCUDACXX_INLINE_VISIBILITY\nconstexpr weekday operator+(const days& __lhs, const weekday& __rhs) noexcept\n{ return __rhs + __lhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\nconstexpr weekday operator-(const weekday& __lhs, const days& __rhs) noexcept\n{ return __lhs + -__rhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\nconstexpr days operator-(const weekday& __lhs, const weekday& __rhs) noexcept\n{\n    // casts are required to work around nvcc bug 3145483\n    const int __wdu = static_cast<int>(__lhs.c_encoding()) - static_cast<int>(__rhs.c_encoding());\n    const int __wk = (__wdu >= 0 ? __wdu : __wdu-6) / 7;\n    return days{__wdu - __wk * 7};\n}\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr weekday& weekday::operator+=(const days& __dd) noexcept\n{ *this = *this + __dd; return *this; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr weekday& weekday::operator-=(const days& __dd) noexcept\n{ *this = *this - __dd; return *this; }\n\n\nclass weekday_indexed {\nprivate:\n    _CUDA_VSTD::chrono::weekday __wd;\n    unsigned char          __idx;\npublic:\n    weekday_indexed() = default;\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr weekday_indexed(const _CUDA_VSTD::chrono::weekday& __wdval, unsigned __idxval) noexcept\n        : __wd{__wdval}, __idx(static_cast<unsigned char>(__idxval)) {}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr _CUDA_VSTD::chrono::weekday weekday() const noexcept { return __wd; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr unsigned                 index() const noexcept { return __idx; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr bool ok() const noexcept { return __wd.ok() && __idx >= 1 && __idx <= 5; }\n};\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator==(const weekday_indexed& __lhs, const weekday_indexed& __rhs) noexcept\n{ return __lhs.weekday() == __rhs.weekday() && __lhs.index() == __rhs.index(); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator!=(const weekday_indexed& __lhs, const weekday_indexed& __rhs) noexcept\n{ return !(__lhs == __rhs); }\n\n\nclass weekday_last {\nprivate:\n    _CUDA_VSTD::chrono::weekday __wd;\npublic:\n    _LIBCUDACXX_INLINE_VISIBILITY\n    explicit constexpr weekday_last(const _CUDA_VSTD::chrono::weekday& __val) noexcept\n        : __wd{__val} {}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr _CUDA_VSTD::chrono::weekday weekday() const noexcept { return __wd; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr bool ok() const noexcept { return __wd.ok(); }\n};\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator==(const weekday_last& __lhs, const weekday_last& __rhs) noexcept\n{ return __lhs.weekday() == __rhs.weekday(); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator!=(const weekday_last& __lhs, const weekday_last& __rhs) noexcept\n{ return !(__lhs == __rhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nweekday_indexed weekday::operator[](unsigned __index) const noexcept { return weekday_indexed{*this, __index}; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nweekday_last    weekday::operator[](last_spec) const noexcept { return weekday_last{*this}; }\n\n\n_LIBCUDACXX_INLINE_VAR constexpr last_spec last{};\n_LIBCUDACXX_INLINE_VAR constexpr weekday   Sunday{0};\n_LIBCUDACXX_INLINE_VAR constexpr weekday   Monday{1};\n_LIBCUDACXX_INLINE_VAR constexpr weekday   Tuesday{2};\n_LIBCUDACXX_INLINE_VAR constexpr weekday   Wednesday{3};\n_LIBCUDACXX_INLINE_VAR constexpr weekday   Thursday{4};\n_LIBCUDACXX_INLINE_VAR constexpr weekday   Friday{5};\n_LIBCUDACXX_INLINE_VAR constexpr weekday   Saturday{6};\n\n_LIBCUDACXX_INLINE_VAR constexpr month January{1};\n_LIBCUDACXX_INLINE_VAR constexpr month February{2};\n_LIBCUDACXX_INLINE_VAR constexpr month March{3};\n_LIBCUDACXX_INLINE_VAR constexpr month April{4};\n_LIBCUDACXX_INLINE_VAR constexpr month May{5};\n_LIBCUDACXX_INLINE_VAR constexpr month June{6};\n_LIBCUDACXX_INLINE_VAR constexpr month July{7};\n_LIBCUDACXX_INLINE_VAR constexpr month August{8};\n_LIBCUDACXX_INLINE_VAR constexpr month September{9};\n_LIBCUDACXX_INLINE_VAR constexpr month October{10};\n_LIBCUDACXX_INLINE_VAR constexpr month November{11};\n_LIBCUDACXX_INLINE_VAR constexpr month December{12};\n\n\nclass month_day {\nprivate:\n   chrono::month __m;\n   chrono::day   __d;\npublic:\n    month_day() = default;\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr month_day(const chrono::month& __mval, const chrono::day& __dval) noexcept\n        : __m{__mval}, __d{__dval} {}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr chrono::month month() const noexcept { return __m; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr chrono::day   day()   const noexcept { return __d; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr bool ok() const noexcept;\n};\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool month_day::ok() const noexcept\n{\n    if (!__m.ok()) return false;\n    const unsigned __dval = static_cast<unsigned>(__d);\n    if (__dval < 1 || __dval > 31) return false;\n    if (__dval <= 29) return true;\n//  Now we've got either 30 or 31\n    const unsigned __mval = static_cast<unsigned>(__m);\n    if (__mval == 2) return false;\n    if (__mval == 4 || __mval == 6 || __mval == 9 || __mval == 11)\n        return __dval == 30;\n    return true;\n}\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator==(const month_day& __lhs, const month_day& __rhs) noexcept\n{ return __lhs.month() == __rhs.month() && __lhs.day() == __rhs.day(); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator!=(const month_day& __lhs, const month_day& __rhs) noexcept\n{ return !(__lhs == __rhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nmonth_day operator/(const month& __lhs, const day& __rhs) noexcept\n{ return month_day{__lhs, __rhs}; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\nconstexpr\nmonth_day operator/(const day& __lhs, const month& __rhs) noexcept\n{ return __rhs / __lhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nmonth_day operator/(const month& __lhs, int __rhs) noexcept\n{ return __lhs / day(__rhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\nconstexpr\nmonth_day operator/(int __lhs, const day& __rhs) noexcept\n{ return month(__lhs) / __rhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\nconstexpr\nmonth_day operator/(const day& __lhs, int __rhs) noexcept\n{ return month(__rhs) / __lhs; }\n\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator< (const month_day& __lhs, const month_day& __rhs) noexcept\n{ return __lhs.month() != __rhs.month() ? __lhs.month() < __rhs.month() : __lhs.day() < __rhs.day(); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator> (const month_day& __lhs, const month_day& __rhs) noexcept\n{ return __rhs < __lhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator<=(const month_day& __lhs, const month_day& __rhs) noexcept\n{ return !(__rhs < __lhs);}\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator>=(const month_day& __lhs, const month_day& __rhs) noexcept\n{ return !(__lhs < __rhs); }\n\n\n\nclass month_day_last {\nprivate:\n    chrono::month __m;\npublic:\n    _LIBCUDACXX_INLINE_VISIBILITY\n    explicit constexpr month_day_last(const chrono::month& __val) noexcept\n        : __m{__val} {}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr chrono::month month() const noexcept { return __m; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr bool ok() const noexcept { return __m.ok(); }\n};\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator==(const month_day_last& __lhs, const month_day_last& __rhs) noexcept\n{ return __lhs.month() == __rhs.month(); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator!=(const month_day_last& __lhs, const month_day_last& __rhs) noexcept\n{ return !(__lhs == __rhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator< (const month_day_last& __lhs, const month_day_last& __rhs) noexcept\n{ return __lhs.month() < __rhs.month(); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator> (const month_day_last& __lhs, const month_day_last& __rhs) noexcept\n{ return __rhs < __lhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator<=(const month_day_last& __lhs, const month_day_last& __rhs) noexcept\n{ return !(__rhs < __lhs);}\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator>=(const month_day_last& __lhs, const month_day_last& __rhs) noexcept\n{ return !(__lhs < __rhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nmonth_day_last operator/(const month& __lhs, last_spec) noexcept\n{ return month_day_last{__lhs}; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nmonth_day_last operator/(last_spec, const month& __rhs) noexcept\n{ return month_day_last{__rhs}; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nmonth_day_last operator/(int __lhs, last_spec) noexcept\n{ return month_day_last{month(__lhs)}; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nmonth_day_last operator/(last_spec, int __rhs) noexcept\n{ return month_day_last{month(__rhs)}; }\n\n\nclass month_weekday {\nprivate:\n    chrono::month __m;\n    chrono::weekday_indexed __wdi;\npublic:\n    month_weekday() = default;\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr month_weekday(const chrono::month& __mval, const chrono::weekday_indexed& __wdival) noexcept\n        : __m{__mval}, __wdi{__wdival} {}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr chrono::month                     month() const noexcept { return __m; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr chrono::weekday_indexed weekday_indexed() const noexcept { return __wdi; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr bool                                 ok() const noexcept { return __m.ok() && __wdi.ok(); }\n};\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator==(const month_weekday& __lhs, const month_weekday& __rhs) noexcept\n{ return __lhs.month() == __rhs.month() && __lhs.weekday_indexed() == __rhs.weekday_indexed(); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator!=(const month_weekday& __lhs, const month_weekday& __rhs) noexcept\n{ return !(__lhs == __rhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nmonth_weekday operator/(const month& __lhs, const weekday_indexed& __rhs) noexcept\n{ return month_weekday{__lhs, __rhs}; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nmonth_weekday operator/(int __lhs, const weekday_indexed& __rhs) noexcept\n{ return month_weekday{month(__lhs), __rhs}; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nmonth_weekday operator/(const weekday_indexed& __lhs, const month& __rhs) noexcept\n{ return month_weekday{__rhs, __lhs}; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nmonth_weekday operator/(const weekday_indexed& __lhs, int __rhs) noexcept\n{ return month_weekday{month(__rhs), __lhs}; }\n\n\nclass month_weekday_last {\n    chrono::month        __m;\n    chrono::weekday_last __wdl;\n  public:\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr month_weekday_last(const chrono::month& __mval, const chrono::weekday_last& __wdlval) noexcept\n        : __m{__mval}, __wdl{__wdlval} {}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr chrono::month               month() const noexcept { return __m; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr chrono::weekday_last weekday_last() const noexcept { return __wdl; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr bool                           ok() const noexcept { return __m.ok() && __wdl.ok(); }\n};\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator==(const month_weekday_last& __lhs, const month_weekday_last& __rhs) noexcept\n{ return __lhs.month() == __rhs.month() && __lhs.weekday_last() == __rhs.weekday_last(); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator!=(const month_weekday_last& __lhs, const month_weekday_last& __rhs) noexcept\n{ return !(__lhs == __rhs); }\n\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nmonth_weekday_last operator/(const month& __lhs, const weekday_last& __rhs) noexcept\n{ return month_weekday_last{__lhs, __rhs}; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nmonth_weekday_last operator/(int __lhs, const weekday_last& __rhs) noexcept\n{ return month_weekday_last{month(__lhs), __rhs}; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nmonth_weekday_last operator/(const weekday_last& __lhs, const month& __rhs) noexcept\n{ return month_weekday_last{__rhs, __lhs}; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nmonth_weekday_last operator/(const weekday_last& __lhs, int __rhs) noexcept\n{ return month_weekday_last{month(__rhs), __lhs}; }\n\n\nclass year_month {\n    chrono::year  __y;\n    chrono::month __m;\npublic:\n    year_month() = default;\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr year_month(const chrono::year& __yval, const chrono::month& __mval) noexcept\n        : __y{__yval}, __m{__mval} {}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr chrono::year  year()  const noexcept { return __y; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr chrono::month month() const noexcept { return __m; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr year_month& operator+=(const months& __dm) noexcept { this->__m += __dm; return *this; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr year_month& operator-=(const months& __dm) noexcept { this->__m -= __dm; return *this; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr year_month& operator+=(const years& __dy)  noexcept { this->__y += __dy; return *this; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr year_month& operator-=(const years& __dy)  noexcept { this->__y -= __dy; return *this; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr bool ok() const noexcept { return __y.ok() && __m.ok(); }\n};\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month operator/(const year& __y, const month& __m) noexcept { return year_month{__y, __m}; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month operator/(const year& __y, int __m) noexcept { return year_month{__y, month(__m)}; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator==(const year_month& __lhs, const year_month& __rhs) noexcept\n{ return __lhs.year() == __rhs.year() && __lhs.month() == __rhs.month(); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator!=(const year_month& __lhs, const year_month& __rhs) noexcept\n{ return !(__lhs == __rhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator< (const year_month& __lhs, const year_month& __rhs) noexcept\n{ return __lhs.year() != __rhs.year() ? __lhs.year() < __rhs.year() : __lhs.month() < __rhs.month(); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator> (const year_month& __lhs, const year_month& __rhs) noexcept\n{ return __rhs < __lhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator<=(const year_month& __lhs, const year_month& __rhs) noexcept\n{ return !(__rhs < __lhs);}\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator>=(const year_month& __lhs, const year_month& __rhs) noexcept\n{ return !(__lhs < __rhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\nconstexpr year_month operator+(const year_month& __lhs, const months& __rhs) noexcept\n{\n    int __dmi = static_cast<int>(static_cast<unsigned>(__lhs.month())) - 1 + __rhs.count();\n    const int __dy = (__dmi >= 0 ? __dmi : __dmi-11) / 12;\n    __dmi = __dmi - __dy * 12 + 1;\n    return (__lhs.year() + years(__dy)) / month(static_cast<unsigned>(__dmi));\n}\n\n_LIBCUDACXX_INLINE_VISIBILITY\nconstexpr year_month operator+(const months& __lhs, const year_month& __rhs) noexcept\n{ return __rhs + __lhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\nconstexpr year_month operator+(const year_month& __lhs, const years& __rhs) noexcept\n{ return (__lhs.year() + __rhs) / __lhs.month(); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\nconstexpr year_month operator+(const years& __lhs, const year_month& __rhs) noexcept\n{ return __rhs + __lhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\nconstexpr months     operator-(const year_month& __lhs, const year_month& __rhs) noexcept\n{ return (__lhs.year() - __rhs.year()) + months(static_cast<unsigned>(__lhs.month()) - static_cast<unsigned>(__rhs.month())); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\nconstexpr year_month operator-(const year_month& __lhs, const months& __rhs) noexcept\n{ return __lhs + -__rhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\nconstexpr year_month operator-(const year_month& __lhs, const years& __rhs) noexcept\n{ return __lhs + -__rhs; }\n\nclass year_month_day_last;\n\nclass year_month_day {\nprivate:\n    chrono::year  __y;\n    chrono::month __m;\n    chrono::day   __d;\npublic:\n     year_month_day() = default;\n     _LIBCUDACXX_INLINE_VISIBILITY\n     inline constexpr year_month_day(\n            const chrono::year& __yval, const chrono::month& __mval, const chrono::day& __dval) noexcept\n            : __y{__yval}, __m{__mval}, __d{__dval} {}\n     _LIBCUDACXX_INLINE_VISIBILITY\n            constexpr year_month_day(const year_month_day_last& __ymdl) noexcept;\n     _LIBCUDACXX_INLINE_VISIBILITY\n     inline constexpr year_month_day(const sys_days& __sysd) noexcept\n            : year_month_day(__from_days(__sysd.time_since_epoch())) {}\n     _LIBCUDACXX_INLINE_VISIBILITY\n     inline explicit constexpr year_month_day(const local_days& __locd) noexcept\n            : year_month_day(__from_days(__locd.time_since_epoch())) {}\n\n     _LIBCUDACXX_INLINE_VISIBILITY\n            constexpr year_month_day& operator+=(const months& __dm) noexcept;\n     _LIBCUDACXX_INLINE_VISIBILITY\n            constexpr year_month_day& operator-=(const months& __dm) noexcept;\n     _LIBCUDACXX_INLINE_VISIBILITY\n            constexpr year_month_day& operator+=(const years& __dy)  noexcept;\n     _LIBCUDACXX_INLINE_VISIBILITY\n            constexpr year_month_day& operator-=(const years& __dy)  noexcept;\n\n     _LIBCUDACXX_INLINE_VISIBILITY\n     inline constexpr chrono::year   year() const noexcept { return __y; }\n     _LIBCUDACXX_INLINE_VISIBILITY\n     inline constexpr chrono::month month() const noexcept { return __m; }\n     _LIBCUDACXX_INLINE_VISIBILITY\n     inline constexpr chrono::day     day() const noexcept { return __d; }\n     _LIBCUDACXX_INLINE_VISIBILITY\n     inline constexpr operator   sys_days() const noexcept          { return   sys_days{__to_days()}; }\n     _LIBCUDACXX_INLINE_VISIBILITY\n     inline explicit constexpr operator local_days() const noexcept { return local_days{__to_days()}; }\n\n     _LIBCUDACXX_INLINE_VISIBILITY\n            constexpr bool             ok() const noexcept;\n\n     _LIBCUDACXX_INLINE_VISIBILITY\n     static constexpr year_month_day __from_days(days __d) noexcept;\n     _LIBCUDACXX_INLINE_VISIBILITY\n     constexpr days __to_days() const noexcept;\n};\n\n\n// https://howardhinnant.github.io/date_algorithms.html#civil_from_days\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_day\nyear_month_day::__from_days(days __d) noexcept\n{\n    static_assert(std::numeric_limits<unsigned>::digits >= 18, \"\");\n    static_assert(std::numeric_limits<int>::digits >= 20     , \"\");\n    const int      __z = __d.count() + 719468;\n    const int      __era = (__z >= 0 ? __z : __z - 146096) / 146097;\n    const unsigned __doe = static_cast<unsigned>(__z - __era * 146097);              // [0, 146096]\n    const unsigned __yoe = (__doe - __doe/1460 + __doe/36524 - __doe/146096) / 365;  // [0, 399]\n    const int      __yr = static_cast<int>(__yoe) + __era * 400;\n    const unsigned __doy = __doe - (365 * __yoe + __yoe/4 - __yoe/100);              // [0, 365]\n    const unsigned __mp = (5 * __doy + 2)/153;                                       // [0, 11]\n    const unsigned __dy = __doy - (153 * __mp + 2)/5 + 1;                            // [1, 31]\n    const unsigned __mth = __mp + static_cast<unsigned>(__mp < 10 ? 3 : -9);         // [1, 12]\n    return year_month_day{chrono::year{__yr + (__mth <= 2)}, chrono::month{__mth}, chrono::day{__dy}};\n}\n\n// https://howardhinnant.github.io/date_algorithms.html#days_from_civil\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr days year_month_day::__to_days() const noexcept\n{\n    static_assert(std::numeric_limits<unsigned>::digits >= 18, \"\");\n    static_assert(std::numeric_limits<int>::digits >= 20     , \"\");\n\n    // nvcc doesn't allow ODR using constexpr globals. Therefore,\n    // make a temporary initialized from the global\n    auto constexpr __Feb = February;\n    const int      __yr  = static_cast<int>(__y) - (__m <= __Feb);\n    const unsigned __mth = static_cast<unsigned>(__m);\n    const unsigned __dy  = static_cast<unsigned>(__d);\n\n    const int      __era = (__yr >= 0 ? __yr : __yr - 399) / 400;\n    const unsigned __yoe = static_cast<unsigned>(__yr - __era * 400);                  // [0, 399]\n    const unsigned __doy = static_cast<unsigned>(\n        (153 * (__mth + static_cast<unsigned>(__mth > 2 ? -3 : 9)) + 2) / 5 + __dy-1); // [0, 365]\n    const unsigned __doe = __yoe * 365 + __yoe/4 - __yoe/100 + __doy;                  // [0, 146096]\n    return days{__era * 146097 + static_cast<int>(__doe) - 719468};\n}\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator==(const year_month_day& __lhs, const year_month_day& __rhs) noexcept\n{ return __lhs.year() == __rhs.year() && __lhs.month() == __rhs.month() && __lhs.day() == __rhs.day(); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator!=(const year_month_day& __lhs, const year_month_day& __rhs) noexcept\n{ return !(__lhs == __rhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator< (const year_month_day& __lhs, const year_month_day& __rhs) noexcept\n{\n    if (__lhs.year() < __rhs.year()) return true;\n    if (__lhs.year() > __rhs.year()) return false;\n    if (__lhs.month() < __rhs.month()) return true;\n    if (__lhs.month() > __rhs.month()) return false;\n    return __lhs.day() < __rhs.day();\n}\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator> (const year_month_day& __lhs, const year_month_day& __rhs) noexcept\n{ return __rhs < __lhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator<=(const year_month_day& __lhs, const year_month_day& __rhs) noexcept\n{ return !(__rhs < __lhs);}\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator>=(const year_month_day& __lhs, const year_month_day& __rhs) noexcept\n{ return !(__lhs < __rhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_day operator/(const year_month& __lhs, const day& __rhs) noexcept\n{ return year_month_day{__lhs.year(), __lhs.month(), __rhs}; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_day operator/(const year_month& __lhs, int __rhs) noexcept\n{ return __lhs / day(__rhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_day operator/(const year& __lhs, const month_day& __rhs) noexcept\n{ return __lhs / __rhs.month() / __rhs.day(); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_day operator/(int __lhs, const month_day& __rhs) noexcept\n{ return year(__lhs) / __rhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_day operator/(const month_day& __lhs, const year& __rhs) noexcept\n{ return __rhs / __lhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_day operator/(const month_day& __lhs, int __rhs) noexcept\n{ return year(__rhs) / __lhs; }\n\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_day operator+(const year_month_day& __lhs, const months& __rhs) noexcept\n{ return (__lhs.year()/__lhs.month() + __rhs)/__lhs.day(); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_day operator+(const months& __lhs, const year_month_day& __rhs) noexcept\n{ return __rhs + __lhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_day operator-(const year_month_day& __lhs, const months& __rhs) noexcept\n{ return __lhs + -__rhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_day operator+(const year_month_day& __lhs, const years& __rhs) noexcept\n{ return (__lhs.year() + __rhs) / __lhs.month() / __lhs.day(); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_day operator+(const years& __lhs, const year_month_day& __rhs) noexcept\n{ return __rhs + __lhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_day operator-(const year_month_day& __lhs, const years& __rhs) noexcept\n{ return __lhs + -__rhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr year_month_day& year_month_day::operator+=(const months& __dm) noexcept { *this = *this + __dm; return *this; }\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr year_month_day& year_month_day::operator-=(const months& __dm) noexcept { *this = *this - __dm; return *this; }\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr year_month_day& year_month_day::operator+=(const years& __dy)  noexcept { *this = *this + __dy; return *this; }\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr year_month_day& year_month_day::operator-=(const years& __dy)  noexcept { *this = *this - __dy; return *this; }\n\nclass year_month_day_last {\nprivate:\n    chrono::year           __y;\n    chrono::month_day_last __mdl;\npublic:\n     _LIBCUDACXX_INLINE_VISIBILITY\n     constexpr year_month_day_last(const year& __yval, const month_day_last& __mdlval) noexcept\n        : __y{__yval}, __mdl{__mdlval} {}\n\n     _LIBCUDACXX_INLINE_VISIBILITY\n     constexpr year_month_day_last& operator+=(const months& __m) noexcept;\n     _LIBCUDACXX_INLINE_VISIBILITY\n     constexpr year_month_day_last& operator-=(const months& __m) noexcept;\n     _LIBCUDACXX_INLINE_VISIBILITY\n     constexpr year_month_day_last& operator+=(const years& __y)  noexcept;\n     _LIBCUDACXX_INLINE_VISIBILITY\n     constexpr year_month_day_last& operator-=(const years& __y)  noexcept;\n\n     _LIBCUDACXX_INLINE_VISIBILITY\n     inline constexpr chrono::year                     year() const noexcept { return __y; }\n     _LIBCUDACXX_INLINE_VISIBILITY\n     inline constexpr chrono::month                   month() const noexcept { return __mdl.month(); }\n     _LIBCUDACXX_INLINE_VISIBILITY\n     inline constexpr chrono::month_day_last month_day_last() const noexcept { return __mdl; }\n     _LIBCUDACXX_INLINE_VISIBILITY\n            constexpr chrono::day                       day() const noexcept;\n     _LIBCUDACXX_INLINE_VISIBILITY\n     inline constexpr operator                     sys_days() const noexcept { return   sys_days{year()/month()/day()}; }\n     _LIBCUDACXX_INLINE_VISIBILITY\n     inline explicit constexpr operator          local_days() const noexcept { return local_days{year()/month()/day()}; }\n     _LIBCUDACXX_INLINE_VISIBILITY\n     inline constexpr bool                               ok() const noexcept { return __y.ok() && __mdl.ok(); }\n};\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nchrono::day year_month_day_last::day() const noexcept\n{\n    constexpr chrono::day __d[] =\n    {\n        chrono::day(31), chrono::day(28), chrono::day(31),\n        chrono::day(30), chrono::day(31), chrono::day(30),\n        chrono::day(31), chrono::day(31), chrono::day(30),\n        chrono::day(31), chrono::day(30), chrono::day(31)\n    };\n\n    // nvcc doesn't allow ODR using constexpr globals. Therefore,\n    // make a temporary initialized from the global\n    auto constexpr __Feb = February;\n    return month() != __Feb || !__y.is_leap() ?\n        __d[static_cast<unsigned>(month()) - 1] : chrono::day{29};\n}\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator==(const year_month_day_last& __lhs, const year_month_day_last& __rhs) noexcept\n{ return __lhs.year() == __rhs.year() && __lhs.month_day_last() == __rhs.month_day_last(); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator!=(const year_month_day_last& __lhs, const year_month_day_last& __rhs) noexcept\n{ return !(__lhs == __rhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator< (const year_month_day_last& __lhs, const year_month_day_last& __rhs) noexcept\n{\n    if (__lhs.year() < __rhs.year()) return true;\n    if (__lhs.year() > __rhs.year()) return false;\n    return __lhs.month_day_last() < __rhs.month_day_last();\n}\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator> (const year_month_day_last& __lhs, const year_month_day_last& __rhs) noexcept\n{ return __rhs < __lhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator<=(const year_month_day_last& __lhs, const year_month_day_last& __rhs) noexcept\n{ return !(__rhs < __lhs);}\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator>=(const year_month_day_last& __lhs, const year_month_day_last& __rhs) noexcept\n{ return !(__lhs < __rhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr year_month_day_last operator/(const year_month& __lhs, last_spec) noexcept\n{ return year_month_day_last{__lhs.year(), month_day_last{__lhs.month()}}; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr year_month_day_last operator/(const year& __lhs, const month_day_last& __rhs) noexcept\n{ return year_month_day_last{__lhs, __rhs}; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr year_month_day_last operator/(int __lhs, const month_day_last& __rhs) noexcept\n{ return year_month_day_last{year{__lhs}, __rhs}; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr year_month_day_last operator/(const month_day_last& __lhs, const year& __rhs) noexcept\n{ return __rhs / __lhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr year_month_day_last operator/(const month_day_last& __lhs, int __rhs) noexcept\n{ return year{__rhs} / __lhs; }\n\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_day_last operator+(const year_month_day_last& __lhs, const months& __rhs) noexcept\n{ return (__lhs.year() / __lhs.month() + __rhs) / last; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_day_last operator+(const months& __lhs, const year_month_day_last& __rhs) noexcept\n{ return __rhs + __lhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_day_last operator-(const year_month_day_last& __lhs, const months& __rhs) noexcept\n{ return __lhs + (-__rhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_day_last operator+(const year_month_day_last& __lhs, const years& __rhs) noexcept\n{ return year_month_day_last{__lhs.year() + __rhs, __lhs.month_day_last()}; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_day_last operator+(const years& __lhs, const year_month_day_last& __rhs) noexcept\n{ return __rhs + __lhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_day_last operator-(const year_month_day_last& __lhs, const years& __rhs) noexcept\n{ return __lhs + (-__rhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr year_month_day_last& year_month_day_last::operator+=(const months& __dm) noexcept { *this = *this + __dm; return *this; }\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr year_month_day_last& year_month_day_last::operator-=(const months& __dm) noexcept { *this = *this - __dm; return *this; }\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr year_month_day_last& year_month_day_last::operator+=(const years& __dy)  noexcept { *this = *this + __dy; return *this; }\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr year_month_day_last& year_month_day_last::operator-=(const years& __dy)  noexcept { *this = *this - __dy; return *this; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr year_month_day::year_month_day(const year_month_day_last& __ymdl) noexcept\n    : __y{__ymdl.year()}, __m{__ymdl.month()}, __d{__ymdl.day()} {}\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr bool year_month_day::ok() const noexcept\n{\n    if (!__y.ok() || !__m.ok()) return false;\n    return chrono::day{1} <= __d && __d <= (__y / __m / last).day();\n}\n\nclass year_month_weekday {\n    chrono::year            __y;\n    chrono::month           __m;\n    chrono::weekday_indexed __wdi;\npublic:\n    year_month_weekday() = default;\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr year_month_weekday(const chrono::year& __yval, const chrono::month& __mval,\n                               const chrono::weekday_indexed& __wdival) noexcept\n        : __y{__yval}, __m{__mval}, __wdi{__wdival} {}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr year_month_weekday(const sys_days& __sysd) noexcept\n            : year_month_weekday(__from_days(__sysd.time_since_epoch())) {}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline explicit constexpr year_month_weekday(const local_days& __locd) noexcept\n            : year_month_weekday(__from_days(__locd.time_since_epoch())) {}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr year_month_weekday& operator+=(const months& m) noexcept;\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr year_month_weekday& operator-=(const months& m) noexcept;\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr year_month_weekday& operator+=(const years& y)  noexcept;\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr year_month_weekday& operator-=(const years& y)  noexcept;\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr chrono::year                       year() const noexcept { return __y; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr chrono::month                     month() const noexcept { return __m; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr chrono::weekday                 weekday() const noexcept { return __wdi.weekday(); }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr unsigned                          index() const noexcept { return __wdi.index(); }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr chrono::weekday_indexed weekday_indexed() const noexcept { return __wdi; }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr                       operator sys_days() const noexcept { return   sys_days{__to_days()}; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline explicit constexpr operator            local_days() const noexcept { return local_days{__to_days()}; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr bool ok() const noexcept\n    {\n        if (!__y.ok() || !__m.ok() || !__wdi.ok()) return false;\n    //  TODO: make sure it's a valid date\n        return true;\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    static constexpr year_month_weekday __from_days(days __d) noexcept;\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr days __to_days() const noexcept;\n};\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_weekday year_month_weekday::__from_days(days __d) noexcept\n{\n    const sys_days      __sysd{__d};\n    const chrono::weekday __wd = chrono::weekday(__sysd);\n    const year_month_day __ymd = year_month_day(__sysd);\n    return year_month_weekday{__ymd.year(), __ymd.month(),\n                              __wd[(static_cast<unsigned>(__ymd.day())-1)/7+1]};\n}\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\ndays year_month_weekday::__to_days() const noexcept\n{\n    const sys_days __sysd = sys_days(__y/__m/1);\n    return (__sysd + (__wdi.weekday() - chrono::weekday(__sysd) + days{(__wdi.index()-1)*7}))\n                .time_since_epoch();\n}\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator==(const year_month_weekday& __lhs, const year_month_weekday& __rhs) noexcept\n{ return __lhs.year() == __rhs.year() && __lhs.month() == __rhs.month() && __lhs.weekday_indexed() == __rhs.weekday_indexed(); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator!=(const year_month_weekday& __lhs, const year_month_weekday& __rhs) noexcept\n{ return !(__lhs == __rhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_weekday operator/(const year_month& __lhs, const weekday_indexed& __rhs) noexcept\n{ return year_month_weekday{__lhs.year(), __lhs.month(), __rhs}; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_weekday operator/(const year& __lhs, const month_weekday& __rhs) noexcept\n{ return year_month_weekday{__lhs, __rhs.month(), __rhs.weekday_indexed()}; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_weekday operator/(int __lhs, const month_weekday& __rhs) noexcept\n{ return year(__lhs) / __rhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_weekday operator/(const month_weekday& __lhs, const year& __rhs) noexcept\n{ return __rhs / __lhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_weekday operator/(const month_weekday& __lhs, int __rhs) noexcept\n{ return year(__rhs) / __lhs; }\n\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_weekday operator+(const year_month_weekday& __lhs, const months& __rhs) noexcept\n{ return (__lhs.year() / __lhs.month() + __rhs) / __lhs.weekday_indexed(); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_weekday operator+(const months& __lhs, const year_month_weekday& __rhs) noexcept\n{ return __rhs + __lhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_weekday operator-(const year_month_weekday& __lhs, const months& __rhs) noexcept\n{ return __lhs + (-__rhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_weekday operator+(const year_month_weekday& __lhs, const years& __rhs) noexcept\n{ return year_month_weekday{__lhs.year() + __rhs, __lhs.month(), __lhs.weekday_indexed()}; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_weekday operator+(const years& __lhs, const year_month_weekday& __rhs) noexcept\n{ return __rhs + __lhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_weekday operator-(const year_month_weekday& __lhs, const years& __rhs) noexcept\n{ return __lhs + (-__rhs); }\n\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr year_month_weekday& year_month_weekday::operator+=(const months& __dm) noexcept { *this = *this + __dm; return *this; }\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr year_month_weekday& year_month_weekday::operator-=(const months& __dm) noexcept { *this = *this - __dm; return *this; }\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr year_month_weekday& year_month_weekday::operator+=(const years& __dy)  noexcept { *this = *this + __dy; return *this; }\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr year_month_weekday& year_month_weekday::operator-=(const years& __dy)  noexcept { *this = *this - __dy; return *this; }\n\nclass year_month_weekday_last {\nprivate:\n    chrono::year         __y;\n    chrono::month        __m;\n    chrono::weekday_last __wdl;\npublic:\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr year_month_weekday_last(const chrono::year& __yval, const chrono::month& __mval,\n                                      const chrono::weekday_last& __wdlval) noexcept\n                : __y{__yval}, __m{__mval}, __wdl{__wdlval} {}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr year_month_weekday_last& operator+=(const months& __dm) noexcept;\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr year_month_weekday_last& operator-=(const months& __dm) noexcept;\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr year_month_weekday_last& operator+=(const years& __dy)  noexcept;\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr year_month_weekday_last& operator-=(const years& __dy)  noexcept;\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr chrono::year                 year() const noexcept { return __y; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr chrono::month               month() const noexcept { return __m; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr chrono::weekday           weekday() const noexcept { return __wdl.weekday(); }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr chrono::weekday_last weekday_last() const noexcept { return __wdl; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr operator                 sys_days() const noexcept { return   sys_days{__to_days()}; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline explicit constexpr operator      local_days() const noexcept { return local_days{__to_days()}; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    inline constexpr bool ok() const noexcept { return __y.ok() && __m.ok() && __wdl.ok(); }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr days __to_days() const noexcept;\n\n};\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\ndays year_month_weekday_last::__to_days() const noexcept\n{\n    const sys_days __last = sys_days{__y/__m/last};\n    return (__last - (chrono::weekday{__last} - __wdl.weekday())).time_since_epoch();\n\n}\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator==(const year_month_weekday_last& __lhs, const year_month_weekday_last& __rhs) noexcept\n{ return __lhs.year() == __rhs.year() && __lhs.month() == __rhs.month() && __lhs.weekday_last() == __rhs.weekday_last(); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nbool operator!=(const year_month_weekday_last& __lhs, const year_month_weekday_last& __rhs) noexcept\n{ return !(__lhs == __rhs); }\n\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_weekday_last operator/(const year_month& __lhs, const weekday_last& __rhs) noexcept\n{ return year_month_weekday_last{__lhs.year(), __lhs.month(), __rhs}; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_weekday_last operator/(const year& __lhs, const month_weekday_last& __rhs) noexcept\n{ return year_month_weekday_last{__lhs, __rhs.month(), __rhs.weekday_last()}; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_weekday_last operator/(int __lhs, const month_weekday_last& __rhs) noexcept\n{ return year(__lhs) / __rhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_weekday_last operator/(const month_weekday_last& __lhs, const year& __rhs) noexcept\n{ return __rhs / __lhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_weekday_last operator/(const month_weekday_last& __lhs, int __rhs) noexcept\n{ return year(__rhs) / __lhs; }\n\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_weekday_last operator+(const year_month_weekday_last& __lhs, const months& __rhs) noexcept\n{ return (__lhs.year() / __lhs.month() + __rhs) / __lhs.weekday_last(); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_weekday_last operator+(const months& __lhs, const year_month_weekday_last& __rhs) noexcept\n{ return __rhs + __lhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_weekday_last operator-(const year_month_weekday_last& __lhs, const months& __rhs) noexcept\n{ return __lhs + (-__rhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_weekday_last operator+(const year_month_weekday_last& __lhs, const years& __rhs) noexcept\n{ return year_month_weekday_last{__lhs.year() + __rhs, __lhs.month(), __lhs.weekday_last()}; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_weekday_last operator+(const years& __lhs, const year_month_weekday_last& __rhs) noexcept\n{ return __rhs + __lhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr\nyear_month_weekday_last operator-(const year_month_weekday_last& __lhs, const years& __rhs) noexcept\n{ return __lhs + (-__rhs); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr year_month_weekday_last& year_month_weekday_last::operator+=(const months& __dm) noexcept { *this = *this + __dm; return *this; }\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr year_month_weekday_last& year_month_weekday_last::operator-=(const months& __dm) noexcept { *this = *this - __dm; return *this; }\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr year_month_weekday_last& year_month_weekday_last::operator+=(const years& __dy)  noexcept { *this = *this + __dy; return *this; }\n_LIBCUDACXX_INLINE_VISIBILITY\ninline constexpr year_month_weekday_last& year_month_weekday_last::operator-=(const years& __dy)  noexcept { *this = *this - __dy; return *this; }\n\ntemplate <class _Duration>\nclass hh_mm_ss\n{\nprivate:\n    static_assert(__is_duration<_Duration>::value, \"template parameter of hh_mm_ss must be a std::chrono::duration\");\n    using __CommonType = typename common_type<_Duration, chrono::seconds>::type;\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    static constexpr uint64_t __pow10(unsigned __exp)\n    {\n        uint64_t __ret = 1;\n        for (unsigned __i = 0; __i < __exp; ++__i)\n            __ret *= 10U;\n        return __ret;\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    static constexpr unsigned __width(uint64_t __n, uint64_t __d = 10, unsigned __w = 0)\n    {\n        if (__n >= 2 && __d != 0 && __w < 19)\n            return 1 + __width(__n, __d % __n * 10, __w+1);\n        return 0;\n    }\n\npublic:\n    static unsigned constexpr fractional_width = __width(__CommonType::period::den) < 19 ?\n                                                 __width(__CommonType::period::den) : 6u;\n    using precision = duration<typename __CommonType::rep, ratio<1, __pow10(fractional_width)>>;\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr hh_mm_ss() noexcept : hh_mm_ss{_Duration::zero()} {}\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr explicit hh_mm_ss(_Duration __d) noexcept :\n        __is_neg(__d < _Duration(0)),\n        __h(duration_cast<chrono::hours>  (abs(__d))),\n        __m(duration_cast<chrono::minutes>(abs(__d) - hours())),\n        __s(duration_cast<chrono::seconds>(abs(__d) - hours() - minutes())),\n        __f(duration_cast<precision>      (abs(__d) - hours() - minutes() - seconds()))\n        {}\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr bool is_negative()        const noexcept { return __is_neg; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr chrono::hours hours()     const noexcept { return __h; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr chrono::minutes minutes() const noexcept { return __m; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr chrono::seconds seconds() const noexcept { return __s; }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr precision subseconds()    const noexcept { return __f; }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr precision to_duration() const noexcept\n    {\n        auto __dur = __h + __m + __s + __f;\n        return __is_neg ? -__dur : __dur;\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr explicit operator precision() const noexcept { return to_duration(); }\n\nprivate:\n    bool            __is_neg;\n    chrono::hours   __h;\n    chrono::minutes __m;\n    chrono::seconds __s;\n    precision       __f;\n};\n\n_LIBCUDACXX_INLINE_VISIBILITY\nconstexpr bool is_am(const hours& __h) noexcept { return __h >= hours( 0) && __h < hours(12); }\n_LIBCUDACXX_INLINE_VISIBILITY\nconstexpr bool is_pm(const hours& __h) noexcept { return __h >= hours(12) && __h < hours(24); }\n\n_LIBCUDACXX_INLINE_VISIBILITY\nconstexpr hours make12(const hours& __h) noexcept\n{\n    if      (__h == hours( 0)) return hours(12);\n    else if (__h <= hours(12)) return __h;\n    else                       return __h - hours(12);\n}\n\n_LIBCUDACXX_INLINE_VISIBILITY\nconstexpr hours make24(const hours& __h, bool __is_pm) noexcept\n{\n    if (__is_pm)\n        return __h == hours(12) ? __h : __h + hours(12);\n    else\n        return __h == hours(12) ? hours(0) : __h;\n}\n#endif // _LIBCUDACXX_STD_VER > 11\n} // chrono\n#if _LIBCUDACXX_STD_VER > 11\n\n// GCC 5 and 6 warn (and then error) on us using the standard reserved UDL names,\n// but have no way of disabling that. Use the system_header pragma on those GCC versions\n// for the remainder of this file, even if it has been requested to disable the pragma\n// earlier.\n#if defined(_LIBCUDACXX_COMPILER_GCC) && (__GNUC__ == 5 || __GNUC__ == 6)\n_Pragma(\"GCC system_header\")\n#endif\n\n// Suffixes for duration literals [time.duration.literals]\ninline namespace literals\n{\n  inline namespace chrono_literals\n  {\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr chrono::hours operator\"\"h(unsigned long long __h)\n    {\n        return chrono::hours(static_cast<chrono::hours::rep>(__h));\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr chrono::duration<_LIBCUDACXX_CHRONO_LITERAL_INTERNAL_T, ratio<3600,1>> operator\"\"h(long double __h)\n    {\n        return chrono::duration<_LIBCUDACXX_CHRONO_LITERAL_INTERNAL_T, ratio<3600,1>>(__h);\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr chrono::minutes operator\"\"min(unsigned long long __m)\n    {\n        return chrono::minutes(static_cast<chrono::minutes::rep>(__m));\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr chrono::duration<_LIBCUDACXX_CHRONO_LITERAL_INTERNAL_T, ratio<60,1>> operator\"\"min(long double __m)\n    {\n        return chrono::duration<_LIBCUDACXX_CHRONO_LITERAL_INTERNAL_T, ratio<60,1>> (__m);\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr chrono::seconds operator\"\"s(unsigned long long __s)\n    {\n        return chrono::seconds(static_cast<chrono::seconds::rep>(__s));\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr chrono::duration<_LIBCUDACXX_CHRONO_LITERAL_INTERNAL_T> operator\"\"s(long double __s)\n    {\n        return chrono::duration<_LIBCUDACXX_CHRONO_LITERAL_INTERNAL_T> (__s);\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr chrono::milliseconds operator\"\"ms(unsigned long long __ms)\n    {\n        return chrono::milliseconds(static_cast<chrono::milliseconds::rep>(__ms));\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr chrono::duration<_LIBCUDACXX_CHRONO_LITERAL_INTERNAL_T, milli> operator\"\"ms(long double __ms)\n    {\n        return chrono::duration<_LIBCUDACXX_CHRONO_LITERAL_INTERNAL_T, milli>(__ms);\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr chrono::microseconds operator\"\"us(unsigned long long __us)\n    {\n        return chrono::microseconds(static_cast<chrono::microseconds::rep>(__us));\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr chrono::duration<_LIBCUDACXX_CHRONO_LITERAL_INTERNAL_T, micro> operator\"\"us(long double __us)\n    {\n        return chrono::duration<_LIBCUDACXX_CHRONO_LITERAL_INTERNAL_T, micro> (__us);\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr chrono::nanoseconds operator\"\"ns(unsigned long long __ns)\n    {\n        return chrono::nanoseconds(static_cast<chrono::nanoseconds::rep>(__ns));\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr chrono::duration<_LIBCUDACXX_CHRONO_LITERAL_INTERNAL_T, nano> operator\"\"ns(long double __ns)\n    {\n        return chrono::duration<_LIBCUDACXX_CHRONO_LITERAL_INTERNAL_T, nano> (__ns);\n    }\n\n#if _LIBCUDACXX_STD_VER > 17 && !defined(_LIBCUDACXX_HAS_NO_CXX20_CHRONO_LITERALS)\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr chrono::day operator\"\"d(unsigned long long __d) noexcept\n    {\n        return chrono::day(static_cast<unsigned>(__d));\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr chrono::year operator\"\"y(unsigned long long __y) noexcept\n    {\n        return chrono::year(static_cast<int>(__y));\n    }\n#endif //_LIBCUDACXX_STD_VER > 17 && !defined(_LIBCUDACXX_HAS_NO_CXX20_CHRONO_LITERALS)\n}}\n\nnamespace chrono { // hoist the literals into namespace std::chrono\n   using namespace literals::chrono_literals;\n}\n\n#endif // _LIBCUDACXX_STD_VER > 11\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#ifndef __cuda_std__\n\n_LIBCUDACXX_BEGIN_NAMESPACE_FILESYSTEM\nstruct _FilesystemClock {\n#if !defined(_LIBCUDACXX_HAS_NO_INT128)\n  typedef __int128_t rep;\n  typedef nano period;\n#else\n  typedef long long rep;\n  typedef nano period;\n#endif\n\n  typedef chrono::duration<rep, period> duration;\n  typedef chrono::time_point<_FilesystemClock> time_point;\n\n  _LIBCUDACXX_EXPORTED_FROM_ABI\n  static _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 const bool is_steady = false;\n\n  _LIBCUDACXX_AVAILABILITY_FILESYSTEM _LIBCUDACXX_FUNC_VIS static time_point now() noexcept;\n\n  _LIBCUDACXX_INLINE_VISIBILITY\n  static time_t to_time_t(const time_point& __t) noexcept {\n      typedef chrono::duration<rep> __secs;\n      return time_t(\n          chrono::duration_cast<__secs>(__t.time_since_epoch()).count());\n  }\n\n  _LIBCUDACXX_INLINE_VISIBILITY\n  static time_point from_time_t(time_t __t) noexcept {\n      typedef chrono::duration<rep> __secs;\n      return time_point(__secs(__t));\n  }\n};\n_LIBCUDACXX_END_NAMESPACE_FILESYSTEM\n#endif // __cuda_std__\n\n_LIBCUDACXX_NV_DIAG_DEFAULT(cuda_demote_unsupported_floating_point)\n\n#ifndef __cuda_std__\n#include <__pragma_pop>\n#else\n#include \"__cuda/chrono.h\"\n#endif // __cuda_std__\n\n#endif  // _LIBCUDACXX_CHRONO\n", "climits": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===--------------------------- climits ----------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX_CLIMITS\n#define _LIBCUDACXX_CLIMITS\n\n/*\n    climits synopsis\n\nMacros:\n\n    CHAR_BIT\n    SCHAR_MIN\n    SCHAR_MAX\n    UCHAR_MAX\n    CHAR_MIN\n    CHAR_MAX\n    MB_LEN_MAX\n    SHRT_MIN\n    SHRT_MAX\n    USHRT_MAX\n    INT_MIN\n    INT_MAX\n    UINT_MAX\n    LONG_MIN\n    LONG_MAX\n    ULONG_MAX\n    LLONG_MIN   // C99\n    LLONG_MAX   // C99\n    ULLONG_MAX  // C99\n\n*/\n\n#ifndef __cuda_std__\n#include <__config>\n#else\n#include \"__cuda/climits_prelude.h\"\n#endif //__cuda_std__\n\n#ifndef __cuda_std__\n#include <limits.h>\n#include <__pragma_push>\n#endif //__cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n#if defined(_LIBCUDACXX_COMPILER_MSVC)\n#include \"support/win32/limits_msvc_win32.h\"\n#endif // _LIBCUDACXX_MSVCRT\n\n#if defined(_LIBCUDACXX_COMPILER_IBM)\n#include \"support/ibm/limits.h\"\n#endif // _LIBCUDACXX_COMPILER_IBM\n\n// ICC defines __CHAR_BIT__ by default\n// accept that, but assert it is what we expect\n#ifdef __CHAR_BIT__\n    static_assert(__CHAR_BIT__ == 8, \"\");\n#else\n    #define __CHAR_BIT__ 8\n#endif\n\n#ifndef __cuda_std__\n#include <__pragma_pop>\n#endif //__cuda_std__\n\n#endif  // _LIBCUDACXX_CLIMITS\n", "concepts": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX_CONCEPTS\n#define _LIBCUDACXX_CONCEPTS\n\n/*\n    concepts synopsis\nnamespace std {\n  // [concepts.lang], language-related concepts\n  // [concept.same], concept same_as\n  template<class T, class U>\n    concept same_as = see below;\n\n  // [concept.derived], concept derived_from\n  template<class Derived, class Base>\n    concept derived_from = see below;\n\n  // [concept.convertible], concept convertible_to\n  template<class From, class To>\n    concept convertible_to = see below;\n\n  // [concept.commonref], concept common_reference_with\n  template<class T, class U>\n    concept common_reference_with = see below;\n\n  // [concept.common], concept common_with\n  template<class T, class U>\n    concept common_with = see below;\n\n  // [concepts.arithmetic], arithmetic concepts\n  template<class T>\n    concept integral = see below;\n  template<class T>\n    concept signed_integral = see below;\n  template<class T>\n    concept unsigned_integral = see below;\n  template<class T>\n    concept floating_point = see below;\n\n  // [concept.assignable], concept assignable_from\n  template<class LHS, class RHS>\n    concept assignable_from = see below;\n\n  // [concept.swappable], concept swappable\n  namespace ranges {\n    inline namespace unspecified {\n      inline constexpr unspecified swap = unspecified;\n    }\n  }\n  template<class T>\n    concept swappable = see below;\n  template<class T, class U>\n    concept swappable_with = see below;\n\n  // [concept.destructible], concept destructible\n  template<class T>\n    concept destructible = see below;\n\n  // [concept.constructible], concept constructible_from\n  template<class T, class... Args>\n    concept constructible_from = see below;\n\n  // [concept.default.init], concept default_initializable\n  template<class T>\n    concept default_initializable = see below;\n\n  // [concept.moveconstructible], concept move_constructible\n  template<class T>\n    concept move_constructible = see below;\n\n  // [concept.copyconstructible], concept copy_constructible\n  template<class T>\n    concept copy_constructible = see below;\n\n  // [concept.equalitycomparable], concept equality_comparable\n  template<class T>\n    concept equality_comparable = see below;\n  template<class T, class U>\n    concept equality_comparable_with = see below;\n\n  // [concept.totallyordered], concept totally_ordered\n  template<class T>\n    concept totally_ordered = see below;\n  template<class T, class U>\n    concept totally_ordered_with = see below;\n\n  // [concepts.object], object concepts\n  template<class T>\n    concept movable = see below;\n  template<class T>\n    concept copyable = see below;\n  template<class T>\n    concept semiregular = see below;\n  template<class T>\n    concept regular = see below;\n\n  // [concepts.callable], callable concepts\n  // [concept.invocable], concept invocable\n  template<class F, class... Args>\n    concept invocable = see below;\n\n  // [concept.regularinvocable], concept regular_invocable\n  template<class F, class... Args>\n    concept regular_invocable = see below;\n\n  // [concept.predicate], concept predicate\n  template<class F, class... Args>\n    concept predicate = see below;\n\n  // [concept.relation], concept relation\n  template<class R, class T, class U>\n    concept relation = see below;\n\n  // [concept.equiv], concept equivalence_relation\n  template<class R, class T, class U>\n    concept equivalence_relation = see below;\n\n  // [concept.strictweakorder], concept strict_weak_order\n  template<class R, class T, class U>\n    concept strict_weak_order = see below;\n}\n\n*/\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"__assert\" // all public C++ headers provide the assertion handler\n#include \"__concepts/__concept_macros.h\"\n#include \"__concepts/_One_of.h\"\n#include \"__concepts/arithmetic.h\"\n#include \"__concepts/assignable.h\"\n#include \"__concepts/boolean_testable.h\"\n#include \"__concepts/class_or_enum.h\"\n#include \"__concepts/common_reference_with.h\"\n#include \"__concepts/common_with.h\"\n#include \"__concepts/constructible.h\"\n#include \"__concepts/convertible_to.h\"\n#include \"__concepts/copyable.h\"\n#include \"__concepts/derived_from.h\"\n#include \"__concepts/destructible.h\"\n#include \"__concepts/different_from.h\"\n#include \"__concepts/equality_comparable.h\"\n#include \"__concepts/invocable.h\"\n#include \"__concepts/movable.h\"\n#include \"__concepts/predicate.h\"\n#include \"__concepts/regular.h\"\n#include \"__concepts/relation.h\"\n#include \"__concepts/same_as.h\"\n#include \"__concepts/semiregular.h\"\n#include \"__concepts/swappable.h\"\n#include \"__concepts/totally_ordered.h\"\n\n#include \"version\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n#endif // _LIBCUDACXX_CONCEPTS\n", "cstddef": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===--------------------------- cstddef ----------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX_CSTDDEF\n#define _LIBCUDACXX_CSTDDEF\n\n/*\n    cstddef synopsis\n\nMacros:\n\n    offsetof(type,member-designator)\n    NULL\n\nnamespace std\n{\n\nTypes:\n\n    ptrdiff_t\n    size_t\n    max_align_t\n    nullptr_t\n    byte // C++17\n\n}  // std\n\n*/\n\n#ifndef __cuda_std__\n#include <__config>\n#else\n#include \"__cuda/cstddef_prelude.h\"\n#endif //__cuda_std__\n\n#include \"__assert\" // all public C++ headers provide the assertion handler\n#include \"__type_traits/enable_if.h\"\n#include \"__type_traits/is_integral.h\"\n#include \"version\"\n\n#ifndef __cuda_std__\n#include <__pragma_push>\n#endif //__cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n#ifndef __cuda_std__\n// Don't include our own <stddef.h>; we don't want to declare ::nullptr_t.\n#ifdef _MSC_VER\n    #include <../ucrt/stddef.h>\n#else\n    #include_next <stddef.h>\n#endif\n#include <__nullptr>\n#endif //__cuda_std__\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\nusing ::ptrdiff_t;\nusing ::size_t;\n\n#if defined(__CLANG_MAX_ALIGN_T_DEFINED) || defined(_GCC_MAX_ALIGN_T) || \\\n    defined(__DEFINED_max_align_t) || defined(__NetBSD__)\n// Re-use the compiler's <stddef.h> max_align_t where possible.\nusing ::max_align_t;\n#else\ntypedef long double max_align_t;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#if _LIBCUDACXX_STD_VER > 11\n#ifdef _LIBCUDACXX_BEGIN_NAMESPACE_STD_NOVERSION\n_LIBCUDACXX_BEGIN_NAMESPACE_STD_NOVERSION\n#else\nnamespace std  // purposefully not versioned\n{\n#endif //_LIBCUDACXX_BEGIN_NAMESPACE_STD_NOVERSION\nenum class byte : unsigned char {};\n\n_LIBCUDACXX_INLINE_VISIBILITY\nconstexpr byte  operator| (byte  __lhs, byte __rhs) noexcept\n{\n    return static_cast<byte>(\n      static_cast<unsigned char>(\n         static_cast<unsigned int>(__lhs) | static_cast<unsigned int>(__rhs)\n    ));\n}\n\n_LIBCUDACXX_INLINE_VISIBILITY\nconstexpr byte& operator|=(byte& __lhs, byte __rhs) noexcept\n{ return __lhs = __lhs | __rhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\nconstexpr byte  operator& (byte  __lhs, byte __rhs) noexcept\n{\n    return static_cast<byte>(\n      static_cast<unsigned char>(\n         static_cast<unsigned int>(__lhs) & static_cast<unsigned int>(__rhs)\n    ));\n}\n\n_LIBCUDACXX_INLINE_VISIBILITY\nconstexpr byte& operator&=(byte& __lhs, byte __rhs) noexcept\n{ return __lhs = __lhs & __rhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\nconstexpr byte  operator^ (byte  __lhs, byte __rhs) noexcept\n{\n    return static_cast<byte>(\n      static_cast<unsigned char>(\n         static_cast<unsigned int>(__lhs) ^ static_cast<unsigned int>(__rhs)\n    ));\n}\n\n_LIBCUDACXX_INLINE_VISIBILITY\nconstexpr byte& operator^=(byte& __lhs, byte __rhs) noexcept\n{ return __lhs = __lhs ^ __rhs; }\n\n_LIBCUDACXX_INLINE_VISIBILITY\nconstexpr byte  operator~ (byte __b) noexcept\n{\n    return static_cast<byte>(\n      static_cast<unsigned char>(\n        ~static_cast<unsigned int>(__b)\n    ));\n}\n\ntemplate <class _Integer>\n  _LIBCUDACXX_INLINE_VISIBILITY\n  constexpr __enable_if_t<is_integral_v<_Integer>, byte> &\n  operator<<=(byte& __lhs, _Integer __shift) noexcept\n  { return __lhs = __lhs << __shift; }\n\ntemplate <class _Integer>\n  _LIBCUDACXX_INLINE_VISIBILITY\n  constexpr __enable_if_t<is_integral_v<_Integer>, byte>\n  operator<< (byte  __lhs, _Integer __shift) noexcept\n  { return static_cast<byte>(static_cast<unsigned char>(static_cast<unsigned int>(__lhs) << __shift)); }\n\ntemplate <class _Integer>\n  _LIBCUDACXX_INLINE_VISIBILITY\n  constexpr __enable_if_t<is_integral_v<_Integer>, byte> &\n  operator>>=(byte& __lhs, _Integer __shift) noexcept\n  { return __lhs = __lhs >> __shift; }\n\ntemplate <class _Integer>\n  _LIBCUDACXX_INLINE_VISIBILITY\n  constexpr __enable_if_t<is_integral_v<_Integer>, byte>\n  operator>> (byte  __lhs, _Integer __shift) noexcept\n  { return static_cast<byte>(static_cast<unsigned char>(static_cast<unsigned int>(__lhs) >> __shift)); }\n\ntemplate <class _Integer>\n  _LIBCUDACXX_INLINE_VISIBILITY\n  constexpr __enable_if_t<is_integral_v<_Integer>, _Integer>\n  to_integer(byte __b) noexcept { return static_cast<_Integer>(__b); }\n\n#ifdef _LIBCUDACXX_END_NAMESPACE_STD_NOVERSION\n_LIBCUDACXX_END_NAMESPACE_STD_NOVERSION\n#else\n}\n#endif //_LIBCUDACXX_END_NAMESPACE_STD_NOVERSION\n#endif // _LIBCUDACXX_STD_VER > 11\n\n#ifndef __cuda_std__\n#include <__pragma_pop>\n#endif //__cuda_std__\n\n#endif  // _LIBCUDACXX_CSTDDEF\n", "cstdint": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===--------------------------- cstdint ----------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX_CSTDINT\n#define _LIBCUDACXX_CSTDINT\n\n/*\n    cstdint synopsis\n\nMacros:\n\n    INT8_MIN\n    INT16_MIN\n    INT32_MIN\n    INT64_MIN\n\n    INT8_MAX\n    INT16_MAX\n    INT32_MAX\n    INT64_MAX\n\n    UINT8_MAX\n    UINT16_MAX\n    UINT32_MAX\n    UINT64_MAX\n\n    INT_LEAST8_MIN\n    INT_LEAST16_MIN\n    INT_LEAST32_MIN\n    INT_LEAST64_MIN\n\n    INT_LEAST8_MAX\n    INT_LEAST16_MAX\n    INT_LEAST32_MAX\n    INT_LEAST64_MAX\n\n    UINT_LEAST8_MAX\n    UINT_LEAST16_MAX\n    UINT_LEAST32_MAX\n    UINT_LEAST64_MAX\n\n    INT_FAST8_MIN\n    INT_FAST16_MIN\n    INT_FAST32_MIN\n    INT_FAST64_MIN\n\n    INT_FAST8_MAX\n    INT_FAST16_MAX\n    INT_FAST32_MAX\n    INT_FAST64_MAX\n\n    UINT_FAST8_MAX\n    UINT_FAST16_MAX\n    UINT_FAST32_MAX\n    UINT_FAST64_MAX\n\n    INTPTR_MIN\n    INTPTR_MAX\n    UINTPTR_MAX\n\n    INTMAX_MIN\n    INTMAX_MAX\n\n    UINTMAX_MAX\n\n    PTRDIFF_MIN\n    PTRDIFF_MAX\n\n    SIG_ATOMIC_MIN\n    SIG_ATOMIC_MAX\n\n    SIZE_MAX\n\n    WCHAR_MIN\n    WCHAR_MAX\n\n    WINT_MIN\n    WINT_MAX\n\n    INT8_C(value)\n    INT16_C(value)\n    INT32_C(value)\n    INT64_C(value)\n\n    UINT8_C(value)\n    UINT16_C(value)\n    UINT32_C(value)\n    UINT64_C(value)\n\n    INTMAX_C(value)\n    UINTMAX_C(value)\n\nnamespace std\n{\n\nTypes:\n\n    int8_t\n    int16_t\n    int32_t\n    int64_t\n\n    uint8_t\n    uint16_t\n    uint32_t\n    uint64_t\n\n    int_least8_t\n    int_least16_t\n    int_least32_t\n    int_least64_t\n\n    uint_least8_t\n    uint_least16_t\n    uint_least32_t\n    uint_least64_t\n\n    int_fast8_t\n    int_fast16_t\n    int_fast32_t\n    int_fast64_t\n\n    uint_fast8_t\n    uint_fast16_t\n    uint_fast32_t\n    uint_fast64_t\n\n    intptr_t\n    uintptr_t\n\n    intmax_t\n    uintmax_t\n\n}  // std\n*/\n\n#ifndef __cuda_std__\n#include <__config>\n#else\n#include \"__cuda/cstdint_prelude.h\"\n#endif //__cuda_std__\n\n#ifndef _LIBCUDACXX_COMPILER_NVRTC\n#include <stdint.h>\n#endif // _LIBCUDACXX_COMPILER_NVRTC\n\n#include \"climits\"\n#include \"version\"\n\n#ifndef __cuda_std__\n#include <__pragma_push>\n#endif //__cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\nusing::int8_t;\nusing::int16_t;\nusing::int32_t;\nusing::int64_t;\n\nusing::uint8_t;\nusing::uint16_t;\nusing::uint32_t;\nusing::uint64_t;\n\nusing::int_least8_t;\nusing::int_least16_t;\nusing::int_least32_t;\nusing::int_least64_t;\n\nusing::uint_least8_t;\nusing::uint_least16_t;\nusing::uint_least32_t;\nusing::uint_least64_t;\n\nusing::int_fast8_t;\nusing::int_fast16_t;\nusing::int_fast32_t;\nusing::int_fast64_t;\n\nusing::uint_fast8_t;\nusing::uint_fast16_t;\nusing::uint_fast32_t;\nusing::uint_fast64_t;\n\nusing::intptr_t;\nusing::uintptr_t;\n\nusing::intmax_t;\nusing::uintmax_t;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#ifndef __cuda_std__\n#include <__pragma_pop>\n#endif //__cuda_std__\n\n#endif  // _LIBCUDACXX_CSTDINT\n", "cstdio": "#ifndef _JITIFY_INCLUDE_GUARD_57A89FD62AF44317\n#define _JITIFY_INCLUDE_GUARD_57A89FD62AF44317\n#define cudaDeviceSynchronize() cudaSuccess\n#include <stddef.h>\n#define FILE int\nint fflush ( FILE * stream );\nint fprintf ( FILE * stream, const char * format, ... );\n\n#endif // _JITIFY_INCLUDE_GUARD_57A89FD62AF44317\n", "ctime": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===---------------------------- ctime -----------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX_CTIME\n#define _LIBCUDACXX_CTIME\n\n/*\n    ctime synopsis\n\nMacros:\n\n    NULL\n    CLOCKS_PER_SEC\n    TIME_UTC // C++17\n\nnamespace std\n{\n\nTypes:\n\n    clock_t\n    size_t\n    time_t\n    tm\n    timespec // C++17\n\nclock_t clock();\ndouble difftime(time_t time1, time_t time0);\ntime_t mktime(tm* timeptr);\ntime_t time(time_t* timer);\nchar* asctime(const tm* timeptr);\nchar* ctime(const time_t* timer);\ntm*    gmtime(const time_t* timer);\ntm* localtime(const time_t* timer);\nsize_t strftime(char* restrict s, size_t maxsize, const char* restrict format,\n                const tm* restrict timeptr);\nint timespec_get( struct timespec *ts, int base); // C++17\n}  // std\n\n*/\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#ifndef _LIBCUDACXX_COMPILER_NVRTC\n#include <time.h>\n#else\ntypedef long long int time_t;\n#endif // _LIBCUDACXX_COMPILER_NVRTC\n\n#ifndef __cuda_std__\n#include <__pragma_push>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\nusing ::clock_t;\nusing ::size_t;\nusing ::time_t;\n\n#ifndef _LIBCUDACXX_COMPILER_NVRTC\n\nusing ::tm;\n#if _LIBCUDACXX_STD_VER > 14 && defined(_LIBCUDACXX_HAS_C11_FEATURES)\nusing ::timespec;\n#endif\nusing ::clock;\nusing ::difftime;\nusing ::mktime;\nusing ::time;\n#ifndef _LIBCUDACXX_HAS_NO_THREAD_UNSAFE_C_FUNCTIONS\nusing ::asctime;\nusing ::ctime;\nusing ::gmtime;\nusing ::localtime;\n#endif\nusing ::strftime;\n#if _LIBCUDACXX_STD_VER > 14 && defined(_LIBCUDACXX_HAS_TIMESPEC_GET)\nusing ::timespec_get;\n#endif\n#endif // _LIBCUDACXX_COMPILER_NVRTC\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#ifndef __cuda_std__\n#include <__pragma_pop>\n#endif //__cuda_std__\n\n#endif  // _LIBCUDACXX_CTIME\n", "cub/block/block_load.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_41DD869113644EF0\n#define _JITIFY_INCLUDE_GUARD_41DD869113644EF0\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2016, NVIDIA CORPORATION.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * Operations for reading linear tiles of data into the CUDA thread block.\n */\n\n#include <iterator>\n#include <type_traits>\n\n#include \"../block/block_exchange.cuh\"\n#include \"../iterator/cache_modified_input_iterator.cuh\"\n#include \"../config.cuh\"\n#include \"../util_ptx.cuh\"\n#include \"../util_type.cuh\"\n\nCUB_NAMESPACE_BEGIN\n\n/**\n * \\addtogroup UtilIo\n * @{\n */\n\n\n/******************************************************************//**\n * \\name Blocked arrangement I/O (direct)\n *********************************************************************/\n//@{\n\n\n/**\n * \\brief Load a linear segment of items into a blocked arrangement across the thread block.\n *\n * \\blocked\n *\n * \\tparam T                    <b>[inferred]</b> The data type to load.\n * \\tparam ITEMS_PER_THREAD     <b>[inferred]</b> The number of consecutive items partitioned onto each thread.\n * \\tparam InputIteratorT       <b>[inferred]</b> The random-access iterator type for input \\iterator.\n */\ntemplate <\n    typename        InputT,\n    int             ITEMS_PER_THREAD,\n    typename        InputIteratorT>\n__device__ __forceinline__ void LoadDirectBlocked(\n    int             linear_tid,                 ///< [in] A suitable 1D thread-identifier for the calling thread (e.g., <tt>(threadIdx.y * blockDim.x) + linear_tid</tt> for 2D thread blocks)\n    InputIteratorT  block_itr,                  ///< [in] The thread block's base input iterator for loading from\n    InputT          (&items)[ITEMS_PER_THREAD]) ///< [out] Data to load\n{\n    // Load directly in thread-blocked order\n    _Pragma(\"unroll\")\n    for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n    {\n        items[ITEM] = block_itr[(linear_tid * ITEMS_PER_THREAD) + ITEM];\n    }\n}\n\n\n/**\n * \\brief Load a linear segment of items into a blocked arrangement across the thread block, guarded by range.\n *\n * \\blocked\n *\n * \\tparam T                    <b>[inferred]</b> The data type to load.\n * \\tparam ITEMS_PER_THREAD     <b>[inferred]</b> The number of consecutive items partitioned onto each thread.\n * \\tparam InputIteratorT       <b>[inferred]</b> The random-access iterator type for input \\iterator.\n */\ntemplate <\n    typename        InputT,\n    int             ITEMS_PER_THREAD,\n    typename        InputIteratorT>\n__device__ __forceinline__ void LoadDirectBlocked(\n    int             linear_tid,                 ///< [in] A suitable 1D thread-identifier for the calling thread (e.g., <tt>(threadIdx.y * blockDim.x) + linear_tid</tt> for 2D thread blocks)\n    InputIteratorT  block_itr,                  ///< [in] The thread block's base input iterator for loading from\n    InputT          (&items)[ITEMS_PER_THREAD], ///< [out] Data to load\n    int             valid_items)                ///< [in] Number of valid items to load\n{\n\n    _Pragma(\"unroll\")\n    for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n    {\n        if ((linear_tid * ITEMS_PER_THREAD) + ITEM < valid_items)\n        {\n            items[ITEM] = block_itr[(linear_tid * ITEMS_PER_THREAD) + ITEM];\n        }\n    }\n}\n\n\n/**\n * \\brief Load a linear segment of items into a blocked arrangement across the thread block, guarded by range, with a fall-back assignment of out-of-bound elements..\n *\n * \\blocked\n *\n * \\tparam T                    <b>[inferred]</b> The data type to load.\n * \\tparam ITEMS_PER_THREAD     <b>[inferred]</b> The number of consecutive items partitioned onto each thread.\n * \\tparam InputIteratorT       <b>[inferred]</b> The random-access iterator type for input \\iterator.\n */\ntemplate <\n    typename        InputT,\n    typename        DefaultT,\n    int             ITEMS_PER_THREAD,\n    typename        InputIteratorT>\n__device__ __forceinline__ void LoadDirectBlocked(\n    int             linear_tid,                 ///< [in] A suitable 1D thread-identifier for the calling thread (e.g., <tt>(threadIdx.y * blockDim.x) + linear_tid</tt> for 2D thread blocks)\n    InputIteratorT  block_itr,                  ///< [in] The thread block's base input iterator for loading from\n    InputT          (&items)[ITEMS_PER_THREAD], ///< [out] Data to load\n    int             valid_items,                ///< [in] Number of valid items to load\n    DefaultT        oob_default)                ///< [in] Default value to assign out-of-bound items\n{\n    _Pragma(\"unroll\")\n    for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n        items[ITEM] = oob_default;\n\n    LoadDirectBlocked(linear_tid, block_itr, items, valid_items);\n}\n\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS    // Do not document\n\n/**\n * Internal implementation for load vectorization\n */\ntemplate <\n    CacheLoadModifier   MODIFIER,\n    typename            T,\n    int                 ITEMS_PER_THREAD>\n__device__ __forceinline__ void InternalLoadDirectBlockedVectorized(\n    int    linear_tid,                 ///< [in] A suitable 1D thread-identifier for the calling thread (e.g., <tt>(threadIdx.y * blockDim.x) + linear_tid</tt> for 2D thread blocks)\n    T      *block_ptr,                 ///< [in] Input pointer for loading from\n    T      (&items)[ITEMS_PER_THREAD]) ///< [out] Data to load\n{\n    // Biggest memory access word that T is a whole multiple of\n    typedef typename UnitWord<T>::DeviceWord DeviceWord;\n\n    enum\n    {\n        TOTAL_WORDS = sizeof(items) / sizeof(DeviceWord),\n\n        VECTOR_SIZE = (TOTAL_WORDS % 4 == 0) ?\n            4 :\n            (TOTAL_WORDS % 2 == 0) ?\n                2 :\n                1,\n\n        VECTORS_PER_THREAD = TOTAL_WORDS / VECTOR_SIZE,\n    };\n\n    // Vector type\n    typedef typename CubVector<DeviceWord, VECTOR_SIZE>::Type Vector;\n\n    // Vector items\n    Vector vec_items[VECTORS_PER_THREAD];\n\n    // Aliased input ptr\n    Vector* vec_ptr = reinterpret_cast<Vector*>(block_ptr) + (linear_tid * VECTORS_PER_THREAD);\n\n    // Load directly in thread-blocked order\n    _Pragma(\"unroll\")\n    for (int ITEM = 0; ITEM < VECTORS_PER_THREAD; ITEM++)\n    {\n        vec_items[ITEM] = ThreadLoad<MODIFIER>(vec_ptr + ITEM);\n    }\n\n    // Copy\n    _Pragma(\"unroll\")\n    for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n    {\n        items[ITEM] = *(reinterpret_cast<T*>(vec_items) + ITEM);\n    }\n}\n\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\n\n/**\n * \\brief Load a linear segment of items into a blocked arrangement across the thread block.\n *\n * \\blocked\n *\n * The input offset (\\p block_ptr + \\p block_offset) must be quad-item aligned\n *\n * The following conditions will prevent vectorization and loading will fall back to cub::BLOCK_LOAD_DIRECT:\n *   - \\p ITEMS_PER_THREAD is odd\n *   - The data type \\p T is not a built-in primitive or CUDA vector type (e.g., \\p short, \\p int2, \\p double, \\p float2, etc.)\n *\n * \\tparam T                    <b>[inferred]</b> The data type to load.\n * \\tparam ITEMS_PER_THREAD     <b>[inferred]</b> The number of consecutive items partitioned onto each thread.\n */\ntemplate <\n    typename        T,\n    int             ITEMS_PER_THREAD>\n__device__ __forceinline__ void LoadDirectBlockedVectorized(\n    int linear_tid,                 ///< [in] A suitable 1D thread-identifier for the calling thread (e.g., <tt>(threadIdx.y * blockDim.x) + linear_tid</tt> for 2D thread blocks)\n    T   *block_ptr,                 ///< [in] Input pointer for loading from\n    T   (&items)[ITEMS_PER_THREAD]) ///< [out] Data to load\n{\n    InternalLoadDirectBlockedVectorized<LOAD_DEFAULT>(linear_tid, block_ptr, items);\n}\n\n\n//@}  end member group\n/******************************************************************//**\n * \\name Striped arrangement I/O (direct)\n *********************************************************************/\n//@{\n\n\n/**\n * \\brief Load a linear segment of items into a striped arrangement across the thread block.\n *\n * \\striped\n *\n * \\tparam BLOCK_THREADS        The thread block size in threads\n * \\tparam T                    <b>[inferred]</b> The data type to load.\n * \\tparam ITEMS_PER_THREAD     <b>[inferred]</b> The number of consecutive items partitioned onto each thread.\n * \\tparam InputIteratorT       <b>[inferred]</b> The random-access iterator type for input \\iterator.\n */\ntemplate <\n    int             BLOCK_THREADS,\n    typename        InputT,\n    int             ITEMS_PER_THREAD,\n    typename        InputIteratorT>\n__device__ __forceinline__ void LoadDirectStriped(\n    int             linear_tid,                 ///< [in] A suitable 1D thread-identifier for the calling thread (e.g., <tt>(threadIdx.y * blockDim.x) + linear_tid</tt> for 2D thread blocks)\n    InputIteratorT  block_itr,                  ///< [in] The thread block's base input iterator for loading from\n    InputT          (&items)[ITEMS_PER_THREAD]) ///< [out] Data to load\n{\n    _Pragma(\"unroll\")\n    for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n    {\n        items[ITEM] = block_itr[linear_tid + ITEM * BLOCK_THREADS];\n    }\n}\n\n\n/**\n * \\brief Load a linear segment of items into a striped arrangement across the thread block, guarded by range\n *\n * \\striped\n *\n * \\tparam BLOCK_THREADS        The thread block size in threads\n * \\tparam T                    <b>[inferred]</b> The data type to load.\n * \\tparam ITEMS_PER_THREAD     <b>[inferred]</b> The number of consecutive items partitioned onto each thread.\n * \\tparam InputIteratorT       <b>[inferred]</b> The random-access iterator type for input \\iterator.\n */\ntemplate <\n    int             BLOCK_THREADS,\n    typename        InputT,\n    int             ITEMS_PER_THREAD,\n    typename        InputIteratorT>\n__device__ __forceinline__ void LoadDirectStriped(\n    int             linear_tid,                 ///< [in] A suitable 1D thread-identifier for the calling thread (e.g., <tt>(threadIdx.y * blockDim.x) + linear_tid</tt> for 2D thread blocks)\n    InputIteratorT  block_itr,                  ///< [in] The thread block's base input iterator for loading from\n    InputT          (&items)[ITEMS_PER_THREAD], ///< [out] Data to load\n    int             valid_items)                ///< [in] Number of valid items to load\n{\n    _Pragma(\"unroll\")\n    for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n    {\n        if (linear_tid + (ITEM * BLOCK_THREADS) < valid_items)\n        {\n            items[ITEM] = block_itr[linear_tid + ITEM * BLOCK_THREADS];\n        }\n    }\n}\n\n\n/**\n * \\brief Load a linear segment of items into a striped arrangement across the thread block, guarded by range, with a fall-back assignment of out-of-bound elements.\n *\n * \\striped\n *\n * \\tparam BLOCK_THREADS        The thread block size in threads\n * \\tparam T                    <b>[inferred]</b> The data type to load.\n * \\tparam ITEMS_PER_THREAD     <b>[inferred]</b> The number of consecutive items partitioned onto each thread.\n * \\tparam InputIteratorT       <b>[inferred]</b> The random-access iterator type for input \\iterator.\n */\ntemplate <\n    int             BLOCK_THREADS,\n    typename        InputT,\n    typename        DefaultT,\n    int             ITEMS_PER_THREAD,\n    typename        InputIteratorT>\n__device__ __forceinline__ void LoadDirectStriped(\n    int             linear_tid,                 ///< [in] A suitable 1D thread-identifier for the calling thread (e.g., <tt>(threadIdx.y * blockDim.x) + linear_tid</tt> for 2D thread blocks)\n    InputIteratorT  block_itr,                  ///< [in] The thread block's base input iterator for loading from\n    InputT          (&items)[ITEMS_PER_THREAD], ///< [out] Data to load\n    int             valid_items,                ///< [in] Number of valid items to load\n    DefaultT        oob_default)                ///< [in] Default value to assign out-of-bound items\n{\n    _Pragma(\"unroll\")\n    for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n        items[ITEM] = oob_default;\n\n    LoadDirectStriped<BLOCK_THREADS>(linear_tid, block_itr, items, valid_items);\n}\n\n\n\n//@}  end member group\n/******************************************************************//**\n * \\name Warp-striped arrangement I/O (direct)\n *********************************************************************/\n//@{\n\n\n/**\n * \\brief Load a linear segment of items into a warp-striped arrangement across the thread block.\n *\n * \\warpstriped\n *\n * \\par Usage Considerations\n * The number of threads in the thread block must be a multiple of the architecture's warp size.\n *\n * \\tparam T                    <b>[inferred]</b> The data type to load.\n * \\tparam ITEMS_PER_THREAD     <b>[inferred]</b> The number of consecutive items partitioned onto each thread.\n * \\tparam InputIteratorT       <b>[inferred]</b> The random-access iterator type for input \\iterator.\n */\ntemplate <\n    typename        InputT,\n    int             ITEMS_PER_THREAD,\n    typename        InputIteratorT>\n__device__ __forceinline__ void LoadDirectWarpStriped(\n    int             linear_tid,                 ///< [in] A suitable 1D thread-identifier for the calling thread (e.g., <tt>(threadIdx.y * blockDim.x) + linear_tid</tt> for 2D thread blocks)\n    InputIteratorT  block_itr,                  ///< [in] The thread block's base input iterator for loading from\n    InputT          (&items)[ITEMS_PER_THREAD]) ///< [out] Data to load\n{\n    int tid                = linear_tid & (CUB_PTX_WARP_THREADS - 1);\n    int wid                = linear_tid >> CUB_PTX_LOG_WARP_THREADS;\n    int warp_offset        = wid * CUB_PTX_WARP_THREADS * ITEMS_PER_THREAD;\n\n    // Load directly in warp-striped order\n    _Pragma(\"unroll\")\n    for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n    {\n        new(&items[ITEM]) InputT(block_itr[warp_offset + tid + (ITEM * CUB_PTX_WARP_THREADS)]);\n    }\n}\n\n\n/**\n * \\brief Load a linear segment of items into a warp-striped arrangement across the thread block, guarded by range\n *\n * \\warpstriped\n *\n * \\par Usage Considerations\n * The number of threads in the thread block must be a multiple of the architecture's warp size.\n *\n * \\tparam T                    <b>[inferred]</b> The data type to load.\n * \\tparam ITEMS_PER_THREAD     <b>[inferred]</b> The number of consecutive items partitioned onto each thread.\n * \\tparam InputIteratorT       <b>[inferred]</b> The random-access iterator type for input \\iterator.\n */\ntemplate <\n    typename        InputT,\n    int             ITEMS_PER_THREAD,\n    typename        InputIteratorT>\n__device__ __forceinline__ void LoadDirectWarpStriped(\n    int             linear_tid,                 ///< [in] A suitable 1D thread-identifier for the calling thread (e.g., <tt>(threadIdx.y * blockDim.x) + linear_tid</tt> for 2D thread blocks)\n    InputIteratorT  block_itr,                  ///< [in] The thread block's base input iterator for loading from\n    InputT          (&items)[ITEMS_PER_THREAD], ///< [out] Data to load\n    int             valid_items)                ///< [in] Number of valid items to load\n{\n    int tid                = linear_tid & (CUB_PTX_WARP_THREADS - 1);\n    int wid                = linear_tid >> CUB_PTX_LOG_WARP_THREADS;\n    int warp_offset        = wid * CUB_PTX_WARP_THREADS * ITEMS_PER_THREAD;\n\n    // Load directly in warp-striped order\n    _Pragma(\"unroll\")\n    for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n    {\n        if (warp_offset + tid + (ITEM * CUB_PTX_WARP_THREADS) < valid_items)\n        {\n            new(&items[ITEM]) InputT(block_itr[warp_offset + tid + (ITEM * CUB_PTX_WARP_THREADS)]);\n        }\n    }\n}\n\n\n/**\n * \\brief Load a linear segment of items into a warp-striped arrangement across the thread block, guarded by range, with a fall-back assignment of out-of-bound elements.\n *\n * \\warpstriped\n *\n * \\par Usage Considerations\n * The number of threads in the thread block must be a multiple of the architecture's warp size.\n *\n * \\tparam T                    <b>[inferred]</b> The data type to load.\n * \\tparam ITEMS_PER_THREAD     <b>[inferred]</b> The number of consecutive items partitioned onto each thread.\n * \\tparam InputIteratorT       <b>[inferred]</b> The random-access iterator type for input \\iterator.\n */\ntemplate <\n    typename        InputT,\n    typename        DefaultT,\n    int             ITEMS_PER_THREAD,\n    typename        InputIteratorT>\n__device__ __forceinline__ void LoadDirectWarpStriped(\n    int             linear_tid,                 ///< [in] A suitable 1D thread-identifier for the calling thread (e.g., <tt>(threadIdx.y * blockDim.x) + linear_tid</tt> for 2D thread blocks)\n    InputIteratorT  block_itr,                  ///< [in] The thread block's base input iterator for loading from\n    InputT          (&items)[ITEMS_PER_THREAD], ///< [out] Data to load\n    int             valid_items,                ///< [in] Number of valid items to load\n    DefaultT        oob_default)                ///< [in] Default value to assign out-of-bound items\n{\n    // Load directly in warp-striped order\n    _Pragma(\"unroll\")\n    for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n        items[ITEM] = oob_default;\n\n    LoadDirectWarpStriped(linear_tid, block_itr, items, valid_items);\n}\n\n\n\n//@}  end member group\n\n/** @} */       // end group UtilIo\n\n\n\n//-----------------------------------------------------------------------------\n// Generic BlockLoad abstraction\n//-----------------------------------------------------------------------------\n\n/**\n * \\brief cub::BlockLoadAlgorithm enumerates alternative algorithms for cub::BlockLoad to read a linear segment of data from memory into a blocked arrangement across a CUDA thread block.\n */\nenum BlockLoadAlgorithm\n{\n    /**\n     * \\par Overview\n     *\n     * A [<em>blocked arrangement</em>](index.html#sec5sec3) of data is read\n     * directly from memory.\n     *\n     * \\par Performance Considerations\n     * The utilization of memory transactions (coalescing) decreases as the\n     * access stride between threads increases (i.e., the number items per thread).\n     */\n    BLOCK_LOAD_DIRECT,\n\n    /**\n     * \\par Overview\n     *\n     * A [<em>striped arrangement</em>](index.html#sec5sec3) of data is read\n     * directly from memory.\n     *\n     * \\par Performance Considerations\n     * The utilization of memory transactions (coalescing) doesn't depend on\n     * the number of items per thread.\n     */\n    BLOCK_LOAD_STRIPED,\n\n    /**\n     * \\par Overview\n     *\n     * A [<em>blocked arrangement</em>](index.html#sec5sec3) of data is read\n     * from memory using CUDA's built-in vectorized loads as a coalescing optimization.\n     * For example, <tt>ld.global.v4.s32</tt> instructions will be generated\n     * when \\p T = \\p int and \\p ITEMS_PER_THREAD % 4 == 0.\n     *\n     * \\par Performance Considerations\n     * - The utilization of memory transactions (coalescing) remains high until the the\n     *   access stride between threads (i.e., the number items per thread) exceeds the\n     *   maximum vector load width (typically 4 items or 64B, whichever is lower).\n     * - The following conditions will prevent vectorization and loading will fall\n     *   back to cub::BLOCK_LOAD_DIRECT:\n     *   - \\p ITEMS_PER_THREAD is odd\n     *   - The \\p InputIteratorT is not a simple pointer type\n     *   - The block input offset is not quadword-aligned\n     *   - The data type \\p T is not a built-in primitive or CUDA vector type\n     *     (e.g., \\p short, \\p int2, \\p double, \\p float2, etc.)\n     */\n    BLOCK_LOAD_VECTORIZE,\n\n    /**\n     * \\par Overview\n     *\n     * A [<em>striped arrangement</em>](index.html#sec5sec3) of data is read\n     * efficiently from memory and then locally transposed into a\n     * [<em>blocked arrangement</em>](index.html#sec5sec3).\n     *\n     * \\par Performance Considerations\n     * - The utilization of memory transactions (coalescing) remains high regardless\n     *   of items loaded per thread.\n     * - The local reordering incurs slightly longer latencies and throughput than the\n     *   direct cub::BLOCK_LOAD_DIRECT and cub::BLOCK_LOAD_VECTORIZE alternatives.\n     */\n    BLOCK_LOAD_TRANSPOSE,\n\n    /**\n     * \\par Overview\n     *\n     * A [<em>warp-striped arrangement</em>](index.html#sec5sec3) of data is\n     * read efficiently from memory and then locally transposed into a\n     * [<em>blocked arrangement</em>](index.html#sec5sec3).\n     *\n     * \\par Usage Considerations\n     * - BLOCK_THREADS must be a multiple of WARP_THREADS\n     *\n     * \\par Performance Considerations\n     * - The utilization of memory transactions (coalescing) remains high regardless\n     *   of items loaded per thread.\n     * - The local reordering incurs slightly larger latencies than the\n     *   direct cub::BLOCK_LOAD_DIRECT and cub::BLOCK_LOAD_VECTORIZE alternatives.\n     * - Provisions more shared storage, but incurs smaller latencies than the\n     *   BLOCK_LOAD_WARP_TRANSPOSE_TIMESLICED alternative.\n     */\n    BLOCK_LOAD_WARP_TRANSPOSE,\n\n    /**\n     * \\par Overview\n     *\n     * Like \\p BLOCK_LOAD_WARP_TRANSPOSE, a [<em>warp-striped arrangement</em>](index.html#sec5sec3)\n     * of data is read directly from memory and then is locally transposed into a\n     * [<em>blocked arrangement</em>](index.html#sec5sec3). To reduce the shared memory\n     * requirement, only one warp's worth of shared memory is provisioned and is\n     * subsequently time-sliced among warps.\n     *\n     * \\par Usage Considerations\n     * - BLOCK_THREADS must be a multiple of WARP_THREADS\n     *\n     * \\par Performance Considerations\n     * - The utilization of memory transactions (coalescing) remains high regardless\n     *   of items loaded per thread.\n     * - Provisions less shared memory temporary storage, but incurs larger\n     *   latencies than the BLOCK_LOAD_WARP_TRANSPOSE alternative.\n     */\n    BLOCK_LOAD_WARP_TRANSPOSE_TIMESLICED,\n};\n\n\n/**\n * \\brief The BlockLoad class provides [<em>collective</em>](index.html#sec0) data movement methods for loading a linear segment of items from memory into a [<em>blocked arrangement</em>](index.html#sec5sec3) across a CUDA thread block.  ![](block_load_logo.png)\n * \\ingroup BlockModule\n * \\ingroup UtilIo\n *\n * \\tparam InputT               The data type to read into (which must be convertible from the input iterator's value type).\n * \\tparam BLOCK_DIM_X          The thread block length in threads along the X dimension\n * \\tparam ITEMS_PER_THREAD     The number of consecutive items partitioned onto each thread.\n * \\tparam ALGORITHM            <b>[optional]</b> cub::BlockLoadAlgorithm tuning policy.  default: cub::BLOCK_LOAD_DIRECT.\n * \\tparam WARP_TIME_SLICING    <b>[optional]</b> Whether or not only one warp's worth of shared memory should be allocated and time-sliced among block-warps during any load-related data transpositions (versus each warp having its own storage). (default: false)\n * \\tparam BLOCK_DIM_Y          <b>[optional]</b> The thread block length in threads along the Y dimension (default: 1)\n * \\tparam BLOCK_DIM_Z          <b>[optional]</b> The thread block length in threads along the Z dimension (default: 1)\n * \\tparam LEGACY_PTX_ARCH      <b>[optional]</b> Unused.\n *\n * \\par Overview\n * - The BlockLoad class provides a single data movement abstraction that can be specialized\n *   to implement different cub::BlockLoadAlgorithm strategies.  This facilitates different\n *   performance policies for different architectures, data types, granularity sizes, etc.\n * - BlockLoad can be optionally specialized by different data movement strategies:\n *   -# <b>cub::BLOCK_LOAD_DIRECT</b>.  A [<em>blocked arrangement</em>](index.html#sec5sec3)\n *      of data is read directly from memory.  [More...](\\ref cub::BlockLoadAlgorithm)\n*    -# <b>cub::BLOCK_LOAD_STRIPED,</b>.  A [<em>striped arrangement</em>](index.html#sec5sec3)\n *      of data is read directly from memory.  [More...](\\ref cub::BlockLoadAlgorithm)\n *   -# <b>cub::BLOCK_LOAD_VECTORIZE</b>.  A [<em>blocked arrangement</em>](index.html#sec5sec3)\n *      of data is read directly from memory using CUDA's built-in vectorized loads as a\n *      coalescing optimization.    [More...](\\ref cub::BlockLoadAlgorithm)\n *   -# <b>cub::BLOCK_LOAD_TRANSPOSE</b>.  A [<em>striped arrangement</em>](index.html#sec5sec3)\n *      of data is read directly from memory and is then locally transposed into a\n *      [<em>blocked arrangement</em>](index.html#sec5sec3).  [More...](\\ref cub::BlockLoadAlgorithm)\n *   -# <b>cub::BLOCK_LOAD_WARP_TRANSPOSE</b>.  A [<em>warp-striped arrangement</em>](index.html#sec5sec3)\n *      of data is read directly from memory and is then locally transposed into a\n *      [<em>blocked arrangement</em>](index.html#sec5sec3).  [More...](\\ref cub::BlockLoadAlgorithm)\n *   -# <b>cub::BLOCK_LOAD_WARP_TRANSPOSE_TIMESLICED,</b>.  A [<em>warp-striped arrangement</em>](index.html#sec5sec3)\n *      of data is read directly from memory and is then locally transposed into a\n *      [<em>blocked arrangement</em>](index.html#sec5sec3) one warp at a time.  [More...](\\ref cub::BlockLoadAlgorithm)\n * - \\rowmajor\n *\n * \\par A Simple Example\n * \\blockcollective{BlockLoad}\n * \\par\n * The code snippet below illustrates the loading of a linear\n * segment of 512 integers into a \"blocked\" arrangement across 128 threads where each\n * thread owns 4 consecutive items.  The load is specialized for \\p BLOCK_LOAD_WARP_TRANSPOSE,\n * meaning memory references are efficiently coalesced using a warp-striped access\n * pattern (after which items are locally reordered among threads).\n * \\par\n * \\code\n * #include <cub/cub.cuh>   // or equivalently <cub/block/block_load.cuh>\n *\n * __global__ void ExampleKernel(int *d_data, ...)\n * {\n *     // Specialize BlockLoad for a 1D block of 128 threads owning 4 integer items each\n *     typedef cub::BlockLoad<int, 128, 4, BLOCK_LOAD_WARP_TRANSPOSE> BlockLoad;\n *\n *     // Allocate shared memory for BlockLoad\n *     __shared__ typename BlockLoad::TempStorage temp_storage;\n *\n *     // Load a segment of consecutive items that are blocked across threads\n *     int thread_data[4];\n *     BlockLoad(temp_storage).Load(d_data, thread_data);\n *\n * \\endcode\n * \\par\n * Suppose the input \\p d_data is <tt>0, 1, 2, 3, 4, 5, ...</tt>.\n * The set of \\p thread_data across the block of threads in those threads will be\n * <tt>{ [0,1,2,3], [4,5,6,7], ..., [508,509,510,511] }</tt>.\n *\n * \\par Re-using dynamically allocating shared memory\n * The following example under the examples/block folder illustrates usage of\n * dynamically shared memory with BlockReduce and how to re-purpose\n * the same memory region:\n * <a href=\"../../examples/block/example_block_reduce_dyn_smem.cu\">example_block_reduce_dyn_smem.cu</a>\n *\n * This example can be easily adapted to the storage required by BlockLoad.\n */\ntemplate <\n    typename            InputT,\n    int                 BLOCK_DIM_X,\n    int                 ITEMS_PER_THREAD,\n    BlockLoadAlgorithm  ALGORITHM           = BLOCK_LOAD_DIRECT,\n    int                 BLOCK_DIM_Y         = 1,\n    int                 BLOCK_DIM_Z         = 1,\n    int                 LEGACY_PTX_ARCH     = 0>\nclass BlockLoad\n{\nprivate:\n\n    /******************************************************************************\n     * Constants and typed definitions\n     ******************************************************************************/\n\n    /// Constants\n    enum\n    {\n        /// The thread block size in threads\n        BLOCK_THREADS = BLOCK_DIM_X * BLOCK_DIM_Y * BLOCK_DIM_Z,\n    };\n\n\n    /******************************************************************************\n     * Algorithmic variants\n     ******************************************************************************/\n\n    /// Load helper\n    template <BlockLoadAlgorithm _POLICY, int DUMMY>\n    struct LoadInternal;\n\n\n    /**\n     * BLOCK_LOAD_DIRECT specialization of load helper\n     */\n    template <int DUMMY>\n    struct LoadInternal<BLOCK_LOAD_DIRECT, DUMMY>\n    {\n        /// Shared memory storage layout type\n        typedef NullType TempStorage;\n\n        /// Linear thread-id\n        int linear_tid;\n\n        /// Constructor\n        __device__ __forceinline__ LoadInternal(\n            TempStorage &/*temp_storage*/,\n            int linear_tid)\n        :\n            linear_tid(linear_tid)\n        {}\n\n        /// Load a linear segment of items from memory\n        template <typename InputIteratorT>\n        __device__ __forceinline__ void Load(\n            InputIteratorT  block_itr,                      ///< [in] The thread block's base input iterator for loading from\n            InputT          (&items)[ITEMS_PER_THREAD])     ///< [out] Data to load\n        {\n            LoadDirectBlocked(linear_tid, block_itr, items);\n        }\n\n        /// Load a linear segment of items from memory, guarded by range\n        template <typename InputIteratorT>\n        __device__ __forceinline__ void Load(\n            InputIteratorT  block_itr,                      ///< [in] The thread block's base input iterator for loading from\n            InputT          (&items)[ITEMS_PER_THREAD],     ///< [out] Data to load\n            int             valid_items)                    ///< [in] Number of valid items to load\n        {\n            LoadDirectBlocked(linear_tid, block_itr, items, valid_items);\n        }\n\n        /// Load a linear segment of items from memory, guarded by range, with a fall-back assignment of out-of-bound elements\n        template <typename InputIteratorT, typename DefaultT>\n        __device__ __forceinline__ void Load(\n            InputIteratorT  block_itr,                      ///< [in] The thread block's base input iterator for loading from\n            InputT          (&items)[ITEMS_PER_THREAD],     ///< [out] Data to load\n            int             valid_items,                    ///< [in] Number of valid items to load\n            DefaultT        oob_default)                    ///< [in] Default value to assign out-of-bound items\n        {\n            LoadDirectBlocked(linear_tid, block_itr, items, valid_items, oob_default);\n        }\n\n    };\n\n\n    /**\n    * BLOCK_LOAD_STRIPED specialization of load helper\n    */\n    template <int DUMMY>\n    struct LoadInternal<BLOCK_LOAD_STRIPED, DUMMY>\n    {\n        /// Shared memory storage layout type\n        typedef NullType TempStorage;\n\n        /// Linear thread-id\n        int linear_tid;\n\n        /// Constructor\n        __device__ __forceinline__ LoadInternal(\n            TempStorage &/*temp_storage*/,\n            int linear_tid)\n        :\n            linear_tid(linear_tid)\n        {}\n\n        /// Load a linear segment of items from memory\n        template <typename InputIteratorT>\n        __device__ __forceinline__ void Load(\n            InputIteratorT  block_itr,                      ///< [in] The thread block's base input iterator for loading from\n            InputT          (&items)[ITEMS_PER_THREAD])     ///< [out] Data to load\n        {\n            LoadDirectStriped<BLOCK_THREADS>(linear_tid, block_itr, items);\n        }\n\n        /// Load a linear segment of items from memory, guarded by range\n        template <typename InputIteratorT>\n        __device__ __forceinline__ void Load(\n            InputIteratorT  block_itr,                      ///< [in] The thread block's base input iterator for loading from\n            InputT          (&items)[ITEMS_PER_THREAD],     ///< [out] Data to load\n            int             valid_items)                    ///< [in] Number of valid items to load\n        {\n            LoadDirectStriped<BLOCK_THREADS>(linear_tid, block_itr, items, valid_items);\n        }\n\n        /// Load a linear segment of items from memory, guarded by range, with a fall-back assignment of out-of-bound elements\n        template <typename InputIteratorT, typename DefaultT>\n        __device__ __forceinline__ void Load(\n            InputIteratorT  block_itr,                      ///< [in] The thread block's base input iterator for loading from\n            InputT          (&items)[ITEMS_PER_THREAD],     ///< [out] Data to load\n            int             valid_items,                    ///< [in] Number of valid items to load\n            DefaultT        oob_default)                    ///< [in] Default value to assign out-of-bound items\n        {\n            LoadDirectStriped<BLOCK_THREADS>(linear_tid, block_itr, items, valid_items, oob_default);\n        }\n\n    };\n\n\n    /**\n     * BLOCK_LOAD_VECTORIZE specialization of load helper\n     */\n    template <int DUMMY>\n    struct LoadInternal<BLOCK_LOAD_VECTORIZE, DUMMY>\n    {\n        /// Shared memory storage layout type\n        typedef NullType TempStorage;\n\n        /// Linear thread-id\n        int linear_tid;\n\n        /// Constructor\n        __device__ __forceinline__ LoadInternal(\n            TempStorage &/*temp_storage*/,\n            int linear_tid)\n        :\n            linear_tid(linear_tid)\n        {}\n\n        /// Load a linear segment of items from memory, specialized for native pointer types (attempts vectorization)\n        template <typename InputIteratorT>\n        __device__ __forceinline__ void Load(\n            InputT               *block_ptr,                     ///< [in] The thread block's base input iterator for loading from\n            InputT               (&items)[ITEMS_PER_THREAD])     ///< [out] Data to load\n        {\n            InternalLoadDirectBlockedVectorized<LOAD_DEFAULT>(linear_tid, block_ptr, items);\n        }\n\n        /// Load a linear segment of items from memory, specialized for native pointer types (attempts vectorization)\n        template <typename InputIteratorT>\n        __device__ __forceinline__ void Load(\n            const InputT         *block_ptr,                     ///< [in] The thread block's base input iterator for loading from\n            InputT               (&items)[ITEMS_PER_THREAD])     ///< [out] Data to load\n        {\n            InternalLoadDirectBlockedVectorized<LOAD_DEFAULT>(linear_tid, block_ptr, items);\n        }\n\n        /// Load a linear segment of items from memory, specialized for native pointer types (attempts vectorization)\n        template <\n            CacheLoadModifier   MODIFIER,\n            typename            ValueType,\n            typename            OffsetT>\n        __device__ __forceinline__ void Load(\n            CacheModifiedInputIterator<MODIFIER, ValueType, OffsetT>    block_itr,                      ///< [in] The thread block's base input iterator for loading from\n            InputT                                                     (&items)[ITEMS_PER_THREAD])     ///< [out] Data to load\n        {\n            InternalLoadDirectBlockedVectorized<MODIFIER>(linear_tid, block_itr.ptr, items);\n        }\n\n        /// Load a linear segment of items from memory, specialized for opaque input iterators (skips vectorization)\n        template <typename _InputIteratorT>\n        __device__ __forceinline__ void Load(\n            _InputIteratorT   block_itr,                    ///< [in] The thread block's base input iterator for loading from\n            InputT           (&items)[ITEMS_PER_THREAD])   ///< [out] Data to load\n        {\n            LoadDirectBlocked(linear_tid, block_itr, items);\n        }\n\n        /// Load a linear segment of items from memory, guarded by range (skips vectorization)\n        template <typename InputIteratorT>\n        __device__ __forceinline__ void Load(\n            InputIteratorT  block_itr,                      ///< [in] The thread block's base input iterator for loading from\n            InputT          (&items)[ITEMS_PER_THREAD],     ///< [out] Data to load\n            int             valid_items)                    ///< [in] Number of valid items to load\n        {\n            LoadDirectBlocked(linear_tid, block_itr, items, valid_items);\n        }\n\n        /// Load a linear segment of items from memory, guarded by range, with a fall-back assignment of out-of-bound elements (skips vectorization)\n        template <typename InputIteratorT, typename DefaultT>\n        __device__ __forceinline__ void Load(\n            InputIteratorT  block_itr,                      ///< [in] The thread block's base input iterator for loading from\n            InputT          (&items)[ITEMS_PER_THREAD],     ///< [out] Data to load\n            int             valid_items,                    ///< [in] Number of valid items to load\n            DefaultT        oob_default)                    ///< [in] Default value to assign out-of-bound items\n        {\n            LoadDirectBlocked(linear_tid, block_itr, items, valid_items, oob_default);\n        }\n\n    };\n\n\n    /**\n     * BLOCK_LOAD_TRANSPOSE specialization of load helper\n     */\n    template <int DUMMY>\n    struct LoadInternal<BLOCK_LOAD_TRANSPOSE, DUMMY>\n    {\n        // BlockExchange utility type for keys\n        typedef BlockExchange<InputT, BLOCK_DIM_X, ITEMS_PER_THREAD, false, BLOCK_DIM_Y, BLOCK_DIM_Z> BlockExchange;\n\n        /// Shared memory storage layout type\n        struct _TempStorage : BlockExchange::TempStorage\n        {};\n\n        /// Alias wrapper allowing storage to be unioned\n        struct TempStorage : Uninitialized<_TempStorage> {};\n\n        /// Thread reference to shared storage\n        _TempStorage &temp_storage;\n\n        /// Linear thread-id\n        int linear_tid;\n\n        /// Constructor\n        __device__ __forceinline__ LoadInternal(\n            TempStorage &temp_storage,\n            int linear_tid)\n        :\n            temp_storage(temp_storage.Alias()),\n            linear_tid(linear_tid)\n        {}\n\n        /// Load a linear segment of items from memory\n        template <typename InputIteratorT>\n        __device__ __forceinline__ void Load(\n            InputIteratorT  block_itr,                      ///< [in] The thread block's base input iterator for loading from\n            InputT          (&items)[ITEMS_PER_THREAD])     ///< [out] Data to load{\n        {\n            LoadDirectStriped<BLOCK_THREADS>(linear_tid, block_itr, items);\n            BlockExchange(temp_storage).StripedToBlocked(items, items);\n        }\n\n        /// Load a linear segment of items from memory, guarded by range\n        template <typename InputIteratorT>\n        __device__ __forceinline__ void Load(\n            InputIteratorT  block_itr,                      ///< [in] The thread block's base input iterator for loading from\n            InputT          (&items)[ITEMS_PER_THREAD],     ///< [out] Data to load\n            int             valid_items)                    ///< [in] Number of valid items to load\n        {\n            LoadDirectStriped<BLOCK_THREADS>(linear_tid, block_itr, items, valid_items);\n            BlockExchange(temp_storage).StripedToBlocked(items, items);\n        }\n\n        /// Load a linear segment of items from memory, guarded by range, with a fall-back assignment of out-of-bound elements\n        template <typename InputIteratorT, typename DefaultT>\n        __device__ __forceinline__ void Load(\n            InputIteratorT  block_itr,                      ///< [in] The thread block's base input iterator for loading from\n            InputT          (&items)[ITEMS_PER_THREAD],     ///< [out] Data to load\n            int             valid_items,                    ///< [in] Number of valid items to load\n            DefaultT        oob_default)                    ///< [in] Default value to assign out-of-bound items\n        {\n            LoadDirectStriped<BLOCK_THREADS>(linear_tid, block_itr, items, valid_items, oob_default);\n            BlockExchange(temp_storage).StripedToBlocked(items, items);\n        }\n\n    };\n\n\n    /**\n     * BLOCK_LOAD_WARP_TRANSPOSE specialization of load helper\n     */\n    template <int DUMMY>\n    struct LoadInternal<BLOCK_LOAD_WARP_TRANSPOSE, DUMMY>\n    {\n        enum\n        {\n            WARP_THREADS = CUB_WARP_THREADS(0)\n        };\n\n        // Assert BLOCK_THREADS must be a multiple of WARP_THREADS\n        CUB_STATIC_ASSERT((int(BLOCK_THREADS) % int(WARP_THREADS) == 0), \"BLOCK_THREADS must be a multiple of WARP_THREADS\");\n\n        // BlockExchange utility type for keys\n        typedef BlockExchange<InputT, BLOCK_DIM_X, ITEMS_PER_THREAD, false, BLOCK_DIM_Y, BLOCK_DIM_Z> BlockExchange;\n\n        /// Shared memory storage layout type\n        struct _TempStorage : BlockExchange::TempStorage\n        {};\n\n        /// Alias wrapper allowing storage to be unioned\n        struct TempStorage : Uninitialized<_TempStorage> {};\n\n        /// Thread reference to shared storage\n        _TempStorage &temp_storage;\n\n        /// Linear thread-id\n        int linear_tid;\n\n        /// Constructor\n        __device__ __forceinline__ LoadInternal(\n            TempStorage &temp_storage,\n            int linear_tid)\n        :\n            temp_storage(temp_storage.Alias()),\n            linear_tid(linear_tid)\n        {}\n\n        /// Load a linear segment of items from memory\n        template <typename InputIteratorT>\n        __device__ __forceinline__ void Load(\n            InputIteratorT  block_itr,                      ///< [in] The thread block's base input iterator for loading from\n            InputT          (&items)[ITEMS_PER_THREAD])     ///< [out] Data to load{\n        {\n            LoadDirectWarpStriped(linear_tid, block_itr, items);\n            BlockExchange(temp_storage).WarpStripedToBlocked(items, items);\n        }\n\n        /// Load a linear segment of items from memory, guarded by range\n        template <typename InputIteratorT>\n        __device__ __forceinline__ void Load(\n            InputIteratorT  block_itr,                      ///< [in] The thread block's base input iterator for loading from\n            InputT          (&items)[ITEMS_PER_THREAD],     ///< [out] Data to load\n            int             valid_items)                    ///< [in] Number of valid items to load\n        {\n            LoadDirectWarpStriped(linear_tid, block_itr, items, valid_items);\n            BlockExchange(temp_storage).WarpStripedToBlocked(items, items);\n        }\n\n\n        /// Load a linear segment of items from memory, guarded by range, with a fall-back assignment of out-of-bound elements\n        template <typename InputIteratorT, typename DefaultT>\n        __device__ __forceinline__ void Load(\n            InputIteratorT  block_itr,                      ///< [in] The thread block's base input iterator for loading from\n            InputT          (&items)[ITEMS_PER_THREAD],     ///< [out] Data to load\n            int             valid_items,                    ///< [in] Number of valid items to load\n            DefaultT        oob_default)                    ///< [in] Default value to assign out-of-bound items\n        {\n            LoadDirectWarpStriped(linear_tid, block_itr, items, valid_items, oob_default);\n            BlockExchange(temp_storage).WarpStripedToBlocked(items, items);\n        }\n    };\n\n\n    /**\n     * BLOCK_LOAD_WARP_TRANSPOSE_TIMESLICED specialization of load helper\n     */\n    template <int DUMMY>\n    struct LoadInternal<BLOCK_LOAD_WARP_TRANSPOSE_TIMESLICED, DUMMY>\n    {\n        enum\n        {\n            WARP_THREADS = CUB_WARP_THREADS(0)\n        };\n\n        // Assert BLOCK_THREADS must be a multiple of WARP_THREADS\n        CUB_STATIC_ASSERT((int(BLOCK_THREADS) % int(WARP_THREADS) == 0), \"BLOCK_THREADS must be a multiple of WARP_THREADS\");\n\n        // BlockExchange utility type for keys\n        typedef BlockExchange<InputT, BLOCK_DIM_X, ITEMS_PER_THREAD, true, BLOCK_DIM_Y, BLOCK_DIM_Z> BlockExchange;\n\n        /// Shared memory storage layout type\n        struct _TempStorage : BlockExchange::TempStorage\n        {};\n\n        /// Alias wrapper allowing storage to be unioned\n        struct TempStorage : Uninitialized<_TempStorage> {};\n\n        /// Thread reference to shared storage\n        _TempStorage &temp_storage;\n\n        /// Linear thread-id\n        int linear_tid;\n\n        /// Constructor\n        __device__ __forceinline__ LoadInternal(\n            TempStorage &temp_storage,\n            int linear_tid)\n        :\n            temp_storage(temp_storage.Alias()),\n            linear_tid(linear_tid)\n        {}\n\n        /// Load a linear segment of items from memory\n        template <typename InputIteratorT>\n        __device__ __forceinline__ void Load(\n            InputIteratorT  block_itr,                      ///< [in] The thread block's base input iterator for loading from\n            InputT          (&items)[ITEMS_PER_THREAD])     ///< [out] Data to load{\n        {\n            LoadDirectWarpStriped(linear_tid, block_itr, items);\n            BlockExchange(temp_storage).WarpStripedToBlocked(items, items);\n        }\n\n        /// Load a linear segment of items from memory, guarded by range\n        template <typename InputIteratorT>\n        __device__ __forceinline__ void Load(\n            InputIteratorT  block_itr,                      ///< [in] The thread block's base input iterator for loading from\n            InputT          (&items)[ITEMS_PER_THREAD],     ///< [out] Data to load\n            int             valid_items)                    ///< [in] Number of valid items to load\n        {\n            LoadDirectWarpStriped(linear_tid, block_itr, items, valid_items);\n            BlockExchange(temp_storage).WarpStripedToBlocked(items, items);\n        }\n\n\n        /// Load a linear segment of items from memory, guarded by range, with a fall-back assignment of out-of-bound elements\n        template <typename InputIteratorT, typename DefaultT>\n        __device__ __forceinline__ void Load(\n            InputIteratorT  block_itr,                      ///< [in] The thread block's base input iterator for loading from\n            InputT          (&items)[ITEMS_PER_THREAD],     ///< [out] Data to load\n            int             valid_items,                    ///< [in] Number of valid items to load\n            DefaultT        oob_default)                    ///< [in] Default value to assign out-of-bound items\n        {\n            LoadDirectWarpStriped(linear_tid, block_itr, items, valid_items, oob_default);\n            BlockExchange(temp_storage).WarpStripedToBlocked(items, items);\n        }\n    };\n\n\n    /******************************************************************************\n     * Type definitions\n     ******************************************************************************/\n\n    /// Internal load implementation to use\n    typedef LoadInternal<ALGORITHM, 0> InternalLoad;\n\n\n    /// Shared memory storage layout type\n    typedef typename InternalLoad::TempStorage _TempStorage;\n\n\n    /******************************************************************************\n     * Utility methods\n     ******************************************************************************/\n\n    /// Internal storage allocator\n    __device__ __forceinline__ _TempStorage& PrivateStorage()\n    {\n        __shared__ _TempStorage private_storage;\n        return private_storage;\n    }\n\n\n    /******************************************************************************\n     * Thread fields\n     ******************************************************************************/\n\n    /// Thread reference to shared storage\n    _TempStorage &temp_storage;\n\n    /// Linear thread-id\n    int linear_tid;\n\npublic:\n\n    /// \\smemstorage{BlockLoad}\n    struct TempStorage : Uninitialized<_TempStorage> {};\n\n\n    /******************************************************************//**\n     * \\name Collective constructors\n     *********************************************************************/\n    //@{\n\n    /**\n     * \\brief Collective constructor using a private static allocation of shared memory as temporary storage.\n     */\n    __device__ __forceinline__ BlockLoad()\n    :\n        temp_storage(PrivateStorage()),\n        linear_tid(RowMajorTid(BLOCK_DIM_X, BLOCK_DIM_Y, BLOCK_DIM_Z))\n    {}\n\n\n    /**\n     * \\brief Collective constructor using the specified memory allocation as temporary storage.\n     */\n    __device__ __forceinline__ BlockLoad(\n        TempStorage &temp_storage)             ///< [in] Reference to memory allocation having layout type TempStorage\n    :\n        temp_storage(temp_storage.Alias()),\n        linear_tid(RowMajorTid(BLOCK_DIM_X, BLOCK_DIM_Y, BLOCK_DIM_Z))\n    {}\n\n\n\n\n    //@}  end member group\n    /******************************************************************//**\n     * \\name Data movement\n     *********************************************************************/\n    //@{\n\n\n    /**\n     * \\brief Load a linear segment of items from memory.\n     *\n     * \\par\n     * - \\blocked\n     * - \\smemreuse\n     *\n     * \\par Snippet\n     * The code snippet below illustrates the loading of a linear\n     * segment of 512 integers into a \"blocked\" arrangement across 128 threads where each\n     * thread owns 4 consecutive items.  The load is specialized for \\p BLOCK_LOAD_WARP_TRANSPOSE,\n     * meaning memory references are efficiently coalesced using a warp-striped access\n     * pattern (after which items are locally reordered among threads).\n     * \\par\n     * \\code\n     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_load.cuh>\n     *\n     * __global__ void ExampleKernel(int *d_data, ...)\n     * {\n     *     // Specialize BlockLoad for a 1D block of 128 threads owning 4 integer items each\n     *     typedef cub::BlockLoad<int, 128, 4, BLOCK_LOAD_WARP_TRANSPOSE> BlockLoad;\n     *\n     *     // Allocate shared memory for BlockLoad\n     *     __shared__ typename BlockLoad::TempStorage temp_storage;\n     *\n     *     // Load a segment of consecutive items that are blocked across threads\n     *     int thread_data[4];\n     *     BlockLoad(temp_storage).Load(d_data, thread_data);\n     *\n     * \\endcode\n     * \\par\n     * Suppose the input \\p d_data is <tt>0, 1, 2, 3, 4, 5, ...</tt>.\n     * The set of \\p thread_data across the block of threads in those threads will be\n     * <tt>{ [0,1,2,3], [4,5,6,7], ..., [508,509,510,511] }</tt>.\n     *\n     */\n    template <typename InputIteratorT>\n    __device__ __forceinline__ void Load(\n        InputIteratorT  block_itr,                  ///< [in] The thread block's base input iterator for loading from\n        InputT          (&items)[ITEMS_PER_THREAD]) ///< [out] Data to load\n    {\n        InternalLoad(temp_storage, linear_tid).Load(block_itr, items);\n    }\n\n\n    /**\n     * \\brief Load a linear segment of items from memory, guarded by range.\n     *\n     * \\par\n     * - \\blocked\n     * - \\smemreuse\n     *\n     * \\par Snippet\n     * The code snippet below illustrates the guarded loading of a linear\n     * segment of 512 integers into a \"blocked\" arrangement across 128 threads where each\n     * thread owns 4 consecutive items.  The load is specialized for \\p BLOCK_LOAD_WARP_TRANSPOSE,\n     * meaning memory references are efficiently coalesced using a warp-striped access\n     * pattern (after which items are locally reordered among threads).\n     * \\par\n     * \\code\n     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_load.cuh>\n     *\n     * __global__ void ExampleKernel(int *d_data, int valid_items, ...)\n     * {\n     *     // Specialize BlockLoad for a 1D block of 128 threads owning 4 integer items each\n     *     typedef cub::BlockLoad<int, 128, 4, BLOCK_LOAD_WARP_TRANSPOSE> BlockLoad;\n     *\n     *     // Allocate shared memory for BlockLoad\n     *     __shared__ typename BlockLoad::TempStorage temp_storage;\n     *\n     *     // Load a segment of consecutive items that are blocked across threads\n     *     int thread_data[4];\n     *     BlockLoad(temp_storage).Load(d_data, thread_data, valid_items);\n     *\n     * \\endcode\n     * \\par\n     * Suppose the input \\p d_data is <tt>0, 1, 2, 3, 4, 5, 6...</tt> and \\p valid_items is \\p 5.\n     * The set of \\p thread_data across the block of threads in those threads will be\n     * <tt>{ [0,1,2,3], [4,?,?,?], ..., [?,?,?,?] }</tt>, with only the first two threads\n     * being unmasked to load portions of valid data (and other items remaining unassigned).\n     *\n     */\n    template <typename InputIteratorT>\n    __device__ __forceinline__ void Load(\n        InputIteratorT  block_itr,                  ///< [in] The thread block's base input iterator for loading from\n        InputT          (&items)[ITEMS_PER_THREAD], ///< [out] Data to load\n        int             valid_items)                ///< [in] Number of valid items to load\n    {\n        InternalLoad(temp_storage, linear_tid).Load(block_itr, items, valid_items);\n    }\n\n\n    /**\n     * \\brief Load a linear segment of items from memory, guarded by range, with a fall-back assignment of out-of-bound elements\n     *\n     * \\par\n     * - \\blocked\n     * - \\smemreuse\n     *\n     * \\par Snippet\n     * The code snippet below illustrates the guarded loading of a linear\n     * segment of 512 integers into a \"blocked\" arrangement across 128 threads where each\n     * thread owns 4 consecutive items.  The load is specialized for \\p BLOCK_LOAD_WARP_TRANSPOSE,\n     * meaning memory references are efficiently coalesced using a warp-striped access\n     * pattern (after which items are locally reordered among threads).\n     * \\par\n     * \\code\n     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_load.cuh>\n     *\n     * __global__ void ExampleKernel(int *d_data, int valid_items, ...)\n     * {\n     *     // Specialize BlockLoad for a 1D block of 128 threads owning 4 integer items each\n     *     typedef cub::BlockLoad<int, 128, 4, BLOCK_LOAD_WARP_TRANSPOSE> BlockLoad;\n     *\n     *     // Allocate shared memory for BlockLoad\n     *     __shared__ typename BlockLoad::TempStorage temp_storage;\n     *\n     *     // Load a segment of consecutive items that are blocked across threads\n     *     int thread_data[4];\n     *     BlockLoad(temp_storage).Load(d_data, thread_data, valid_items, -1);\n     *\n     * \\endcode\n     * \\par\n     * Suppose the input \\p d_data is <tt>0, 1, 2, 3, 4, 5, 6...</tt>,\n     * \\p valid_items is \\p 5, and the out-of-bounds default is \\p -1.\n     * The set of \\p thread_data across the block of threads in those threads will be\n     * <tt>{ [0,1,2,3], [4,-1,-1,-1], ..., [-1,-1,-1,-1] }</tt>, with only the first two threads\n     * being unmasked to load portions of valid data (and other items are assigned \\p -1)\n     *\n     */\n    template <typename InputIteratorT, typename DefaultT>\n    __device__ __forceinline__ void Load(\n        InputIteratorT  block_itr,                  ///< [in] The thread block's base input iterator for loading from\n        InputT          (&items)[ITEMS_PER_THREAD], ///< [out] Data to load\n        int             valid_items,                ///< [in] Number of valid items to load\n        DefaultT        oob_default)                ///< [in] Default value to assign out-of-bound items\n    {\n        InternalLoad(temp_storage, linear_tid).Load(block_itr, items, valid_items, oob_default);\n    }\n\n\n    //@}  end member group\n\n};\n\ntemplate <class Policy,\n          class It,\n          class T = cub::detail::value_t<It>>\nstruct BlockLoadType\n{\n  using type = cub::BlockLoad<T,\n                              Policy::BLOCK_THREADS,\n                              Policy::ITEMS_PER_THREAD,\n                              Policy::LOAD_ALGORITHM>;\n};\n\n\nCUB_NAMESPACE_END\n\n\n#endif // _JITIFY_INCLUDE_GUARD_41DD869113644EF0\n", "cub/block/block_reduce.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_821AC9A4031AA112\n#define _JITIFY_INCLUDE_GUARD_821AC9A4031AA112\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.\n * \n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n * \n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * The cub::BlockReduce class provides [<em>collective</em>](index.html#sec0) methods for computing a parallel reduction of items partitioned across a CUDA thread block.\n */\n\n#include \"specializations/block_reduce_raking.cuh\"\n#include \"specializations/block_reduce_raking_commutative_only.cuh\"\n#include \"specializations/block_reduce_warp_reductions.cuh\"\n#include \"../config.cuh\"\n#include \"../util_ptx.cuh\"\n#include \"../util_type.cuh\"\n#include \"../thread/thread_operators.cuh\"\n\nCUB_NAMESPACE_BEGIN\n\n\n\n/******************************************************************************\n * Algorithmic variants\n ******************************************************************************/\n\n/**\n * BlockReduceAlgorithm enumerates alternative algorithms for parallel\n * reduction across a CUDA thread block.\n */\nenum BlockReduceAlgorithm\n{\n\n    /**\n     * \\par Overview\n     * An efficient \"raking\" reduction algorithm that only supports commutative\n     * reduction operators (true for most operations, e.g., addition).\n     *\n     * \\par\n     * Execution is comprised of three phases:\n     * -# Upsweep sequential reduction in registers (if threads contribute more\n     *    than one input each).  Threads in warps other than the first warp place\n     *    their partial reductions into shared memory.\n     * -# Upsweep sequential reduction in shared memory.  Threads within the first\n     *    warp continue to accumulate by raking across segments of shared partial reductions\n     * -# A warp-synchronous Kogge-Stone style reduction within the raking warp.\n     *\n     * \\par\n     * \\image html block_reduce.png\n     * <div class=\"centercaption\">\\p BLOCK_REDUCE_RAKING data flow for a hypothetical 16-thread thread block and 4-thread raking warp.</div>\n     *\n     * \\par Performance Considerations\n     * - This variant performs less communication than BLOCK_REDUCE_RAKING_NON_COMMUTATIVE\n     *   and is preferable when the reduction operator is commutative.  This variant\n     *   applies fewer reduction operators  than BLOCK_REDUCE_WARP_REDUCTIONS, and can provide higher overall\n     *   throughput across the GPU when suitably occupied.  However, turn-around latency may be\n     *   higher than to BLOCK_REDUCE_WARP_REDUCTIONS and thus less-desirable\n     *   when the GPU is under-occupied.\n     */\n    BLOCK_REDUCE_RAKING_COMMUTATIVE_ONLY,\n\n\n    /**\n     * \\par Overview\n     * An efficient \"raking\" reduction algorithm that supports commutative\n     * (e.g., addition) and non-commutative (e.g., string concatenation) reduction\n     * operators. \\blocked.\n     *\n     * \\par\n     * Execution is comprised of three phases:\n     * -# Upsweep sequential reduction in registers (if threads contribute more\n     *    than one input each).  Each thread then places the partial reduction\n     *    of its item(s) into shared memory.\n     * -# Upsweep sequential reduction in shared memory.  Threads within a\n     *    single warp rake across segments of shared partial reductions.\n     * -# A warp-synchronous Kogge-Stone style reduction within the raking warp.\n     *\n     * \\par\n     * \\image html block_reduce.png\n     * <div class=\"centercaption\">\\p BLOCK_REDUCE_RAKING data flow for a hypothetical 16-thread thread block and 4-thread raking warp.</div>\n     *\n     * \\par Performance Considerations\n     * - This variant performs more communication than BLOCK_REDUCE_RAKING\n     *   and is only preferable when the reduction operator is non-commutative.  This variant\n     *   applies fewer reduction operators than BLOCK_REDUCE_WARP_REDUCTIONS, and can provide higher overall\n     *   throughput across the GPU when suitably occupied.  However, turn-around latency may be\n     *   higher than to BLOCK_REDUCE_WARP_REDUCTIONS and thus less-desirable\n     *   when the GPU is under-occupied.\n     */\n    BLOCK_REDUCE_RAKING,\n\n\n    /**\n     * \\par Overview\n     * A quick \"tiled warp-reductions\" reduction algorithm that supports commutative\n     * (e.g., addition) and non-commutative (e.g., string concatenation) reduction\n     * operators.\n     *\n     * \\par\n     * Execution is comprised of four phases:\n     * -# Upsweep sequential reduction in registers (if threads contribute more\n     *    than one input each).  Each thread then places the partial reduction\n     *    of its item(s) into shared memory.\n     * -# Compute a shallow, but inefficient warp-synchronous Kogge-Stone style\n     *    reduction within each warp.\n     * -# A propagation phase where the warp reduction outputs in each warp are\n     *    updated with the aggregate from each preceding warp.\n     *\n     * \\par\n     * \\image html block_scan_warpscans.png\n     * <div class=\"centercaption\">\\p BLOCK_REDUCE_WARP_REDUCTIONS data flow for a hypothetical 16-thread thread block and 4-thread raking warp.</div>\n     *\n     * \\par Performance Considerations\n     * - This variant applies more reduction operators than BLOCK_REDUCE_RAKING\n     *   or BLOCK_REDUCE_RAKING_NON_COMMUTATIVE, which may result in lower overall\n     *   throughput across the GPU.  However turn-around latency may be lower and\n     *   thus useful when the GPU is under-occupied.\n     */\n    BLOCK_REDUCE_WARP_REDUCTIONS,\n};\n\n\n/******************************************************************************\n * Block reduce\n ******************************************************************************/\n\n/**\n * \\brief The BlockReduce class provides [<em>collective</em>](index.html#sec0) methods for computing a parallel reduction of items partitioned across a CUDA thread block. ![](reduce_logo.png)\n * \\ingroup BlockModule\n *\n * \\tparam T                Data type being reduced\n * \\tparam BLOCK_DIM_X      The thread block length in threads along the X dimension\n * \\tparam ALGORITHM        <b>[optional]</b> cub::BlockReduceAlgorithm enumerator specifying the underlying algorithm to use (default: cub::BLOCK_REDUCE_WARP_REDUCTIONS)\n * \\tparam BLOCK_DIM_Y      <b>[optional]</b> The thread block length in threads along the Y dimension (default: 1)\n * \\tparam BLOCK_DIM_Z      <b>[optional]</b> The thread block length in threads along the Z dimension (default: 1)\n * \\tparam LEGACY_PTX_ARCH  <b>[optional]</b> Unused.\n *\n * \\par Overview\n * - A <a href=\"http://en.wikipedia.org/wiki/Reduce_(higher-order_function)\"><em>reduction</em></a> (or <em>fold</em>)\n *   uses a binary combining operator to compute a single aggregate from a list of input elements.\n * - \\rowmajor\n * - BlockReduce can be optionally specialized by algorithm to accommodate different latency/throughput workload profiles:\n *   -# <b>cub::BLOCK_REDUCE_RAKING_COMMUTATIVE_ONLY</b>.  An efficient \"raking\" reduction algorithm that only supports commutative reduction operators. [More...](\\ref cub::BlockReduceAlgorithm)\n *   -# <b>cub::BLOCK_REDUCE_RAKING</b>.  An efficient \"raking\" reduction algorithm that supports commutative and non-commutative reduction operators. [More...](\\ref cub::BlockReduceAlgorithm)\n *   -# <b>cub::BLOCK_REDUCE_WARP_REDUCTIONS</b>.  A quick \"tiled warp-reductions\" reduction algorithm that supports commutative and non-commutative reduction operators. [More...](\\ref cub::BlockReduceAlgorithm)\n *\n * \\par Performance Considerations\n * - \\granularity\n * - Very efficient (only one synchronization barrier).\n * - Incurs zero bank conflicts for most types\n * - Computation is slightly more efficient (i.e., having lower instruction overhead) for:\n *   - Summation (<b><em>vs.</em></b> generic reduction)\n *   - \\p BLOCK_THREADS is a multiple of the architecture's warp size\n *   - Every thread has a valid input (i.e., full <b><em>vs.</em></b> partial-tiles)\n * - See cub::BlockReduceAlgorithm for performance details regarding algorithmic alternatives\n *\n * \\par A Simple Example\n * \\blockcollective{BlockReduce}\n * \\par\n * The code snippet below illustrates a sum reduction of 512 integer items that\n * are partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3) across 128 threads\n * where each thread owns 4 consecutive items.\n * \\par\n * \\code\n * #include <cub/cub.cuh>   // or equivalently <cub/block/block_reduce.cuh>\n *\n * __global__ void ExampleKernel(...)\n * {\n *     // Specialize BlockReduce for a 1D block of 128 threads of type int\n *     typedef cub::BlockReduce<int, 128> BlockReduce;\n *\n *     // Allocate shared memory for BlockReduce\n *     __shared__ typename BlockReduce::TempStorage temp_storage;\n *\n *     // Obtain a segment of consecutive items that are blocked across threads\n *     int thread_data[4];\n *     ...\n *\n *     // Compute the block-wide sum for thread0\n *     int aggregate = BlockReduce(temp_storage).Sum(thread_data);\n *\n * \\endcode\n *\n * \\par Re-using dynamically allocating shared memory\n * The following example under the examples/block folder illustrates usage of\n * dynamically shared memory with BlockReduce and how to re-purpose\n * the same memory region:\n * <a href=\"../../examples/block/example_block_reduce_dyn_smem.cu\">example_block_reduce_dyn_smem.cu</a>\n */\ntemplate <\n    typename                T,\n    int                     BLOCK_DIM_X,\n    BlockReduceAlgorithm    ALGORITHM       = BLOCK_REDUCE_WARP_REDUCTIONS,\n    int                     BLOCK_DIM_Y     = 1,\n    int                     BLOCK_DIM_Z     = 1,\n    int                     LEGACY_PTX_ARCH = 0>\nclass BlockReduce\n{\nprivate:\n\n    /******************************************************************************\n     * Constants and type definitions\n     ******************************************************************************/\n\n    /// Constants\n    enum\n    {\n        /// The thread block size in threads\n        BLOCK_THREADS = BLOCK_DIM_X * BLOCK_DIM_Y * BLOCK_DIM_Z,\n    };\n\n    typedef BlockReduceWarpReductions<T, BLOCK_DIM_X, BLOCK_DIM_Y, BLOCK_DIM_Z>           WarpReductions;\n    typedef BlockReduceRakingCommutativeOnly<T, BLOCK_DIM_X, BLOCK_DIM_Y, BLOCK_DIM_Z>    RakingCommutativeOnly;\n    typedef BlockReduceRaking<T, BLOCK_DIM_X, BLOCK_DIM_Y, BLOCK_DIM_Z>                   Raking;\n\n    /// Internal specialization type\n    using InternalBlockReduce = cub::detail::conditional_t<\n      ALGORITHM == BLOCK_REDUCE_WARP_REDUCTIONS,\n      WarpReductions,\n      cub::detail::conditional_t<ALGORITHM == BLOCK_REDUCE_RAKING_COMMUTATIVE_ONLY,\n                                 RakingCommutativeOnly,\n                                 Raking>>; // BlockReduceRaking\n\n    /// Shared memory storage layout type for BlockReduce\n    typedef typename InternalBlockReduce::TempStorage _TempStorage;\n\n\n    /******************************************************************************\n     * Utility methods\n     ******************************************************************************/\n\n    /// Internal storage allocator\n    __device__ __forceinline__ _TempStorage& PrivateStorage()\n    {\n        __shared__ _TempStorage private_storage;\n        return private_storage;\n    }\n\n\n    /******************************************************************************\n     * Thread fields\n     ******************************************************************************/\n\n    /// Shared storage reference\n    _TempStorage &temp_storage;\n\n    /// Linear thread-id\n    unsigned int linear_tid;\n\n\npublic:\n\n    /// \\smemstorage{BlockReduce}\n    struct TempStorage : Uninitialized<_TempStorage> {};\n\n\n    /******************************************************************//**\n     * \\name Collective constructors\n     *********************************************************************/\n    //@{\n\n    /**\n     * \\brief Collective constructor using a private static allocation of shared memory as temporary storage.\n     */\n    __device__ __forceinline__ BlockReduce()\n    :\n        temp_storage(PrivateStorage()),\n        linear_tid(RowMajorTid(BLOCK_DIM_X, BLOCK_DIM_Y, BLOCK_DIM_Z))\n    {}\n\n\n    /**\n     * \\brief Collective constructor using the specified memory allocation as temporary storage.\n     */\n    __device__ __forceinline__ BlockReduce(\n        TempStorage &temp_storage)             ///< [in] Reference to memory allocation having layout type TempStorage\n    :\n        temp_storage(temp_storage.Alias()),\n        linear_tid(RowMajorTid(BLOCK_DIM_X, BLOCK_DIM_Y, BLOCK_DIM_Z))\n    {}\n\n\n    //@}  end member group\n    /******************************************************************//**\n     * \\name Generic reductions\n     *********************************************************************/\n    //@{\n\n\n    /**\n     * \\brief Computes a block-wide reduction for thread<sub>0</sub> using the specified binary reduction functor.  Each thread contributes one input element.\n     *\n     * \\par\n     * - The return value is undefined in threads other than thread<sub>0</sub>.\n     * - \\rowmajor\n     * - \\smemreuse\n     *\n     * \\par Snippet\n     * The code snippet below illustrates a max reduction of 128 integer items that\n     * are partitioned across 128 threads.\n     * \\par\n     * \\code\n     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_reduce.cuh>\n     *\n     * __global__ void ExampleKernel(...)\n     * {\n     *     // Specialize BlockReduce for a 1D block of 128 threads of type int\n     *     typedef cub::BlockReduce<int, 128> BlockReduce;\n     *\n     *     // Allocate shared memory for BlockReduce\n     *     __shared__ typename BlockReduce::TempStorage temp_storage;\n     *\n     *     // Each thread obtains an input item\n     *     int thread_data;\n     *     ...\n     *\n     *     // Compute the block-wide max for thread0\n     *     int aggregate = BlockReduce(temp_storage).Reduce(thread_data, cub::Max());\n     *\n     * \\endcode\n     *\n     * \\tparam ReductionOp          <b>[inferred]</b> Binary reduction functor  type having member <tt>T operator()(const T &a, const T &b)</tt>\n     */\n    template <typename ReductionOp>\n    __device__ __forceinline__ T Reduce(\n        T               input,                      ///< [in] Calling thread's input\n        ReductionOp     reduction_op)               ///< [in] Binary reduction functor \n    {\n        return InternalBlockReduce(temp_storage).template Reduce<true>(input, BLOCK_THREADS, reduction_op);\n    }\n\n\n    /**\n     * \\brief Computes a block-wide reduction for thread<sub>0</sub> using the specified binary reduction functor.  Each thread contributes an array of consecutive input elements.\n     *\n     * \\par\n     * - The return value is undefined in threads other than thread<sub>0</sub>.\n     * - \\granularity\n     * - \\smemreuse\n     *\n     * \\par Snippet\n     * The code snippet below illustrates a max reduction of 512 integer items that\n     * are partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3) across 128 threads\n     * where each thread owns 4 consecutive items.\n     * \\par\n     * \\code\n     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_reduce.cuh>\n     *\n     * __global__ void ExampleKernel(...)\n     * {\n     *     // Specialize BlockReduce for a 1D block of 128 threads of type int\n     *     typedef cub::BlockReduce<int, 128> BlockReduce;\n     *\n     *     // Allocate shared memory for BlockReduce\n     *     __shared__ typename BlockReduce::TempStorage temp_storage;\n     *\n     *     // Obtain a segment of consecutive items that are blocked across threads\n     *     int thread_data[4];\n     *     ...\n     *\n     *     // Compute the block-wide max for thread0\n     *     int aggregate = BlockReduce(temp_storage).Reduce(thread_data, cub::Max());\n     *\n     * \\endcode\n     *\n     * \\tparam ITEMS_PER_THREAD     <b>[inferred]</b> The number of consecutive items partitioned onto each thread.\n     * \\tparam ReductionOp          <b>[inferred]</b> Binary reduction functor  type having member <tt>T operator()(const T &a, const T &b)</tt>\n     */\n    template <\n        int ITEMS_PER_THREAD,\n        typename ReductionOp>\n    __device__ __forceinline__ T Reduce(\n        T               (&inputs)[ITEMS_PER_THREAD],    ///< [in] Calling thread's input segment\n        ReductionOp     reduction_op)                   ///< [in] Binary reduction functor \n    {\n        // Reduce partials\n        T partial = internal::ThreadReduce(inputs, reduction_op);\n        return Reduce(partial, reduction_op);\n    }\n\n\n    /**\n     * \\brief Computes a block-wide reduction for thread<sub>0</sub> using the specified binary reduction functor.  The first \\p num_valid threads each contribute one input element.\n     *\n     * \\par\n     * - The return value is undefined in threads other than thread<sub>0</sub>.\n     * - \\rowmajor\n     * - \\smemreuse\n     *\n     * \\par Snippet\n     * The code snippet below illustrates a max reduction of a partially-full tile of integer items that\n     * are partitioned across 128 threads.\n     * \\par\n     * \\code\n     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_reduce.cuh>\n     *\n     * __global__ void ExampleKernel(int num_valid, ...)\n     * {\n     *     // Specialize BlockReduce for a 1D block of 128 threads of type int\n     *     typedef cub::BlockReduce<int, 128> BlockReduce;\n     *\n     *     // Allocate shared memory for BlockReduce\n     *     __shared__ typename BlockReduce::TempStorage temp_storage;\n     *\n     *     // Each thread obtains an input item\n     *     int thread_data;\n     *     if (threadIdx.x < num_valid) thread_data = ...\n     *\n     *     // Compute the block-wide max for thread0\n     *     int aggregate = BlockReduce(temp_storage).Reduce(thread_data, cub::Max(), num_valid);\n     *\n     * \\endcode\n     *\n     * \\tparam ReductionOp          <b>[inferred]</b> Binary reduction functor  type having member <tt>T operator()(const T &a, const T &b)</tt>\n     */\n    template <typename ReductionOp>\n    __device__ __forceinline__ T Reduce(\n        T                   input,                  ///< [in] Calling thread's input\n        ReductionOp         reduction_op,           ///< [in] Binary reduction functor \n        int                 num_valid)              ///< [in] Number of threads containing valid elements (may be less than BLOCK_THREADS)\n    {\n        // Determine if we skip bounds checking\n        if (num_valid >= BLOCK_THREADS)\n        {\n            return InternalBlockReduce(temp_storage).template Reduce<true>(input, num_valid, reduction_op);\n        }\n        else\n        {\n            return InternalBlockReduce(temp_storage).template Reduce<false>(input, num_valid, reduction_op);\n        }\n    }\n\n\n    //@}  end member group\n    /******************************************************************//**\n     * \\name Summation reductions\n     *********************************************************************/\n    //@{\n\n\n    /**\n     * \\brief Computes a block-wide reduction for thread<sub>0</sub> using addition (+) as the reduction operator.  Each thread contributes one input element.\n     *\n     * \\par\n     * - The return value is undefined in threads other than thread<sub>0</sub>.\n     * - \\rowmajor\n     * - \\smemreuse\n     *\n     * \\par Snippet\n     * The code snippet below illustrates a sum reduction of 128 integer items that\n     * are partitioned across 128 threads.\n     * \\par\n     * \\code\n     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_reduce.cuh>\n     *\n     * __global__ void ExampleKernel(...)\n     * {\n     *     // Specialize BlockReduce for a 1D block of 128 threads of type int\n     *     typedef cub::BlockReduce<int, 128> BlockReduce;\n     *\n     *     // Allocate shared memory for BlockReduce\n     *     __shared__ typename BlockReduce::TempStorage temp_storage;\n     *\n     *     // Each thread obtains an input item\n     *     int thread_data;\n     *     ...\n     *\n     *     // Compute the block-wide sum for thread0\n     *     int aggregate = BlockReduce(temp_storage).Sum(thread_data);\n     *\n     * \\endcode\n     *\n     */\n    __device__ __forceinline__ T Sum(\n        T   input)                      ///< [in] Calling thread's input\n    {\n        return InternalBlockReduce(temp_storage).template Sum<true>(input, BLOCK_THREADS);\n    }\n\n    /**\n     * \\brief Computes a block-wide reduction for thread<sub>0</sub> using addition (+) as the reduction operator.  Each thread contributes an array of consecutive input elements.\n     *\n     * \\par\n     * - The return value is undefined in threads other than thread<sub>0</sub>.\n     * - \\granularity\n     * - \\smemreuse\n     *\n     * \\par Snippet\n     * The code snippet below illustrates a sum reduction of 512 integer items that\n     * are partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3) across 128 threads\n     * where each thread owns 4 consecutive items.\n     * \\par\n     * \\code\n     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_reduce.cuh>\n     *\n     * __global__ void ExampleKernel(...)\n     * {\n     *     // Specialize BlockReduce for a 1D block of 128 threads of type int\n     *     typedef cub::BlockReduce<int, 128> BlockReduce;\n     *\n     *     // Allocate shared memory for BlockReduce\n     *     __shared__ typename BlockReduce::TempStorage temp_storage;\n     *\n     *     // Obtain a segment of consecutive items that are blocked across threads\n     *     int thread_data[4];\n     *     ...\n     *\n     *     // Compute the block-wide sum for thread0\n     *     int aggregate = BlockReduce(temp_storage).Sum(thread_data);\n     *\n     * \\endcode\n     *\n     * \\tparam ITEMS_PER_THREAD     <b>[inferred]</b> The number of consecutive items partitioned onto each thread.\n     */\n    template <int ITEMS_PER_THREAD>\n    __device__ __forceinline__ T Sum(\n        T   (&inputs)[ITEMS_PER_THREAD])    ///< [in] Calling thread's input segment\n    {\n        // Reduce partials\n        T partial = internal::ThreadReduce(inputs, cub::Sum());\n        return Sum(partial);\n    }\n\n\n    /**\n     * \\brief Computes a block-wide reduction for thread<sub>0</sub> using addition (+) as the reduction operator.  The first \\p num_valid threads each contribute one input element.\n     *\n     * \\par\n     * - The return value is undefined in threads other than thread<sub>0</sub>.\n     * - \\rowmajor\n     * - \\smemreuse\n     *\n     * \\par Snippet\n     * The code snippet below illustrates a sum reduction of a partially-full tile of integer items that\n     * are partitioned across 128 threads.\n     * \\par\n     * \\code\n     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_reduce.cuh>\n     *\n     * __global__ void ExampleKernel(int num_valid, ...)\n     * {\n     *     // Specialize BlockReduce for a 1D block of 128 threads of type int\n     *     typedef cub::BlockReduce<int, 128> BlockReduce;\n     *\n     *     // Allocate shared memory for BlockReduce\n     *     __shared__ typename BlockReduce::TempStorage temp_storage;\n     *\n     *     // Each thread obtains an input item (up to num_items)\n     *     int thread_data;\n     *     if (threadIdx.x < num_valid)\n     *         thread_data = ...\n     *\n     *     // Compute the block-wide sum for thread0\n     *     int aggregate = BlockReduce(temp_storage).Sum(thread_data, num_valid);\n     *\n     * \\endcode\n     *\n     */\n    __device__ __forceinline__ T Sum(\n        T   input,                  ///< [in] Calling thread's input\n        int num_valid)              ///< [in] Number of threads containing valid elements (may be less than BLOCK_THREADS)\n    {\n        // Determine if we skip bounds checking\n        if (num_valid >= BLOCK_THREADS)\n        {\n            return InternalBlockReduce(temp_storage).template Sum<true>(input, num_valid);\n        }\n        else\n        {\n            return InternalBlockReduce(temp_storage).template Sum<false>(input, num_valid);\n        }\n    }\n\n\n    //@}  end member group\n};\n\n/**\n * \\example example_block_reduce.cu\n */\n\nCUB_NAMESPACE_END\n\n\n#endif // _JITIFY_INCLUDE_GUARD_821AC9A4031AA112\n", "cub/config.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_1D666048A344D3BF\n#define _JITIFY_INCLUDE_GUARD_1D666048A344D3BF\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * Static configuration header for the CUB project.\n */\n\n#include \"util_arch.cuh\"\n#include \"util_compiler.cuh\"\n#include \"util_cpp_dialect.cuh\"\n#include \"util_deprecated.cuh\"\n#include \"util_macro.cuh\"\n#include \"util_namespace.cuh\"\n\n#endif // _JITIFY_INCLUDE_GUARD_1D666048A344D3BF\n", "cub/detail/detect_cuda_runtime.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_E0F4FC2C1089A641\n#define _JITIFY_INCLUDE_GUARD_E0F4FC2C1089A641\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * Utilities for CUDA dynamic parallelism.\n */\n\n#include <cub/util_namespace.cuh>\n\n//#include <cuda_runtime_api.h>\n\nCUB_NAMESPACE_BEGIN\nnamespace detail\n{\n\n#ifdef DOXYGEN_SHOULD_SKIP_THIS // Only parse this during doxygen passes:\n\n/**\n * \\def CUB_DISABLE_CDP\n *\n * If defined, support for device-side usage of CUB is disabled.\n */\n#define CUB_DISABLE_CDP\n\n/**\n * \\def CUB_RDC_ENABLED\n *\n * Defined if RDC is enabled and CUB_DISABLE_CDP is not defined.\n */\n#define CUB_RDC_ENABLED\n\n/**\n * \\def CUB_RUNTIME_FUNCTION\n *\n * Execution space for functions that can use the CUDA runtime API (`__host__`\n * when RDC is off, `__host__ __device__` when RDC is on).\n */\n#define CUB_RUNTIME_FUNCTION\n\n/**\n * \\def CUB_RUNTIME_ENABLED\n *\n * Whether or not the active compiler pass is allowed to invoke device kernels\n * or methods from the CUDA runtime API.\n *\n * This macro should not be used in CUB, as it depends on `__CUDA_ARCH__`\n * and is not compatible with `NV_IF_TARGET`. It is provided for legacy\n * purposes only.\n *\n * Replace any usages with `CUB_RDC_ENABLED` and `NV_IF_TARGET`.\n */\n#define CUB_RUNTIME_ENABLED\n\n#else // Non-doxygen pass:\n\n#ifndef CUB_RUNTIME_FUNCTION\n\n#if defined(__CUDACC_RDC__) && !defined(CUB_DISABLE_CDP)\n\n#define CUB_RDC_ENABLED\n#define CUB_RUNTIME_FUNCTION __host__ __device__\n\n#else // RDC disabled:\n\n#define CUB_RUNTIME_FUNCTION __host__\n\n#endif // RDC enabled\n\n#if !defined(__CUDA_ARCH__) || defined(__CUDACC_RDC__)\n// Legacy only -- do not use in new code.\n#define CUB_RUNTIME_ENABLED\n#endif\n\n#endif // CUB_RUNTIME_FUNCTION predefined\n\n#ifdef CUB_RDC_ENABLED\n// Detect available version of CDP:\n#if __CUDACC_VER_MAJOR__ < 12 || defined(CUDA_FORCE_CDP1_IF_SUPPORTED)\n#define CUB_DETAIL_CDPv1\n#else\n#define CUB_DETAIL_CDPv2\n#endif\n#endif\n\n#endif // Do not document\n\n} // namespace detail\nCUB_NAMESPACE_END\n\n#endif // _JITIFY_INCLUDE_GUARD_E0F4FC2C1089A641\n", "cub/detail/type_traits.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_86A4C0B05335F23D\n#define _JITIFY_INCLUDE_GUARD_86A4C0B05335F23D\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * Wrappers and extensions around <type_traits> utilities.\n */\n\n#include <cub/util_cpp_dialect.cuh>\n#include <cub/util_namespace.cuh>\n\n#include <cuda/std/type_traits>\n\n\nCUB_NAMESPACE_BEGIN\nnamespace detail {\n\ntemplate <typename Invokable, typename... Args>\nusing invoke_result_t =\n#if CUB_CPP_DIALECT < 2017\n  typename ::cuda::std::result_of<Invokable(Args...)>::type;\n#else // 2017+\n  ::cuda::std::invoke_result_t<Invokable, Args...>;\n#endif\n\n/// The type of intermediate accumulator (according to P2322R6)\ntemplate <typename Invokable, typename InitT, typename InputT>\nusing accumulator_t = \n  typename ::cuda::std::decay<invoke_result_t<Invokable, InitT, InputT>>::type;\n\n} // namespace detail\nCUB_NAMESPACE_END\n\n#endif // _JITIFY_INCLUDE_GUARD_86A4C0B05335F23D\n", "cub/detail/uninitialized_copy.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_3CD9006125E1ABF0\n#define _JITIFY_INCLUDE_GUARD_3CD9006125E1ABF0\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011-2022, NVIDIA CORPORATION.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" \n * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE \n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE \n * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n#include <cub/config.cuh>\n\n#include <cuda/std/type_traits>\n\nCUB_NAMESPACE_BEGIN\n\n\nnamespace detail\n{\n\n#if defined(_NVHPC_CUDA)\ntemplate <typename T, typename U>\n__host__ __device__ void uninitialized_copy(T *ptr, U &&val)\n{\n  // NVBug 3384810\n  new (ptr) T(::cuda::std::forward<U>(val));\n}\n#else\ntemplate <typename T,\n          typename U,\n          typename ::cuda::std::enable_if<\n            ::cuda::std::is_trivially_copyable<T>::value, \n            int\n          >::type = 0>\n__host__ __device__ void uninitialized_copy(T *ptr, U &&val)\n{\n  *ptr = ::cuda::std::forward<U>(val);\n}\n\ntemplate <typename T, \n         typename U,\n         typename ::cuda::std::enable_if<\n           !::cuda::std::is_trivially_copyable<T>::value,\n           int\n         >::type = 0>\n__host__ __device__ void uninitialized_copy(T *ptr, U &&val)\n{\n  new (ptr) T(::cuda::std::forward<U>(val));\n}\n#endif\n\n} // namespace detail\n\n\nCUB_NAMESPACE_END\n\n\n#endif // _JITIFY_INCLUDE_GUARD_3CD9006125E1ABF0\n", "cub/thread/thread_operators.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_94E67823631CB18F\n#define _JITIFY_INCLUDE_GUARD_94E67823631CB18F\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2022, NVIDIA CORPORATION.  All rights reserved.\n * \n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n * \n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" \n * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE \n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE \n * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * @file\n * Simple binary operator functor types\n */\n\n/******************************************************************************\n * Simple functor operators\n ******************************************************************************/\n\n#include <cub/config.cuh>\n#include <cub/util_cpp_dialect.cuh>\n#include <cub/util_type.cuh>\n\n#include <cuda/std/functional>\n#include <cuda/std/type_traits>\n#include <cuda/std/utility>\n\nCUB_NAMESPACE_BEGIN\n\n\n/**\n * @addtogroup UtilModule\n * @{\n */\n\n/// @brief Inequality functor (wraps equality functor)\ntemplate <typename EqualityOp>\nstruct InequalityWrapper\n{\n  /// Wrapped equality operator\n  EqualityOp op;\n\n  /// Constructor\n  __host__ __device__ __forceinline__ InequalityWrapper(EqualityOp op)\n      : op(op)\n  {}\n\n  /// Boolean inequality operator, returns `t != u`\n  template <typename T, typename U>\n  __host__ __device__ __forceinline__ bool operator()(T &&t, U &&u)\n  {\n    return !op(::cuda::std::forward<T>(t), ::cuda::std::forward<U>(u));\n  }\n};\n\n#if CUB_CPP_DIALECT > 2011\nusing Equality = ::cuda::std::equal_to<>;\nusing Inequality = ::cuda::std::not_equal_to<>;\nusing Sum = ::cuda::std::plus<>;\nusing Difference = ::cuda::std::minus<>;\nusing Division = ::cuda::std::divides<>;\n#else\n/// @brief Default equality functor\nstruct Equality\n{\n  /// Boolean equality operator, returns `t == u`\n  template <typename T, typename U>\n  __host__ __device__ __forceinline__ bool operator()(T &&t, U &&u) const\n  {\n    return ::cuda::std::forward<T>(t) == ::cuda::std::forward<U>(u);\n  }\n};\n\n/// @brief Default inequality functor\nstruct Inequality\n{\n  /// Boolean inequality operator, returns `t != u`\n  template <typename T, typename U>\n  __host__ __device__ __forceinline__ bool operator()(T &&t, U &&u) const\n  {\n    return ::cuda::std::forward<T>(t) != ::cuda::std::forward<U>(u);\n  }\n};\n\n/// @brief Default sum functor\nstruct Sum\n{\n  /// Binary sum operator, returns `t + u`\n  template <typename T, typename U>\n  __host__ __device__ __forceinline__ auto operator()(T &&t, U &&u) const\n    -> decltype(::cuda::std::forward<T>(t) + ::cuda::std::forward<U>(u))\n  {\n    return ::cuda::std::forward<T>(t) + ::cuda::std::forward<U>(u);\n  }\n};\n\n/// @brief Default difference functor\nstruct Difference\n{\n  /// Binary difference operator, returns `t - u`\n  template <typename T, typename U>\n  __host__ __device__ __forceinline__ auto operator()(T &&t, U &&u) const\n    -> decltype(::cuda::std::forward<T>(t) - ::cuda::std::forward<U>(u))\n  {\n    return ::cuda::std::forward<T>(t) - ::cuda::std::forward<U>(u);\n  }\n};\n\n/// @brief Default division functor\nstruct Division\n{\n  /// Binary division operator, returns `t / u`\n  template <typename T, typename U>\n  __host__ __device__ __forceinline__ auto operator()(T &&t, U &&u) const\n    -> decltype(::cuda::std::forward<T>(t) / ::cuda::std::forward<U>(u))\n  {\n    return ::cuda::std::forward<T>(t) / ::cuda::std::forward<U>(u);\n  }\n};\n#endif\n\n/// @brief Default max functor\nstruct Max\n{\n  /// Boolean max operator, returns `(t > u) ? t : u`\n  template <typename T, typename U>\n  __host__ __device__ __forceinline__\n    typename ::cuda::std::common_type<T, U>::type\n    operator()(T &&t, U &&u) const\n  {\n    return CUB_MAX(t, u);\n  }\n};\n\n/// @brief Arg max functor (keeps the value and offset of the first occurrence\n///        of the larger item)\nstruct ArgMax\n{\n  /// Boolean max operator, preferring the item having the smaller offset in\n  /// case of ties\n  template <typename T, typename OffsetT>\n  __host__ __device__ __forceinline__ KeyValuePair<OffsetT, T>\n  operator()(const KeyValuePair<OffsetT, T> &a,\n             const KeyValuePair<OffsetT, T> &b) const\n  {\n    // Mooch BUG (device reduce argmax gk110 3.2 million random fp32)\n    // return ((b.value > a.value) || \n    //         ((a.value == b.value) && (b.key < a.key))) \n    //      ? b : a;\n\n    if ((b.value > a.value) || ((a.value == b.value) && (b.key < a.key)))\n    {\n      return b;\n    }\n\n    return a;\n  }\n};\n\n/// @brief Default min functor\nstruct Min\n{\n  /// Boolean min operator, returns `(t < u) ? t : u`\n  template <typename T, typename U>\n  __host__ __device__ __forceinline__\n    typename ::cuda::std::common_type<T, U>::type\n    operator()(T &&t, U &&u) const\n  {\n    return CUB_MIN(t, u);\n  }\n};\n\n/// @brief Arg min functor (keeps the value and offset of the first occurrence\n///        of the smallest item)\nstruct ArgMin\n{\n  /// Boolean min operator, preferring the item having the smaller offset in\n  /// case of ties\n  template <typename T, typename OffsetT>\n  __host__ __device__ __forceinline__ KeyValuePair<OffsetT, T>\n  operator()(const KeyValuePair<OffsetT, T> &a,\n             const KeyValuePair<OffsetT, T> &b) const\n  {\n    // Mooch BUG (device reduce argmax gk110 3.2 million random fp32)\n    // return ((b.value < a.value) ||\n    //         ((a.value == b.value) && (b.key < a.key)))\n    //      ? b : a;\n\n    if ((b.value < a.value) || ((a.value == b.value) && (b.key < a.key)))\n    {\n      return b;\n    }\n\n    return a;\n  }\n};\n\nnamespace detail\n{\ntemplate <class OpT>\nstruct basic_binary_op_t\n{\n  static constexpr bool value = false;\n};\n\ntemplate <>\nstruct basic_binary_op_t<Sum>\n{\n  static constexpr bool value = true;\n};\n\ntemplate <>\nstruct basic_binary_op_t<Min>\n{\n  static constexpr bool value = true;\n};\n\ntemplate <>\nstruct basic_binary_op_t<Max>\n{\n  static constexpr bool value = true;\n};\n} // namespace detail\n\n/// @brief Default cast functor\ntemplate <typename B>\nstruct CastOp\n{\n  /// Cast operator, returns `(B) a`\n  template <typename A>\n  __host__ __device__ __forceinline__ B operator()(A &&a) const\n  {\n    return (B)a;\n  }\n};\n\n/// @brief Binary operator wrapper for switching non-commutative scan arguments\ntemplate <typename ScanOp>\nclass SwizzleScanOp\n{\nprivate:\n  /// Wrapped scan operator\n  ScanOp scan_op;\n\npublic:\n  /// Constructor\n  __host__ __device__ __forceinline__ SwizzleScanOp(ScanOp scan_op)\n      : scan_op(scan_op)\n  {}\n\n  /// Switch the scan arguments\n  template <typename T>\n  __host__ __device__ __forceinline__ T operator()(const T &a, const T &b)\n  {\n    T _a(a);\n    T _b(b);\n\n    return scan_op(_b, _a);\n  }\n};\n\n/**\n * @brief Reduce-by-segment functor.\n *\n * Given two cub::KeyValuePair inputs `a` and `b` and a binary associative \n * combining operator `f(const T &x, const T &y)`, an instance of this functor \n * returns a cub::KeyValuePair whose `key` field is `a.key + b.key`, and whose \n * `value` field is either `b.value` if `b.key` is non-zero, or \n * `f(a.value, b.value)` otherwise.\n *\n * ReduceBySegmentOp is an associative, non-commutative binary combining \n * operator for input sequences of cub::KeyValuePair pairings. Such sequences \n * are typically used to represent a segmented set of values to be reduced\n * and a corresponding set of {0,1}-valued integer \"head flags\" demarcating the\n * first value of each segment.\n *\n * @tparam ReductionOpT Binary reduction operator to apply to values\n */\ntemplate <typename ReductionOpT>\nstruct ReduceBySegmentOp\n{\n  /// Wrapped reduction operator\n  ReductionOpT op;\n\n  /// Constructor\n  __host__ __device__ __forceinline__ ReduceBySegmentOp() {}\n\n  /// Constructor\n  __host__ __device__ __forceinline__ ReduceBySegmentOp(ReductionOpT op)\n      : op(op)\n  {}\n\n  /**\n   * @brief Scan operator\n   *\n   * @tparam KeyValuePairT\n   *   KeyValuePair pairing of T (value) and OffsetT (head flag)\n   *\n   * @param[in] first\n   *   First partial reduction\n   *\n   * @param[in] second\n   *   Second partial reduction\n   */\n  template <typename KeyValuePairT>\n  __host__ __device__ __forceinline__ KeyValuePairT\n  operator()(const KeyValuePairT &first, const KeyValuePairT &second)\n  {\n    KeyValuePairT retval;\n    retval.key = first.key + second.key;\n#ifdef _NVHPC_CUDA // WAR bug on nvc++\n    if (second.key)\n    {\n      retval.value = second.value;\n    }\n    else\n    {\n      // If second.value isn't copied into a temporary here, nvc++ will\n      // crash while compiling the TestScanByKeyWithLargeTypes test in\n      // thrust/testing/scan_by_key.cu:\n      auto v2      = second.value;\n      retval.value = op(first.value, v2);\n    }\n#else // not nvc++:\n    // if (second.key) {\n    //   The second partial reduction spans a segment reset, so it's value\n    //   aggregate becomes the running aggregate\n    // else {\n    //   The second partial reduction does not span a reset, so accumulate both\n    //   into the running aggregate\n    // } \n    retval.value = (second.key) ? second.value : op(first.value, second.value);\n#endif\n    return retval;\n  }\n};\n\n/**\n * @tparam ReductionOpT Binary reduction operator to apply to values\n */\ntemplate <typename ReductionOpT>\nstruct ReduceByKeyOp\n{\n  /// Wrapped reduction operator\n  ReductionOpT op;\n\n  /// Constructor\n  __host__ __device__ __forceinline__ ReduceByKeyOp() {}\n\n  /// Constructor\n  __host__ __device__ __forceinline__ ReduceByKeyOp(ReductionOpT op)\n      : op(op)\n  {}\n\n  /**\n   * @brief Scan operator\n   *\n   * @param[in] first First partial reduction\n   * @param[in] second Second partial reduction\n   */\n  template <typename KeyValuePairT>\n  __host__ __device__ __forceinline__ KeyValuePairT\n  operator()(const KeyValuePairT &first, const KeyValuePairT &second)\n  {\n    KeyValuePairT retval = second;\n\n    if (first.key == second.key)\n    {\n      retval.value = op(first.value, retval.value);\n    }\n\n    return retval;\n  }\n};\n\ntemplate <typename BinaryOpT>\nstruct BinaryFlip\n{\n  BinaryOpT binary_op;\n\n  __device__ __host__ explicit BinaryFlip(BinaryOpT binary_op)\n      : binary_op(binary_op)\n  {}\n\n  template <typename T, typename U>\n  __device__ auto\n  operator()(T &&t, U &&u) -> decltype(binary_op(::cuda::std::forward<U>(u),\n                                                 ::cuda::std::forward<T>(t)))\n  {\n    return binary_op(::cuda::std::forward<U>(u), ::cuda::std::forward<T>(t));\n  }\n};\n\ntemplate <typename BinaryOpT>\n__device__ __host__ BinaryFlip<BinaryOpT> MakeBinaryFlip(BinaryOpT binary_op)\n{\n  return BinaryFlip<BinaryOpT>(binary_op);\n}\n\n/** @} */       // end group UtilModule\n\n\nCUB_NAMESPACE_END\n\n#endif // _JITIFY_INCLUDE_GUARD_94E67823631CB18F\n", "cub/util_arch.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_99336375E0757819\n#define _JITIFY_INCLUDE_GUARD_99336375E0757819\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * Static architectural properties by SM version.\n */\n\n#include <cub/util_cpp_dialect.cuh>\n#include <cub/util_namespace.cuh>\n#include <cub/util_macro.cuh>\n\n// Legacy include; this functionality used to be defined in here.\n#include <cub/detail/detect_cuda_runtime.cuh>\n\nCUB_NAMESPACE_BEGIN\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS    // Do not document\n\n// \\deprecated [Since 2.1.0] \n#define CUB_USE_COOPERATIVE_GROUPS\n\n/// In device code, CUB_PTX_ARCH expands to the PTX version for which we are\n/// compiling. In host code, CUB_PTX_ARCH's value is implementation defined.\n#ifndef CUB_PTX_ARCH\n    #if defined(_NVHPC_CUDA)\n        // __NVCOMPILER_CUDA_ARCH__ is the target PTX version, and is defined\n        // when compiling both host code and device code. Currently, only one\n        // PTX version can be targeted.\n        #define CUB_PTX_ARCH __NVCOMPILER_CUDA_ARCH__\n    #elif !defined(__CUDA_ARCH__)\n        #define CUB_PTX_ARCH 0\n    #else\n        #define CUB_PTX_ARCH __CUDA_ARCH__\n    #endif\n#endif\n\n// These definitions were intended for internal use only and are now obsolete.\n// If you relied on them, consider porting your code to use the functionality\n// in libcu++'s <nv/target> header.\n// For a temporary workaround, define CUB_PROVIDE_LEGACY_ARCH_MACROS to make\n// them available again. These should be considered deprecated and will be\n// fully removed in a future version.\n#ifdef CUB_PROVIDE_LEGACY_ARCH_MACROS\n    #ifndef CUB_IS_DEVICE_CODE\n        #if defined(_NVHPC_CUDA)\n            #define CUB_IS_DEVICE_CODE __builtin_is_device_code()\n            #define CUB_IS_HOST_CODE (!__builtin_is_device_code())\n            #define CUB_INCLUDE_DEVICE_CODE 1\n            #define CUB_INCLUDE_HOST_CODE 1\n        #elif CUB_PTX_ARCH > 0\n            #define CUB_IS_DEVICE_CODE 1\n            #define CUB_IS_HOST_CODE 0\n            #define CUB_INCLUDE_DEVICE_CODE 1\n            #define CUB_INCLUDE_HOST_CODE 0\n        #else\n            #define CUB_IS_DEVICE_CODE 0\n            #define CUB_IS_HOST_CODE 1\n            #define CUB_INCLUDE_DEVICE_CODE 0\n            #define CUB_INCLUDE_HOST_CODE 1\n        #endif\n    #endif\n#endif // CUB_PROVIDE_LEGACY_ARCH_MACROS\n\n/// Maximum number of devices supported.\n#ifndef CUB_MAX_DEVICES\n    #define CUB_MAX_DEVICES (128)\n#endif\n\nstatic_assert(CUB_MAX_DEVICES > 0, \"CUB_MAX_DEVICES must be greater than 0.\");\n\n\n/// Number of threads per warp\n#ifndef CUB_LOG_WARP_THREADS\n    #define CUB_LOG_WARP_THREADS(unused) (5)\n    #define CUB_WARP_THREADS(unused) (1 << CUB_LOG_WARP_THREADS(0))\n\n    #define CUB_PTX_WARP_THREADS        CUB_WARP_THREADS(0)\n    #define CUB_PTX_LOG_WARP_THREADS    CUB_LOG_WARP_THREADS(0)\n#endif\n\n\n/// Number of smem banks\n#ifndef CUB_LOG_SMEM_BANKS\n    #define CUB_LOG_SMEM_BANKS(unused) (5)\n    #define CUB_SMEM_BANKS(unused) (1 << CUB_LOG_SMEM_BANKS(0))\n\n    #define CUB_PTX_LOG_SMEM_BANKS      CUB_LOG_SMEM_BANKS(0)\n    #define CUB_PTX_SMEM_BANKS          CUB_SMEM_BANKS\n#endif\n\n\n/// Oversubscription factor\n#ifndef CUB_SUBSCRIPTION_FACTOR\n    #define CUB_SUBSCRIPTION_FACTOR(unused) (5)\n    #define CUB_PTX_SUBSCRIPTION_FACTOR CUB_SUBSCRIPTION_FACTOR(0)\n#endif\n\n\n/// Prefer padding overhead vs X-way conflicts greater than this threshold\n#ifndef CUB_PREFER_CONFLICT_OVER_PADDING\n    #define CUB_PREFER_CONFLICT_OVER_PADDING(unused) (1)\n    #define CUB_PTX_PREFER_CONFLICT_OVER_PADDING CUB_PREFER_CONFLICT_OVER_PADDING(0)\n#endif\n\n\ntemplate <\n    int NOMINAL_4B_BLOCK_THREADS,\n    int NOMINAL_4B_ITEMS_PER_THREAD,\n    typename T>\nstruct RegBoundScaling\n{\n    enum {\n        ITEMS_PER_THREAD    = CUB_MAX(1, NOMINAL_4B_ITEMS_PER_THREAD * 4 / CUB_MAX(4, sizeof(T))),\n        BLOCK_THREADS       = CUB_MIN(NOMINAL_4B_BLOCK_THREADS, (((1024 * 48) / (sizeof(T) * ITEMS_PER_THREAD)) + 31) / 32 * 32),\n    };\n};\n\n\ntemplate <\n    int NOMINAL_4B_BLOCK_THREADS,\n    int NOMINAL_4B_ITEMS_PER_THREAD,\n    typename T>\nstruct MemBoundScaling\n{\n    enum {\n        ITEMS_PER_THREAD    = CUB_MAX(1, CUB_MIN(NOMINAL_4B_ITEMS_PER_THREAD * 4 / sizeof(T), NOMINAL_4B_ITEMS_PER_THREAD * 2)),\n        BLOCK_THREADS       = CUB_MIN(NOMINAL_4B_BLOCK_THREADS, (((1024 * 48) / (sizeof(T) * ITEMS_PER_THREAD)) + 31) / 32 * 32),\n    };\n};\n\n\n\n\n#endif  // Do not document\n\nCUB_NAMESPACE_END\n\n#endif // _JITIFY_INCLUDE_GUARD_99336375E0757819\n", "cub/util_compiler.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_1969ACC0820923D8\n#define _JITIFY_INCLUDE_GUARD_1969ACC0820923D8\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n *AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n *IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * Detect compiler information.\n */\n\n// enumerate host compilers we know about\n#define CUB_HOST_COMPILER_UNKNOWN 0\n#define CUB_HOST_COMPILER_MSVC 1\n#define CUB_HOST_COMPILER_GCC 2\n#define CUB_HOST_COMPILER_CLANG 3\n\n// enumerate device compilers we know about\n#define CUB_DEVICE_COMPILER_UNKNOWN 0\n#define CUB_DEVICE_COMPILER_MSVC 1\n#define CUB_DEVICE_COMPILER_GCC 2\n#define CUB_DEVICE_COMPILER_NVCC 3\n#define CUB_DEVICE_COMPILER_CLANG 4\n\n// figure out which host compiler we're using\n#if defined(_MSC_VER)\n#  define CUB_HOST_COMPILER CUB_HOST_COMPILER_MSVC\n#  define CUB_MSVC_VERSION _MSC_VER\n#  define CUB_MSVC_VERSION_FULL _MSC_FULL_VER\n#elif defined(__clang__)\n#  define CUB_HOST_COMPILER CUB_HOST_COMPILER_CLANG\n#  define CUB_CLANG_VERSION                                                    \\\n    (__clang_major__ * 10000 + __clang_minor__ * 100 + __clang_patchlevel__)\n#elif defined(__GNUC__)\n#  define CUB_HOST_COMPILER CUB_HOST_COMPILER_GCC\n#  define CUB_GCC_VERSION                                                      \\\n    (__GNUC__ * 10000 + __GNUC_MINOR__ * 100 + __GNUC_PATCHLEVEL__)\n#else\n#  define CUB_HOST_COMPILER CUB_HOST_COMPILER_UNKNOWN\n#endif // CUB_HOST_COMPILER\n\n// figure out which device compiler we're using\n#if defined(__CUDACC__) || defined(_NVHPC_CUDA)\n#  define CUB_DEVICE_COMPILER CUB_DEVICE_COMPILER_NVCC\n#elif CUB_HOST_COMPILER == CUB_HOST_COMPILER_MSVC\n#  define CUB_DEVICE_COMPILER CUB_DEVICE_COMPILER_MSVC\n#elif CUB_HOST_COMPILER == CUB_HOST_COMPILER_GCC\n#  define CUB_DEVICE_COMPILER CUB_DEVICE_COMPILER_GCC\n#elif CUB_HOST_COMPILER == CUB_HOST_COMPILER_CLANG\n// CUDA-capable clang should behave similar to NVCC.\n#  if defined(__CUDA__)\n#    define CUB_DEVICE_COMPILER CUB_DEVICE_COMPILER_NVCC\n#  else\n#    define CUB_DEVICE_COMPILER CUB_DEVICE_COMPILER_CLANG\n#  endif\n#else\n#  define CUB_DEVICE_COMPILER CUB_DEVICE_COMPILER_UNKNOWN\n#endif\n\n#endif // _JITIFY_INCLUDE_GUARD_1969ACC0820923D8\n", "cub/util_cpp_dialect.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_44D938EB20B46E5E\n#define _JITIFY_INCLUDE_GUARD_44D938EB20B46E5E\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n *AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n *IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/*! \\file\n *  \\brief Detect the version of the C++ standard used by the compiler.\n */\n\n#include \"util_compiler.cuh\"\n\n// Deprecation warnings may be silenced by defining the following macros. These\n// may be combined.\n// - CUB_IGNORE_DEPRECATED_CPP_DIALECT:\n//   Ignore all deprecated C++ dialects and outdated compilers.\n// - CUB_IGNORE_DEPRECATED_CPP_11:\n//   Ignore deprecation warnings when compiling with C++11. C++03 and outdated\n//   compilers will still issue warnings.\n// - CUB_IGNORE_DEPRECATED_COMPILER\n//   Ignore deprecation warnings when using deprecated compilers. Compiling\n//   with C++03 and C++11 will still issue warnings.\n\n// Check for the thrust opt-outs as well:\n#if !defined(CUB_IGNORE_DEPRECATED_CPP_DIALECT) && \\\n     defined(THRUST_IGNORE_DEPRECATED_CPP_DIALECT)\n#  define    CUB_IGNORE_DEPRECATED_CPP_DIALECT\n#endif\n#if !defined(CUB_IGNORE_DEPRECATED_CPP_11) && \\\n     defined(THRUST_IGNORE_DEPRECATED_CPP_11)\n#  define    CUB_IGNORE_DEPRECATED_CPP_11\n#endif\n#if !defined(CUB_IGNORE_DEPRECATED_COMPILER) && \\\n     defined(THRUST_IGNORE_DEPRECATED_COMPILER)\n#  define    CUB_IGNORE_DEPRECATED_COMPILER\n#endif\n\n#ifdef CUB_IGNORE_DEPRECATED_CPP_DIALECT\n#  define CUB_IGNORE_DEPRECATED_CPP_11\n#  define CUB_IGNORE_DEPRECATED_COMPILER\n#endif\n\n// Define this to override the built-in detection.\n#ifndef CUB_CPP_DIALECT\n\n// MSVC does not define __cplusplus correctly. _MSVC_LANG is used instead.\n// This macro is only defined in MSVC 2015U3+.\n#  ifdef _MSVC_LANG // Do not replace with CUB_HOST_COMPILER test (see above)\n// MSVC2015 reports C++14 but lacks extended constexpr support. Treat as C++11.\n#    if CUB_MSVC_VERSION < 1910 && _MSVC_LANG > 201103L /* MSVC < 2017 && CPP > 2011 */\n#      define CUB_CPLUSPLUS 201103L /* Fix to 2011 */\n#    else\n#      define CUB_CPLUSPLUS _MSVC_LANG /* We'll trust this for now. */\n#    endif // MSVC 2015 C++14 fix\n#  else\n#    define CUB_CPLUSPLUS __cplusplus\n#  endif\n\n// Detect current dialect:\n#  if CUB_CPLUSPLUS < 201103L\n#    define CUB_CPP_DIALECT 2003\n#  elif CUB_CPLUSPLUS < 201402L\n#    define CUB_CPP_DIALECT 2011\n#  elif CUB_CPLUSPLUS < 201703L\n#    define CUB_CPP_DIALECT 2014\n#  elif CUB_CPLUSPLUS == 201703L\n#    define CUB_CPP_DIALECT 2017\n#  elif CUB_CPLUSPLUS > 201703L // unknown, but is higher than 2017.\n#    define CUB_CPP_DIALECT 2020\n#  endif\n\n#  undef CUB_CPLUSPLUS // cleanup\n\n#endif // !CUB_CPP_DIALECT\n\n// Define CUB_COMPILER_DEPRECATION macro:\n#if CUB_HOST_COMPILER == CUB_HOST_COMPILER_MSVC\n#  define CUB_COMP_DEPR_IMPL(msg) \\\n    __pragma(message(__FILE__ \":\" CUB_COMP_DEPR_IMPL0(__LINE__) \": warning: \" #msg))\n#  define CUB_COMP_DEPR_IMPL0(x) CUB_COMP_DEPR_IMPL1(x)\n#  define CUB_COMP_DEPR_IMPL1(x) #x\n#else // clang / gcc:\n#  define CUB_COMP_DEPR_IMPL(msg) CUB_COMP_DEPR_IMPL0(GCC warning #msg)\n#  define CUB_COMP_DEPR_IMPL0(expr) _Pragma(#expr)\n#  define CUB_COMP_DEPR_IMPL1 /* intentionally blank */\n#endif\n\n#define CUB_COMPILER_DEPRECATION(REQ) \\\n  CUB_COMP_DEPR_IMPL(CUB requires at least REQ. Define CUB_IGNORE_DEPRECATED_CPP_DIALECT to suppress this message.)\n\n#define CUB_COMPILER_DEPRECATION_SOFT(REQ, CUR) \\\n  CUB_COMP_DEPR_IMPL(CUB requires at least REQ. CUR is deprecated but still supported. CUR support will be removed in a future release. Define CUB_IGNORE_DEPRECATED_CPP_DIALECT to suppress this message.)\n\n#ifndef CUB_IGNORE_DEPRECATED_COMPILER\n\n// Compiler checks:\n#  if CUB_HOST_COMPILER == CUB_HOST_COMPILER_GCC && CUB_GCC_VERSION < 50000\n     CUB_COMPILER_DEPRECATION(GCC 5.0);\n#  elif CUB_HOST_COMPILER == CUB_HOST_COMPILER_CLANG && CUB_CLANG_VERSION < 70000\n     CUB_COMPILER_DEPRECATION(Clang 7.0);\n#  elif CUB_HOST_COMPILER == CUB_HOST_COMPILER_MSVC && CUB_MSVC_VERSION < 1910\n     // <2017. Hard upgrade message:\n     CUB_COMPILER_DEPRECATION(MSVC 2019 (19.20/16.0/14.20));\n#  elif CUB_HOST_COMPILER == CUB_HOST_COMPILER_MSVC && CUB_MSVC_VERSION < 1920\n     // >=2017, <2019. Soft deprecation message:\n     CUB_COMPILER_DEPRECATION_SOFT(MSVC 2019 (19.20/16.0/14.20), MSVC 2017);\n#  endif\n\n#endif // CUB_IGNORE_DEPRECATED_COMPILER\n\n#ifndef CUB_IGNORE_DEPRECATED_DIALECT\n\n// Dialect checks:\n#  if CUB_CPP_DIALECT < 2011\n     // <C++11. Hard upgrade message:\n     CUB_COMPILER_DEPRECATION(C++14);\n#  elif CUB_CPP_DIALECT == 2011 && !defined(CUB_IGNORE_DEPRECATED_CPP_11)\n     // =C++11. Soft upgrade message:\n     CUB_COMPILER_DEPRECATION_SOFT(C++14, C++11);\n#  endif\n\n#endif // CUB_IGNORE_DEPRECATED_DIALECT\n\n#undef CUB_COMPILER_DEPRECATION_SOFT\n#undef CUB_COMPILER_DEPRECATION\n#undef CUB_COMP_DEPR_IMPL\n#undef CUB_COMP_DEPR_IMPL0\n#undef CUB_COMP_DEPR_IMPL1\n\n#endif // _JITIFY_INCLUDE_GUARD_44D938EB20B46E5E\n", "cub/util_debug.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_E187B871CF4D598D\n#define _JITIFY_INCLUDE_GUARD_E187B871CF4D598D\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2022, NVIDIA CORPORATION.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * Error and event logging routines.\n *\n * The following macros definitions are supported:\n * - \\p CUB_LOG.  Simple event messages are printed to \\p stdout.\n */\n\n#include <cub/util_namespace.cuh>\n#include <cub/util_arch.cuh>\n\n#include <nv/target>\n\n#include <cstdio>\n\nCUB_NAMESPACE_BEGIN\n\n\n#ifdef DOXYGEN_SHOULD_SKIP_THIS // Only parse this during doxygen passes:\n\n/**\n * @def CUB_DEBUG_LOG\n *\n * Causes kernel launch configurations to be printed to the console\n */\n#define CUB_DEBUG_LOG\n\n/**\n * @def CUB_DEBUG_SYNC\n *\n * Causes synchronization of the stream after every kernel launch to check \n * for errors. Also causes kernel launch configurations to be printed to the \n * console.\n */\n#define CUB_DEBUG_SYNC\n\n/**\n * @def CUB_DEBUG_HOST_ASSERTIONS\n *\n * Extends `CUB_DEBUG_SYNC` effects by checking host-side precondition \n * assertions.\n */\n#define CUB_DEBUG_HOST_ASSERTIONS\n\n/**\n * @def CUB_DEBUG_DEVICE_ASSERTIONS\n *\n * Extends `CUB_DEBUG_HOST_ASSERTIONS` effects by checking device-side \n * precondition assertions.\n */\n#define CUB_DEBUG_DEVICE_ASSERTIONS\n\n/**\n * @def CUB_DEBUG_ALL\n *\n * Causes host and device-side precondition assertions to be checked. Apart \n * from that, causes synchronization of the stream after every kernel launch to \n * check for errors. Also causes kernel launch configurations to be printed to \n * the console.\n */\n#define CUB_DEBUG_ALL\n\n#endif // DOXYGEN_SHOULD_SKIP_THIS \n\n/**\n * \\addtogroup UtilMgmt\n * @{\n */\n\n\n// `CUB_DETAIL_DEBUG_LEVEL_*`: Implementation details, internal use only:\n\n#define CUB_DETAIL_DEBUG_LEVEL_NONE 0\n#define CUB_DETAIL_DEBUG_LEVEL_HOST_ASSERTIONS_ONLY 1\n#define CUB_DETAIL_DEBUG_LEVEL_LOG 2\n#define CUB_DETAIL_DEBUG_LEVEL_SYNC 3\n#define CUB_DETAIL_DEBUG_LEVEL_HOST_ASSERTIONS 4\n#define CUB_DETAIL_DEBUG_LEVEL_DEVICE_ASSERTIONS 5\n#define CUB_DETAIL_DEBUG_LEVEL_ALL 1000\n\n// `CUB_DEBUG_*`: User interfaces:\n\n// Extra logging, no syncs\n#ifdef CUB_DEBUG_LOG\n#define CUB_DETAIL_DEBUG_LEVEL CUB_DETAIL_DEBUG_LEVEL_LOG\n#endif\n\n// Logging + syncs\n#ifdef CUB_DEBUG_SYNC\n#define CUB_DETAIL_DEBUG_LEVEL CUB_DETAIL_DEBUG_LEVEL_SYNC\n#endif\n\n// Logging + syncs + host assertions\n#ifdef CUB_DEBUG_HOST_ASSERTIONS\n#define CUB_DETAIL_DEBUG_LEVEL CUB_DETAIL_DEBUG_LEVEL_HOST_ASSERTIONS\n#endif\n\n// Logging + syncs + host assertions + device assertions\n#ifdef CUB_DEBUG_DEVICE_ASSERTIONS\n#define CUB_DETAIL_DEBUG_LEVEL CUB_DETAIL_DEBUG_LEVEL_DEVICE_ASSERTIONS\n#endif\n\n// All\n#ifdef CUB_DEBUG_ALL\n#define CUB_DETAIL_DEBUG_LEVEL CUB_DETAIL_DEBUG_LEVEL_ALL \n#endif\n\n// Default case, no extra debugging:\n#ifndef CUB_DETAIL_DEBUG_LEVEL\n#ifdef NDEBUG\n#define CUB_DETAIL_DEBUG_LEVEL CUB_DETAIL_DEBUG_LEVEL_NONE\n#else\n#define CUB_DETAIL_DEBUG_LEVEL CUB_DETAIL_DEBUG_LEVEL_HOST_ASSERTIONS_ONLY\n#endif\n#endif\n\n/*\n * `CUB_DETAIL_DEBUG_ENABLE_*`:\n * Internal implementation details, used for testing enabled debug features:\n */\n\n#if CUB_DETAIL_DEBUG_LEVEL >= CUB_DETAIL_DEBUG_LEVEL_LOG\n#define CUB_DETAIL_DEBUG_ENABLE_LOG\n#endif\n\n#if CUB_DETAIL_DEBUG_LEVEL >= CUB_DETAIL_DEBUG_LEVEL_SYNC\n#define CUB_DETAIL_DEBUG_ENABLE_SYNC\n#endif\n\n#if (CUB_DETAIL_DEBUG_LEVEL >= CUB_DETAIL_DEBUG_LEVEL_HOST_ASSERTIONS) || \\\n    (CUB_DETAIL_DEBUG_LEVEL == CUB_DETAIL_DEBUG_LEVEL_HOST_ASSERTIONS_ONLY)\n#define CUB_DETAIL_DEBUG_ENABLE_HOST_ASSERTIONS\n#endif\n\n#if CUB_DETAIL_DEBUG_LEVEL >= CUB_DETAIL_DEBUG_LEVEL_DEVICE_ASSERTIONS\n#define CUB_DETAIL_DEBUG_ENABLE_DEVICE_ASSERTIONS\n#endif\n\n\n/// CUB error reporting macro (prints error messages to stderr)\n#if (defined(DEBUG) || defined(_DEBUG)) && !defined(CUB_STDERR)\n    #define CUB_STDERR\n#endif\n\n/**\n * \\brief %If \\p CUB_STDERR is defined and \\p error is not \\p cudaSuccess, the\n * corresponding error message is printed to \\p stderr (or \\p stdout in device\n * code) along with the supplied source context.\n *\n * \\return The CUDA error.\n */\n__host__ __device__\n__forceinline__\ncudaError_t Debug(cudaError_t error, const char *filename, int line)\n{\n  // Clear the global CUDA error state which may have been set by the last\n  // call. Otherwise, errors may \"leak\" to unrelated kernel launches.\n\n  // clang-format off\n  #ifndef CUB_RDC_ENABLED\n  #define CUB_TEMP_DEVICE_CODE\n  #else\n  #define CUB_TEMP_DEVICE_CODE cudaGetLastError()\n  #endif\n\n  NV_IF_TARGET(\n    NV_IS_HOST, \n    (cudaGetLastError();),\n    (CUB_TEMP_DEVICE_CODE;)\n  );\n  \n  #undef CUB_TEMP_DEVICE_CODE\n  // clang-format on\n\n#ifdef CUB_STDERR\n  if (error)\n  {\n    NV_IF_TARGET(\n      NV_IS_HOST, (\n        fprintf(stderr,\n                \"CUDA error %d [%s, %d]: %s\\n\",\n                error,\n                filename,\n                line,\n                cudaGetErrorString(error));\n        fflush(stderr);\n      ),\n      (\n        printf(\"CUDA error %d [block (%d,%d,%d) thread (%d,%d,%d), %s, %d]\\n\",\n               error,\n               blockIdx.z,\n               blockIdx.y,\n               blockIdx.x,\n               threadIdx.z,\n               threadIdx.y,\n               threadIdx.x,\n               filename,\n               line);\n      )\n    );\n  }\n#else\n  (void)filename;\n  (void)line;\n#endif\n\n  return error;\n}\n\n/**\n * \\brief Debug macro\n */\n#ifndef CubDebug\n    #define CubDebug(e) CUB_NS_QUALIFIER::Debug((cudaError_t) (e), __FILE__, __LINE__)\n#endif\n\n\n/**\n * \\brief Debug macro with exit\n */\n#ifndef CubDebugExit\n    #define CubDebugExit(e) if (CUB_NS_QUALIFIER::Debug((cudaError_t) (e), __FILE__, __LINE__)) { exit(1); }\n#endif\n\n\n/**\n * \\brief Log macro for printf statements.\n */\n#if !defined(_CubLog)\n#if defined(_NVHPC_CUDA) || !(defined(__clang__) && defined(__CUDA__))\n\n// NVCC / NVC++\n#define _CubLog(format, ...)                                                   \\\n  do                                                                           \\\n  {                                                                            \\\n    NV_IF_TARGET(NV_IS_HOST,                                                   \\\n                 (printf(format, __VA_ARGS__);),                               \\\n                 (printf(\"[block (%d,%d,%d), thread (%d,%d,%d)]: \" format,     \\\n                         blockIdx.z,                                           \\\n                         blockIdx.y,                                           \\\n                         blockIdx.x,                                           \\\n                         threadIdx.z,                                          \\\n                         threadIdx.y,                                          \\\n                         threadIdx.x,                                          \\\n                         __VA_ARGS__);));                                      \\\n  } while (false)\n\n#else // Clang:\n\n// XXX shameless hack for clang around variadic printf...\n//     Compilies w/o supplying -std=c++11 but shows warning,\n//     so we silence them :)\n_Pragma(\"clang diagnostic ignored \\\"-Wc++11-extensions\\\"\")\n_Pragma(\"clang diagnostic ignored \\\"-Wunnamed-type-template-args\\\"\")\ntemplate <class... Args>\ninline __host__ __device__ void va_printf(char const *format,\n                                          Args const &...args)\n{\n#ifdef __CUDA_ARCH__\n  printf(format,\n         blockIdx.z,\n         blockIdx.y,\n         blockIdx.x,\n         threadIdx.z,\n         threadIdx.y,\n         threadIdx.x,\n         args...);\n#else\n  printf(format, args...);\n#endif\n}\n#ifndef __CUDA_ARCH__\n#define _CubLog(format, ...) CUB_NS_QUALIFIER::va_printf(format, __VA_ARGS__);\n#else\n#define _CubLog(format, ...)                                                   \\\n  CUB_NS_QUALIFIER::va_printf(\"[block (%d,%d,%d), thread \"                     \\\n                              \"(%d,%d,%d)]: \" format,                          \\\n                              __VA_ARGS__);\n#endif\n#endif\n#endif\n\n/** @} */       // end group UtilMgmt\n\nCUB_NAMESPACE_END\n\n#endif // _JITIFY_INCLUDE_GUARD_E187B871CF4D598D\n", "cub/util_deprecated.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_D74F77C0B5838493\n#define _JITIFY_INCLUDE_GUARD_D74F77C0B5838493\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n *AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n *IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * Define CUB_DEPRECATED macro.\n */\n\n\n#include <cub/detail/type_traits.cuh>\n#include <cub/util_compiler.cuh>\n#include <cub/util_cpp_dialect.cuh>\n#include <cub/util_debug.cuh>\n\n\n#if defined(THRUST_IGNORE_DEPRECATED_API) && !defined(CUB_IGNORE_DEPRECATED_API)\n#  define CUB_IGNORE_DEPRECATED_API\n#endif\n\n#ifdef CUB_IGNORE_DEPRECATED_API\n#  define CUB_DEPRECATED\n#  define CUB_DEPRECATED_BECAUSE(MSG)\n#elif CUB_CPP_DIALECT >= 2014\n#  define CUB_DEPRECATED [[deprecated]]\n#  define CUB_DEPRECATED_BECAUSE(MSG) [[deprecated(MSG)]]\n#elif CUB_HOST_COMPILER == CUB_HOST_COMPILER_MSVC\n#  define CUB_DEPRECATED __declspec(deprecated)\n#  define CUB_DEPRECATED_BECAUSE(MSG) __declspec(deprecated(MSG))\n#elif CUB_HOST_COMPILER == CUB_HOST_COMPILER_CLANG\n#  define CUB_DEPRECATED __attribute__((deprecated))\n#  define CUB_DEPRECATED_BECAUSE(MSG) __attribute__((deprecated(MSG)))\n#elif CUB_HOST_COMPILER == CUB_HOST_COMPILER_GCC\n#  define CUB_DEPRECATED __attribute__((deprecated))\n#  define CUB_DEPRECATED_BECAUSE(MSG) __attribute__((deprecated(MSG)))\n#else\n#  define CUB_DEPRECATED\n#  define CUB_DEPRECATED_BECAUSE(MSG)\n#endif\n\n#define CUB_DETAIL_RUNTIME_DEBUG_SYNC_IS_NOT_SUPPORTED                         \\\n  CUB_DEPRECATED_BECAUSE(                                                      \\\n    \"CUB no longer accepts `debug_synchronous` parameter. \"                    \\\n    \"Define CUB_DEBUG_SYNC instead, or silence this message with \"             \\\n    \"CUB_IGNORE_DEPRECATED_API.\")\n\n#define CUB_DETAIL_RUNTIME_DEBUG_SYNC_USAGE_LOG                                \\\n  if (debug_synchronous)                                                       \\\n  {                                                                            \\\n    _CubLog(\"%s\\n\",                                                            \\\n            \"CUB no longer accepts `debug_synchronous` parameter. \"            \\\n            \"Define CUB_DEBUG_SYNC instead.\");                                 \\\n  }\n\n\n#endif // _JITIFY_INCLUDE_GUARD_D74F77C0B5838493\n", "cub/util_macro.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_45CEB8529D0A8BEE\n#define _JITIFY_INCLUDE_GUARD_45CEB8529D0A8BEE\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.\n * \n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n * \n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/******************************************************************************\n * Common C/C++ macro utilities\n ******************************************************************************/\n\n#include <cuda/std/utility>\n\n#include \"util_namespace.cuh\"\n\nCUB_NAMESPACE_BEGIN\n\n\n/**\n * \\addtogroup UtilModule\n * @{\n */\n\n#ifndef CUB_ALIGN\n    #if defined(_WIN32) || defined(_WIN64)\n        /// Align struct\n        #define CUB_ALIGN(bytes) __declspec(align(32))\n    #else\n        /// Align struct\n        #define CUB_ALIGN(bytes) __attribute__((aligned(bytes)))\n    #endif\n#endif\n\n#define CUB_PREVENT_MACRO_SUBSTITUTION\n\ntemplate <typename T, typename U>\nconstexpr __host__ __device__ auto min CUB_PREVENT_MACRO_SUBSTITUTION(T &&t,\n                                                                      U &&u)\n  -> decltype(t < u ? ::cuda::std::forward<T>(t) : ::cuda::std::forward<U>(u))\n{\n  return t < u ? ::cuda::std::forward<T>(t) : ::cuda::std::forward<U>(u);\n}\n\ntemplate <typename T, typename U>\nconstexpr __host__ __device__ auto max CUB_PREVENT_MACRO_SUBSTITUTION(T &&t,\n                                                                      U &&u)\n  -> decltype(t < u ? ::cuda::std::forward<U>(u) : ::cuda::std::forward<T>(t))\n{\n  return t < u ? ::cuda::std::forward<U>(u) : ::cuda::std::forward<T>(t);\n}\n\n#ifndef CUB_MAX\n    /// Select maximum(a, b)\n    #define CUB_MAX(a, b) (((b) > (a)) ? (b) : (a))\n#endif\n\n#ifndef CUB_MIN\n    /// Select minimum(a, b)\n    #define CUB_MIN(a, b) (((b) < (a)) ? (b) : (a))\n#endif\n\n#ifndef CUB_QUOTIENT_FLOOR\n    /// Quotient of x/y rounded down to nearest integer\n    #define CUB_QUOTIENT_FLOOR(x, y) ((x) / (y))\n#endif\n\n#ifndef CUB_QUOTIENT_CEILING\n    /// Quotient of x/y rounded up to nearest integer\n    #define CUB_QUOTIENT_CEILING(x, y) (((x) + (y) - 1) / (y))\n#endif\n\n#ifndef CUB_ROUND_UP_NEAREST\n    /// x rounded up to the nearest multiple of y\n    #define CUB_ROUND_UP_NEAREST(x, y) ((((x) + (y) - 1) / (y)) * y)\n#endif\n\n#ifndef CUB_ROUND_DOWN_NEAREST\n    /// x rounded down to the nearest multiple of y\n    #define CUB_ROUND_DOWN_NEAREST(x, y) (((x) / (y)) * y)\n#endif\n\n\n#ifndef CUB_STATIC_ASSERT\n    #ifndef DOXYGEN_SHOULD_SKIP_THIS    // Do not document\n        #define CUB_CAT_(a, b) a ## b\n        #define CUB_CAT(a, b) CUB_CAT_(a, b)\n    #endif // DOXYGEN_SHOULD_SKIP_THIS\n\n    /// Static assert\n    #define CUB_STATIC_ASSERT(cond, msg) typedef int CUB_CAT(cub_static_assert, __LINE__)[(cond) ? 1 : -1]\n#endif\n\n/** @} */       // end group UtilModule\n\nCUB_NAMESPACE_END\n\n#endif // _JITIFY_INCLUDE_GUARD_45CEB8529D0A8BEE\n", "cub/util_namespace.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_AE4EE898DA832DA0\n#define _JITIFY_INCLUDE_GUARD_AE4EE898DA832DA0\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file util_namespace.cuh\n * \\brief Utilities that allow `cub::` to be placed inside an\n * application-specific namespace.\n */\n\n\n// This is not used by this file; this is a hack so that we can detect the\n// CUB version from Thrust on older versions of CUB that did not have\n// version.cuh.\n#include \"version.cuh\"\n\n// Prior to 1.13.1, only the PREFIX/POSTFIX macros were used. Notify users\n// that they must now define the qualifier macro, too.\n#if (defined(CUB_NS_PREFIX) || defined(CUB_NS_POSTFIX)) && !defined(CUB_NS_QUALIFIER)\n#error CUB requires a definition of CUB_NS_QUALIFIER when CUB_NS_PREFIX/POSTFIX are defined.\n#endif\n\n/**\n * \\def THRUST_CUB_WRAPPED_NAMESPACE\n * If defined, this value will be used as the name of a namespace that wraps the\n * `thrust::` and `cub::` namespaces.\n * This macro should not be used with any other CUB namespace macros.\n */\n#ifdef THRUST_CUB_WRAPPED_NAMESPACE\n#define CUB_WRAPPED_NAMESPACE THRUST_CUB_WRAPPED_NAMESPACE\n#endif\n\n/**\n * \\def CUB_WRAPPED_NAMESPACE\n * If defined, this value will be used as the name of a namespace that wraps the\n * `cub::` namespace.\n * If THRUST_CUB_WRAPPED_NAMESPACE is set, this will inherit that macro's value.\n * This macro should not be used with any other CUB namespace macros.\n */\n#ifdef CUB_WRAPPED_NAMESPACE\n#define CUB_NS_PREFIX                                                       \\\n  namespace CUB_WRAPPED_NAMESPACE                                           \\\n  {\n\n#define CUB_NS_POSTFIX }\n\n#define CUB_NS_QUALIFIER ::CUB_WRAPPED_NAMESPACE::cub\n#endif\n\n/**\n * \\def CUB_NS_PREFIX\n * This macro is inserted prior to all `namespace cub { ... }` blocks. It is\n * derived from CUB_WRAPPED_NAMESPACE, if set, and will be empty otherwise.\n * It may be defined by users, in which case CUB_NS_PREFIX,\n * CUB_NS_POSTFIX, and CUB_NS_QUALIFIER must all be set consistently.\n */\n#ifndef CUB_NS_PREFIX\n#define CUB_NS_PREFIX\n#endif\n\n/**\n * \\def CUB_NS_POSTFIX\n * This macro is inserted following the closing braces of all\n * `namespace cub { ... }` block. It is defined appropriately when\n * CUB_WRAPPED_NAMESPACE is set, and will be empty otherwise. It may be\n * defined by users, in which case CUB_NS_PREFIX, CUB_NS_POSTFIX, and\n * CUB_NS_QUALIFIER must all be set consistently.\n */\n#ifndef CUB_NS_POSTFIX\n#define CUB_NS_POSTFIX\n#endif\n\n/**\n * \\def CUB_NS_QUALIFIER\n * This macro is used to qualify members of cub:: when accessing them from\n * outside of their namespace. By default, this is just `::cub`, and will be\n * set appropriately when CUB_WRAPPED_NAMESPACE is defined. This macro may be\n * defined by users, in which case CUB_NS_PREFIX, CUB_NS_POSTFIX, and\n * CUB_NS_QUALIFIER must all be set consistently.\n */\n#ifndef CUB_NS_QUALIFIER\n#define CUB_NS_QUALIFIER ::cub\n#endif\n\n#if !defined(CUB_DETAIL_MAGIC_NS_NAME)\n#define CUB_DETAIL_COUNT_N(_1, _2, _3, _4, _5, _6, _7, _8, _9, _10, _11, _12, _13, \\\n                           _14, _15, _16, _17, _18, _19, _20, N, ...)              \\\n                           N\n#define CUB_DETAIL_COUNT(...)                                                      \\\n  CUB_DETAIL_IDENTITY(CUB_DETAIL_COUNT_N(__VA_ARGS__, 20, 19, 18, 17, 16, 15, 14, 13, 12, \\\n                                         11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1))\n#define CUB_DETAIL_IDENTITY(N) N\n#define CUB_DETAIL_APPLY(MACRO, ...) CUB_DETAIL_IDENTITY(MACRO(__VA_ARGS__))\n#define CUB_DETAIL_MAGIC_NS_NAME1(P1) \\\n    CUB_##P1##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME2(P1, P2) \\\n    CUB_##P1##_##P2##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME3(P1, P2, P3) \\\n    CUB_##P1##_##P2##_##P3##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME4(P1, P2, P3, P4) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME5(P1, P2, P3, P4, P5) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_##P5##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME6(P1, P2, P3, P4, P5, P6) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_##P5##_##P6##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME7(P1, P2, P3, P4, P5, P6, P7) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_##P5##_##P6##_##P7##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME8(P1, P2, P3, P4, P5, P6, P7, P8) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_##P5##_##P6##_##P7##_##P8##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME9(P1, P2, P3, P4, P5, P6, P7, P8, P9) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_##P5##_##P6##_##P7##_##P8##_##P9##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME10(P1, P2, P3, P4, P5, P6, P7, P8, P9, P10) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_##P5##_##P6##_##P7##_##P8##_##P9##_##P10##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME11(P1, P2, P3, P4, P5, P6, P7, P8, P9, P10, P11) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_##P5##_##P6##_##P7##_##P8##_##P9##_##P10##_##P11##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME12(P1, P2, P3, P4, P5, P6, P7, P8, P9, P10, P11, P12) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_##P5##_##P6##_##P7##_##P8##_##P9##_##P10##_##P11##_##P12##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME13(P1, P2, P3, P4, P5, P6, P7, P8, P9, P10, P11, P12, P13) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_##P5##_##P6##_##P7##_##P8##_##P9##_##P10##_##P11##_##P12##_##P13##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME14(P1, P2, P3, P4, P5, P6, P7, P8, P9, P10, P11, P12, P13, P14) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_##P5##_##P6##_##P7##_##P8##_##P9##_##P10##_##P11##_##P12##_##P13##_##P14##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME15(P1, P2, P3, P4, P5, P6, P7, P8, P9, P10, P11, P12, P13, P14, P15) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_##P5##_##P6##_##P7##_##P8##_##P9##_##P10##_##P11##_##P12##_##P13##_##P14##_##P15##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME16(P1, P2, P3, P4, P5, P6, P7, P8, P9, P10, P11, P12, P13, P14, P15, P16) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_##P5##_##P6##_##P7##_##P8##_##P9##_##P10##_##P11##_##P12##_##P13##_##P14##_##P15##_##P16##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME17(P1, P2, P3, P4, P5, P6, P7, P8, P9, P10, P11, P12, P13, P14, P15, P16, P17) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_##P5##_##P6##_##P7##_##P8##_##P9##_##P10##_##P11##_##P12##_##P13##_##P14##_##P15##_##P16##_##P17##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME18(P1, P2, P3, P4, P5, P6, P7, P8, P9, P10, P11, P12, P13, P14, P15, P16, P17, P18) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_##P5##_##P6##_##P7##_##P8##_##P9##_##P10##_##P11##_##P12##_##P13##_##P14##_##P15##_##P16##_##P17##_##P18##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME19(P1, P2, P3, P4, P5, P6, P7, P8, P9, P10, P11, P12, P13, P14, P15, P16, P17, P18, P19) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_##P5##_##P6##_##P7##_##P8##_##P9##_##P10##_##P11##_##P12##_##P13##_##P14##_##P15##_##P16##_##P17##_##P18##_##P19##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME20(P1, P2, P3, P4, P5, P6, P7, P8, P9, P10, P11, P12, P13, P14, P15, P16, P17, P18, P19, P20) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_##P5##_##P6##_##P7##_##P8##_##P9##_##P10##_##P11##_##P12##_##P13##_##P14##_##P15##_##P16##_##P17##_##P18##_##P19##_##P20##_NS\n#define CUB_DETAIL_DISPATCH(N) CUB_DETAIL_MAGIC_NS_NAME ## N\n#define CUB_DETAIL_MAGIC_NS_NAME(...) CUB_DETAIL_IDENTITY(CUB_DETAIL_APPLY(CUB_DETAIL_DISPATCH, CUB_DETAIL_COUNT(__VA_ARGS__))(__VA_ARGS__))\n#endif // !defined(CUB_DETAIL_MAGIC_NS_NAME)\n\n#if defined(CUB_DISABLE_NAMESPACE_MAGIC)\n#if !defined(CUB_WRAPPED_NAMESPACE)\n#if !defined(CUB_IGNORE_NAMESPACE_MAGIC_ERROR)\n#error \"Disabling namespace magic is unsafe without wrapping namespace\"\n#endif // !defined(CUB_IGNORE_NAMESPACE_MAGIC_ERROR)\n#endif // !defined(CUB_WRAPPED_NAMESPACE)\n#define CUB_DETAIL_MAGIC_NS_BEGIN\n#define CUB_DETAIL_MAGIC_NS_END\n#else // not defined(CUB_DISABLE_NAMESPACE_MAGIC)\n#if defined(_NVHPC_CUDA)\n#define CUB_DETAIL_MAGIC_NS_BEGIN inline namespace CUB_DETAIL_MAGIC_NS_NAME(CUB_VERSION, NV_TARGET_SM_INTEGER_LIST) {\n#define CUB_DETAIL_MAGIC_NS_END }\n#else // not defined(_NVHPC_CUDA)\n#define CUB_DETAIL_MAGIC_NS_BEGIN inline namespace CUB_DETAIL_MAGIC_NS_NAME(CUB_VERSION, __CUDA_ARCH_LIST__) {\n#define CUB_DETAIL_MAGIC_NS_END }\n#endif // not defined(_NVHPC_CUDA)\n#endif // not defined(CUB_DISABLE_NAMESPACE_MAGIC)\n\n/**\n * \\def CUB_NAMESPACE_BEGIN\n * This macro is used to open a `cub::` namespace block, along with any\n * enclosing namespaces requested by CUB_WRAPPED_NAMESPACE, etc.\n * This macro is defined by CUB and may not be overridden.\n */\n#define CUB_NAMESPACE_BEGIN                                                 \\\n  CUB_NS_PREFIX                                                             \\\n  namespace cub                                                             \\\n  {                                                                         \\\n  CUB_DETAIL_MAGIC_NS_BEGIN                                                        \n\n/**\n * \\def CUB_NAMESPACE_END\n * This macro is used to close a `cub::` namespace block, along with any\n * enclosing namespaces requested by CUB_WRAPPED_NAMESPACE, etc.\n * This macro is defined by CUB and may not be overridden.\n */\n#define CUB_NAMESPACE_END                                                   \\\n  CUB_DETAIL_MAGIC_NS_END                                                   \\\n  } /* end namespace cub */                                                 \\\n  CUB_NS_POSTFIX\n\n// Declare these namespaces here for the purpose of Doxygenating them\nCUB_NS_PREFIX\n\n/*! \\namespace cub\n *  \\brief \\p cub is the top-level namespace which contains all CUB\n *         functions and types.\n */\nnamespace cub\n{\n}\n\nCUB_NS_POSTFIX\n\n#endif // _JITIFY_INCLUDE_GUARD_AE4EE898DA832DA0\n", "cub/util_ptx.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_8EAE810BB36A0FF0\n#define _JITIFY_INCLUDE_GUARD_8EAE810BB36A0FF0\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.\n * \n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n * \n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * PTX intrinsics\n */\n\n\n#include \"util_type.cuh\"\n#include \"util_arch.cuh\"\n#include \"util_namespace.cuh\"\n#include \"util_debug.cuh\"\n\n\nCUB_NAMESPACE_BEGIN\n\n\n/**\n * \\addtogroup UtilPtx\n * @{\n */\n\n\n/******************************************************************************\n * PTX helper macros\n ******************************************************************************/\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS    // Do not document\n\n/**\n * Register modifier for pointer-types (for inlining PTX assembly)\n */\n#if defined(_WIN64) || defined(__LP64__)\n    #define __CUB_LP64__ 1\n    // 64-bit register modifier for inlined asm\n    #define _CUB_ASM_PTR_ \"l\"\n    #define _CUB_ASM_PTR_SIZE_ \"u64\"\n#else\n    #define __CUB_LP64__ 0\n    // 32-bit register modifier for inlined asm\n    #define _CUB_ASM_PTR_ \"r\"\n    #define _CUB_ASM_PTR_SIZE_ \"u32\"\n#endif\n\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\n\n/******************************************************************************\n * Inlined PTX intrinsics\n ******************************************************************************/\n\nnamespace detail\n{\n/**\n * @brief Shifts @p val left by the amount specified by unsigned 32-bit value in @p num_bits. If @p\n * num_bits is larger than 32 bits, @p num_bits is clamped to 32.\n */\n__device__ __forceinline__ uint32_t LogicShiftLeft(uint32_t val, uint32_t num_bits)\n{\n  uint32_t ret{};\n  asm(\"shl.b32 %0, %1, %2;\" : \"=r\"(ret) : \"r\"(val), \"r\"(num_bits));\n  return ret;\n}\n\n/**\n * @brief Shifts @p val right by the amount specified by unsigned 32-bit value in @p num_bits. If @p\n * num_bits is larger than 32 bits, @p num_bits is clamped to 32.\n */\n__device__ __forceinline__ uint32_t LogicShiftRight(uint32_t val, uint32_t num_bits)\n{\n  uint32_t ret{};\n  asm(\"shr.b32 %0, %1, %2;\" : \"=r\"(ret) : \"r\"(val), \"r\"(num_bits));\n  return ret;\n}\n} // namespace detail\n\n/**\n * \\brief Shift-right then add.  Returns (\\p x >> \\p shift) + \\p addend.\n */\n__device__ __forceinline__ unsigned int SHR_ADD(\n    unsigned int x,\n    unsigned int shift,\n    unsigned int addend)\n{\n    unsigned int ret;\n    asm (\"vshr.u32.u32.u32.clamp.add %0, %1, %2, %3;\" :\n        \"=r\"(ret) : \"r\"(x), \"r\"(shift), \"r\"(addend));\n    return ret;\n}\n\n\n/**\n * \\brief Shift-left then add.  Returns (\\p x << \\p shift) + \\p addend.\n */\n__device__ __forceinline__ unsigned int SHL_ADD(\n    unsigned int x,\n    unsigned int shift,\n    unsigned int addend)\n{\n    unsigned int ret;\n    asm (\"vshl.u32.u32.u32.clamp.add %0, %1, %2, %3;\" :\n        \"=r\"(ret) : \"r\"(x), \"r\"(shift), \"r\"(addend));\n    return ret;\n}\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS    // Do not document\n\n/**\n * Bitfield-extract.\n */\ntemplate <typename UnsignedBits, int BYTE_LEN>\n__device__ __forceinline__ unsigned int BFE(\n    UnsignedBits            source,\n    unsigned int            bit_start,\n    unsigned int            num_bits,\n    Int2Type<BYTE_LEN>      /*byte_len*/)\n{\n    unsigned int bits;\n    asm (\"bfe.u32 %0, %1, %2, %3;\" : \"=r\"(bits) : \"r\"((unsigned int) source), \"r\"(bit_start), \"r\"(num_bits));\n    return bits;\n}\n\n\n/**\n * Bitfield-extract for 64-bit types.\n */\ntemplate <typename UnsignedBits>\n__device__ __forceinline__ unsigned int BFE(\n    UnsignedBits            source,\n    unsigned int            bit_start,\n    unsigned int            num_bits,\n    Int2Type<8>             /*byte_len*/)\n{\n    const unsigned long long MASK = (1ull << num_bits) - 1;\n    return (source >> bit_start) & MASK;\n}\n\n#if CUB_IS_INT128_ENABLED \n/**\n * Bitfield-extract for 128-bit types.\n */\ntemplate <typename UnsignedBits>\n__device__ __forceinline__ unsigned int BFE(\n    UnsignedBits            source,\n    unsigned int            bit_start,\n    unsigned int            num_bits,\n    Int2Type<16>            /*byte_len*/)\n{\n    const __uint128_t MASK = (__uint128_t{1} << num_bits) - 1;\n    return (source >> bit_start) & MASK;\n}\n#endif\n\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\n/**\n * \\brief Bitfield-extract.  Extracts \\p num_bits from \\p source starting at bit-offset \\p bit_start.  The input \\p source may be an 8b, 16b, 32b, or 64b unsigned integer type.\n */\ntemplate <typename UnsignedBits>\n__device__ __forceinline__ unsigned int BFE(\n    UnsignedBits source,\n    unsigned int bit_start,\n    unsigned int num_bits)\n{\n    return BFE(source, bit_start, num_bits, Int2Type<sizeof(UnsignedBits)>());\n}\n\n\n/**\n * \\brief Bitfield insert.  Inserts the \\p num_bits least significant bits of \\p y into \\p x at bit-offset \\p bit_start.\n */\n__device__ __forceinline__ void BFI(\n    unsigned int &ret,\n    unsigned int x,\n    unsigned int y,\n    unsigned int bit_start,\n    unsigned int num_bits)\n{\n    asm (\"bfi.b32 %0, %1, %2, %3, %4;\" :\n        \"=r\"(ret) : \"r\"(y), \"r\"(x), \"r\"(bit_start), \"r\"(num_bits));\n}\n\n\n/**\n * \\brief Three-operand add.  Returns \\p x + \\p y + \\p z.\n */\n__device__ __forceinline__ unsigned int IADD3(unsigned int x, unsigned int y, unsigned int z)\n{\n    asm (\"vadd.u32.u32.u32.add %0, %1, %2, %3;\" : \"=r\"(x) : \"r\"(x), \"r\"(y), \"r\"(z));\n    return x;\n}\n\n\n/**\n * \\brief Byte-permute. Pick four arbitrary bytes from two 32-bit registers, and reassemble them into a 32-bit destination register.  For SM2.0 or later.\n *\n * \\par\n * The bytes in the two source registers \\p a and \\p b are numbered from 0 to 7:\n * {\\p b, \\p a} = {{b7, b6, b5, b4}, {b3, b2, b1, b0}}. For each of the four bytes\n * {b3, b2, b1, b0} selected in the return value, a 4-bit selector is defined within\n * the four lower \"nibbles\" of \\p index: {\\p index } = {n7, n6, n5, n4, n3, n2, n1, n0}\n *\n * \\par Snippet\n * The code snippet below illustrates byte-permute.\n * \\par\n * \\code\n * #include <cub/cub.cuh>\n *\n * __global__ void ExampleKernel(...)\n * {\n *     int a        = 0x03020100;\n *     int b        = 0x07060504;\n *     int index    = 0x00007531;\n *\n *     int selected = PRMT(a, b, index);    // 0x07050301\n *\n * \\endcode\n *\n */\n__device__ __forceinline__ int PRMT(unsigned int a, unsigned int b, unsigned int index)\n{\n    int ret;\n    asm (\"prmt.b32 %0, %1, %2, %3;\" : \"=r\"(ret) : \"r\"(a), \"r\"(b), \"r\"(index));\n    return ret;\n}\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS    // Do not document\n\n/**\n * Sync-threads barrier.\n */\n__device__ __forceinline__ void BAR(int count)\n{\n    asm volatile(\"bar.sync 1, %0;\" : : \"r\"(count));\n}\n\n/**\n * CTA barrier\n */\n__device__  __forceinline__ void CTA_SYNC()\n{\n    __syncthreads();\n}\n\n\n/**\n * CTA barrier with predicate\n */\n__device__  __forceinline__ int CTA_SYNC_AND(int p)\n{\n    return __syncthreads_and(p);\n}\n\n\n/**\n * CTA barrier with predicate\n */\n__device__  __forceinline__ int CTA_SYNC_OR(int p)\n{\n    return __syncthreads_or(p);\n}\n\n\n/**\n * Warp barrier\n */\n__device__  __forceinline__ void WARP_SYNC(unsigned int member_mask)\n{\n    __syncwarp(member_mask);\n}\n\n\n/**\n * Warp any\n */\n__device__  __forceinline__ int WARP_ANY(int predicate, unsigned int member_mask)\n{\n    return __any_sync(member_mask, predicate);\n}\n\n\n/**\n * Warp any\n */\n__device__  __forceinline__ int WARP_ALL(int predicate, unsigned int member_mask)\n{\n    return __all_sync(member_mask, predicate);\n}\n\n\n/**\n * Warp ballot\n */\n__device__  __forceinline__ int WARP_BALLOT(int predicate, unsigned int member_mask)\n{\n    return __ballot_sync(member_mask, predicate);\n}\n\n\n/**\n * Warp synchronous shfl_up\n */\n__device__ __forceinline__ \nunsigned int SHFL_UP_SYNC(unsigned int word, int src_offset, int flags, unsigned int member_mask)\n{\n    asm volatile(\"shfl.sync.up.b32 %0, %1, %2, %3, %4;\"\n        : \"=r\"(word) : \"r\"(word), \"r\"(src_offset), \"r\"(flags), \"r\"(member_mask));\n    return word;\n}\n\n/**\n * Warp synchronous shfl_down\n */\n__device__ __forceinline__ \nunsigned int SHFL_DOWN_SYNC(unsigned int word, int src_offset, int flags, unsigned int member_mask)\n{\n    asm volatile(\"shfl.sync.down.b32 %0, %1, %2, %3, %4;\"\n        : \"=r\"(word) : \"r\"(word), \"r\"(src_offset), \"r\"(flags), \"r\"(member_mask));\n    return word;\n}\n\n/**\n * Warp synchronous shfl_idx\n */\n__device__ __forceinline__ \nunsigned int SHFL_IDX_SYNC(unsigned int word, int src_lane, int flags, unsigned int member_mask)\n{\n    asm volatile(\"shfl.sync.idx.b32 %0, %1, %2, %3, %4;\"\n        : \"=r\"(word) : \"r\"(word), \"r\"(src_lane), \"r\"(flags), \"r\"(member_mask));\n    return word;\n}\n\n/**\n * Warp synchronous shfl_idx\n */\n__device__ __forceinline__ \nunsigned int SHFL_IDX_SYNC(unsigned int word, int src_lane, unsigned int member_mask)\n{\n    return __shfl_sync(member_mask, word, src_lane);\n}\n\n/**\n * Floating point multiply. (Mantissa LSB rounds towards zero.)\n */\n__device__ __forceinline__ float FMUL_RZ(float a, float b)\n{\n    float d;\n    asm (\"mul.rz.f32 %0, %1, %2;\" : \"=f\"(d) : \"f\"(a), \"f\"(b));\n    return d;\n}\n\n\n/**\n * Floating point multiply-add. (Mantissa LSB rounds towards zero.)\n */\n__device__ __forceinline__ float FFMA_RZ(float a, float b, float c)\n{\n    float d;\n    asm (\"fma.rz.f32 %0, %1, %2, %3;\" : \"=f\"(d) : \"f\"(a), \"f\"(b), \"f\"(c));\n    return d;\n}\n\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\n/**\n * \\brief Terminates the calling thread\n */\n__device__ __forceinline__ void ThreadExit() {\n    asm volatile(\"exit;\");\n}    \n\n\n/**\n * \\brief  Abort execution and generate an interrupt to the host CPU\n */\n__device__ __forceinline__ void ThreadTrap() {\n    asm volatile(\"trap;\");\n}\n\n\n/**\n * \\brief Returns the row-major linear thread identifier for a multidimensional thread block\n */\n__device__ __forceinline__ int RowMajorTid(int block_dim_x, int block_dim_y, int block_dim_z)\n{\n    return ((block_dim_z == 1) ? 0 : (threadIdx.z * block_dim_x * block_dim_y)) +\n            ((block_dim_y == 1) ? 0 : (threadIdx.y * block_dim_x)) +\n            threadIdx.x;\n}\n\n\n/**\n * \\brief Returns the warp lane ID of the calling thread\n */\n__device__ __forceinline__ unsigned int LaneId()\n{\n    unsigned int ret;\n    asm (\"mov.u32 %0, %%laneid;\" : \"=r\"(ret) );\n    return ret;\n}\n\n\n/**\n * \\brief Returns the warp ID of the calling thread.  Warp ID is guaranteed to be unique among warps, but may not correspond to a zero-based ranking within the thread block.\n */\n__device__ __forceinline__ unsigned int WarpId()\n{\n    unsigned int ret;\n    asm (\"mov.u32 %0, %%warpid;\" : \"=r\"(ret) );\n    return ret;\n}\n\n/**\n * @brief Returns the warp mask for a warp of @p LOGICAL_WARP_THREADS threads\n *\n * @par\n * If the number of threads assigned to the virtual warp is not a power of two,\n * it's assumed that only one virtual warp exists.\n *\n * @tparam LOGICAL_WARP_THREADS <b>[optional]</b> The number of threads per\n *                              \"logical\" warp (may be less than the number of\n *                              hardware warp threads).\n * @param warp_id Id of virtual warp within architectural warp\n */\ntemplate <int LOGICAL_WARP_THREADS, int LEGACY_PTX_ARCH = 0>\n__host__ __device__ __forceinline__\nunsigned int WarpMask(unsigned int warp_id)\n{\n  constexpr bool is_pow_of_two = PowerOfTwo<LOGICAL_WARP_THREADS>::VALUE;\n  constexpr bool is_arch_warp  = LOGICAL_WARP_THREADS == CUB_WARP_THREADS(0);\n\n  unsigned int member_mask = 0xFFFFFFFFu >>\n                             (CUB_WARP_THREADS(0) - LOGICAL_WARP_THREADS);\n\n  if (is_pow_of_two && !is_arch_warp)\n  {\n    member_mask <<= warp_id * LOGICAL_WARP_THREADS;\n  }\n\n  return member_mask;\n}\n\n/**\n * \\brief Returns the warp lane mask of all lanes less than the calling thread\n */\n__device__ __forceinline__ unsigned int LaneMaskLt()\n{\n    unsigned int ret;\n    asm (\"mov.u32 %0, %%lanemask_lt;\" : \"=r\"(ret) );\n    return ret;\n}\n\n/**\n * \\brief Returns the warp lane mask of all lanes less than or equal to the calling thread\n */\n__device__ __forceinline__ unsigned int LaneMaskLe()\n{\n    unsigned int ret;\n    asm (\"mov.u32 %0, %%lanemask_le;\" : \"=r\"(ret) );\n    return ret;\n}\n\n/**\n * \\brief Returns the warp lane mask of all lanes greater than the calling thread\n */\n__device__ __forceinline__ unsigned int LaneMaskGt()\n{\n    unsigned int ret;\n    asm (\"mov.u32 %0, %%lanemask_gt;\" : \"=r\"(ret) );\n    return ret;\n}\n\n/**\n * \\brief Returns the warp lane mask of all lanes greater than or equal to the calling thread\n */\n__device__ __forceinline__ unsigned int LaneMaskGe()\n{\n    unsigned int ret;\n    asm (\"mov.u32 %0, %%lanemask_ge;\" : \"=r\"(ret) );\n    return ret;\n}\n\n/** @} */       // end group UtilPtx\n\n\n\n\n/**\n * \\brief Shuffle-up for any data type.  Each <em>warp-lane<sub>i</sub></em> obtains the value \\p input contributed by <em>warp-lane</em><sub><em>i</em>-<tt>src_offset</tt></sub>.  For thread lanes \\e i < src_offset, the thread's own \\p input is returned to the thread. ![](shfl_up_logo.png)\n * \\ingroup WarpModule\n *\n * \\tparam LOGICAL_WARP_THREADS     The number of threads per \"logical\" warp.  Must be a power-of-two <= 32.\n * \\tparam T                        <b>[inferred]</b> The input/output element type\n *\n * \\par\n * - Available only for SM3.0 or newer\n *\n * \\par Snippet\n * The code snippet below illustrates each thread obtaining a \\p double value from the\n * predecessor of its predecessor.\n * \\par\n * \\code\n * #include <cub/cub.cuh>   // or equivalently <cub/util_ptx.cuh>\n *\n * __global__ void ExampleKernel(...)\n * {\n *     // Obtain one input item per thread\n *     double thread_data = ...\n *\n *     // Obtain item from two ranks below\n *     double peer_data = ShuffleUp<32>(thread_data, 2, 0, 0xffffffff);\n *\n * \\endcode\n * \\par\n * Suppose the set of input \\p thread_data across the first warp of threads is <tt>{1.0, 2.0, 3.0, 4.0, 5.0, ..., 32.0}</tt>.\n * The corresponding output \\p peer_data will be <tt>{1.0, 2.0, 1.0, 2.0, 3.0, ..., 30.0}</tt>.\n *\n */\ntemplate <\n    int LOGICAL_WARP_THREADS,   ///< Number of threads per logical warp\n    typename T>\n__device__ __forceinline__ T ShuffleUp(\n    T               input,              ///< [in] The value to broadcast\n    int             src_offset,         ///< [in] The relative down-offset of the peer to read from\n    int             first_thread,       ///< [in] Index of first lane in logical warp (typically 0)\n    unsigned int    member_mask)        ///< [in] 32-bit mask of participating warp lanes\n{\n    /// The 5-bit SHFL mask for logically splitting warps into sub-segments starts 8-bits up\n    enum {\n        SHFL_C = (32 - LOGICAL_WARP_THREADS) << 8\n    };\n\n    typedef typename UnitWord<T>::ShuffleWord ShuffleWord;\n\n    const int       WORDS           = (sizeof(T) + sizeof(ShuffleWord) - 1) / sizeof(ShuffleWord);\n \n    T               output;\n    ShuffleWord     *output_alias   = reinterpret_cast<ShuffleWord *>(&output);\n    ShuffleWord     *input_alias    = reinterpret_cast<ShuffleWord *>(&input);\n\n    unsigned int shuffle_word;\n    shuffle_word = SHFL_UP_SYNC((unsigned int)input_alias[0], src_offset, first_thread | SHFL_C, member_mask);\n    output_alias[0] = shuffle_word;\n\n    _Pragma(\"unroll\")\n    for (int WORD = 1; WORD < WORDS; ++WORD)\n    {\n        shuffle_word       = SHFL_UP_SYNC((unsigned int)input_alias[WORD], src_offset, first_thread | SHFL_C, member_mask);\n        output_alias[WORD] = shuffle_word;\n    }\n\n    return output;\n}\n\n\n/**\n * \\brief Shuffle-down for any data type.  Each <em>warp-lane<sub>i</sub></em> obtains the value \\p input contributed by <em>warp-lane</em><sub><em>i</em>+<tt>src_offset</tt></sub>.  For thread lanes \\e i >= WARP_THREADS, the thread's own \\p input is returned to the thread.  ![](shfl_down_logo.png)\n * \\ingroup WarpModule\n *\n * \\tparam LOGICAL_WARP_THREADS     The number of threads per \"logical\" warp.  Must be a power-of-two <= 32.\n * \\tparam T                        <b>[inferred]</b> The input/output element type\n *\n * \\par\n * - Available only for SM3.0 or newer\n *\n * \\par Snippet\n * The code snippet below illustrates each thread obtaining a \\p double value from the\n * successor of its successor.\n * \\par\n * \\code\n * #include <cub/cub.cuh>   // or equivalently <cub/util_ptx.cuh>\n *\n * __global__ void ExampleKernel(...)\n * {\n *     // Obtain one input item per thread\n *     double thread_data = ...\n *\n *     // Obtain item from two ranks below\n *     double peer_data = ShuffleDown<32>(thread_data, 2, 31, 0xffffffff);\n *\n * \\endcode\n * \\par\n * Suppose the set of input \\p thread_data across the first warp of threads is <tt>{1.0, 2.0, 3.0, 4.0, 5.0, ..., 32.0}</tt>.\n * The corresponding output \\p peer_data will be <tt>{3.0, 4.0, 5.0, 6.0, 7.0, ..., 32.0}</tt>.\n *\n */\ntemplate <\n    int LOGICAL_WARP_THREADS,   ///< Number of threads per logical warp\n    typename T>\n__device__ __forceinline__ T ShuffleDown(\n    T               input,              ///< [in] The value to broadcast\n    int             src_offset,         ///< [in] The relative up-offset of the peer to read from\n    int             last_thread,        ///< [in] Index of last thread in logical warp (typically 31 for a 32-thread warp)\n    unsigned int    member_mask)        ///< [in] 32-bit mask of participating warp lanes\n{\n    /// The 5-bit SHFL mask for logically splitting warps into sub-segments starts 8-bits up\n    enum {\n        SHFL_C = (32 - LOGICAL_WARP_THREADS) << 8\n    };\n\n    typedef typename UnitWord<T>::ShuffleWord ShuffleWord;\n\n    const int       WORDS           = (sizeof(T) + sizeof(ShuffleWord) - 1) / sizeof(ShuffleWord);\n\n    T               output;\n    ShuffleWord     *output_alias   = reinterpret_cast<ShuffleWord *>(&output);\n    ShuffleWord     *input_alias    = reinterpret_cast<ShuffleWord *>(&input);\n\n    unsigned int shuffle_word;\n    shuffle_word    = SHFL_DOWN_SYNC((unsigned int)input_alias[0], src_offset, last_thread | SHFL_C, member_mask);\n    output_alias[0] = shuffle_word;\n\n    _Pragma(\"unroll\")\n    for (int WORD = 1; WORD < WORDS; ++WORD)\n    {\n        shuffle_word       = SHFL_DOWN_SYNC((unsigned int)input_alias[WORD], src_offset, last_thread | SHFL_C, member_mask);\n        output_alias[WORD] = shuffle_word;\n    }\n\n    return output;\n}\n\n\n/**\n * \\brief Shuffle-broadcast for any data type.  Each <em>warp-lane<sub>i</sub></em> obtains the value \\p input\n * contributed by <em>warp-lane</em><sub><tt>src_lane</tt></sub>.  For \\p src_lane < 0 or \\p src_lane >= WARP_THREADS,\n * then the thread's own \\p input is returned to the thread. ![](shfl_broadcast_logo.png)\n *\n * \\tparam LOGICAL_WARP_THREADS     The number of threads per \"logical\" warp.  Must be a power-of-two <= 32.\n * \\tparam T                        <b>[inferred]</b> The input/output element type\n *\n * \\ingroup WarpModule\n *\n * \\par\n * - Available only for SM3.0 or newer\n *\n * \\par Snippet\n * The code snippet below illustrates each thread obtaining a \\p double value from <em>warp-lane</em><sub>0</sub>.\n *\n * \\par\n * \\code\n * #include <cub/cub.cuh>   // or equivalently <cub/util_ptx.cuh>\n *\n * __global__ void ExampleKernel(...)\n * {\n *     // Obtain one input item per thread\n *     double thread_data = ...\n *\n *     // Obtain item from thread 0\n *     double peer_data = ShuffleIndex<32>(thread_data, 0, 0xffffffff);\n *\n * \\endcode\n * \\par\n * Suppose the set of input \\p thread_data across the first warp of threads is <tt>{1.0, 2.0, 3.0, 4.0, 5.0, ..., 32.0}</tt>.\n * The corresponding output \\p peer_data will be <tt>{1.0, 1.0, 1.0, 1.0, 1.0, ..., 1.0}</tt>.\n *\n */\ntemplate <\n    int LOGICAL_WARP_THREADS,   ///< Number of threads per logical warp\n    typename T>\n__device__ __forceinline__ T ShuffleIndex(\n    T               input,                  ///< [in] The value to broadcast\n    int             src_lane,               ///< [in] Which warp lane is to do the broadcasting\n    unsigned int    member_mask)            ///< [in] 32-bit mask of participating warp lanes\n{\n    /// The 5-bit SHFL mask for logically splitting warps into sub-segments starts 8-bits up\n    enum {\n        SHFL_C = ((32 - LOGICAL_WARP_THREADS) << 8) | (LOGICAL_WARP_THREADS - 1)\n    };\n\n    typedef typename UnitWord<T>::ShuffleWord ShuffleWord;\n\n    const int       WORDS           = (sizeof(T) + sizeof(ShuffleWord) - 1) / sizeof(ShuffleWord);\n\n    T               output;\n    ShuffleWord     *output_alias   = reinterpret_cast<ShuffleWord *>(&output);\n    ShuffleWord     *input_alias    = reinterpret_cast<ShuffleWord *>(&input);\n\n    unsigned int shuffle_word;\n    shuffle_word = SHFL_IDX_SYNC((unsigned int)input_alias[0],\n                                 src_lane,\n                                 SHFL_C,\n                                 member_mask);\n\n    output_alias[0] = shuffle_word;\n\n    _Pragma(\"unroll\")\n    for (int WORD = 1; WORD < WORDS; ++WORD)\n    {\n        shuffle_word = SHFL_IDX_SYNC((unsigned int)input_alias[WORD],\n                                     src_lane,\n                                     SHFL_C,\n                                     member_mask);\n\n        output_alias[WORD] = shuffle_word;\n    }\n\n    return output;\n}\n\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS    // Do not document\nnamespace detail \n{\n\n/** \n * Implementation detail for `MatchAny`. It provides specializations for full and partial warps. \n * For partial warps, inactive threads must be masked out. This is done in the partial warp \n * specialization below. \n * Usage:\n * ```\n * // returns a mask of threads with the same 4 least-significant bits of `label` \n * // in a warp with 16 active threads\n * warp_matcher_t<4, 16>::match_any(label); \n *\n * // returns a mask of threads with the same 4 least-significant bits of `label` \n * // in a warp with 32 active threads (no extra work is done)\n * warp_matcher_t<4, 32>::match_any(label); \n * ```\n */\ntemplate <int LABEL_BITS, int WARP_ACTIVE_THREADS>\nstruct warp_matcher_t \n{\n\n  static __device__ unsigned int match_any(unsigned int label)\n  {\n    return warp_matcher_t<LABEL_BITS, 32>::match_any(label) & ~(~0 << WARP_ACTIVE_THREADS);\n  }\n\n};\n\ntemplate <int LABEL_BITS>\nstruct warp_matcher_t<LABEL_BITS, CUB_PTX_WARP_THREADS> \n{\n\n  // match.any.sync.b32 is slower when matching a few bits\n  // using a ballot loop instead\n  static __device__ unsigned int match_any(unsigned int label)\n  {\n      unsigned int retval;\n\n      // Extract masks of common threads for each bit\n      _Pragma(\"unroll\")\n      for (int BIT = 0; BIT < LABEL_BITS; ++BIT)\n      {\n          unsigned int mask;\n          unsigned int current_bit = 1 << BIT;\n          asm (\"{\\n\"\n              \"    .reg .pred p;\\n\"\n              \"    and.b32 %0, %1, %2;\"\n              \"    setp.eq.u32 p, %0, %2;\\n\"\n              \"    vote.ballot.sync.b32 %0, p, 0xffffffff;\\n\"\n              \"    @!p not.b32 %0, %0;\\n\"\n              \"}\\n\" : \"=r\"(mask) : \"r\"(label), \"r\"(current_bit));\n\n          // Remove peers who differ\n          retval = (BIT == 0) ? mask : retval & mask;\n      }\n\n      return retval;\n  }\n\n};\n\n} // namespace detail\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\n/**\n * Compute a 32b mask of threads having the same least-significant\n * LABEL_BITS of \\p label as the calling thread.\n */\ntemplate <int LABEL_BITS, int WARP_ACTIVE_THREADS = CUB_PTX_WARP_THREADS>\ninline __device__ unsigned int MatchAny(unsigned int label)\n{\n  return detail::warp_matcher_t<LABEL_BITS, WARP_ACTIVE_THREADS>::match_any(label);\n}\n\nCUB_NAMESPACE_END\n\n#endif // _JITIFY_INCLUDE_GUARD_8EAE810BB36A0FF0\n", "cub/util_type.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_1375AABB974C3530\n#define _JITIFY_INCLUDE_GUARD_1375AABB974C3530\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.\n * \n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n * \n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * Common type manipulation (metaprogramming) utilities\n */\n\n#include <cfloat>\n#include <iostream>\n#include <iterator>\n#include <limits>\n\n#include <cuda.h>\n\n#if !_NVHPC_CUDA\n    #include <cuda_fp16.h>\n#endif\n#if !_NVHPC_CUDA && !defined(CUB_DISABLE_BF16_SUPPORT)\n    #include <cuda_bf16.h>\n#endif\n\n#include <cub/detail/uninitialized_copy.cuh>\n#include <cub/util_arch.cuh>\n#include <cub/util_compiler.cuh>\n#include <cub/util_deprecated.cuh>\n#include <cub/util_macro.cuh>\n#include <cub/util_namespace.cuh>\n\n#include <cuda/std/type_traits>\n\nCUB_NAMESPACE_BEGIN\n\n#ifndef CUB_IS_INT128_ENABLED\n#if defined(__CUDACC_RTC__)\n#if defined(__CUDACC_RTC_INT128__)\n#define CUB_IS_INT128_ENABLED 1\n#endif // !defined(__CUDACC_RTC_INT128__)\n#else  // !defined(__CUDACC_RTC__)\n#if CUDA_VERSION >= 11050\n#if (CUB_HOST_COMPILER == CUB_HOST_COMPILER_GCC) || \\\n    (CUB_HOST_COMPILER == CUB_HOST_COMPILER_CLANG) || \\\n    defined(__ICC) || defined(_NVHPC_CUDA)\n#define CUB_IS_INT128_ENABLED 1\n#endif // GCC || CLANG || ICC || NVHPC\n#endif // CTK >= 11.5\n#endif // !defined(__CUDACC_RTC__)\n#endif // !defined(CUB_IS_INT128_ENABLED)\n\n/**\n * \\addtogroup UtilModule\n * @{\n */\n\n\n\n/******************************************************************************\n * Conditional types\n ******************************************************************************/\n\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS // Do not document\nnamespace detail\n{\n\n\ntemplate <bool Test, class T1, class T2>\nusing conditional_t = typename std::conditional<Test, T1, T2>::type;\n\n\ntemplate <typename Iterator>\nusing value_t = typename std::iterator_traits<Iterator>::value_type;\n\ntemplate <typename It,\n          typename FallbackT,\n          bool = ::cuda::std::is_same<\n            typename ::cuda::std::remove_cv<typename ::cuda::std::remove_pointer<It>::type>::type,\n            void>::value>\nstruct non_void_value_impl\n{\n  using type = FallbackT;\n};\n\ntemplate <typename It, typename FallbackT>\nstruct non_void_value_impl<It, FallbackT, false>\n{\n  using type = typename ::cuda::std::conditional<\n    ::cuda::std::is_same<typename std::iterator_traits<It>::value_type, void>::value,\n    FallbackT,\n    typename std::iterator_traits<It>::value_type>::type;\n};\n\n/**\n * The output value type\n * type = (if IteratorT's value type is void) ?\n * ... then the FallbackT,\n * ... else the IteratorT's value type\n */\ntemplate <typename It, typename FallbackT>\nusing non_void_value_t = typename non_void_value_impl<It, FallbackT>::type;\n} // namespace detail\n\n\n/**\n * \\brief Type selection (<tt>IF ? ThenType : ElseType</tt>)\n *\n * \\deprecated [Since 1.16.0] The cub::If APIs are deprecated.\n *             Use cub::detail::conditional_t instead.\n */\ntemplate <bool IF, typename ThenType, typename ElseType>\nstruct CUB_DEPRECATED If\n{\n  using Type = cub::detail::conditional_t<IF, ThenType, ElseType>;\n};\n\n\n/******************************************************************************\n * Type equality\n ******************************************************************************/\n\n/**\n * \\brief Type equality test\n *\n * \\deprecated [Since 1.16.0] The cub::Equals APIs are deprecated.\n *             Use std::is_same instead.\n */\ntemplate <typename A, typename B>\nstruct CUB_DEPRECATED Equals\n{\n  static constexpr int VALUE = std::is_same<A, B>::value ? 1 : 0;\n  static constexpr int NEGATE = VALUE ? 0 : 1;\n};\n\n\n/******************************************************************************\n * Static math\n ******************************************************************************/\n\n/**\n * \\brief Statically determine log2(N), rounded up.\n *\n * For example:\n *     Log2<8>::VALUE   // 3\n *     Log2<3>::VALUE   // 2\n */\ntemplate <int N, int CURRENT_VAL = N, int COUNT = 0>\nstruct Log2\n{\n    /// Static logarithm value\n    enum { VALUE = Log2<N, (CURRENT_VAL >> 1), COUNT + 1>::VALUE };         // Inductive case\n};\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS    // Do not document\n\ntemplate <int N, int COUNT>\nstruct Log2<N, 0, COUNT>\n{\n    enum {VALUE = (1 << (COUNT - 1) < N) ?                                  // Base case\n        COUNT :\n        COUNT - 1 };\n};\n\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\n/**\n * \\brief Statically determine if N is a power-of-two\n */\ntemplate <int N>\nstruct PowerOfTwo\n{\n    enum { VALUE = ((N & (N - 1)) == 0) };\n};\n\n\n\n/******************************************************************************\n * Pointer vs. iterator detection\n ******************************************************************************/\n\n/**\n * \\brief Pointer vs. iterator\n *\n * \\deprecated [Since 1.16.0] The cub::IsPointer APIs are deprecated.\n *             Use std::is_pointer instead.\n */\ntemplate <typename Tp>\nstruct CUB_DEPRECATED IsPointer\n{\n  static constexpr int VALUE = std::is_pointer<Tp>::value;\n};\n\n\n/******************************************************************************\n * Qualifier detection\n ******************************************************************************/\n\n/**\n * \\brief Volatile modifier test\n *\n * \\deprecated [Since 1.16.0] The cub::IsVolatile APIs are deprecated.\n *             Use std::is_volatile instead.\n */\ntemplate <typename Tp>\nstruct CUB_DEPRECATED IsVolatile\n{\n  static constexpr int VALUE = std::is_volatile<Tp>::value;\n};\n\n/******************************************************************************\n * Qualifier removal\n ******************************************************************************/\n\n/**\n * \\brief Removes \\p const and \\p volatile qualifiers from type \\p Tp.\n *\n * \\deprecated [Since 1.16.0] The cub::RemoveQualifiers APIs are deprecated.\n *             Use std::remove_cv instead.\n *\n * For example:\n *     <tt>typename RemoveQualifiers<volatile int>::Type         // int;</tt>\n */\ntemplate <typename Tp, typename Up = Tp>\nstruct CUB_DEPRECATED RemoveQualifiers\n{\n  using Type = typename std::remove_cv<Tp>::type;\n};\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\n\n/******************************************************************************\n * Marker types\n ******************************************************************************/\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS    // Do not document\n\n/**\n * \\brief A simple \"NULL\" marker type\n */\nstruct NullType\n{\n    using value_type = NullType;\n\n    template <typename T>\n    __host__ __device__ __forceinline__ NullType& operator =(const T&) { return *this; }\n\n    __host__ __device__ __forceinline__ bool operator ==(const NullType&) { return true; }\n\n    __host__ __device__ __forceinline__ bool operator !=(const NullType&) { return false; }\n};\n\n\n/**\n * \\brief Allows for the treatment of an integral constant as a type at compile-time (e.g., to achieve static call dispatch based on constant integral values)\n */\ntemplate <int A>\nstruct Int2Type\n{\n    enum {VALUE = A};\n};\n\n/**\n * \\brief Allows algorithms that take a value as input to take a future value that is not computed yet at launch time.\n *\n * Note that it is user's responsibility to ensure that the result will be ready before use via external synchronization\n * or stream-ordering dependencies.\n *\n * \\code\n * int *d_intermediate_result;\n * allocator.DeviceAllocate((void **)&d_intermediate_result, sizeof(int));\n * compute_intermediate_result<<<blocks, threads>>>(\n *     d_intermediate_result,  // output\n *     arg1,                   // input\n *     arg2);                  // input\n * cub::FutureValue<int> init_value(d_intermediate_result);\n * cub::DeviceScan::ExclusiveScan(\n *     d_temp_storage,\n *     temp_storage_bytes,\n *     d_in,\n *     d_out,\n *     cub::Sum(),\n *     init_value,\n *     num_items);\n * allocator.DeviceFree(d_intermediate_result);\n * \\endcode\n */\ntemplate <typename T, typename IterT = T*>\nstruct FutureValue\n{\n    using value_type = T;\n    using iterator_type = IterT;\n    explicit __host__ __device__ __forceinline__ FutureValue(IterT iter):m_iter(iter) {}\n    __host__ __device__ __forceinline__ operator T() {\n        return *m_iter;\n    }\n\nprivate:\n    IterT m_iter;\n};\n\nnamespace detail {\n\n/**\n * \\brief Allows algorithms to instantiate a single kernel to support both immediate value and future value.\n */\ntemplate <typename T, typename IterT = T*>\nstruct InputValue\n{\n    using value_type = T;\n    using iterator_type = IterT;\n    __host__ __device__ __forceinline__ operator T() {\n        if (m_is_future) {\n            return m_future_value;\n        }\n        return m_immediate_value;\n    }\n    explicit __host__ __device__ __forceinline__ InputValue(T immediate_value): m_is_future(false), m_immediate_value(immediate_value) {}\n    explicit __host__ __device__ __forceinline__ InputValue(FutureValue<T, IterT> future_value): m_is_future(true), m_future_value(future_value) {}\n    __host__ __device__ __forceinline__ InputValue(const InputValue &other): m_is_future(other.m_is_future) {\n        if (m_is_future) {\n            m_future_value = other.m_future_value;\n        } else {\n          detail::uninitialized_copy(&m_immediate_value,\n                                     other.m_immediate_value);\n        }\n    }\n\nprivate:\n    bool m_is_future;\n    union\n    {\n        FutureValue<T, IterT> m_future_value;\n        T m_immediate_value;\n    };\n};\n\n} // namespace detail\n\n\n/******************************************************************************\n * Size and alignment\n ******************************************************************************/\n\n/// Structure alignment\ntemplate <typename T>\nstruct AlignBytes\n{\n    struct Pad\n    {\n        T       val;\n        char    byte;\n    };\n\n    enum\n    {\n        /// The \"true CUDA\" alignment of T in bytes\n        ALIGN_BYTES = sizeof(Pad) - sizeof(T)\n    };\n\n    /// The \"truly aligned\" type\n    typedef T Type;\n};\n\n// Specializations where host C++ compilers (e.g., 32-bit Windows) may disagree\n// with device C++ compilers (EDG) on types passed as template parameters through\n// kernel functions\n\n#define __CUB_ALIGN_BYTES(t, b)         \\\n    template <> struct AlignBytes<t>    \\\n    { enum { ALIGN_BYTES = b }; typedef __align__(b) t Type; };\n\n__CUB_ALIGN_BYTES(short4, 8)\n__CUB_ALIGN_BYTES(ushort4, 8)\n__CUB_ALIGN_BYTES(int2, 8)\n__CUB_ALIGN_BYTES(uint2, 8)\n__CUB_ALIGN_BYTES(long long, 8)\n__CUB_ALIGN_BYTES(unsigned long long, 8)\n__CUB_ALIGN_BYTES(float2, 8)\n__CUB_ALIGN_BYTES(double, 8)\n#ifdef _WIN32\n    __CUB_ALIGN_BYTES(long2, 8)\n    __CUB_ALIGN_BYTES(ulong2, 8)\n#else\n    __CUB_ALIGN_BYTES(long2, 16)\n    __CUB_ALIGN_BYTES(ulong2, 16)\n#endif\n__CUB_ALIGN_BYTES(int4, 16)\n__CUB_ALIGN_BYTES(uint4, 16)\n__CUB_ALIGN_BYTES(float4, 16)\n__CUB_ALIGN_BYTES(long4, 16)\n__CUB_ALIGN_BYTES(ulong4, 16)\n__CUB_ALIGN_BYTES(longlong2, 16)\n__CUB_ALIGN_BYTES(ulonglong2, 16)\n__CUB_ALIGN_BYTES(double2, 16)\n__CUB_ALIGN_BYTES(longlong4, 16)\n__CUB_ALIGN_BYTES(ulonglong4, 16)\n__CUB_ALIGN_BYTES(double4, 16)\n\n// clang-format off\ntemplate <typename T> struct AlignBytes<volatile T> : AlignBytes<T> {};\ntemplate <typename T> struct AlignBytes<const T> : AlignBytes<T> {};\ntemplate <typename T> struct AlignBytes<const volatile T> : AlignBytes<T> {};\n// clang-format on\n\n/// Unit-words of data movement\ntemplate <typename T>\nstruct UnitWord\n{\n    enum {\n        ALIGN_BYTES = AlignBytes<T>::ALIGN_BYTES\n    };\n\n    template <typename Unit>\n    struct IsMultiple\n    {\n        enum {\n            UNIT_ALIGN_BYTES    = AlignBytes<Unit>::ALIGN_BYTES,\n            IS_MULTIPLE         = (sizeof(T) % sizeof(Unit) == 0) && (int(ALIGN_BYTES) % int(UNIT_ALIGN_BYTES) == 0)\n        };\n    };\n\n    /// Biggest shuffle word that T is a whole multiple of and is not larger than\n    /// the alignment of T\n    using ShuffleWord = cub::detail::conditional_t<\n      IsMultiple<int>::IS_MULTIPLE,\n      unsigned int,\n      cub::detail::conditional_t<IsMultiple<short>::IS_MULTIPLE,\n                                 unsigned short,\n                                 unsigned char>>;\n\n    /// Biggest volatile word that T is a whole multiple of and is not larger than\n    /// the alignment of T\n    using VolatileWord =\n      cub::detail::conditional_t<IsMultiple<long long>::IS_MULTIPLE,\n                                 unsigned long long,\n                                 ShuffleWord>;\n\n    /// Biggest memory-access word that T is a whole multiple of and is not larger\n    /// than the alignment of T\n    using DeviceWord =\n      cub::detail::conditional_t<IsMultiple<longlong2>::IS_MULTIPLE,\n                                 ulonglong2,\n                                 VolatileWord>;\n\n    /// Biggest texture reference word that T is a whole multiple of and is not\n    /// larger than the alignment of T\n    using TextureWord = cub::detail::conditional_t<\n      IsMultiple<int4>::IS_MULTIPLE,\n      uint4,\n      cub::detail::conditional_t<IsMultiple<int2>::IS_MULTIPLE, uint2, ShuffleWord>>;\n};\n\n// float2 specialization workaround (for SM10-SM13)\ntemplate <>\nstruct UnitWord <float2>\n{\n    typedef int         ShuffleWord;\n    typedef unsigned long long   VolatileWord;\n    typedef unsigned long long   DeviceWord;\n    typedef float2      TextureWord;\n};\n\n// float4 specialization workaround (for SM10-SM13)\ntemplate <>\nstruct UnitWord <float4>\n{\n    typedef int         ShuffleWord;\n    typedef unsigned long long  VolatileWord;\n    typedef ulonglong2          DeviceWord;\n    typedef float4              TextureWord;\n};\n\n\n// char2 specialization workaround (for SM10-SM13)\ntemplate <>\nstruct UnitWord <char2>\n{\n    typedef unsigned short      ShuffleWord;\n    typedef unsigned short      VolatileWord;\n    typedef unsigned short      DeviceWord;\n    typedef unsigned short      TextureWord;\n};\n\n// clang-format off\ntemplate <typename T> struct UnitWord<volatile T> : UnitWord<T> {};\ntemplate <typename T> struct UnitWord<const T> : UnitWord<T> {};\ntemplate <typename T> struct UnitWord<const volatile T> : UnitWord<T> {};\n// clang-format on\n\n\n/******************************************************************************\n * Vector type inference utilities.\n ******************************************************************************/\n\n/**\n * \\brief Exposes a member typedef \\p Type that names the corresponding CUDA vector type if one exists.  Otherwise \\p Type refers to the CubVector structure itself, which will wrap the corresponding \\p x, \\p y, etc. vector fields.\n */\ntemplate <typename T, int vec_elements> struct CubVector;\n\n\nenum\n{\n    /// The maximum number of elements in CUDA vector types\n    MAX_VEC_ELEMENTS = 4,\n};\n\n\n/**\n * Generic vector-1 type\n */\ntemplate <typename T>\nstruct CubVector<T, 1>\n{\n    T x;\n\n    typedef T BaseType;\n    typedef CubVector<T, 1> Type;\n};\n\n/**\n * Generic vector-2 type\n */\ntemplate <typename T>\nstruct CubVector<T, 2>\n{\n    T x;\n    T y;\n\n    typedef T BaseType;\n    typedef CubVector<T, 2> Type;\n};\n\n/**\n * Generic vector-3 type\n */\ntemplate <typename T>\nstruct CubVector<T, 3>\n{\n    T x;\n    T y;\n    T z;\n\n    typedef T BaseType;\n    typedef CubVector<T, 3> Type;\n};\n\n/**\n * Generic vector-4 type\n */\ntemplate <typename T>\nstruct CubVector<T, 4>\n{\n    T x;\n    T y;\n    T z;\n    T w;\n\n    typedef T BaseType;\n    typedef CubVector<T, 4> Type;\n};\n\n\n/**\n * Macro for expanding partially-specialized built-in vector types\n */\n#define CUB_DEFINE_VECTOR_TYPE(base_type,short_type)                                                    \\\n                                                                                                        \\\n    template<> struct CubVector<base_type, 1> : short_type##1                                           \\\n    {                                                                                                   \\\n      typedef base_type       BaseType;                                                                 \\\n      typedef short_type##1   Type;                                                                     \\\n      __host__ __device__ __forceinline__ CubVector operator+(const CubVector &other) const {           \\\n          CubVector retval;                                                                             \\\n          retval.x = x + other.x;                                                                       \\\n          return retval;                                                                                \\\n      }                                                                                                 \\\n      __host__ __device__ __forceinline__ CubVector operator-(const CubVector &other) const {           \\\n          CubVector retval;                                                                             \\\n          retval.x = x - other.x;                                                                       \\\n          return retval;                                                                                \\\n      }                                                                                                 \\\n    };                                                                                                  \\\n                                                                                                        \\\n    template<> struct CubVector<base_type, 2> : short_type##2                                           \\\n    {                                                                                                   \\\n        typedef base_type       BaseType;                                                               \\\n        typedef short_type##2   Type;                                                                   \\\n        __host__ __device__ __forceinline__ CubVector operator+(const CubVector &other) const {         \\\n            CubVector retval;                                                                           \\\n            retval.x = x + other.x;                                                                     \\\n            retval.y = y + other.y;                                                                     \\\n            return retval;                                                                              \\\n        }                                                                                               \\\n        __host__ __device__ __forceinline__ CubVector operator-(const CubVector &other) const {         \\\n            CubVector retval;                                                                           \\\n            retval.x = x - other.x;                                                                     \\\n            retval.y = y - other.y;                                                                     \\\n            return retval;                                                                              \\\n        }                                                                                               \\\n    };                                                                                                  \\\n                                                                                                        \\\n    template<> struct CubVector<base_type, 3> : short_type##3                                           \\\n    {                                                                                                   \\\n        typedef base_type       BaseType;                                                               \\\n        typedef short_type##3   Type;                                                                   \\\n        __host__ __device__ __forceinline__ CubVector operator+(const CubVector &other) const {         \\\n            CubVector retval;                                                                           \\\n            retval.x = x + other.x;                                                                     \\\n            retval.y = y + other.y;                                                                     \\\n            retval.z = z + other.z;                                                                     \\\n            return retval;                                                                              \\\n        }                                                                                               \\\n        __host__ __device__ __forceinline__ CubVector operator-(const CubVector &other) const {         \\\n            CubVector retval;                                                                           \\\n            retval.x = x - other.x;                                                                     \\\n            retval.y = y - other.y;                                                                     \\\n            retval.z = z - other.z;                                                                     \\\n            return retval;                                                                              \\\n        }                                                                                               \\\n    };                                                                                                  \\\n                                                                                                        \\\n    template<> struct CubVector<base_type, 4> : short_type##4                                           \\\n    {                                                                                                   \\\n        typedef base_type       BaseType;                                                               \\\n        typedef short_type##4   Type;                                                                   \\\n        __host__ __device__ __forceinline__ CubVector operator+(const CubVector &other) const {         \\\n            CubVector retval;                                                                           \\\n            retval.x = x + other.x;                                                                     \\\n            retval.y = y + other.y;                                                                     \\\n            retval.z = z + other.z;                                                                     \\\n            retval.w = w + other.w;                                                                     \\\n            return retval;                                                                              \\\n        }                                                                                               \\\n        __host__ __device__ __forceinline__ CubVector operator-(const CubVector &other) const {         \\\n            CubVector retval;                                                                           \\\n            retval.x = x - other.x;                                                                     \\\n            retval.y = y - other.y;                                                                     \\\n            retval.z = z - other.z;                                                                     \\\n            retval.w = w - other.w;                                                                     \\\n            return retval;                                                                              \\\n        }                                                                                               \\\n    };\n\n\n\n// Expand CUDA vector types for built-in primitives\n// clang-format off\nCUB_DEFINE_VECTOR_TYPE(char,               char)\nCUB_DEFINE_VECTOR_TYPE(signed char,        char)\nCUB_DEFINE_VECTOR_TYPE(short,              short)\nCUB_DEFINE_VECTOR_TYPE(int,                int)\nCUB_DEFINE_VECTOR_TYPE(long,               long)\nCUB_DEFINE_VECTOR_TYPE(long long,          longlong)\nCUB_DEFINE_VECTOR_TYPE(unsigned char,      uchar)\nCUB_DEFINE_VECTOR_TYPE(unsigned short,     ushort)\nCUB_DEFINE_VECTOR_TYPE(unsigned int,       uint)\nCUB_DEFINE_VECTOR_TYPE(unsigned long,      ulong)\nCUB_DEFINE_VECTOR_TYPE(unsigned long long, ulonglong)\nCUB_DEFINE_VECTOR_TYPE(float,              float)\nCUB_DEFINE_VECTOR_TYPE(double,             double)\nCUB_DEFINE_VECTOR_TYPE(bool,               uchar)\n// clang-format on\n\n// Undefine macros\n#undef CUB_DEFINE_VECTOR_TYPE\n\n\n/******************************************************************************\n * Wrapper types\n ******************************************************************************/\n\n/**\n * \\brief A storage-backing wrapper that allows types with non-trivial constructors to be aliased in unions\n */\ntemplate <typename T>\nstruct Uninitialized\n{\n    /// Biggest memory-access word that T is a whole multiple of and is not larger than the alignment of T\n    typedef typename UnitWord<T>::DeviceWord DeviceWord;\n\n    static constexpr std::size_t DATA_SIZE = sizeof(T);\n    static constexpr std::size_t WORD_SIZE = sizeof(DeviceWord);\n    static constexpr std::size_t WORDS = DATA_SIZE / WORD_SIZE;\n\n    /// Backing storage\n    DeviceWord storage[WORDS];\n\n    /// Alias\n    __host__ __device__ __forceinline__ T& Alias()\n    {\n        return reinterpret_cast<T&>(*this);\n    }\n};\n\n\n/**\n * \\brief A key identifier paired with a corresponding value\n */\ntemplate <\n    typename    _Key,\n    typename    _Value\n#if defined(_WIN32) && !defined(_WIN64)\n    , bool KeyIsLT = (AlignBytes<_Key>::ALIGN_BYTES < AlignBytes<_Value>::ALIGN_BYTES)\n    , bool ValIsLT = (AlignBytes<_Value>::ALIGN_BYTES < AlignBytes<_Key>::ALIGN_BYTES)\n#endif // #if defined(_WIN32) && !defined(_WIN64)\n    >\nstruct KeyValuePair\n{\n    typedef _Key    Key;                ///< Key data type\n    typedef _Value  Value;              ///< Value data type\n\n    Key     key;                        ///< Item key\n    Value   value;                      ///< Item value\n\n    /// Constructor\n    __host__ __device__ __forceinline__\n    KeyValuePair() {}\n\n    /// Constructor\n    __host__ __device__ __forceinline__\n    KeyValuePair(Key const& key, Value const& value) : key(key), value(value) {}\n\n    /// Inequality operator\n    __host__ __device__ __forceinline__ bool operator !=(const KeyValuePair &b)\n    {\n        return (value != b.value) || (key != b.key);\n    }\n};\n\n#if defined(_WIN32) && !defined(_WIN64)\n\n/**\n * Win32 won't do 16B alignment.  This can present two problems for\n * should-be-16B-aligned (but actually 8B aligned) built-in and intrinsics members:\n * 1) If a smaller-aligned item were to be listed first, the host compiler places the\n *    should-be-16B item at too early an offset (and disagrees with device compiler)\n * 2) Or, if a smaller-aligned item lists second, the host compiler gets the size\n *    of the struct wrong (and disagrees with device compiler)\n *\n * So we put the larger-should-be-aligned item first, and explicitly pad the\n * end of the struct\n */\n\n/// Smaller key specialization\ntemplate <typename K, typename V>\nstruct KeyValuePair<K, V, true, false>\n{\n    typedef K Key;\n    typedef V Value;\n\n    typedef char Pad[AlignBytes<V>::ALIGN_BYTES - AlignBytes<K>::ALIGN_BYTES];\n\n    Value   value;  // Value has larger would-be alignment and goes first\n    Key     key;\n    Pad     pad;\n\n    /// Constructor\n    __host__ __device__ __forceinline__\n    KeyValuePair() {}\n\n    /// Constructor\n    __host__ __device__ __forceinline__\n    KeyValuePair(Key const& key, Value const& value) : key(key), value(value) {}\n\n    /// Inequality operator\n    __host__ __device__ __forceinline__ bool operator !=(const KeyValuePair &b)\n    {\n        return (value != b.value) || (key != b.key);\n    }\n};\n\n\n/// Smaller value specialization\ntemplate <typename K, typename V>\nstruct KeyValuePair<K, V, false, true>\n{\n    typedef K Key;\n    typedef V Value;\n\n    typedef char Pad[AlignBytes<K>::ALIGN_BYTES - AlignBytes<V>::ALIGN_BYTES];\n\n    Key     key;    // Key has larger would-be alignment and goes first\n    Value   value;\n    Pad     pad;\n\n    /// Constructor\n    __host__ __device__ __forceinline__\n    KeyValuePair() {}\n\n    /// Constructor\n    __host__ __device__ __forceinline__\n    KeyValuePair(Key const& key, Value const& value) : key(key), value(value) {}\n\n    /// Inequality operator\n    __host__ __device__ __forceinline__ bool operator !=(const KeyValuePair &b)\n    {\n        return (value != b.value) || (key != b.key);\n    }\n};\n\n#endif // #if defined(_WIN32) && !defined(_WIN64)\n\n\n/**\n * \\brief A wrapper for passing simple static arrays as kernel parameters\n */\ntemplate <typename T, int COUNT>\nstruct ArrayWrapper\n{\n\n    /// Statically-sized array of type \\p T\n    T array[COUNT];\n\n    /// Constructor\n    __host__ __device__ __forceinline__ ArrayWrapper() {}\n};\n\n\n/**\n * \\brief Double-buffer storage wrapper for multi-pass stream transformations that require more than one storage array for streaming intermediate results back and forth.\n *\n * Many multi-pass computations require a pair of \"ping-pong\" storage\n * buffers (e.g., one for reading from and the other for writing to, and then\n * vice-versa for the subsequent pass).  This structure wraps a set of device\n * buffers and a \"selector\" member to track which is \"current\".\n */\ntemplate <typename T>\nstruct DoubleBuffer\n{\n    /// Pair of device buffer pointers\n    T *d_buffers[2];\n\n    ///  Selector into \\p d_buffers (i.e., the active/valid buffer)\n    int selector;\n\n    /// \\brief Constructor\n    __host__ __device__ __forceinline__ DoubleBuffer()\n    {\n        selector = 0;\n        d_buffers[0] = NULL;\n        d_buffers[1] = NULL;\n    }\n\n    /// \\brief Constructor\n    __host__ __device__ __forceinline__ DoubleBuffer(\n        T *d_current,         ///< The currently valid buffer\n        T *d_alternate)       ///< Alternate storage buffer of the same size as \\p d_current\n    {\n        selector = 0;\n        d_buffers[0] = d_current;\n        d_buffers[1] = d_alternate;\n    }\n\n    /// \\brief Return pointer to the currently valid buffer\n    __host__ __device__ __forceinline__ T* Current() { return d_buffers[selector]; }\n\n    /// \\brief Return pointer to the currently invalid buffer\n    __host__ __device__ __forceinline__ T* Alternate() { return d_buffers[selector ^ 1]; }\n\n};\n\n\n\n/******************************************************************************\n * Typedef-detection\n ******************************************************************************/\n\n\n/**\n * \\brief Defines a structure \\p detector_name that is templated on type \\p T.  The \\p detector_name struct exposes a constant member \\p VALUE indicating whether or not parameter \\p T exposes a nested type \\p nested_type_name\n */\n#define CUB_DEFINE_DETECT_NESTED_TYPE(detector_name, nested_type_name)  \\\n    template <typename T>                                               \\\n    struct detector_name                                                \\\n    {                                                                   \\\n        template <typename C>                                           \\\n        static char& test(typename C::nested_type_name*);               \\\n        template <typename>                                             \\\n        static int& test(...);                                          \\\n        enum                                                            \\\n        {                                                               \\\n            VALUE = sizeof(test<T>(0)) < sizeof(int)                    \\\n        };                                                              \\\n    };\n\n\n\n/******************************************************************************\n * Simple enable-if (similar to Boost)\n ******************************************************************************/\n\n/**\n * \\brief Simple enable-if (similar to Boost)\n *\n * \\deprecated [Since 1.16.0] The cub::If APIs are deprecated.\n *             Use std::enable_if instead.\n */\ntemplate <bool Condition, class T = void>\nstruct CUB_DEPRECATED EnableIf\n{\n  using Type = typename std::enable_if<Condition, T>::type;\n};\n\n/******************************************************************************\n * Typedef-detection\n ******************************************************************************/\n\n/**\n * \\brief Determine whether or not BinaryOp's functor is of the form <tt>bool operator()(const T& a, const T&b)</tt> or <tt>bool operator()(const T& a, const T&b, unsigned int idx)</tt>\n */\ntemplate <typename T, typename BinaryOp>\nstruct BinaryOpHasIdxParam\n{\nprivate:\n/*\n    template <typename BinaryOpT, bool (BinaryOpT::*)(const T &a, const T &b, unsigned int idx) const>  struct SFINAE1 {};\n    template <typename BinaryOpT, bool (BinaryOpT::*)(const T &a, const T &b, unsigned int idx)>        struct SFINAE2 {};\n    template <typename BinaryOpT, bool (BinaryOpT::*)(T a, T b, unsigned int idx) const>                struct SFINAE3 {};\n    template <typename BinaryOpT, bool (BinaryOpT::*)(T a, T b, unsigned int idx)>                      struct SFINAE4 {};\n*/\n    template <typename BinaryOpT, bool (BinaryOpT::*)(const T &a, const T &b, int idx) const>           struct SFINAE5 {};\n    template <typename BinaryOpT, bool (BinaryOpT::*)(const T &a, const T &b, int idx)>                 struct SFINAE6 {};\n    template <typename BinaryOpT, bool (BinaryOpT::*)(T a, T b, int idx) const>                         struct SFINAE7 {};\n    template <typename BinaryOpT, bool (BinaryOpT::*)(T a, T b, int idx)>                               struct SFINAE8 {};\n/*\n    template <typename BinaryOpT> static char Test(SFINAE1<BinaryOpT, &BinaryOpT::operator()> *);\n    template <typename BinaryOpT> static char Test(SFINAE2<BinaryOpT, &BinaryOpT::operator()> *);\n    template <typename BinaryOpT> static char Test(SFINAE3<BinaryOpT, &BinaryOpT::operator()> *);\n    template <typename BinaryOpT> static char Test(SFINAE4<BinaryOpT, &BinaryOpT::operator()> *);\n*/\n    template <typename BinaryOpT> __host__ __device__ static char Test(SFINAE5<BinaryOpT, &BinaryOpT::operator()> *);\n    template <typename BinaryOpT> __host__ __device__ static char Test(SFINAE6<BinaryOpT, &BinaryOpT::operator()> *);\n    template <typename BinaryOpT> __host__ __device__ static char Test(SFINAE7<BinaryOpT, &BinaryOpT::operator()> *);\n    template <typename BinaryOpT> __host__ __device__ static char Test(SFINAE8<BinaryOpT, &BinaryOpT::operator()> *);\n\n    template <typename BinaryOpT> static int Test(...);\n\npublic:\n\n    /// Whether the functor BinaryOp has a third <tt>unsigned int</tt> index param\n    static const bool HAS_PARAM = sizeof(Test<BinaryOp>(NULL)) == sizeof(char);\n};\n\n\n\n\n/******************************************************************************\n * Simple type traits utilities.\n *\n * For example:\n *     Traits<int>::CATEGORY             // SIGNED_INTEGER\n *     Traits<NullType>::NULL_TYPE       // true\n *     Traits<uint4>::CATEGORY           // NOT_A_NUMBER\n *     Traits<uint4>::PRIMITIVE;         // false\n *\n ******************************************************************************/\n\n/**\n * \\brief Basic type traits categories\n */\nenum Category\n{\n    NOT_A_NUMBER,\n    SIGNED_INTEGER,\n    UNSIGNED_INTEGER,\n    FLOATING_POINT\n};\n\n\n/**\n * \\brief Basic type traits\n */\ntemplate <Category _CATEGORY, bool _PRIMITIVE, bool _NULL_TYPE, typename _UnsignedBits, typename T>\nstruct BaseTraits\n{\n    /// Category\n    static const Category CATEGORY      = _CATEGORY;\n    enum\n    {\n        PRIMITIVE       = _PRIMITIVE,\n        NULL_TYPE       = _NULL_TYPE,\n    };\n};\n\n\n/**\n * Basic type traits (unsigned primitive specialization)\n */\ntemplate <typename _UnsignedBits, typename T>\nstruct BaseTraits<UNSIGNED_INTEGER, true, false, _UnsignedBits, T>\n{\n    typedef _UnsignedBits       UnsignedBits;\n\n    static const Category       CATEGORY    = UNSIGNED_INTEGER;\n    static const UnsignedBits   LOWEST_KEY  = UnsignedBits(0);\n    static const UnsignedBits   MAX_KEY     = UnsignedBits(-1);\n\n    enum\n    {\n        PRIMITIVE       = true,\n        NULL_TYPE       = false,\n    };\n\n\n    static __host__ __device__ __forceinline__ UnsignedBits TwiddleIn(UnsignedBits key)\n    {\n        return key;\n    }\n\n    static __host__ __device__ __forceinline__ UnsignedBits TwiddleOut(UnsignedBits key)\n    {\n        return key;\n    }\n\n    static __host__ __device__ __forceinline__ T Max()\n    {\n        UnsignedBits retval_bits = MAX_KEY;\n        T retval;\n        memcpy(&retval, &retval_bits, sizeof(T));\n        return retval;\n    }\n\n    static __host__ __device__ __forceinline__ T Lowest()\n    {\n        UnsignedBits retval_bits = LOWEST_KEY;\n        T retval;\n        memcpy(&retval, &retval_bits, sizeof(T));\n        return retval;\n    }\n};\n\n\n/**\n * Basic type traits (signed primitive specialization)\n */\ntemplate <typename _UnsignedBits, typename T>\nstruct BaseTraits<SIGNED_INTEGER, true, false, _UnsignedBits, T>\n{\n    typedef _UnsignedBits       UnsignedBits;\n\n    static const Category       CATEGORY    = SIGNED_INTEGER;\n    static const UnsignedBits   HIGH_BIT    = UnsignedBits(1) << ((sizeof(UnsignedBits) * 8) - 1);\n    static const UnsignedBits   LOWEST_KEY  = HIGH_BIT;\n    static const UnsignedBits   MAX_KEY     = UnsignedBits(-1) ^ HIGH_BIT;\n\n    enum\n    {\n        PRIMITIVE       = true,\n        NULL_TYPE       = false,\n    };\n\n    static __host__ __device__ __forceinline__ UnsignedBits TwiddleIn(UnsignedBits key)\n    {\n        return key ^ HIGH_BIT;\n    };\n\n    static __host__ __device__ __forceinline__ UnsignedBits TwiddleOut(UnsignedBits key)\n    {\n        return key ^ HIGH_BIT;\n    };\n\n    static __host__ __device__ __forceinline__ T Max()\n    {\n        UnsignedBits retval = MAX_KEY;\n        return reinterpret_cast<T&>(retval);\n    }\n\n    static __host__ __device__ __forceinline__ T Lowest()\n    {\n        UnsignedBits retval = LOWEST_KEY;\n        return reinterpret_cast<T&>(retval);\n    }\n};\n\ntemplate <typename _T>\nstruct FpLimits;\n\ntemplate <>\nstruct FpLimits<float>\n{\n    static __host__ __device__ __forceinline__ float Max() {\n        return FLT_MAX;\n    }\n\n    static __host__ __device__ __forceinline__ float Lowest() {\n        return FLT_MAX * float(-1);\n    }\n};\n\ntemplate <>\nstruct FpLimits<double>\n{\n    static __host__ __device__ __forceinline__ double Max() {\n        return DBL_MAX;\n    }\n\n    static __host__ __device__ __forceinline__ double Lowest() {\n        return DBL_MAX  * double(-1);\n    }\n};\n\n#if !_NVHPC_CUDA\ntemplate <>\nstruct FpLimits<__half>\n{\n    static __host__ __device__ __forceinline__ __half Max() {\n        unsigned short max_word = 0x7BFF;\n        return reinterpret_cast<__half&>(max_word);\n    }\n\n    static __host__ __device__ __forceinline__ __half Lowest() {\n        unsigned short lowest_word = 0xFBFF;\n        return reinterpret_cast<__half&>(lowest_word);\n    }\n};\n#endif\n\n#if !_NVHPC_CUDA && !defined(CUB_DISABLE_BF16_SUPPORT)\ntemplate <>\nstruct FpLimits<__nv_bfloat16>\n{\n    static __host__ __device__ __forceinline__ __nv_bfloat16 Max() {\n        unsigned short max_word = 0x7F7F;\n        return reinterpret_cast<__nv_bfloat16&>(max_word);\n    }\n\n    static __host__ __device__ __forceinline__ __nv_bfloat16 Lowest() {\n        unsigned short lowest_word = 0xFF7F;\n        return reinterpret_cast<__nv_bfloat16&>(lowest_word);\n    }\n};\n#endif\n\n/**\n * Basic type traits (fp primitive specialization)\n */\ntemplate <typename _UnsignedBits, typename T>\nstruct BaseTraits<FLOATING_POINT, true, false, _UnsignedBits, T>\n{\n    typedef _UnsignedBits       UnsignedBits;\n\n    static const Category       CATEGORY    = FLOATING_POINT;\n    static const UnsignedBits   HIGH_BIT    = UnsignedBits(1) << ((sizeof(UnsignedBits) * 8) - 1);\n    static const UnsignedBits   LOWEST_KEY  = UnsignedBits(-1);\n    static const UnsignedBits   MAX_KEY     = UnsignedBits(-1) ^ HIGH_BIT;\n\n    enum\n    {\n        PRIMITIVE       = true,\n        NULL_TYPE       = false,\n    };\n\n    static __host__ __device__ __forceinline__ UnsignedBits TwiddleIn(UnsignedBits key)\n    {\n        UnsignedBits mask = (key & HIGH_BIT) ? UnsignedBits(-1) : HIGH_BIT;\n        return key ^ mask;\n    };\n\n    static __host__ __device__ __forceinline__ UnsignedBits TwiddleOut(UnsignedBits key)\n    {\n        UnsignedBits mask = (key & HIGH_BIT) ? HIGH_BIT : UnsignedBits(-1);\n        return key ^ mask;\n    };\n\n    static __host__ __device__ __forceinline__ T Max() {\n        return FpLimits<T>::Max();\n    }\n\n    static __host__ __device__ __forceinline__ T Lowest() {\n        return FpLimits<T>::Lowest();\n    }\n};\n\n\n/**\n * \\brief Numeric type traits\n */\n// clang-format off\ntemplate <typename T> struct NumericTraits :            BaseTraits<NOT_A_NUMBER, false, false, T, T> {};\n\ntemplate <> struct NumericTraits<NullType> :            BaseTraits<NOT_A_NUMBER, false, true, NullType, NullType> {};\n\ntemplate <> struct NumericTraits<char> :                BaseTraits<(std::numeric_limits<char>::is_signed) ? SIGNED_INTEGER : UNSIGNED_INTEGER, true, false, unsigned char, char> {};\ntemplate <> struct NumericTraits<signed char> :         BaseTraits<SIGNED_INTEGER, true, false, unsigned char, signed char> {};\ntemplate <> struct NumericTraits<short> :               BaseTraits<SIGNED_INTEGER, true, false, unsigned short, short> {};\ntemplate <> struct NumericTraits<int> :                 BaseTraits<SIGNED_INTEGER, true, false, unsigned int, int> {};\ntemplate <> struct NumericTraits<long> :                BaseTraits<SIGNED_INTEGER, true, false, unsigned long, long> {};\ntemplate <> struct NumericTraits<long long> :           BaseTraits<SIGNED_INTEGER, true, false, unsigned long long, long long> {};\n\ntemplate <> struct NumericTraits<unsigned char> :       BaseTraits<UNSIGNED_INTEGER, true, false, unsigned char, unsigned char> {};\ntemplate <> struct NumericTraits<unsigned short> :      BaseTraits<UNSIGNED_INTEGER, true, false, unsigned short, unsigned short> {};\ntemplate <> struct NumericTraits<unsigned int> :        BaseTraits<UNSIGNED_INTEGER, true, false, unsigned int, unsigned int> {};\ntemplate <> struct NumericTraits<unsigned long> :       BaseTraits<UNSIGNED_INTEGER, true, false, unsigned long, unsigned long> {};\ntemplate <> struct NumericTraits<unsigned long long> :  BaseTraits<UNSIGNED_INTEGER, true, false, unsigned long long, unsigned long long> {};\n\n\n#if CUB_IS_INT128_ENABLED \ntemplate <>\nstruct NumericTraits<__uint128_t>\n{\n  using T = __uint128_t;\n  using UnsignedBits = __uint128_t;\n\n  static constexpr Category       CATEGORY    = UNSIGNED_INTEGER;\n  static constexpr UnsignedBits   LOWEST_KEY  = UnsignedBits(0);\n  static constexpr UnsignedBits   MAX_KEY     = UnsignedBits(-1);\n\n  static constexpr bool PRIMITIVE = false;\n  static constexpr bool NULL_TYPE = false;\n\n  static __host__ __device__ __forceinline__ UnsignedBits TwiddleIn(UnsignedBits key)\n  {\n    return key;\n  }\n\n  static __host__ __device__ __forceinline__ UnsignedBits TwiddleOut(UnsignedBits key)\n  {\n    return key;\n  }\n\n  static __host__ __device__ __forceinline__ T Max()\n  {\n    return MAX_KEY;\n  }\n\n  static __host__ __device__ __forceinline__ T Lowest()\n  {\n    return LOWEST_KEY;\n  }\n};\n\ntemplate <>\nstruct NumericTraits<__int128_t>\n{\n  using T = __int128_t;\n  using UnsignedBits = __uint128_t;\n\n  static constexpr Category       CATEGORY    = SIGNED_INTEGER;\n  static constexpr UnsignedBits   HIGH_BIT    = UnsignedBits(1) << ((sizeof(UnsignedBits) * 8) - 1);\n  static constexpr UnsignedBits   LOWEST_KEY  = HIGH_BIT;\n  static constexpr UnsignedBits   MAX_KEY     = UnsignedBits(-1) ^ HIGH_BIT;\n\n  static constexpr bool PRIMITIVE = false;\n  static constexpr bool NULL_TYPE = false;\n\n  static __host__ __device__ __forceinline__ UnsignedBits TwiddleIn(UnsignedBits key)\n  {\n    return key ^ HIGH_BIT;\n  };\n\n  static __host__ __device__ __forceinline__ UnsignedBits TwiddleOut(UnsignedBits key)\n  {\n    return key ^ HIGH_BIT;\n  };\n\n  static __host__ __device__ __forceinline__ T Max()\n  {\n    UnsignedBits retval = MAX_KEY;\n    return reinterpret_cast<T&>(retval);\n  }\n\n  static __host__ __device__ __forceinline__ T Lowest()\n  {\n    UnsignedBits retval = LOWEST_KEY;\n    return reinterpret_cast<T&>(retval);\n  }\n};\n#endif\n\ntemplate <> struct NumericTraits<float> :               BaseTraits<FLOATING_POINT, true, false, unsigned int, float> {};\ntemplate <> struct NumericTraits<double> :              BaseTraits<FLOATING_POINT, true, false, unsigned long long, double> {};\n#if !_NVHPC_CUDA\n    template <> struct NumericTraits<__half> :          BaseTraits<FLOATING_POINT, true, false, unsigned short, __half> {};\n#endif\n#if !_NVHPC_CUDA && !defined(CUB_DISABLE_BF16_SUPPORT)\n    template <> struct NumericTraits<__nv_bfloat16> :   BaseTraits<FLOATING_POINT, true, false, unsigned short, __nv_bfloat16> {};\n#endif\n\ntemplate <> struct NumericTraits<bool> :                BaseTraits<UNSIGNED_INTEGER, true, false, typename UnitWord<bool>::VolatileWord, bool> {};\n// clang-format on\n\n/**\n * \\brief Type traits\n */\ntemplate <typename T>\nstruct Traits : NumericTraits<typename std::remove_cv<T>::type> {};\n\n\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\n\n/** @} */       // end group UtilModule\n\nCUB_NAMESPACE_END\n\n#endif // _JITIFY_INCLUDE_GUARD_1375AABB974C3530\n", "cub/warp/specializations/warp_exchange_shfl.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_F33B81BC5307E5E5\n#define _JITIFY_INCLUDE_GUARD_F33B81BC5307E5E5\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011-2023, NVIDIA CORPORATION.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n#include <cub/config.cuh>\n#include <cub/util_ptx.cuh>\n#include <cub/util_type.cuh>\n\nCUB_NAMESPACE_BEGIN\n\nnamespace detail\n{\n\ntemplate <typename InputT,\n          int ITEMS_PER_THREAD,\n          int LOGICAL_WARP_THREADS = CUB_PTX_WARP_THREADS>\nclass WarpExchangeShfl\n{\n  static_assert(PowerOfTwo<LOGICAL_WARP_THREADS>::VALUE,\n                \"LOGICAL_WARP_THREADS must be a power of two\");\n\n  static_assert(ITEMS_PER_THREAD == LOGICAL_WARP_THREADS,\n                \"WARP_EXCHANGE_SHUFFLE currently only works when ITEMS_PER_THREAD == \"\n                \"LOGICAL_WARP_THREADS\");\n\n  constexpr static bool IS_ARCH_WARP = LOGICAL_WARP_THREADS == CUB_WARP_THREADS(0);\n\n  // concrete recursion class\n  template <typename OutputT, int IDX, int SIZE> \n  class CompileTimeArray : protected CompileTimeArray<OutputT, IDX + 1, SIZE>\n  {\n  protected:\n    InputT val;\n\n    template <int NUM_ENTRIES>\n    __device__ void Foreach(const bool xor_bit_set, const unsigned mask)\n    {\n      // The implementation here is a recursive divide-and-conquer approach\n      // that takes inspiration from:\n      // https://forums.developer.nvidia.com/t/transposing-register-held-matrices-with-warp-shuffles-need-help/38652/2\n      //\n      // At its core, the problem can be boiled down to transposing the matrix\n      //\n      //   A B\n      //   C D\n      //\n      // by swapping the off-diagonal elements/sub-matrices B and C recursively.\n      //\n      // This implementation requires power-of-two matrices. In order to avoid\n      // the use of local or shared memory, all index computation has to occur\n      // at compile-time, since registers cannot be indexed dynamically.\n      // Furthermore, using recursive templates reduces the mental load on the\n      // optimizer, since lowering for-loops into registers oftentimes requires\n      // finagling them with #pragma unroll, which leads to brittle code.\n      //\n      // To illustrate this algorithm, let's pretend we have warpSize = 8,\n      // where t0, ..., t7 denote the 8 threads, and thread i has an array of\n      // size 8 with data = [Ai, Bi, ..., Hi] (the columns in the schematics).\n      //\n      // In the first round, we exchange the largest 4x4 off-diagonal\n      // submatrix. Boxes illustrate the submatrices to be exchanged.\n      //\n      //       ROUND 1\n      //       =======\n      //  t0  t1  t2  t3  t4  t5  t6  t7\n      //                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      //  A0  A1  A2  A3 \u2502A4  A5  A6  A7\u2502    NUM_ENTRIES == 4 tells us how many\n      //                 \u2502              \u2502       entries we have in a submatrix,\n      //                 \u2502              \u2502       in this case 4 and the size of\n      //  B0  B1  B2  B3 \u2502B4  B5  B6  B7\u2502       the jumps between submatrices.\n      //                 \u2502              \u2502\n      //                 \u2502              \u2502  1. t[0,1,2,3] data[4] swap with t[4,5,6,7]'s data[0]\n      //  C0  C1  C2  C3 \u2502C4  C5  C6  C7\u2502  2. t[0,1,2,3] data[5] swap with t[4,5,6,7]'s data[1]\n      //                 \u2502              \u2502  3. t[0,1,2,3] data[6] swap with t[4,5,6,7]'s data[2]\n      //                 \u2502              \u2502  4. t[0,1,2,3] data[7] swap with t[4,5,6,7]'s data[3]\n      //  D0  D1  D2  D3 \u2502D4  D5  D6  D7\u2502\n      //                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n      // \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      // \u2502E0  E1  E2  E3\u2502 E4  E5  E6  E7\n      // \u2502              \u2502\n      // \u2502              \u2502\n      // \u2502F0  F1  F2  F3\u2502 F4  F5  F6  F7\n      // \u2502              \u2502\n      // \u2502              \u2502\n      // \u2502G0  G1  G2  G3\u2502 G4  G5  G6  G7\n      // \u2502              \u2502\n      // \u2502              \u2502\n      // \u2502H0  H1  H2  H3\u2502 H4  H5  H6  H7\n      // \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n      //\n      //       ROUND 2\n      //       =======\n      //  t0  t1  t2  t3  t4  t5  t6  t7\n      //         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      //  A0  A1 \u2502A2  A3\u2502 E0  E1 \u2502E2  E3\u2502    NUM_ENTRIES == 2 so we have 2\n      //         \u2502      \u2502        \u2502      \u2502       submatrices per thread and there\n      //         \u2502      \u2502        \u2502      \u2502       are 2 elements between these\n      //  B0  B1 \u2502B2  B3\u2502 F0  F1 \u2502F2  F3\u2502       submatrices.\n      //         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n      // \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510          1. t[0,1,4,5] data[2] swap with t[2,3,6,7]'s data[0]\n      // \u2502C0  C1\u2502 C2  C3 \u2502G0  G1\u2502 G2  G3   2. t[0,1,4,5] data[3] swap with t[2,3,6,7]'s data[1]\n      // \u2502      \u2502        \u2502      \u2502          3. t[0,1,4,5] data[6] swap with t[2,3,6,7]'s data[4]\n      // \u2502      \u2502        \u2502      \u2502          4. t[0,1,4,5] data[7] swap with t[2,3,6,7]'s data[5]\n      // \u2502D0  D1\u2502 D2  D3 \u2502H0  H1\u2502 H2  H3\n      // \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n      //         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      //  A4  A5 \u2502A6  A7\u2502 E4  E5 \u2502E6  E7\u2502\n      //         \u2502      \u2502        \u2502      \u2502\n      //         \u2502      \u2502        \u2502      \u2502\n      //  B4  B5 \u2502B6  B7\u2502 F4  F5 \u2502F6  F7\u2502\n      //         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n      // \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      // \u2502C4  C5\u2502 C6  C7 \u2502G4  G5\u2502 G6  G7\n      // \u2502      \u2502        \u2502      \u2502\n      // \u2502      \u2502        \u2502      \u2502\n      // \u2502D4  D5\u2502 D6  D7 \u2502H4  H5\u2502 H6  H7\n      // \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n      //\n      //       ROUND 3\n      //       =======\n      //  t0  t1  t2  t3  t4  t5  t6  t7\n      //     \u250c\u2500\u2500\u2510    \u250c\u2500\u2500\u2510    \u250c\u2500\u2500\u2510    \u250c\u2500\u2500\u2510\n      //  A0 \u2502A1\u2502 C0 \u2502C1\u2502 E0 \u2502E1\u2502 G0 \u2502G1\u2502    NUM_ENTRIES == 1 so we have 4\n      //     \u2514\u2500\u2500\u2518    \u2514\u2500\u2500\u2518    \u2514\u2500\u2500\u2518    \u2514\u2500\u2500\u2518       submatrices per thread and there\n      // \u250c\u2500\u2500\u2510    \u250c\u2500\u2500\u2510    \u250c\u2500\u2500\u2510    \u250c\u2500\u2500\u2510           is 1 element between these\n      // \u2502B0\u2502 B1 \u2502D0\u2502 D1 \u2502F0\u2502 F1 \u2502H0\u2502 H1        submatrices.\n      // \u2514\u2500\u2500\u2518    \u2514\u2500\u2500\u2518    \u2514\u2500\u2500\u2518    \u2514\u2500\u2500\u2518\n      //     \u250c\u2500\u2500\u2510    \u250c\u2500\u2500\u2510    \u250c\u2500\u2500\u2510    \u250c\u2500\u2500\u2510  1. t[0,2,4,6] data[1] swap with t[1,3,5,7]'s data[0]\n      //  A2 \u2502A3\u2502 C2 \u2502C3\u2502 E2 \u2502E3\u2502 G2 \u2502G3\u2502  2. t[0,2,4,6] data[3] swap with t[1,3,5,7]'s data[2]\n      //     \u2514\u2500\u2500\u2518    \u2514\u2500\u2500\u2518    \u2514\u2500\u2500\u2518    \u2514\u2500\u2500\u2518  3. t[0,2,4,6] data[5] swap with t[1,3,5,7]'s data[4]\n      // \u250c\u2500\u2500\u2510    \u250c\u2500\u2500\u2510    \u250c\u2500\u2500\u2510    \u250c\u2500\u2500\u2510      4. t[0,2,4,6] data[7] swap with t[1,3,5,7]'s data[6]\n      // \u2502B2\u2502 B3 \u2502D2\u2502 D3 \u2502F2\u2502 F3 \u2502H2\u2502 H3\n      // \u2514\u2500\u2500\u2518    \u2514\u2500\u2500\u2518    \u2514\u2500\u2500\u2518    \u2514\u2500\u2500\u2518\n      //     \u250c\u2500\u2500\u2510    \u250c\u2500\u2500\u2510    \u250c\u2500\u2500\u2510    \u250c\u2500\u2500\u2510\n      //  A4 \u2502A5\u2502 C4 \u2502C5\u2502 E4 \u2502E5\u2502 G4 \u2502G5\u2502\n      //     \u2514\u2500\u2500\u2518    \u2514\u2500\u2500\u2518    \u2514\u2500\u2500\u2518    \u2514\u2500\u2500\u2518\n      // \u250c\u2500\u2500\u2510    \u250c\u2500\u2500\u2510    \u250c\u2500\u2500\u2510    \u250c\u2500\u2500\u2510\n      // \u2502B4\u2502 B5 \u2502D4\u2502 D5 \u2502F4\u2502 F5 \u2502H4\u2502 H5\n      // \u2514\u2500\u2500\u2518    \u2514\u2500\u2500\u2518    \u2514\u2500\u2500\u2518    \u2514\u2500\u2500\u2518\n      //     \u250c\u2500\u2500\u2510    \u250c\u2500\u2500\u2510    \u250c\u2500\u2500\u2510    \u250c\u2500\u2500\u2510\n      //  A6 \u2502A7\u2502 C6 \u2502C7\u2502 E6 \u2502E7\u2502 G6 \u2502G7\u2502\n      //     \u2514\u2500\u2500\u2518    \u2514\u2500\u2500\u2518    \u2514\u2500\u2500\u2518    \u2514\u2500\u2500\u2518\n      // \u250c\u2500\u2500\u2510    \u250c\u2500\u2500\u2510    \u250c\u2500\u2500\u2510    \u250c\u2500\u2500\u2510\n      // \u2502B6\u2502 B7 \u2502D6\u2502 D7 \u2502F6\u2502 F7 \u2502H6\u2502 H7\n      // \u2514\u2500\u2500\u2518    \u2514\u2500\u2500\u2518    \u2514\u2500\u2500\u2518    \u2514\u2500\u2500\u2518\n      //\n      //       RESULT\n      //       ======\n      //  t0  t1  t2  t3  t4  t5  t6  t7\n      //\n      //  A0  B0  C0  D0  E0  F0  G0  H0\n      //\n      //\n      //  A1  B1  C1  D1  E1  F1  G1  H1\n      //\n      //\n      //  A2  B2  C2  D2  E2  F2  G2  H2\n      //\n      //\n      //  A3  B3  C3  D3  E3  F3  G3  H3\n      //\n      //\n      //  A4  B4  C4  D4  E4  F4  G4  H4\n      //\n      //\n      //  A5  B5  C5  D5  E5  F5  G5  H5\n      //\n      //\n      //  A6  B6  C6  D6  E6  F6  G6  H6\n      //\n      //\n      //  A7  B7  C7  D7  E7  F7  G7  H7\n      //\n\n      // NOTE: Do *NOT* try to refactor this code to use a reference, since nvcc\n      //       tends to choke on it and then drop everything into local memory.\n      const InputT send_val = (xor_bit_set ? CompileTimeArray<OutputT, IDX, SIZE>::val\n                                           : CompileTimeArray<OutputT, IDX + NUM_ENTRIES, SIZE>::val);\n      const InputT recv_val = __shfl_xor_sync(mask, send_val, NUM_ENTRIES, LOGICAL_WARP_THREADS);\n      (xor_bit_set ? CompileTimeArray<OutputT, IDX, SIZE>::val\n                   : CompileTimeArray<OutputT, IDX + NUM_ENTRIES, SIZE>::val) = recv_val;\n\n      constexpr int next_idx = IDX + 1 + ((IDX + 1) % NUM_ENTRIES == 0) * NUM_ENTRIES;\n      CompileTimeArray<OutputT, next_idx, SIZE>::template Foreach<NUM_ENTRIES>(xor_bit_set, mask);\n    }\n\n    // terminate recursion\n    __device__ void TransposeImpl(unsigned int, unsigned int, Int2Type<0>) {}\n\n    template <int NUM_ENTRIES>\n    __device__ void TransposeImpl(const unsigned int lane_id,\n                                  const unsigned int mask,\n                                  Int2Type<NUM_ENTRIES>)\n    {\n      const bool xor_bit_set = lane_id & NUM_ENTRIES;\n      Foreach<NUM_ENTRIES>(xor_bit_set, mask);\n\n      TransposeImpl(lane_id, mask, Int2Type<NUM_ENTRIES / 2>());\n    }\n\n  public:\n    __device__ CompileTimeArray(const InputT (&input_items)[ITEMS_PER_THREAD],\n                                OutputT (&output_items)[ITEMS_PER_THREAD])\n        : CompileTimeArray<OutputT, IDX + 1, SIZE>{input_items, output_items}\n        , val{input_items[IDX]}\n    {}\n\n    __device__ ~CompileTimeArray() { this->output_items[IDX] = val; }\n\n    __device__ void Transpose(const unsigned int lane_id, const unsigned int mask)\n    {\n      TransposeImpl(lane_id, mask, Int2Type<ITEMS_PER_THREAD / 2>());\n    }\n  };\n\n  // terminating partial specialization\n  template <typename OutputT, int SIZE> \n  class CompileTimeArray<OutputT, SIZE, SIZE>\n  {\n  protected:\n    // used for dumping back the individual values after transposing\n    InputT (&output_items)[ITEMS_PER_THREAD];\n\n    template <int>\n    __device__ void Foreach(bool, unsigned)\n    {}\n\n  public:\n    __device__ CompileTimeArray(const InputT (&)[ITEMS_PER_THREAD],\n                                OutputT (&output_items)[ITEMS_PER_THREAD])\n        : output_items{output_items}\n    {}\n  };\n\n\n  const unsigned int lane_id;\n  const unsigned int warp_id;\n  const unsigned int member_mask;\n\npublic:\n  using TempStorage = NullType;\n\n  WarpExchangeShfl() = delete;\n\n  explicit __device__ __forceinline__ WarpExchangeShfl(TempStorage &)\n      : lane_id(IS_ARCH_WARP ? LaneId() : (LaneId() % LOGICAL_WARP_THREADS))\n      , warp_id(IS_ARCH_WARP ? 0 : (LaneId() / LOGICAL_WARP_THREADS))\n      , member_mask(WarpMask<LOGICAL_WARP_THREADS>(warp_id))\n  {}\n\n  template <typename OutputT>\n  __device__ __forceinline__ void BlockedToStriped(const InputT (&input_items)[ITEMS_PER_THREAD],\n                                                   OutputT (&output_items)[ITEMS_PER_THREAD])\n  {\n    CompileTimeArray<OutputT, 0, ITEMS_PER_THREAD> arr{input_items, output_items};\n    arr.Transpose(lane_id, member_mask);\n  }\n\n  template <typename OutputT>\n  __device__ __forceinline__ void StripedToBlocked(const InputT (&input_items)[ITEMS_PER_THREAD],\n                                                   OutputT (&output_items)[ITEMS_PER_THREAD])\n  {\n    BlockedToStriped(input_items, output_items);\n  }\n\n  // Trick to keep the compiler from inferring that the\n  // condition in the static_assert is always false.\n  template <typename T>\n  struct dependent_false\n  {\n    static constexpr bool value = false;\n  };\n\n  template <typename OffsetT>\n  __device__ __forceinline__ void\n  ScatterToStriped(InputT (&)[ITEMS_PER_THREAD],\n                   OffsetT (&)[ITEMS_PER_THREAD])\n  {\n    static_assert(dependent_false<OffsetT>::value,\n                  \"Shuffle specialization of warp exchange does not support\\n\"\n                  \"ScatterToStriped(InputT (&items)[ITEMS_PER_THREAD],\\n\"\n                  \"                 OffsetT (&ranks)[ITEMS_PER_THREAD])\");\n  }\n\n  template <typename OutputT,\n            typename OffsetT>\n  __device__ __forceinline__ void\n  ScatterToStriped(const InputT (&)[ITEMS_PER_THREAD],\n                   OutputT (&)[ITEMS_PER_THREAD],\n                   OffsetT (&)[ITEMS_PER_THREAD])\n  {\n    static_assert(dependent_false<OffsetT>::value,\n                  \"Shuffle specialization of warp exchange does not support\\n\"\n                  \"ScatterToStriped(const InputT (&input_items)[ITEMS_PER_THREAD],\\n\"\n                  \"                 OutputT (&output_items)[ITEMS_PER_THREAD],\\n\"\n                  \"                 OffsetT (&ranks)[ITEMS_PER_THREAD])\");\n  }\n};\n\n} // namespace detail\n\nCUB_NAMESPACE_END\n\n#endif // _JITIFY_INCLUDE_GUARD_F33B81BC5307E5E5\n", "cub/warp/specializations/warp_exchange_smem.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_8733337D6CD84000\n#define _JITIFY_INCLUDE_GUARD_8733337D6CD84000\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011-2023, NVIDIA CORPORATION.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * @file\n * The cub::WarpExchangeSmem class provides [<em>collective</em>](index.html#sec0)\n * methods for rearranging data partitioned across a CUDA warp.\n */\n\n#include <cub/config.cuh>\n#include <cub/util_ptx.cuh>\n#include <cub/util_type.cuh>\n\nCUB_NAMESPACE_BEGIN\n\nnamespace detail\n{\n\ntemplate <typename InputT,\n          int ITEMS_PER_THREAD,\n          int LOGICAL_WARP_THREADS  = CUB_PTX_WARP_THREADS>\nclass WarpExchangeSmem\n{\n  static_assert(PowerOfTwo<LOGICAL_WARP_THREADS>::VALUE,\n                \"LOGICAL_WARP_THREADS must be a power of two\");\n\n  constexpr static int ITEMS_PER_TILE =\n    ITEMS_PER_THREAD * LOGICAL_WARP_THREADS + 1;\n\n  constexpr static bool IS_ARCH_WARP = LOGICAL_WARP_THREADS == CUB_WARP_THREADS(0);\n\n  constexpr static int LOG_SMEM_BANKS = CUB_LOG_SMEM_BANKS(0);\n\n  // Insert padding if the number of items per thread is a power of two\n  // and > 4 (otherwise we can typically use 128b loads)\n  constexpr static bool INSERT_PADDING = (ITEMS_PER_THREAD > 4) &&\n                                         (PowerOfTwo<ITEMS_PER_THREAD>::VALUE);\n\n  constexpr static int PADDING_ITEMS = INSERT_PADDING\n                                     ? (ITEMS_PER_TILE >> LOG_SMEM_BANKS)\n                                     : 0;\n\n  union _TempStorage\n  {\n    InputT items_shared[ITEMS_PER_TILE + PADDING_ITEMS];\n  }; // union TempStorage\n\n  /// Shared storage reference\n  _TempStorage &temp_storage;\n\n  const unsigned int lane_id;\n  const unsigned int warp_id;\n  const unsigned int member_mask;\n\npublic:\n\n  struct TempStorage : Uninitialized<_TempStorage> {};\n\n  WarpExchangeSmem() = delete;\n\n  explicit __device__ __forceinline__\n  WarpExchangeSmem(TempStorage &temp_storage)\n      : temp_storage(temp_storage.Alias())\n      , lane_id(IS_ARCH_WARP ? LaneId() : (LaneId() % LOGICAL_WARP_THREADS))\n      , warp_id(IS_ARCH_WARP ? 0 : (LaneId() / LOGICAL_WARP_THREADS))\n      , member_mask(WarpMask<LOGICAL_WARP_THREADS>(warp_id))\n  {}\n\n  template <typename OutputT>\n  __device__ __forceinline__ void\n  BlockedToStriped(const InputT (&input_items)[ITEMS_PER_THREAD],\n                   OutputT (&output_items)[ITEMS_PER_THREAD])\n  {\n    for (int item = 0; item < ITEMS_PER_THREAD; item++)\n    {\n      const int idx = ITEMS_PER_THREAD * lane_id + item;\n      temp_storage.items_shared[idx] = input_items[item];\n    }\n    WARP_SYNC(member_mask);\n\n    for (int item = 0; item < ITEMS_PER_THREAD; item++)\n    {\n      const int idx = LOGICAL_WARP_THREADS * item + lane_id;\n      output_items[item] = temp_storage.items_shared[idx];\n    }\n  }\n\n  template <typename OutputT>\n  __device__ __forceinline__ void\n  StripedToBlocked(const InputT (&input_items)[ITEMS_PER_THREAD],\n                   OutputT (&output_items)[ITEMS_PER_THREAD])\n  {\n    for (int item = 0; item < ITEMS_PER_THREAD; item++)\n    {\n      const int idx = LOGICAL_WARP_THREADS * item + lane_id;\n      temp_storage.items_shared[idx] = input_items[item];\n    }\n    WARP_SYNC(member_mask);\n\n    for (int item = 0; item < ITEMS_PER_THREAD; item++)\n    {\n      const int idx = ITEMS_PER_THREAD * lane_id + item;\n      output_items[item] = temp_storage.items_shared[idx];\n    }\n  }\n\n  template <typename OffsetT>\n  __device__ __forceinline__ void\n  ScatterToStriped(InputT (&items)[ITEMS_PER_THREAD],\n                   OffsetT (&ranks)[ITEMS_PER_THREAD])\n  {\n    ScatterToStriped(items, items, ranks);\n  }\n\n  template <typename OutputT,\n            typename OffsetT>\n  __device__ __forceinline__ void\n  ScatterToStriped(const InputT (&input_items)[ITEMS_PER_THREAD],\n                   OutputT (&output_items)[ITEMS_PER_THREAD],\n                   OffsetT (&ranks)[ITEMS_PER_THREAD])\n  {\n    _Pragma(\"unroll\")\n    for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n    {\n      if (INSERT_PADDING)\n      {\n        ranks[ITEM] = SHR_ADD(ranks[ITEM], LOG_SMEM_BANKS, ranks[ITEM]);\n      }\n\n      temp_storage.items_shared[ranks[ITEM]] = input_items[ITEM];\n    }\n\n    WARP_SYNC(member_mask);\n\n    _Pragma(\"unroll\")\n    for (int ITEM = 0; ITEM < ITEMS_PER_THREAD; ITEM++)\n    {\n      int item_offset = (ITEM * LOGICAL_WARP_THREADS) + lane_id;\n\n      if (INSERT_PADDING)\n      {\n        item_offset = SHR_ADD(item_offset, LOG_SMEM_BANKS, item_offset);\n      }\n\n      output_items[ITEM] = temp_storage.items_shared[item_offset];\n    }\n  }\n};\n\n} // namespace detail\n\nCUB_NAMESPACE_END\n\n#endif // _JITIFY_INCLUDE_GUARD_8733337D6CD84000\n", "cub/warp/specializations/warp_reduce_shfl.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_EEF564931BA0D156\n#define _JITIFY_INCLUDE_GUARD_EEF564931BA0D156\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * cub::WarpReduceShfl provides SHFL-based variants of parallel reduction of items partitioned across a CUDA thread warp.\n */\n\n#include \"../../config.cuh\"\n#include \"../../thread/thread_operators.cuh\"\n#include \"../../util_ptx.cuh\"\n#include \"../../util_type.cuh\"\n\n#include <stdint.h>\n\n#include <cuda/std/type_traits>\n#include <nv/target>\n\nCUB_NAMESPACE_BEGIN\n\n\nnamespace detail \n{\n\ntemplate <class A = int, class = A>\nstruct reduce_add_exists : ::cuda::std::false_type \n{};\n\ntemplate <class T>\nstruct reduce_add_exists<T, decltype(__reduce_add_sync(0xFFFFFFFF, T{}))> : ::cuda::std::true_type \n{};\n\ntemplate <class T = int, class = T>\nstruct reduce_min_exists : ::cuda::std::false_type \n{};\n\ntemplate <class T>\nstruct reduce_min_exists<T, decltype(__reduce_min_sync(0xFFFFFFFF, T{}))> : ::cuda::std::true_type \n{};\n\ntemplate <class T = int, class = T>\nstruct reduce_max_exists : ::cuda::std::false_type \n{};\n\ntemplate <class T>\nstruct reduce_max_exists<T, decltype(__reduce_max_sync(0xFFFFFFFF, T{}))> : ::cuda::std::true_type \n{};\n\n}\n\n\n/**\n * \\brief WarpReduceShfl provides SHFL-based variants of parallel reduction of items partitioned across a CUDA thread warp.\n *\n * LOGICAL_WARP_THREADS must be a power-of-two\n */\ntemplate <\n    typename    T,                      ///< Data type being reduced\n    int         LOGICAL_WARP_THREADS,   ///< Number of threads per logical warp\n    int         LEGACY_PTX_ARCH = 0>    ///< The PTX compute capability for which to to specialize this collective\nstruct WarpReduceShfl\n{\n    static_assert(PowerOfTwo<LOGICAL_WARP_THREADS>::VALUE,\n                  \"LOGICAL_WARP_THREADS must be a power of two\");\n\n    //---------------------------------------------------------------------\n    // Constants and type definitions\n    //---------------------------------------------------------------------\n\n    enum\n    {\n        /// Whether the logical warp size and the PTX warp size coincide\n        IS_ARCH_WARP = (LOGICAL_WARP_THREADS == CUB_WARP_THREADS(0)),\n\n        /// The number of warp reduction steps\n        STEPS = Log2<LOGICAL_WARP_THREADS>::VALUE,\n\n        /// Number of logical warps in a PTX warp\n        LOGICAL_WARPS = CUB_WARP_THREADS(0) / LOGICAL_WARP_THREADS,\n\n        /// The 5-bit SHFL mask for logically splitting warps into sub-segments starts 8-bits up\n        SHFL_C = (CUB_WARP_THREADS(0) - LOGICAL_WARP_THREADS) << 8\n\n    };\n\n    template <typename S>\n    struct IsInteger\n    {\n        enum {\n            ///Whether the data type is a small (32b or less) integer for which we can use a single SFHL instruction per exchange\n            IS_SMALL_UNSIGNED = (Traits<S>::CATEGORY == UNSIGNED_INTEGER) && (sizeof(S) <= sizeof(unsigned int))\n        };\n    };\n\n\n    /// Shared memory storage layout type\n    typedef NullType TempStorage;\n\n\n    //---------------------------------------------------------------------\n    // Thread fields\n    //---------------------------------------------------------------------\n\n    /// Lane index in logical warp\n    int lane_id;\n\n    /// Logical warp index in 32-thread physical warp\n    int warp_id;\n\n    /// 32-thread physical warp member mask of logical warp\n    uint32_t member_mask;\n\n\n    //---------------------------------------------------------------------\n    // Construction\n    //---------------------------------------------------------------------\n\n    /// Constructor\n    __device__ __forceinline__ WarpReduceShfl(\n        TempStorage &/*temp_storage*/)\n        : lane_id(static_cast<int>(LaneId()))\n        , warp_id(IS_ARCH_WARP ? 0 : (lane_id / LOGICAL_WARP_THREADS))\n        , member_mask(WarpMask<LOGICAL_WARP_THREADS>(warp_id))\n    {\n        if (!IS_ARCH_WARP)\n        {\n            lane_id = lane_id % LOGICAL_WARP_THREADS;\n        }\n    }\n\n\n    //---------------------------------------------------------------------\n    // Reduction steps\n    //---------------------------------------------------------------------\n\n    /// Reduction (specialized for summation across uint32 types)\n    __device__ __forceinline__ unsigned int ReduceStep(\n        unsigned int    input,              ///< [in] Calling thread's input item.\n        cub::Sum        /*reduction_op*/,   ///< [in] Binary reduction operator\n        int             last_lane,          ///< [in] Index of last lane in segment\n        int             offset)             ///< [in] Up-offset to pull from\n    {\n        unsigned int output;\n        int shfl_c = last_lane | SHFL_C;   // Shuffle control (mask and last_lane)\n\n        // Use predicate set from SHFL to guard against invalid peers\n        asm volatile(\n            \"{\"\n            \"  .reg .u32 r0;\"\n            \"  .reg .pred p;\"\n            \"  shfl.sync.down.b32 r0|p, %1, %2, %3, %5;\"\n            \"  @p add.u32 r0, r0, %4;\"\n            \"  mov.u32 %0, r0;\"\n            \"}\"\n            : \"=r\"(output) : \"r\"(input), \"r\"(offset), \"r\"(shfl_c), \"r\"(input), \"r\"(member_mask));\n\n        return output;\n    }\n\n\n    /// Reduction (specialized for summation across fp32 types)\n    __device__ __forceinline__ float ReduceStep(\n        float           input,              ///< [in] Calling thread's input item.\n        cub::Sum        /*reduction_op*/,   ///< [in] Binary reduction operator\n        int             last_lane,          ///< [in] Index of last lane in segment\n        int             offset)             ///< [in] Up-offset to pull from\n    {\n        float output;\n        int shfl_c = last_lane | SHFL_C;   // Shuffle control (mask and last_lane)\n\n        // Use predicate set from SHFL to guard against invalid peers\n        asm volatile(\n            \"{\"\n            \"  .reg .f32 r0;\"\n            \"  .reg .pred p;\"\n            \"  shfl.sync.down.b32 r0|p, %1, %2, %3, %5;\"\n            \"  @p add.f32 r0, r0, %4;\"\n            \"  mov.f32 %0, r0;\"\n            \"}\"\n            : \"=f\"(output) : \"f\"(input), \"r\"(offset), \"r\"(shfl_c), \"f\"(input), \"r\"(member_mask));\n\n        return output;\n    }\n\n\n    /// Reduction (specialized for summation across unsigned long long types)\n    __device__ __forceinline__ unsigned long long ReduceStep(\n        unsigned long long  input,              ///< [in] Calling thread's input item.\n        cub::Sum            /*reduction_op*/,   ///< [in] Binary reduction operator\n        int                 last_lane,          ///< [in] Index of last lane in segment\n        int                 offset)             ///< [in] Up-offset to pull from\n    {\n        unsigned long long output;\n        int shfl_c = last_lane | SHFL_C;   // Shuffle control (mask and last_lane)\n\n        asm volatile(\n            \"{\"\n            \"  .reg .u32 lo;\"\n            \"  .reg .u32 hi;\"\n            \"  .reg .pred p;\"\n            \"  mov.b64 {lo, hi}, %1;\"\n            \"  shfl.sync.down.b32 lo|p, lo, %2, %3, %4;\"\n            \"  shfl.sync.down.b32 hi|p, hi, %2, %3, %4;\"\n            \"  mov.b64 %0, {lo, hi};\"\n            \"  @p add.u64 %0, %0, %1;\"\n            \"}\"\n            : \"=l\"(output) : \"l\"(input), \"r\"(offset), \"r\"(shfl_c), \"r\"(member_mask));\n\n        return output;\n    }\n\n\n    /// Reduction (specialized for summation across long long types)\n    __device__ __forceinline__ long long ReduceStep(\n        long long           input,              ///< [in] Calling thread's input item.\n        cub::Sum            /*reduction_op*/,   ///< [in] Binary reduction operator\n        int                 last_lane,          ///< [in] Index of last lane in segment\n        int                 offset)             ///< [in] Up-offset to pull from\n    {\n        long long output;\n        int shfl_c = last_lane | SHFL_C;   // Shuffle control (mask and last_lane)\n\n        // Use predicate set from SHFL to guard against invalid peers\n        asm volatile(\n            \"{\"\n            \"  .reg .u32 lo;\"\n            \"  .reg .u32 hi;\"\n            \"  .reg .pred p;\"\n            \"  mov.b64 {lo, hi}, %1;\"\n            \"  shfl.sync.down.b32 lo|p, lo, %2, %3, %4;\"\n            \"  shfl.sync.down.b32 hi|p, hi, %2, %3, %4;\"\n            \"  mov.b64 %0, {lo, hi};\"\n            \"  @p add.s64 %0, %0, %1;\"\n            \"}\"\n            : \"=l\"(output) : \"l\"(input), \"r\"(offset), \"r\"(shfl_c), \"r\"(member_mask));\n\n        return output;\n    }\n\n\n    /// Reduction (specialized for summation across double types)\n    __device__ __forceinline__ double ReduceStep(\n        double              input,              ///< [in] Calling thread's input item.\n        cub::Sum            /*reduction_op*/,   ///< [in] Binary reduction operator\n        int                 last_lane,          ///< [in] Index of last lane in segment\n        int                 offset)             ///< [in] Up-offset to pull from\n    {\n        double output;\n        int shfl_c = last_lane | SHFL_C;   // Shuffle control (mask and last_lane)\n\n        // Use predicate set from SHFL to guard against invalid peers\n        asm volatile(\n            \"{\"\n            \"  .reg .u32 lo;\"\n            \"  .reg .u32 hi;\"\n            \"  .reg .pred p;\"\n            \"  .reg .f64 r0;\"\n            \"  mov.b64 %0, %1;\"\n            \"  mov.b64 {lo, hi}, %1;\"\n            \"  shfl.sync.down.b32 lo|p, lo, %2, %3, %4;\"\n            \"  shfl.sync.down.b32 hi|p, hi, %2, %3, %4;\"\n            \"  mov.b64 r0, {lo, hi};\"\n            \"  @p add.f64 %0, %0, r0;\"\n            \"}\"\n            : \"=d\"(output) : \"d\"(input), \"r\"(offset), \"r\"(shfl_c), \"r\"(member_mask));\n\n        return output;\n    }\n\n\n    /// Reduction (specialized for swizzled ReduceByKeyOp<cub::Sum> across KeyValuePair<KeyT, ValueT> types)\n    template <typename ValueT, typename KeyT>\n    __device__ __forceinline__ KeyValuePair<KeyT, ValueT> ReduceStep(\n        KeyValuePair<KeyT, ValueT>                  input,              ///< [in] Calling thread's input item.\n        SwizzleScanOp<ReduceByKeyOp<cub::Sum> >     /*reduction_op*/,   ///< [in] Binary reduction operator\n        int                                         last_lane,          ///< [in] Index of last lane in segment\n        int                                         offset)             ///< [in] Up-offset to pull from\n    {\n        KeyValuePair<KeyT, ValueT> output;\n\n        KeyT other_key = ShuffleDown<LOGICAL_WARP_THREADS>(input.key, offset, last_lane, member_mask);\n\n        output.key = input.key;\n        output.value = ReduceStep(\n            input.value,\n            cub::Sum(),\n            last_lane,\n            offset,\n            Int2Type<IsInteger<ValueT>::IS_SMALL_UNSIGNED>());\n\n        if (input.key != other_key)\n            output.value = input.value;\n\n        return output;\n    }\n\n\n\n    /// Reduction (specialized for swizzled ReduceBySegmentOp<cub::Sum> across KeyValuePair<OffsetT, ValueT> types)\n    template <typename ValueT, typename OffsetT>\n    __device__ __forceinline__ KeyValuePair<OffsetT, ValueT> ReduceStep(\n        KeyValuePair<OffsetT, ValueT>                 input,              ///< [in] Calling thread's input item.\n        SwizzleScanOp<ReduceBySegmentOp<cub::Sum> >   /*reduction_op*/,   ///< [in] Binary reduction operator\n        int                                           last_lane,          ///< [in] Index of last lane in segment\n        int                                           offset)             ///< [in] Up-offset to pull from\n    {\n        KeyValuePair<OffsetT, ValueT> output;\n\n        output.value = ReduceStep(input.value, cub::Sum(), last_lane, offset, Int2Type<IsInteger<ValueT>::IS_SMALL_UNSIGNED>());\n        output.key = ReduceStep(input.key, cub::Sum(), last_lane, offset, Int2Type<IsInteger<OffsetT>::IS_SMALL_UNSIGNED>());\n\n        if (input.key > 0)\n            output.value = input.value;\n\n        return output;\n    }\n\n\n    /// Reduction step (generic)\n    template <typename _T, typename ReductionOp>\n    __device__ __forceinline__ _T ReduceStep(\n        _T                  input,              ///< [in] Calling thread's input item.\n        ReductionOp         reduction_op,       ///< [in] Binary reduction operator\n        int                 last_lane,          ///< [in] Index of last lane in segment\n        int                 offset)             ///< [in] Up-offset to pull from\n    {\n        _T output = input;\n\n        _T temp = ShuffleDown<LOGICAL_WARP_THREADS>(output, offset, last_lane, member_mask);\n\n        // Perform reduction op if valid\n        if (offset + lane_id <= last_lane)\n            output = reduction_op(input, temp);\n\n        return output;\n    }\n\n\n    /// Reduction step (specialized for small unsigned integers size 32b or less)\n    template <typename _T, typename ReductionOp>\n    __device__ __forceinline__ _T ReduceStep(\n        _T              input,                  ///< [in] Calling thread's input item.\n        ReductionOp     reduction_op,           ///< [in] Binary reduction operator\n        int             last_lane,              ///< [in] Index of last lane in segment\n        int             offset,                 ///< [in] Up-offset to pull from\n        Int2Type<true>  /*is_small_unsigned*/)  ///< [in] Marker type indicating whether T is a small unsigned integer\n    {\n        return ReduceStep(input, reduction_op, last_lane, offset);\n    }\n\n\n    /// Reduction step (specialized for types other than small unsigned integers size 32b or less)\n    template <typename _T, typename ReductionOp>\n    __device__ __forceinline__ _T ReduceStep(\n        _T              input,                  ///< [in] Calling thread's input item.\n        ReductionOp     reduction_op,           ///< [in] Binary reduction operator\n        int             last_lane,              ///< [in] Index of last lane in segment\n        int             offset,                 ///< [in] Up-offset to pull from\n        Int2Type<false> /*is_small_unsigned*/)  ///< [in] Marker type indicating whether T is a small unsigned integer\n    {\n        return ReduceStep(input, reduction_op, last_lane, offset);\n    }\n\n\n    //---------------------------------------------------------------------\n    // Templated reduction iteration\n    //---------------------------------------------------------------------\n\n    template <typename ReductionOp, int STEP>\n    __device__ __forceinline__ void ReduceStep(\n        T&              input,              ///< [in] Calling thread's input item.\n        ReductionOp     reduction_op,       ///< [in] Binary reduction operator\n        int             last_lane,          ///< [in] Index of last lane in segment\n        Int2Type<STEP>  /*step*/)\n    {\n        input = ReduceStep(input, reduction_op, last_lane, 1 << STEP, Int2Type<IsInteger<T>::IS_SMALL_UNSIGNED>());\n\n        ReduceStep(input, reduction_op, last_lane, Int2Type<STEP + 1>());\n    }\n\n    template <typename ReductionOp>\n    __device__ __forceinline__ void ReduceStep(\n        T&              /*input*/,              ///< [in] Calling thread's input item.\n        ReductionOp     /*reduction_op*/,       ///< [in] Binary reduction operator\n        int             /*last_lane*/,          ///< [in] Index of last lane in segment\n        Int2Type<STEPS> /*step*/)\n    {}\n\n\n    //---------------------------------------------------------------------\n    // Reduction operations\n    //---------------------------------------------------------------------\n    template <typename ReductionOp>\n    __device__ __forceinline__ T ReduceImpl(\n        Int2Type<0>     /* all_lanes_valid */, \n        T               input,                  ///< [in] Calling thread's input\n        int             valid_items,            ///< [in] Total number of valid items across the logical warp\n        ReductionOp     reduction_op)           ///< [in] Binary reduction operator\n    {\n        int last_lane = valid_items - 1;\n\n        T output = input;\n\n        // Template-iterate reduction steps\n        ReduceStep(output, reduction_op, last_lane, Int2Type<0>());\n\n        return output;\n    }\n\n    template <typename ReductionOp>\n    __device__ __forceinline__ T ReduceImpl(\n        Int2Type<1>     /* all_lanes_valid */, \n        T               input,                  ///< [in] Calling thread's input\n        int             /* valid_items */,      ///< [in] Total number of valid items across the logical warp\n        ReductionOp     reduction_op)           ///< [in] Binary reduction operator\n    {\n        int last_lane = LOGICAL_WARP_THREADS - 1;\n\n        T output = input;\n\n        // Template-iterate reduction steps\n        ReduceStep(output, reduction_op, last_lane, Int2Type<0>());\n\n        return output;\n    }\n\n    template <class U = T>\n    __device__ __forceinline__ \n    typename std::enable_if<\n               (std::is_same<int, U>::value || std::is_same<unsigned int, U>::value)\n            && detail::reduce_add_exists<>::value, T>::type\n    ReduceImpl(Int2Type<1> /* all_lanes_valid */,\n               T input,\n               int /* valid_items */,\n               cub::Sum /* reduction_op */)\n    {\n      T output = input;\n\n      NV_IF_TARGET(NV_PROVIDES_SM_80,\n                   (output = __reduce_add_sync(member_mask, input);),\n                   (output = ReduceImpl<cub::Sum>(Int2Type<1>{},\n                                                  input,\n                                                  LOGICAL_WARP_THREADS,\n                                                  cub::Sum{});));\n\n      return output;\n    }\n\n    template <class U = T>\n    __device__ __forceinline__ \n    typename std::enable_if<\n               (std::is_same<int, U>::value || std::is_same<unsigned int, U>::value)\n            && detail::reduce_min_exists<>::value, T>::type\n    ReduceImpl(Int2Type<1> /* all_lanes_valid */,\n               T input,\n               int /* valid_items */,\n               cub::Min /* reduction_op */)\n    {\n      T output = input;\n\n      NV_IF_TARGET(NV_PROVIDES_SM_80,\n                   (output = __reduce_min_sync(member_mask, input);),\n                   (output = ReduceImpl<cub::Min>(Int2Type<1>{},\n                                                  input,\n                                                  LOGICAL_WARP_THREADS,\n                                                  cub::Min{});));\n\n      return output;\n    }\n\n    template <class U = T>\n    __device__ __forceinline__ \n    typename std::enable_if<\n               (std::is_same<int, U>::value || std::is_same<unsigned int, U>::value)\n            && detail::reduce_max_exists<>::value, T>::type\n    ReduceImpl(Int2Type<1> /* all_lanes_valid */,\n               T input,\n               int /* valid_items */,\n               cub::Max /* reduction_op */)\n    {\n      T output = input;\n\n      NV_IF_TARGET(NV_PROVIDES_SM_80,\n                   (output = __reduce_max_sync(member_mask, input);),\n                   (output = ReduceImpl<cub::Max>(Int2Type<1>{},\n                                                  input,\n                                                  LOGICAL_WARP_THREADS,\n                                                  cub::Max{});));\n\n      return output;\n    }\n\n    /// Reduction\n    template <\n        bool            ALL_LANES_VALID,        ///< Whether all lanes in each warp are contributing a valid fold of items\n        typename        ReductionOp>\n    __device__ __forceinline__ T Reduce(\n        T               input,                  ///< [in] Calling thread's input\n        int             valid_items,            ///< [in] Total number of valid items across the logical warp\n        ReductionOp     reduction_op)           ///< [in] Binary reduction operator\n    {\n        return ReduceImpl(\n            Int2Type<ALL_LANES_VALID>{}, input, valid_items, reduction_op);\n    }\n\n\n    /// Segmented reduction\n    template <\n        bool            HEAD_SEGMENTED,     ///< Whether flags indicate a segment-head or a segment-tail\n        typename        FlagT,\n        typename        ReductionOp>\n    __device__ __forceinline__ T SegmentedReduce(\n        T               input,              ///< [in] Calling thread's input\n        FlagT           flag,               ///< [in] Whether or not the current lane is a segment head/tail\n        ReductionOp     reduction_op)       ///< [in] Binary reduction operator\n    {\n        // Get the start flags for each thread in the warp.\n        int warp_flags = WARP_BALLOT(flag, member_mask);\n\n        // Convert to tail-segmented\n        if (HEAD_SEGMENTED)\n            warp_flags >>= 1;\n\n        // Mask out the bits below the current thread\n        warp_flags &= LaneMaskGe();\n\n        // Mask of physical lanes outside the logical warp and convert to logical lanemask\n        if (!IS_ARCH_WARP)\n        {\n            warp_flags = (warp_flags & member_mask) >> (warp_id * LOGICAL_WARP_THREADS);\n        }\n\n        // Mask in the last lane of logical warp\n        warp_flags |= 1u << (LOGICAL_WARP_THREADS - 1);\n\n        // Find the next set flag\n        int last_lane = __clz(__brev(warp_flags));\n\n        T output = input;\n\n//        // Iterate reduction steps\n//        #pragma unroll\n//        for (int STEP = 0; STEP < STEPS; STEP++)\n//        {\n//            output = ReduceStep(output, reduction_op, last_lane, 1 << STEP, Int2Type<IsInteger<T>::IS_SMALL_UNSIGNED>());\n//        }\n\n        // Template-iterate reduction steps\n        ReduceStep(output, reduction_op, last_lane, Int2Type<0>());\n\n        return output;\n    }\n};\n\n\nCUB_NAMESPACE_END\n\n#endif // _JITIFY_INCLUDE_GUARD_EEF564931BA0D156\n", "cub/warp/specializations/warp_reduce_smem.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_9450AB87423F6391\n#define _JITIFY_INCLUDE_GUARD_9450AB87423F6391\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.\n * \n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n * \n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * cub::WarpReduceSmem provides smem-based variants of parallel reduction of items partitioned across a CUDA thread warp.\n */\n\n#include \"../../config.cuh\"\n#include \"../../thread/thread_operators.cuh\"\n#include \"../../thread/thread_load.cuh\"\n#include \"../../thread/thread_store.cuh\"\n#include \"../../util_type.cuh\"\n\nCUB_NAMESPACE_BEGIN\n\n/**\n * \\brief WarpReduceSmem provides smem-based variants of parallel reduction of items partitioned across a CUDA thread warp.\n */\ntemplate <\n    typename    T,                      ///< Data type being reduced\n    int         LOGICAL_WARP_THREADS,   ///< Number of threads per logical warp\n    int         LEGACY_PTX_ARCH = 0>    ///< The PTX compute capability for which to to specialize this collective\nstruct WarpReduceSmem\n{\n    /******************************************************************************\n     * Constants and type definitions\n     ******************************************************************************/\n\n    enum\n    {\n        /// Whether the logical warp size and the PTX warp size coincide\n        IS_ARCH_WARP = (LOGICAL_WARP_THREADS == CUB_WARP_THREADS(0)),\n\n        /// Whether the logical warp size is a power-of-two\n        IS_POW_OF_TWO = PowerOfTwo<LOGICAL_WARP_THREADS>::VALUE,\n\n        /// The number of warp reduction steps\n        STEPS = Log2<LOGICAL_WARP_THREADS>::VALUE,\n\n        /// The number of threads in half a warp\n        HALF_WARP_THREADS = 1 << (STEPS - 1),\n\n        /// The number of shared memory elements per warp\n        WARP_SMEM_ELEMENTS =  LOGICAL_WARP_THREADS + HALF_WARP_THREADS,\n\n        /// FlagT status (when not using ballot)\n        UNSET   = 0x0,  // Is initially unset\n        SET     = 0x1,  // Is initially set\n        SEEN    = 0x2,  // Has seen another head flag from a successor peer\n    };\n\n    /// Shared memory flag type\n    typedef unsigned char SmemFlag;\n\n    /// Shared memory storage layout type (1.5 warps-worth of elements for each warp)\n    struct _TempStorage\n    {\n        T           reduce[WARP_SMEM_ELEMENTS];\n        SmemFlag    flags[WARP_SMEM_ELEMENTS];\n    };\n\n    // Alias wrapper allowing storage to be unioned\n    struct TempStorage : Uninitialized<_TempStorage> {};\n\n\n    /******************************************************************************\n     * Thread fields\n     ******************************************************************************/\n\n    _TempStorage    &temp_storage;\n    unsigned int    lane_id;\n    unsigned int    member_mask;\n\n\n    /******************************************************************************\n     * Construction\n     ******************************************************************************/\n\n    /// Constructor\n    explicit __device__ __forceinline__ WarpReduceSmem(TempStorage &temp_storage)\n        : temp_storage(temp_storage.Alias())\n        , lane_id(IS_ARCH_WARP ? LaneId() : LaneId() % LOGICAL_WARP_THREADS)\n        , member_mask(\n            WarpMask<LOGICAL_WARP_THREADS>(LaneId() / LOGICAL_WARP_THREADS))\n    {}\n\n    /******************************************************************************\n     * Utility methods\n     ******************************************************************************/\n\n    //---------------------------------------------------------------------\n    // Regular reduction\n    //---------------------------------------------------------------------\n\n    /**\n     * Reduction step\n     */\n    template <\n        bool                ALL_LANES_VALID,        ///< Whether all lanes in each warp are contributing a valid fold of items\n        typename            ReductionOp,\n        int                 STEP>\n    __device__ __forceinline__ T ReduceStep(\n        T                   input,                  ///< [in] Calling thread's input\n        int                 valid_items,            ///< [in] Total number of valid items across the logical warp\n        ReductionOp         reduction_op,           ///< [in] Reduction operator\n        Int2Type<STEP>      /*step*/)\n    {\n        const int OFFSET = 1 << STEP;\n\n        // Share input through buffer\n        ThreadStore<STORE_VOLATILE>(&temp_storage.reduce[lane_id], input);\n\n        WARP_SYNC(member_mask);\n\n        // Update input if peer_addend is in range\n        if ((ALL_LANES_VALID && IS_POW_OF_TWO) || ((lane_id + OFFSET) < valid_items))\n        {\n            T peer_addend = ThreadLoad<LOAD_VOLATILE>(&temp_storage.reduce[lane_id + OFFSET]);\n            input = reduction_op(input, peer_addend);\n        }\n\n        WARP_SYNC(member_mask);\n\n        return ReduceStep<ALL_LANES_VALID>(input, valid_items, reduction_op, Int2Type<STEP + 1>());\n    }\n\n\n    /**\n     * Reduction step (terminate)\n     */\n    template <\n        bool                ALL_LANES_VALID,            ///< Whether all lanes in each warp are contributing a valid fold of items\n        typename            ReductionOp>\n    __device__ __forceinline__ T ReduceStep(\n        T                   input,                      ///< [in] Calling thread's input\n        int                 valid_items,                ///< [in] Total number of valid items across the logical warp\n        ReductionOp         /*reduction_op*/,           ///< [in] Reduction operator\n        Int2Type<STEPS>     /*step*/)\n    {\n        return input;\n    }\n\n\n    //---------------------------------------------------------------------\n    // Segmented reduction\n    //---------------------------------------------------------------------\n\n\n    /**\n     * Ballot-based segmented reduce\n     */\n    template <\n        bool            HEAD_SEGMENTED,     ///< Whether flags indicate a segment-head or a segment-tail\n        typename        FlagT,\n        typename        ReductionOp>\n    __device__ __forceinline__ T SegmentedReduce(\n        T               input,                  ///< [in] Calling thread's input\n        FlagT           flag,                   ///< [in] Whether or not the current lane is a segment head/tail\n        ReductionOp     reduction_op,           ///< [in] Reduction operator\n        Int2Type<true>  /*has_ballot*/)         ///< [in] Marker type for whether the target arch has ballot functionality\n    {\n        // Get the start flags for each thread in the warp.\n        int warp_flags = WARP_BALLOT(flag, member_mask);\n\n        if (!HEAD_SEGMENTED)\n            warp_flags <<= 1;\n\n        // Keep bits above the current thread.\n        warp_flags &= LaneMaskGt();\n\n        // Accommodate packing of multiple logical warps in a single physical warp\n        if (!IS_ARCH_WARP)\n        {\n            warp_flags >>= (LaneId() / LOGICAL_WARP_THREADS) * LOGICAL_WARP_THREADS;\n        }\n\n        // Find next flag\n        int next_flag = __clz(__brev(warp_flags));\n\n        // Clip the next segment at the warp boundary if necessary\n        if (LOGICAL_WARP_THREADS != 32)\n            next_flag = CUB_MIN(next_flag, LOGICAL_WARP_THREADS);\n\n        _Pragma(\"unroll\")\n        for (int STEP = 0; STEP < STEPS; STEP++)\n        {\n            const int OFFSET = 1 << STEP;\n\n            // Share input into buffer\n            ThreadStore<STORE_VOLATILE>(&temp_storage.reduce[lane_id], input);\n\n            WARP_SYNC(member_mask);\n\n            // Update input if peer_addend is in range\n            if (OFFSET + lane_id < next_flag)\n            {\n                T peer_addend = ThreadLoad<LOAD_VOLATILE>(&temp_storage.reduce[lane_id + OFFSET]);\n                input = reduction_op(input, peer_addend);\n            }\n\n            WARP_SYNC(member_mask);\n        }\n\n        return input;\n    }\n\n\n    /**\n     * Smem-based segmented reduce\n     */\n    template <\n        bool            HEAD_SEGMENTED,     ///< Whether flags indicate a segment-head or a segment-tail\n        typename        FlagT,\n        typename        ReductionOp>\n    __device__ __forceinline__ T SegmentedReduce(\n        T               input,                  ///< [in] Calling thread's input\n        FlagT           flag,                   ///< [in] Whether or not the current lane is a segment head/tail\n        ReductionOp     reduction_op,           ///< [in] Reduction operator\n        Int2Type<false> /*has_ballot*/)         ///< [in] Marker type for whether the target arch has ballot functionality\n    {\n        enum\n        {\n            UNSET   = 0x0,  // Is initially unset\n            SET     = 0x1,  // Is initially set\n            SEEN    = 0x2,  // Has seen another head flag from a successor peer\n        };\n\n        // Alias flags onto shared data storage\n        volatile SmemFlag *flag_storage = temp_storage.flags;\n\n        SmemFlag flag_status = (flag) ? SET : UNSET;\n\n        for (int STEP = 0; STEP < STEPS; STEP++)\n        {\n            const int OFFSET = 1 << STEP;\n\n            // Share input through buffer\n            ThreadStore<STORE_VOLATILE>(&temp_storage.reduce[lane_id], input);\n\n            WARP_SYNC(member_mask);\n\n            // Get peer from buffer\n            T peer_addend = ThreadLoad<LOAD_VOLATILE>(&temp_storage.reduce[lane_id + OFFSET]);\n\n            WARP_SYNC(member_mask);\n\n            // Share flag through buffer\n            flag_storage[lane_id] = flag_status;\n\n            // Get peer flag from buffer\n            SmemFlag peer_flag_status = flag_storage[lane_id + OFFSET];\n\n            // Update input if peer was in range\n            if (lane_id < LOGICAL_WARP_THREADS - OFFSET)\n            {\n                if (HEAD_SEGMENTED)\n                {\n                    // Head-segmented\n                    if ((flag_status & SEEN) == 0)\n                    {\n                        // Has not seen a more distant head flag\n                        if (peer_flag_status & SET)\n                        {\n                            // Has now seen a head flag\n                            flag_status |= SEEN;\n                        }\n                        else\n                        {\n                            // Peer is not a head flag: grab its count\n                            input = reduction_op(input, peer_addend);\n                        }\n\n                        // Update seen status to include that of peer\n                        flag_status |= (peer_flag_status & SEEN);\n                    }\n                }\n                else\n                {\n                    // Tail-segmented.  Simply propagate flag status\n                    if (!flag_status)\n                    {\n                        input = reduction_op(input, peer_addend);\n                        flag_status |= peer_flag_status;\n                    }\n\n                }\n            }\n        }\n\n        return input;\n    }\n\n\n    /******************************************************************************\n     * Interface\n     ******************************************************************************/\n\n    /**\n     * Reduction\n     */\n    template <\n        bool                ALL_LANES_VALID,        ///< Whether all lanes in each warp are contributing a valid fold of items\n        typename            ReductionOp>\n    __device__ __forceinline__ T Reduce(\n        T                   input,                  ///< [in] Calling thread's input\n        int                 valid_items,            ///< [in] Total number of valid items across the logical warp\n        ReductionOp         reduction_op)           ///< [in] Reduction operator\n    {\n        return ReduceStep<ALL_LANES_VALID>(input, valid_items, reduction_op, Int2Type<0>());\n    }\n\n\n    /**\n     * Segmented reduction\n     */\n    template <\n        bool            HEAD_SEGMENTED,     ///< Whether flags indicate a segment-head or a segment-tail\n        typename        FlagT,\n        typename        ReductionOp>\n    __device__ __forceinline__ T SegmentedReduce(\n        T               input,              ///< [in] Calling thread's input\n        FlagT            flag,               ///< [in] Whether or not the current lane is a segment head/tail\n        ReductionOp     reduction_op)       ///< [in] Reduction operator\n    {\n        return SegmentedReduce<HEAD_SEGMENTED>(input, flag, reduction_op, Int2Type<true>());\n    }\n\n\n};\n\n\nCUB_NAMESPACE_END\n\n#endif // _JITIFY_INCLUDE_GUARD_9450AB87423F6391\n", "cub/warp/warp_exchange.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_48B70CD5880D9166\n#define _JITIFY_INCLUDE_GUARD_48B70CD5880D9166\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * @file\n * The cub::WarpExchange class provides [<em>collective</em>](index.html#sec0)\n * methods for rearranging data partitioned across a CUDA warp.\n */\n\n#include <cub/config.cuh>\n#include <cub/util_ptx.cuh>\n#include <cub/util_type.cuh>\n#include <cub/warp/specializations/warp_exchange_shfl.cuh>\n#include <cub/warp/specializations/warp_exchange_smem.cuh>\n\n\nCUB_NAMESPACE_BEGIN\n\n\nenum WarpExchangeAlgorithm\n{\n  WARP_EXCHANGE_SMEM,\n  WARP_EXCHANGE_SHUFFLE,\n};\n\nnamespace detail\n{\ntemplate <typename InputT,\n          int ITEMS_PER_THREAD,\n          int LOGICAL_WARP_THREADS,\n          WarpExchangeAlgorithm WARP_EXCHANGE_ALGORITHM>\nusing InternalWarpExchangeImpl =\n  cub::detail::conditional_t<WARP_EXCHANGE_ALGORITHM == WARP_EXCHANGE_SMEM,\n                             WarpExchangeSmem<InputT, ITEMS_PER_THREAD, LOGICAL_WARP_THREADS>,\n                             WarpExchangeShfl<InputT, ITEMS_PER_THREAD, LOGICAL_WARP_THREADS>>;\n} // namespace detail\n\n/**\n * @brief The WarpExchange class provides [<em>collective</em>](index.html#sec0)\n *        methods for rearranging data partitioned across a CUDA warp.\n * @ingroup WarpModule\n *\n * @tparam T\n *   The data type to be exchanged.\n *\n * @tparam ITEMS_PER_THREAD\n *   The number of items partitioned onto each thread.\n *\n * @tparam LOGICAL_WARP_THREADS\n *   <b>[optional]</b> The number of threads per \"logical\" warp (may be less\n *   than the number of hardware warp threads). Default is the warp size of the\n *   targeted CUDA compute-capability (e.g., 32 threads for SM86). Must be a\n *   power of two.\n *\n * @tparam LEGACY_PTX_ARCH\n *   Unused.\n *\n * @par Overview\n * - It is commonplace for a warp of threads to rearrange data items between\n *   threads. For example, the global memory accesses prefer patterns where\n *   data items are \"striped\" across threads (where consecutive threads access\n *   consecutive items), yet most warp-wide operations prefer a \"blocked\"\n *   partitioning of items across threads (where consecutive items belong to a\n *   single thread).\n * - WarpExchange supports the following types of data exchanges:\n *   - Transposing between [<em>blocked</em>](index.html#sec5sec3) and\n *     [<em>striped</em>](index.html#sec5sec3) arrangements\n *   - Scattering ranked items to a\n *     [<em>striped arrangement</em>](index.html#sec5sec3)\n *\n * @par A Simple Example\n * @par\n * The code snippet below illustrates the conversion from a \"blocked\" to a\n * \"striped\" arrangement of 64 integer items partitioned across 16 threads where\n * each thread owns 4 items.\n * @par\n * @code\n * #include <cub/cub.cuh>   // or equivalently <cub/warp/warp_exchange.cuh>\n *\n * __global__ void ExampleKernel(int *d_data, ...)\n * {\n *     constexpr int warp_threads = 16;\n *     constexpr int block_threads = 256;\n *     constexpr int items_per_thread = 4;\n *     constexpr int warps_per_block = block_threads / warp_threads;\n *     const int warp_id = static_cast<int>(threadIdx.x) / warp_threads;\n *\n *     // Specialize WarpExchange for a virtual warp of 16 threads owning 4 integer items each\n *     using WarpExchangeT =\n *       cub::WarpExchange<int, items_per_thread, warp_threads>;\n *\n *     // Allocate shared memory for WarpExchange\n *     __shared__ typename WarpExchangeT::TempStorage temp_storage[warps_per_block];\n *\n *     // Load a tile of data striped across threads\n *     int thread_data[items_per_thread];\n *     // ...\n *\n *     // Collectively exchange data into a blocked arrangement across threads\n *     WarpExchangeT(temp_storage[warp_id]).StripedToBlocked(thread_data, thread_data);\n * @endcode\n * @par\n * Suppose the set of striped input @p thread_data across the block of threads\n * is <tt>{ [0,16,32,48], [1,17,33,49], ..., [15, 32, 47, 63] }</tt>.\n * The corresponding output @p thread_data in those threads will be\n * <tt>{ [0,1,2,3], [4,5,6,7], [8,9,10,11], ..., [60,61,62,63] }</tt>.\n */\ntemplate <typename InputT,\n          int ITEMS_PER_THREAD,\n          int LOGICAL_WARP_THREADS  = CUB_PTX_WARP_THREADS,\n          int LEGACY_PTX_ARCH       = 0,\n          WarpExchangeAlgorithm WARP_EXCHANGE_ALGORITHM = WARP_EXCHANGE_SMEM>\nclass WarpExchange : private detail::InternalWarpExchangeImpl<\n    InputT,\n    ITEMS_PER_THREAD,\n    LOGICAL_WARP_THREADS,\n    WARP_EXCHANGE_ALGORITHM>\n{\n  using InternalWarpExchange = detail::InternalWarpExchangeImpl<\n    InputT,\n    ITEMS_PER_THREAD,\n    LOGICAL_WARP_THREADS,\n    WARP_EXCHANGE_ALGORITHM>;\n\npublic:\n\n  /// \\smemstorage{WarpExchange}\n  using TempStorage = typename InternalWarpExchange::TempStorage;\n\n  /*************************************************************************//**\n   * @name Collective constructors\n   ****************************************************************************/\n  //@{\n\n  WarpExchange() = delete;\n\n  /**\n   * @brief Collective constructor using the specified memory allocation as\n   *        temporary storage.\n   */\n  explicit __device__ __forceinline__\n  WarpExchange(TempStorage &temp_storage)\n      : InternalWarpExchange(temp_storage)\n  {\n\n  }\n\n  //@}  end member group\n  /*************************************************************************//**\n   * @name Data movement\n   ****************************************************************************/\n  //@{\n\n  /**\n   * @brief Transposes data items from <em>blocked</em> arrangement to\n   *        <em>striped</em> arrangement.\n   *\n   * @par\n   * @smemwarpreuse\n   *\n   * @par Snippet\n   * The code snippet below illustrates the conversion from a \"blocked\" to a\n   * \"striped\" arrangement of 64 integer items partitioned across 16 threads\n   * where each thread owns 4 items.\n   * @par\n   * @code\n   * #include <cub/cub.cuh>   // or equivalently <cub/warp/warp_exchange.cuh>\n   *\n   * __global__ void ExampleKernel(int *d_data, ...)\n   * {\n   *     constexpr int warp_threads = 16;\n   *     constexpr int block_threads = 256;\n   *     constexpr int items_per_thread = 4;\n   *     constexpr int warps_per_block = block_threads / warp_threads;\n   *     const int warp_id = static_cast<int>(threadIdx.x) / warp_threads;\n   *\n   *     // Specialize WarpExchange for a virtual warp of 16 threads owning 4 integer items each\n   *     using WarpExchangeT = cub::WarpExchange<int, items_per_thread, warp_threads>;\n   *\n   *     // Allocate shared memory for WarpExchange\n   *     __shared__ typename WarpExchangeT::TempStorage temp_storage[warps_per_block];\n   *\n   *     // Obtain a segment of consecutive items that are blocked across threads\n   *     int thread_data[items_per_thread];\n   *     // ...\n   *\n   *     // Collectively exchange data into a striped arrangement across threads\n   *     WarpExchangeT(temp_storage[warp_id]).BlockedToStriped(thread_data, thread_data);\n   * @endcode\n   * @par\n   * Suppose the set of striped input @p thread_data across the block of threads\n   * is <tt>{ [0,1,2,3], [4,5,6,7], [8,9,10,11], ..., [60,61,62,63] }</tt>.\n   * The corresponding output @p thread_data in those threads will be\n   * <tt>{ [0,16,32,48], [1,17,33,49], ..., [15, 32, 47, 63] }</tt>.\n   *\n   * @param[in] input_items\n   *   Items to exchange, converting between <em>blocked</em> and\n   *   <em>striped</em> arrangements.\n   *\n   * @param[out] output_items\n   *   Items from exchange, converting between <em>striped</em> and\n   *   <em>blocked</em> arrangements. May be aliased to @p input_items.\n   */\n  template <typename OutputT>\n  __device__ __forceinline__ void\n  BlockedToStriped(const InputT (&input_items)[ITEMS_PER_THREAD],\n                   OutputT (&output_items)[ITEMS_PER_THREAD])\n  {\n    InternalWarpExchange::BlockedToStriped(input_items, output_items);\n  }\n\n  /**\n   * @brief Transposes data items from <em>striped</em> arrangement to\n   *        <em>blocked</em> arrangement.\n   *\n   * @par\n   * @smemwarpreuse\n   *\n   * @par Snippet\n   * The code snippet below illustrates the conversion from a \"striped\" to a\n   * \"blocked\" arrangement of 64 integer items partitioned across 16 threads\n   * where each thread owns 4 items.\n   * @par\n   * @code\n   * #include <cub/cub.cuh>   // or equivalently <cub/warp/warp_exchange.cuh>\n   *\n   * __global__ void ExampleKernel(int *d_data, ...)\n   * {\n   *     constexpr int warp_threads = 16;\n   *     constexpr int block_threads = 256;\n   *     constexpr int items_per_thread = 4;\n   *     constexpr int warps_per_block = block_threads / warp_threads;\n   *     const int warp_id = static_cast<int>(threadIdx.x) / warp_threads;\n   *\n   *     // Specialize WarpExchange for a virtual warp of 16 threads owning 4 integer items each\n   *     using WarpExchangeT = cub::WarpExchange<int, items_per_thread, warp_threads>;\n   *\n   *     // Allocate shared memory for WarpExchange\n   *     __shared__ typename WarpExchangeT::TempStorage temp_storage[warps_per_block];\n   *\n   *     // Load a tile of data striped across threads\n   *     int thread_data[items_per_thread];\n   *     // ...\n   *\n   *     // Collectively exchange data into a blocked arrangement across threads\n   *     WarpExchangeT(temp_storage[warp_id]).StripedToBlocked(thread_data, thread_data);\n   * @endcode\n   * @par\n   * Suppose the set of striped input @p thread_data across the block of threads\n   * is <tt>{ [0,16,32,48], [1,17,33,49], ..., [15, 32, 47, 63] }</tt>.\n   * The corresponding output @p thread_data in those threads will be\n   * <tt>{ [0,1,2,3], [4,5,6,7], [8,9,10,11], ..., [60,61,62,63] }</tt>.\n   *\n   * @param[in] input_items\n   *   Items to exchange\n   *\n   * @param[out] output_items\n   *   Items from exchange. May be aliased to @p input_items.\n   */\n  template <typename OutputT>\n  __device__ __forceinline__ void\n  StripedToBlocked(const InputT (&input_items)[ITEMS_PER_THREAD],\n                   OutputT (&output_items)[ITEMS_PER_THREAD])\n  {\n    InternalWarpExchange::StripedToBlocked(input_items, output_items);\n  }\n\n  /**\n   * @brief Exchanges valid data items annotated by rank\n   *        into <em>striped</em> arrangement.\n   *\n   * @par\n   * @smemwarpreuse\n   *\n   * @par Snippet\n   * The code snippet below illustrates the conversion from a \"scatter\" to a\n   * \"striped\" arrangement of 64 integer items partitioned across 16 threads\n   * where each thread owns 4 items.\n   * @par\n   * @code\n   * #include <cub/cub.cuh>   // or equivalently <cub/warp/warp_exchange.cuh>\n   *\n   * __global__ void ExampleKernel(int *d_data, ...)\n   * {\n   *     constexpr int warp_threads = 16;\n   *     constexpr int block_threads = 256;\n   *     constexpr int items_per_thread = 4;\n   *     constexpr int warps_per_block = block_threads / warp_threads;\n   *     const int warp_id = static_cast<int>(threadIdx.x) / warp_threads;\n   *\n   *     // Specialize WarpExchange for a virtual warp of 16 threads owning 4 integer items each\n   *     using WarpExchangeT = cub::WarpExchange<int, items_per_thread, warp_threads>;\n   *\n   *     // Allocate shared memory for WarpExchange\n   *     __shared__ typename WarpExchangeT::TempStorage temp_storage[warps_per_block];\n   *\n   *     // Obtain a segment of consecutive items that are blocked across threads\n   *     int thread_data[items_per_thread];\n   *     int thread_ranks[items_per_thread];\n   *     // ...\n   *\n   *     // Collectively exchange data into a striped arrangement across threads\n   *     WarpExchangeT(temp_storage[warp_id]).ScatterToStriped(\n   *       thread_data, thread_ranks);\n   * @endcode\n   * @par\n   * Suppose the set of input @p thread_data across the block of threads\n   * is `{ [0,1,2,3], [4,5,6,7], ..., [60,61,62,63] }`, and the set of\n   * @p thread_ranks is `{ [63,62,61,60], ..., [7,6,5,4], [3,2,1,0] }`. The\n   * corresponding output @p thread_data in those threads will be\n   * `{ [63, 47, 31, 15], [62, 46, 30, 14], ..., [48, 32, 16, 0] }`.\n   *\n   * @tparam OffsetT <b>[inferred]</b> Signed integer type for local offsets\n   *\n   * @param[in,out] items Items to exchange\n   * @param[in] ranks Corresponding scatter ranks\n   */\n  template <typename OffsetT>\n  __device__ __forceinline__ void\n  ScatterToStriped(InputT (&items)[ITEMS_PER_THREAD],\n                   OffsetT (&ranks)[ITEMS_PER_THREAD])\n  {\n    InternalWarpExchange::ScatterToStriped(items, ranks);\n  }\n\n  /**\n   * @brief Exchanges valid data items annotated by rank\n   *        into <em>striped</em> arrangement.\n   *\n   * @par\n   * @smemwarpreuse\n   *\n   * @par Snippet\n   * The code snippet below illustrates the conversion from a \"scatter\" to a\n   * \"striped\" arrangement of 64 integer items partitioned across 16 threads\n   * where each thread owns 4 items.\n   * @par\n   * @code\n   * #include <cub/cub.cuh>   // or equivalently <cub/warp/warp_exchange.cuh>\n   *\n   * __global__ void ExampleKernel(int *d_data, ...)\n   * {\n   *     constexpr int warp_threads = 16;\n   *     constexpr int block_threads = 256;\n   *     constexpr int items_per_thread = 4;\n   *     constexpr int warps_per_block = block_threads / warp_threads;\n   *     const int warp_id = static_cast<int>(threadIdx.x) / warp_threads;\n   *\n   *     // Specialize WarpExchange for a virtual warp of 16 threads owning 4 integer items each\n   *     using WarpExchangeT = cub::WarpExchange<int, items_per_thread, warp_threads>;\n   *\n   *     // Allocate shared memory for WarpExchange\n   *     __shared__ typename WarpExchangeT::TempStorage temp_storage[warps_per_block];\n   *\n   *     // Obtain a segment of consecutive items that are blocked across threads\n   *     int thread_input[items_per_thread];\n   *     int thread_ranks[items_per_thread];\n   *     // ...\n   *\n   *     // Collectively exchange data into a striped arrangement across threads\n   *     int thread_output[items_per_thread];\n   *     WarpExchangeT(temp_storage[warp_id]).ScatterToStriped(\n   *       thread_input, thread_output, thread_ranks);\n   * @endcode\n   * @par\n   * Suppose the set of input @p thread_input across the block of threads\n   * is `{ [0,1,2,3], [4,5,6,7], ..., [60,61,62,63] }`, and the set of\n   * @p thread_ranks is `{ [63,62,61,60], ..., [7,6,5,4], [3,2,1,0] }`. The\n   * corresponding @p thread_output in those threads will be\n   * `{ [63, 47, 31, 15], [62, 46, 30, 14], ..., [48, 32, 16, 0] }`.\n   *\n   * @tparam OffsetT <b>[inferred]</b> Signed integer type for local offsets\n   *\n   * @param[in] input_items\n   *   Items to exchange\n   *\n   * @param[out] output_items\n   *   Items from exchange. May be aliased to @p input_items.\n   *\n   * @param[in] ranks\n   *   Corresponding scatter ranks\n   */\n  template <typename OutputT,\n            typename OffsetT>\n  __device__ __forceinline__ void\n  ScatterToStriped(const InputT (&input_items)[ITEMS_PER_THREAD],\n                   OutputT (&output_items)[ITEMS_PER_THREAD],\n                   OffsetT (&ranks)[ITEMS_PER_THREAD])\n  {\n    InternalWarpExchange::ScatterToStriped(input_items, output_items, ranks);\n  }\n\n  //@}  end member group\n};\n\n\nCUB_NAMESPACE_END\n\n#endif // _JITIFY_INCLUDE_GUARD_48B70CD5880D9166\n", "cub/warp/warp_reduce.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_EAEF1640776910F4\n#define _JITIFY_INCLUDE_GUARD_EAEF1640776910F4\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n//! @file\n//! @rst\n//! The ``cub::WarpReduce`` class provides :ref:`collective <collective-primitives>` methods for\n//! computing a parallel reduction of items partitioned across a CUDA thread warp.\n//! @endrst\n\n#include <cub/config.cuh>\n#include <cub/thread/thread_operators.cuh>\n#include <cub/util_type.cuh>\n#include <cub/warp/specializations/warp_reduce_shfl.cuh>\n#include <cub/warp/specializations/warp_reduce_smem.cuh>\n\nCUB_NAMESPACE_BEGIN\n\n//! @rst\n//! The ``WarpReduce`` class provides :ref:`collective <collective-primitives>` methods for\n//! computing a parallel reduction of items partitioned across a CUDA thread warp.\n//!\n//! .. image:: ../img/warp_reduce_logo.png\n//!     :align: center\n//!\n//! Overview\n//! ++++++++++++++++++++++++++\n//!\n//! - A `reduction <http://en.wikipedia.org/wiki/Reduce_(higher-order_function)>`__ (or *fold*)\n//!   uses a binary combining operator to compute a single aggregate from a list of input elements.\n//! - Supports \"logical\" warps smaller than the physical warp size (e.g., logical warps of 8\n//!   threads)\n//! - The number of entrant threads must be an multiple of ``LOGICAL_WARP_THREADS``\n//!\n//! Performance Considerations\n//! ++++++++++++++++++++++++++\n//!\n//! - Uses special instructions when applicable (e.g., warp ``SHFL`` instructions)\n//! - Uses synchronization-free communication between warp lanes when applicable\n//! - Incurs zero bank conflicts for most types\n//! - Computation is slightly more efficient (i.e., having lower instruction overhead) for:\n//!\n//!   - Summation (**vs.** generic reduction)\n//!   - The architecture's warp size is a whole multiple of ``LOGICAL_WARP_THREADS``\n//!\n//! Simple Examples\n//! ++++++++++++++++++++++++++\n//!\n//! @warpcollective{WarpReduce}\n//!\n//! The code snippet below illustrates four concurrent warp sum reductions within a block of\n//! 128 threads (one per each of the 32-thread warps).\n//!\n//! .. code-block:: c++\n//!\n//!    #include <cub/cub.cuh>\n//!\n//!    __global__ void ExampleKernel(...)\n//!    {\n//!        // Specialize WarpReduce for type int\n//!        typedef cub::WarpReduce<int> WarpReduce;\n//!\n//!        // Allocate WarpReduce shared memory for 4 warps\n//!        __shared__ typename WarpReduce::TempStorage temp_storage[4];\n//!\n//!        // Obtain one input item per thread\n//!        int thread_data = ...\n//!\n//!        // Return the warp-wide sums to each lane0 (threads 0, 32, 64, and 96)\n//!        int warp_id = threadIdx.x / 32;\n//!        int aggregate = WarpReduce(temp_storage[warp_id]).Sum(thread_data);\n//!\n//! Suppose the set of input ``thread_data`` across the block of threads is\n//! ``{0, 1, 2, 3, ..., 127}``. The corresponding output ``aggregate`` in threads 0, 32, 64, and 96\n//! will be ``496``, ``1520``, ``2544``, and ``3568``, respectively\n//! (and is undefined in other threads).\n//!\n//! The code snippet below illustrates a single warp sum reduction within a block of\n//! 128 threads.\n//!\n//! .. code-block:: c++\n//!\n//!    #include <cub/cub.cuh>\n//!\n//!    __global__ void ExampleKernel(...)\n//!    {\n//!        // Specialize WarpReduce for type int\n//!        typedef cub::WarpReduce<int> WarpReduce;\n//!\n//!        // Allocate WarpReduce shared memory for one warp\n//!        __shared__ typename WarpReduce::TempStorage temp_storage;\n//!        ...\n//!\n//!        // Only the first warp performs a reduction\n//!        if (threadIdx.x < 32)\n//!        {\n//!            // Obtain one input item per thread\n//!            int thread_data = ...\n//!\n//!            // Return the warp-wide sum to lane0\n//!            int aggregate = WarpReduce(temp_storage).Sum(thread_data);\n//!\n//! Suppose the set of input ``thread_data`` across the warp of threads is\n//! ``{0, 1, 2, 3, ..., 31}``. The corresponding output ``aggregate`` in thread0 will be ``496``\n//! (and is undefined in other threads).\n//! @endrst\n//!\n//! @tparam T\n//!   The reduction input/output element type\n//!\n//! @tparam LOGICAL_WARP_THREADS\n//!   <b>[optional]</b> The number of threads per \"logical\" warp (may be less than the number of\n//!   hardware warp threads).  Default is the warp size of the targeted CUDA compute-capability\n//!   (e.g., 32 threads for SM20).\n//!\n//! @tparam LEGACY_PTX_ARCH\n//!   <b>[optional]</b> Unused.\ntemplate <typename T, int LOGICAL_WARP_THREADS = CUB_PTX_WARP_THREADS, int LEGACY_PTX_ARCH = 0>\nclass WarpReduce\n{\nprivate:\n  /******************************************************************************\n   * Constants and type definitions\n   ******************************************************************************/\n\n  enum\n  {\n    /// Whether the logical warp size and the PTX warp size coincide\n    IS_ARCH_WARP = (LOGICAL_WARP_THREADS == CUB_WARP_THREADS(0)),\n\n    /// Whether the logical warp size is a power-of-two\n    IS_POW_OF_TWO = PowerOfTwo<LOGICAL_WARP_THREADS>::VALUE,\n  };\n\npublic:\n#ifndef DOXYGEN_SHOULD_SKIP_THIS // Do not document\n\n  /// Internal specialization.\n  /// Use SHFL-based reduction if LOGICAL_WARP_THREADS is a power-of-two\n  using InternalWarpReduce = cub::detail::conditional_t<IS_POW_OF_TWO,\n                                                        WarpReduceShfl<T, LOGICAL_WARP_THREADS>,\n                                                        WarpReduceSmem<T, LOGICAL_WARP_THREADS>>;\n\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\nprivate:\n  /// Shared memory storage layout type for WarpReduce\n  using _TempStorage = typename InternalWarpReduce::TempStorage;\n\n  /******************************************************************************\n   * Thread fields\n   ******************************************************************************/\n\n  /// Shared storage reference\n  _TempStorage &temp_storage;\n\n  /******************************************************************************\n   * Utility methods\n   ******************************************************************************/\n\npublic:\n  /// \\smemstorage{WarpReduce}\n  struct TempStorage : Uninitialized<_TempStorage>\n  {};\n\n  //! @name Collective constructors\n  //! @{\n\n  //! @rst\n  //! Collective constructor using the specified memory allocation as temporary storage.\n  //! Logical warp and lane identifiers are constructed from ``threadIdx.x``.\n  //! @endrst\n  //!\n  //! @param[in] temp_storage Reference to memory allocation having layout type TempStorage\n  __device__ __forceinline__ WarpReduce(TempStorage &temp_storage)\n      : temp_storage(temp_storage.Alias())\n  {}\n\n  //! @}  end member group\n  //! @name Summation reductions\n  //! @{\n\n  //! @rst\n  //! Computes a warp-wide sum in the calling warp.\n  //! The output is valid in warp *lane*\\ :sub:`0`.\n  //!\n  //! @smemwarpreuse\n  //!\n  //! Snippet\n  //! +++++++\n  //!\n  //! The code snippet below illustrates four concurrent warp sum reductions within a block of\n  //! 128 threads (one per each of the 32-thread warps).\n  //!\n  //! .. code-block:: c++\n  //!\n  //!    #include <cub/cub.cuh>\n  //!\n  //!    __global__ void ExampleKernel(...)\n  //!    {\n  //!        // Specialize WarpReduce for type int\n  //!        typedef cub::WarpReduce<int> WarpReduce;\n  //!\n  //!        // Allocate WarpReduce shared memory for 4 warps\n  //!        __shared__ typename WarpReduce::TempStorage temp_storage[4];\n  //!\n  //!        // Obtain one input item per thread\n  //!        int thread_data = ...\n  //!\n  //!        // Return the warp-wide sums to each lane0\n  //!        int warp_id = threadIdx.x / 32;\n  //!        int aggregate = WarpReduce(temp_storage[warp_id]).Sum(thread_data);\n  //!\n  //! Suppose the set of input ``thread_data`` across the block of threads is\n  //! ``{0, 1, 2, 3, ..., 127}``.\n  //! The corresponding output ``aggregate`` in threads 0, 32, 64, and 96 will ``496``, ``1520``,\n  //! ``2544``, and ``3568``, respectively (and is undefined in other threads).\n  //! @endrst\n  //!\n  //! @param[in] input Calling thread's input\n  __device__ __forceinline__ T Sum(T input)\n  {\n    return InternalWarpReduce(temp_storage)\n      .template Reduce<true>(input, LOGICAL_WARP_THREADS, cub::Sum());\n  }\n\n  //! @rst\n  //! Computes a partially-full warp-wide sum in the calling warp.\n  //! The output is valid in warp *lane*\\ :sub:`0`.\n  //!\n  //! All threads across the calling warp must agree on the same value for ``valid_items``.\n  //! Otherwise the result is undefined.\n  //!\n  //! @smemwarpreuse\n  //!\n  //! Snippet\n  //! +++++++\n  //!\n  //! The code snippet below illustrates a sum reduction within a single, partially-full\n  //! block of 32 threads (one warp).\n  //!\n  //! .. code-block:: c++\n  //!\n  //!    #include <cub/cub.cuh>\n  //!\n  //!    __global__ void ExampleKernel(int *d_data, int valid_items)\n  //!    {\n  //!        // Specialize WarpReduce for type int\n  //!        typedef cub::WarpReduce<int> WarpReduce;\n  //!\n  //!        // Allocate WarpReduce shared memory for one warp\n  //!        __shared__ typename WarpReduce::TempStorage temp_storage;\n  //!\n  //!        // Obtain one input item per thread if in range\n  //!        int thread_data;\n  //!        if (threadIdx.x < valid_items)\n  //!            thread_data = d_data[threadIdx.x];\n  //!\n  //!        // Return the warp-wide sums to each lane0\n  //!        int aggregate = WarpReduce(temp_storage).Sum(\n  //!            thread_data, valid_items);\n  //!\n  //! Suppose the input ``d_data`` is ``{0, 1, 2, 3, 4, ...`` and ``valid_items`` is ``4``.\n  //! The corresponding output ``aggregate`` in *lane*\\ :sub:`0` is ``6``\n  //! (and is undefined in other threads).\n  //! @endrst\n  //!\n  //! @param[in] input\n  //!   Calling thread's input\n  //!\n  //! @param[in] valid_items\n  //!   Total number of valid items in the calling thread's logical warp\n  //!   (may be less than ``LOGICAL_WARP_THREADS``)\n  __device__ __forceinline__ T Sum(T input, int valid_items)\n  {\n    // Determine if we don't need bounds checking\n    return InternalWarpReduce(temp_storage).template Reduce<false>(input, valid_items, cub::Sum());\n  }\n\n  //! @rst\n  //! Computes a segmented sum in the calling warp where segments are defined by head-flags.\n  //! The sum of each segment is returned to the first lane in that segment\n  //! (which always includes *lane*\\ :sub:`0`).\n  //!\n  //! @smemwarpreuse\n  //!\n  //! Snippet\n  //! +++++++\n  //!\n  //! The code snippet below illustrates a head-segmented warp sum\n  //! reduction within a block of 32 threads (one warp).\n  //!\n  //! .. code-block:: c++\n  //!\n  //!    #include <cub/cub.cuh>\n  //!\n  //!    __global__ void ExampleKernel(...)\n  //!    {\n  //!        // Specialize WarpReduce for type int\n  //!        typedef cub::WarpReduce<int> WarpReduce;\n  //!\n  //!        // Allocate WarpReduce shared memory for one warp\n  //!        __shared__ typename WarpReduce::TempStorage temp_storage;\n  //!\n  //!        // Obtain one input item and flag per thread\n  //!        int thread_data = ...\n  //!        int head_flag = ...\n  //!\n  //!        // Return the warp-wide sums to each lane0\n  //!        int aggregate = WarpReduce(temp_storage).HeadSegmentedSum(\n  //!            thread_data, head_flag);\n  //!\n  //! Suppose the set of input ``thread_data`` and ``head_flag`` across the block of threads\n  //! is ``{0, 1, 2, 3, ..., 31`` and is ``{1, 0, 0, 0, 1, 0, 0, 0, ..., 1, 0, 0, 0``,\n  //! respectively. The corresponding output ``aggregate`` in threads 0, 4, 8, etc. will be\n  //! ``6``, ``22``, ``38``, etc. (and is undefined in other threads).\n  //! @endrst\n  //!\n  //! @tparam ReductionOp\n  //!   **[inferred]** Binary reduction operator type having member\n  //!   `T operator()(const T &a, const T &b)`\n  //!\n  //! @param[in] input\n  //!   Calling thread's input\n  //!\n  //! @param[in] head_flag\n  //!   Head flag denoting whether or not `input` is the start of a new segment\n  template <typename FlagT>\n  __device__ __forceinline__ T HeadSegmentedSum(T input, FlagT head_flag)\n  {\n    return HeadSegmentedReduce(input, head_flag, cub::Sum());\n  }\n\n  //! @rst\n  //! Computes a segmented sum in the calling warp where segments are defined by tail-flags.\n  //! The sum of each segment is returned to the first lane in that segment\n  //! (which always includes *lane*\\ :sub:`0`).\n  //!\n  //! @smemwarpreuse\n  //!\n  //! Snippet\n  //! +++++++\n  //!\n  //! The code snippet below illustrates a tail-segmented warp sum reduction within a block of 32\n  //! threads (one warp).\n  //!\n  //! .. code-block:: c++\n  //!\n  //!    #include <cub/cub.cuh>\n  //!\n  //!    __global__ void ExampleKernel(...)\n  //!    {\n  //!        // Specialize WarpReduce for type int\n  //!        typedef cub::WarpReduce<int> WarpReduce;\n  //!\n  //!        // Allocate WarpReduce shared memory for one warp\n  //!        __shared__ typename WarpReduce::TempStorage temp_storage;\n  //!\n  //!        // Obtain one input item and flag per thread\n  //!        int thread_data = ...\n  //!        int tail_flag = ...\n  //!\n  //!        // Return the warp-wide sums to each lane0\n  //!        int aggregate = WarpReduce(temp_storage).TailSegmentedSum(\n  //!            thread_data, tail_flag);\n  //!\n  //! Suppose the set of input ``thread_data`` and ``tail_flag`` across the block of threads\n  //! is ``{0, 1, 2, 3, ..., 31}`` and is ``{0, 0, 0, 1, 0, 0, 0, 1, ..., 0, 0, 0, 1}``,\n  //! respectively. The corresponding output ``aggregate`` in threads 0, 4, 8, etc. will be\n  //! ``6``, ``22``, ``38``, etc. (and is undefined in other threads).\n  //! @endrst\n  //!\n  //! @tparam ReductionOp\n  //!   **[inferred]** Binary reduction operator type having member\n  //!   `T operator()(const T &a, const T &b)`\n  //!\n  //! @param[in] input\n  //!   Calling thread's input\n  //!\n  //! @param[in] tail_flag\n  //!   Head flag denoting whether or not `input` is the start of a new segment\n  template <typename FlagT>\n  __device__ __forceinline__ T TailSegmentedSum(T input, FlagT tail_flag)\n  {\n    return TailSegmentedReduce(input, tail_flag, cub::Sum());\n  }\n\n  //! @}  end member group\n  //! @name Generic reductions\n  //! @{\n\n  //! @rst\n  //! Computes a warp-wide reduction in the calling warp using the specified binary reduction\n  //! functor. The output is valid in warp *lane*\\ :sub:`0`.\n  //!\n  //! Supports non-commutative reduction operators\n  //!\n  //! @smemwarpreuse\n  //!\n  //! Snippet\n  //! +++++++\n  //!\n  //! The code snippet below illustrates four concurrent warp max reductions within a block of\n  //! 128 threads (one per each of the 32-thread warps).\n  //!\n  //! .. code-block:: c++\n  //!\n  //!    #include <cub/cub.cuh>\n  //!\n  //!    __global__ void ExampleKernel(...)\n  //!    {\n  //!        // Specialize WarpReduce for type int\n  //!        typedef cub::WarpReduce<int> WarpReduce;\n  //!\n  //!        // Allocate WarpReduce shared memory for 4 warps\n  //!        __shared__ typename WarpReduce::TempStorage temp_storage[4];\n  //!\n  //!        // Obtain one input item per thread\n  //!        int thread_data = ...\n  //!\n  //!        // Return the warp-wide reductions to each lane0\n  //!        int warp_id = threadIdx.x / 32;\n  //!        int aggregate = WarpReduce(temp_storage[warp_id]).Reduce(\n  //!            thread_data, cub::Max());\n  //!\n  //! Suppose the set of input ``thread_data`` across the block of threads is\n  //! ``{0, 1, 2, 3, ..., 127}``. The corresponding output ``aggregate`` in threads 0, 32, 64, and\n  //! 96 will be ``31``, ``63``, ``95``, and ``127``, respectively\n  //! (and is undefined in other threads).\n  //! @endrst\n  //!\n  //! @tparam ReductionOp\n  //!   **[inferred]** Binary reduction operator type having member\n  //!   `T operator()(const T &a, const T &b)`\n  //!\n  //! @param[in] input\n  //!   Calling thread's input\n  //!\n  //! @param[in] reduction_op\n  //!   Binary reduction operator\n  template <typename ReductionOp>\n  __device__ __forceinline__ T Reduce(T input, ReductionOp reduction_op)\n  {\n    return InternalWarpReduce(temp_storage)\n      .template Reduce<true>(input, LOGICAL_WARP_THREADS, reduction_op);\n  }\n\n  //! @rst\n  //! Computes a partially-full warp-wide reduction in the calling warp using the specified binary\n  //! reduction functor. The output is valid in warp *lane*\\ :sub:`0`.\n  //!\n  //! All threads across the calling warp must agree on the same value for ``valid_items``.\n  //! Otherwise the result is undefined.\n  //!\n  //! Supports non-commutative reduction operators\n  //!\n  //! @smemwarpreuse\n  //!\n  //! Snippet\n  //! +++++++\n  //!\n  //! The code snippet below illustrates a max reduction within a single, partially-full\n  //! block of 32 threads (one warp).\n  //!\n  //! .. code-block:: c++\n  //!\n  //!    #include <cub/cub.cuh>\n  //!\n  //!    __global__ void ExampleKernel(int *d_data, int valid_items)\n  //!    {\n  //!        // Specialize WarpReduce for type int\n  //!        typedef cub::WarpReduce<int> WarpReduce;\n  //!\n  //!        // Allocate WarpReduce shared memory for one warp\n  //!        __shared__ typename WarpReduce::TempStorage temp_storage;\n  //!\n  //!        // Obtain one input item per thread if in range\n  //!        int thread_data;\n  //!        if (threadIdx.x < valid_items)\n  //!            thread_data = d_data[threadIdx.x];\n  //!\n  //!        // Return the warp-wide reductions to each lane0\n  //!        int aggregate = WarpReduce(temp_storage).Reduce(\n  //!            thread_data, cub::Max(), valid_items);\n  //!\n  //! Suppose the input ``d_data`` is ``{0, 1, 2, 3, 4, ... }`` and ``valid_items``\n  //! is ``4``. The corresponding output ``aggregate`` in thread0 is ``3`` (and is\n  //! undefined in other threads).\n  //! @endrst\n  //!\n  //! @tparam ReductionOp\n  //!   **[inferred]** Binary reduction operator type having member\n  //!   `T operator()(const T &a, const T &b)`\n  //!\n  //! @param[in] input\n  //!   Calling thread's input\n  //!\n  //! @param[in] reduction_op\n  //!   Binary reduction operator\n  //!\n  //! @param[in] valid_items\n  //!   Total number of valid items in the calling thread's logical warp\n  //!   (may be less than ``LOGICAL_WARP_THREADS``)\n  template <typename ReductionOp>\n  __device__ __forceinline__ T Reduce(T input, ReductionOp reduction_op, int valid_items)\n  {\n    return InternalWarpReduce(temp_storage).template Reduce<false>(input, valid_items, reduction_op);\n  }\n\n  //! @rst\n  //! Computes a segmented reduction in the calling warp where segments are defined by head-flags.\n  //! The reduction of each segment is returned to the first lane in that segment\n  //! (which always includes *lane*\\ :sub:`0`).\n  //!\n  //! Supports non-commutative reduction operators\n  //!\n  //! @smemwarpreuse\n  //!\n  //! Snippet\n  //! +++++++\n  //!\n  //! The code snippet below illustrates a head-segmented warp max\n  //! reduction within a block of 32 threads (one warp).\n  //!\n  //! .. code-block:: c++\n  //!\n  //!    #include <cub/cub.cuh>\n  //!\n  //!    __global__ void ExampleKernel(...)\n  //!    {\n  //!        // Specialize WarpReduce for type int\n  //!        typedef cub::WarpReduce<int> WarpReduce;\n  //!\n  //!        // Allocate WarpReduce shared memory for one warp\n  //!        __shared__ typename WarpReduce::TempStorage temp_storage;\n  //!\n  //!        // Obtain one input item and flag per thread\n  //!        int thread_data = ...\n  //!        int head_flag = ...\n  //!\n  //!        // Return the warp-wide reductions to each lane0\n  //!        int aggregate = WarpReduce(temp_storage).HeadSegmentedReduce(\n  //!            thread_data, head_flag, cub::Max());\n  //!\n  //! Suppose the set of input ``thread_data`` and ``head_flag`` across the block of threads\n  //! is ``{0, 1, 2, 3, ..., 31}`` and is ``{1, 0, 0, 0, 1, 0, 0, 0, ..., 1, 0, 0, 0}``,\n  //! respectively. The corresponding output ``aggregate`` in threads 0, 4, 8, etc. will be\n  //! ``3``, ``7``, ``11``, etc. (and is undefined in other threads).\n  //! @endrst\n  //!\n  //! @tparam ReductionOp\n  //!   **[inferred]** Binary reduction operator type having member\n  //!   `T operator()(const T &a, const T &b)`\n  //!\n  //! @param[in] input\n  //!   Calling thread's input\n  //!\n  //! @param[in] head_flag\n  //!   Head flag denoting whether or not `input` is the start of a new segment\n  //!\n  //! @param[in] reduction_op\n  //!   Reduction operator\n  template <typename ReductionOp, typename FlagT>\n  __device__ __forceinline__ T HeadSegmentedReduce(T input,\n                                                   FlagT head_flag,\n                                                   ReductionOp reduction_op)\n  {\n    return InternalWarpReduce(temp_storage)\n      .template SegmentedReduce<true>(input, head_flag, reduction_op);\n  }\n\n  //! @rst\n  //! Computes a segmented reduction in the calling warp where segments are defined by tail-flags.\n  //! The reduction of each segment is returned to the first lane in that segment\n  //! (which always includes *lane*\\ :sub:`0`).\n  //!\n  //! Supports non-commutative reduction operators\n  //!\n  //! @smemwarpreuse\n  //!\n  //! Snippet\n  //! +++++++\n  //!\n  //! The code snippet below illustrates a tail-segmented warp max\n  //! reduction within a block of 32 threads (one warp).\n  //!\n  //! .. code-block:: c++\n  //!\n  //!    #include <cub/cub.cuh>\n  //!\n  //!    __global__ void ExampleKernel(...)\n  //!    {\n  //!        // Specialize WarpReduce for type int\n  //!        typedef cub::WarpReduce<int> WarpReduce;\n  //!\n  //!        // Allocate WarpReduce shared memory for one warp\n  //!        __shared__ typename WarpReduce::TempStorage temp_storage;\n  //!\n  //!        // Obtain one input item and flag per thread\n  //!        int thread_data = ...\n  //!        int tail_flag = ...\n  //!\n  //!        // Return the warp-wide reductions to each lane0\n  //!        int aggregate = WarpReduce(temp_storage).TailSegmentedReduce(\n  //!            thread_data, tail_flag, cub::Max());\n  //!\n  //! Suppose the set of input ``thread_data`` and ``tail_flag`` across the block of threads\n  //! is ``{0, 1, 2, 3, ..., 31}`` and is ``{0, 0, 0, 1, 0, 0, 0, 1, ..., 0, 0, 0, 1}``,\n  //! respectively. The corresponding output ``aggregate`` in threads 0, 4, 8, etc. will be\n  //! ``3``, ``7``, ``11``, etc. (and is undefined in other threads).\n  //! @endrst\n  //!\n  //! @tparam ReductionOp\n  //!   **[inferred]** Binary reduction operator type having member\n  //!   `T operator()(const T &a, const T &b)`\n  //!\n  //! @param[in] input\n  //!   Calling thread's input\n  //!\n  //! @param[in] tail_flag\n  //!   Tail flag denoting whether or not \\p input is the end of the current segment\n  //!\n  //! @param[in] reduction_op\n  //!   Reduction operator\n  template <typename ReductionOp, typename FlagT>\n  __device__ __forceinline__ T TailSegmentedReduce(T input,\n                                                   FlagT tail_flag,\n                                                   ReductionOp reduction_op)\n  {\n    return InternalWarpReduce(temp_storage)\n      .template SegmentedReduce<false>(input, tail_flag, reduction_op);\n  }\n\n  //! @}  end member group\n};\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS // Do not document\ntemplate <typename T, int LEGACY_PTX_ARCH>\nclass WarpReduce<T, 1, LEGACY_PTX_ARCH>\n{\nprivate:\n  using _TempStorage = cub::NullType;\n\npublic:\n  struct InternalWarpReduce\n  {\n    struct TempStorage : Uninitialized<_TempStorage>\n    {};\n\n    __device__ __forceinline__ InternalWarpReduce(TempStorage & /*temp_storage */) {}\n\n    template <bool ALL_LANES_VALID, typename ReductionOp>\n    __device__ __forceinline__ T Reduce(T input,\n                                        int /* valid_items */,\n                                        ReductionOp /* reduction_op */)\n    {\n      return input;\n    }\n\n    template <bool HEAD_SEGMENTED, typename FlagT, typename ReductionOp>\n    __device__ __forceinline__ T SegmentedReduce(T input,\n                                                 FlagT /* flag */,\n                                                 ReductionOp /* reduction_op */)\n    {\n      return input;\n    }\n  };\n\n  using TempStorage = typename InternalWarpReduce::TempStorage;\n\n  __device__ __forceinline__ WarpReduce(TempStorage & /*temp_storage */) {}\n\n  __device__ __forceinline__ T Sum(T input) { return input; }\n\n  __device__ __forceinline__ T Sum(T input, int /* valid_items */) { return input; }\n\n  template <typename FlagT>\n  __device__ __forceinline__ T HeadSegmentedSum(T input, FlagT /* head_flag */)\n  {\n    return input;\n  }\n\n  template <typename FlagT>\n  __device__ __forceinline__ T TailSegmentedSum(T input, FlagT /* tail_flag */)\n  {\n    return input;\n  }\n\n  template <typename ReductionOp>\n  __device__ __forceinline__ T Reduce(T input, ReductionOp /* reduction_op */)\n  {\n    return input;\n  }\n\n  template <typename ReductionOp>\n  __device__ __forceinline__ T Reduce(T input,\n                                      ReductionOp /* reduction_op */,\n                                      int /* valid_items */)\n  {\n    return input;\n  }\n\n  template <typename ReductionOp, typename FlagT>\n  __device__ __forceinline__ T HeadSegmentedReduce(T input,\n                                                   FlagT /* head_flag */,\n                                                   ReductionOp /* reduction_op */)\n  {\n    return input;\n  }\n\n  template <typename ReductionOp, typename FlagT>\n  __device__ __forceinline__ T TailSegmentedReduce(T input,\n                                                   FlagT /* tail_flag */,\n                                                   ReductionOp /* reduction_op */)\n  {\n    return input;\n  }\n};\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\nCUB_NAMESPACE_END\n\n#endif // _JITIFY_INCLUDE_GUARD_EAEF1640776910F4\n", "cuda/barrier": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of libcu++, the C++ Standard Library for your entire system,\n// under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _CUDA_BARRIER\n#define _CUDA_BARRIER\n\n#include \"std/barrier\"\n\n#endif // _CUDA_BARRIER\n", "cuda/std/functional": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of libcu++, the C++ Standard Library for your entire system,\n// under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _CUDA_STD_FUNCTIONAL\n#define _CUDA_STD_FUNCTIONAL\n\n#include \"detail/__config\"\n\n#include \"detail/__pragma_push\"\n\n#include \"detail/libcxx/include/functional\"\n\n#include \"detail/__pragma_pop\"\n\n#endif // _CUDA_STD_FUNCTIONAL\n", "cuda/std/limits": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of libcu++, the C++ Standard Library for your entire system,\n// under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _CUDA_STD_LIMITS\n#define _CUDA_STD_LIMITS\n\n#include \"detail/__config\"\n\n#include \"detail/__pragma_push\"\n\n#include \"detail/libcxx/include/limits\"\n\n#include \"detail/__pragma_pop\"\n\n#endif // _CUDA_STD_LIMITS\n", "cuda/std/tuple": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of libcu++, the C++ Standard Library for your entire system,\n// under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _CUDA_STD_TUPLE\n#define _CUDA_STD_TUPLE\n\n#include \"detail/__config\"\n\n#include \"detail/__pragma_push\"\n\n#include \"detail/libcxx/include/tuple\"\n\n#include \"detail/__pragma_pop\"\n\n#endif // _CUDA_STD_TUPLE\n", "cuda/std/type_traits": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of libcu++, the C++ Standard Library for your entire system,\n// under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _CUDA_STD_TYPE_TRAITS\n#define _CUDA_STD_TYPE_TRAITS\n\n#include \"detail/__config\"\n\n#include \"detail/__pragma_push\"\n\n#include \"detail/libcxx/include/type_traits\"\n\n#include \"detail/__pragma_pop\"\n\n#endif // _CUDA_STD_TYPE_TRAITS\n", "cuda/std/utility": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of libcu++, the C++ Standard Library for your entire system,\n// under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _CUDA_STD_UTILITY\n#define _CUDA_STD_UTILITY\n\n#include \"detail/__config\"\n\n#include \"detail/__pragma_push\"\n\n#include \"detail/libcxx/include/utility\"\n\n#include \"detail/__pragma_pop\"\n\n#endif // _CUDA_STD_UTILITY\n", "cuda_fp16.h": "#define cudaDeviceSynchronize() cudaSuccess\n/*\n* Copyright 1993-2021 NVIDIA Corporation.  All rights reserved.\n*\n* NOTICE TO LICENSEE:\n*\n* This source code and/or documentation (\"Licensed Deliverables\") are\n* subject to NVIDIA intellectual property rights under U.S. and\n* international Copyright laws.\n*\n* These Licensed Deliverables contained herein is PROPRIETARY and\n* CONFIDENTIAL to NVIDIA and is being provided under the terms and\n* conditions of a form of NVIDIA software license agreement by and\n* between NVIDIA and Licensee (\"License Agreement\") or electronically\n* accepted by Licensee.  Notwithstanding any terms or conditions to\n* the contrary in the License Agreement, reproduction or disclosure\n* of the Licensed Deliverables to any third party without the express\n* written consent of NVIDIA is prohibited.\n*\n* NOTWITHSTANDING ANY TERMS OR CONDITIONS TO THE CONTRARY IN THE\n* LICENSE AGREEMENT, NVIDIA MAKES NO REPRESENTATION ABOUT THE\n* SUITABILITY OF THESE LICENSED DELIVERABLES FOR ANY PURPOSE.  IT IS\n* PROVIDED \"AS IS\" WITHOUT EXPRESS OR IMPLIED WARRANTY OF ANY KIND.\n* NVIDIA DISCLAIMS ALL WARRANTIES WITH REGARD TO THESE LICENSED\n* DELIVERABLES, INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY,\n* NONINFRINGEMENT, AND FITNESS FOR A PARTICULAR PURPOSE.\n* NOTWITHSTANDING ANY TERMS OR CONDITIONS TO THE CONTRARY IN THE\n* LICENSE AGREEMENT, IN NO EVENT SHALL NVIDIA BE LIABLE FOR ANY\n* SPECIAL, INDIRECT, INCIDENTAL, OR CONSEQUENTIAL DAMAGES, OR ANY\n* DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS,\n* WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS\n* ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE\n* OF THESE LICENSED DELIVERABLES.\n*\n* U.S. Government End Users.  These Licensed Deliverables are a\n* \"commercial item\" as that term is defined at 48 C.F.R. 2.101 (OCT\n* 1995), consisting of \"commercial computer software\" and \"commercial\n* computer software documentation\" as such terms are used in 48\n* C.F.R. 12.212 (SEPT 1995) and is provided to the U.S. Government\n* only as a commercial end item.  Consistent with 48 C.F.R.12.212 and\n* 48 C.F.R. 227.7202-1 through 227.7202-4 (JUNE 1995), all\n* U.S. Government End Users acquire the Licensed Deliverables with\n* only those rights set forth herein.\n*\n* Any use of the Licensed Deliverables in individual and commercial\n* software must include, in the user documentation and internal\n* comments to the code, the above Disclaimer and U.S. Government End\n* Users Notice.\n*/\n\n/**\n* \\defgroup CUDA_MATH_INTRINSIC_HALF Half Precision Intrinsics\n* This section describes half precision intrinsic functions that are\n* only supported in device code.\n* To use these functions, include the header file \\p cuda_fp16.h in your program.\n* The following macros are available to help users selectively enable/disable\n* various definitions present in the header file:\n* - \\p CUDA_NO_HALF - If defined, this macro will prevent the definition of\n* additional type aliases in the global namespace, helping to avoid potential\n* conflicts with symbols defined in the user program.\n* - \\p __CUDA_NO_HALF_CONVERSIONS__ - If defined, this macro will prevent the\n* use of the C++ type conversions (converting constructors and conversion\n* operators) that are common for built-in floating-point types, but may be\n* undesirable for \\p half which is essentially a user-defined type.\n* - \\p __CUDA_NO_HALF_OPERATORS__ and \\p __CUDA_NO_HALF2_OPERATORS__ - If\n* defined, these macros will prevent the inadvertent use of usual arithmetic\n* and comparison operators. This enforces the storage-only type semantics and\n* prevents C++ style computations on \\p half and \\p half2 types.\n*/\n\n/**\n* \\defgroup CUDA_MATH__HALF_ARITHMETIC Half Arithmetic Functions\n* \\ingroup CUDA_MATH_INTRINSIC_HALF\n* To use these functions, include the header file \\p cuda_fp16.h in your program.\n*/\n\n/**\n* \\defgroup CUDA_MATH__HALF2_ARITHMETIC Half2 Arithmetic Functions\n* \\ingroup CUDA_MATH_INTRINSIC_HALF\n* To use these functions, include the header file \\p cuda_fp16.h in your program.\n*/\n\n/**\n* \\defgroup CUDA_MATH__HALF_COMPARISON Half Comparison Functions\n* \\ingroup CUDA_MATH_INTRINSIC_HALF\n* To use these functions, include the header file \\p cuda_fp16.h in your program.\n*/\n\n/**\n* \\defgroup CUDA_MATH__HALF2_COMPARISON Half2 Comparison Functions\n* \\ingroup CUDA_MATH_INTRINSIC_HALF\n* To use these functions, include the header file \\p cuda_fp16.h in your program.\n*/\n\n/**\n* \\defgroup CUDA_MATH__HALF_MISC Half Precision Conversion and Data Movement\n* \\ingroup CUDA_MATH_INTRINSIC_HALF\n* To use these functions, include the header file \\p cuda_fp16.h in your program.\n*/\n\n/**\n* \\defgroup CUDA_MATH__HALF_FUNCTIONS Half Math Functions\n* \\ingroup CUDA_MATH_INTRINSIC_HALF\n* To use these functions, include the header file \\p cuda_fp16.h in your program.\n*/\n\n/**\n* \\defgroup CUDA_MATH__HALF2_FUNCTIONS Half2 Math Functions\n* \\ingroup CUDA_MATH_INTRINSIC_HALF\n* To use these functions, include the header file \\p cuda_fp16.h in your program.\n*/\n\n#ifndef __CUDA_FP16_H__\n#define __CUDA_FP16_H__\n\n#define ___CUDA_FP16_STRINGIFY_INNERMOST(x) #x\n#define __CUDA_FP16_STRINGIFY(x) ___CUDA_FP16_STRINGIFY_INNERMOST(x)\n\n#if defined(__cplusplus)\n#if defined(__CUDACC__)\n#define __CUDA_FP16_DECL__ static __device__ __inline__\n#define __CUDA_HOSTDEVICE_FP16_DECL__ static __host__ __device__ __inline__\n#else\n#define __CUDA_HOSTDEVICE_FP16_DECL__ static\n#endif /* defined(__CUDACC__) */\n\n#define __CUDA_FP16_TYPES_EXIST__\n\n/* Forward-declaration of structures defined in \"cuda_fp16.hpp\" */\n\n/**\n * \\brief half datatype \n * \n * \\details This structure implements the datatype for storing \n * half-precision floating-point numbers. The structure implements \n * assignment operators and type conversions. \n * 16 bits are being used in total: 1 sign bit, 5 bits for the exponent, \n * and the significand is being stored in 10 bits. \n * The total precision is 11 bits. There are 15361 representable \n * numbers within the interval [0.0, 1.0], endpoints included. \n * On average we have log10(2**11) ~ 3.311 decimal digits. \n * \n * \\internal\n * \\req IEEE 754-2008 compliant implementation of half-precision \n * floating-point numbers. \n * \\endinternal\n */\nstruct __half;\n\n/**\n * \\brief half2 datatype\n * \n * \\details This structure implements the datatype for storing two \n * half-precision floating-point numbers. \n * The structure implements assignment operators and type conversions. \n * \n * \\internal\n * \\req Vectorified version of half. \n * \\endinternal\n */\nstruct __half2;\n\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Converts double number to half precision in round-to-nearest-even mode\n* and returns \\p half with converted value.\n*\n* \\details Converts double number \\p a to half precision in round-to-nearest-even mode.\n* \\param[in] a - double. Is only being read.\n* \\returns half\n* - \\p a converted to half.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_HOSTDEVICE_FP16_DECL__ __half __double2half(const double a);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Converts float number to half precision in round-to-nearest-even mode\n* and returns \\p half with converted value. \n* \n* \\details Converts float number \\p a to half precision in round-to-nearest-even mode. \n* \\param[in] a - float. Is only being read. \n* \\returns half\n* - \\p a converted to half. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_HOSTDEVICE_FP16_DECL__ __half __float2half(const float a);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Converts float number to half precision in round-to-nearest-even mode\n* and returns \\p half with converted value.\n*\n* \\details Converts float number \\p a to half precision in round-to-nearest-even mode.\n* \\param[in] a - float. Is only being read. \n* \\returns half\n* - \\p a converted to half. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_HOSTDEVICE_FP16_DECL__ __half __float2half_rn(const float a);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Converts float number to half precision in round-towards-zero mode\n* and returns \\p half with converted value.\n* \n* \\details Converts float number \\p a to half precision in round-towards-zero mode.\n* \\param[in] a - float. Is only being read. \n* \\returns half\n* - \\p a converted to half. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_HOSTDEVICE_FP16_DECL__ __half __float2half_rz(const float a);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Converts float number to half precision in round-down mode\n* and returns \\p half with converted value.\n* \n* \\details Converts float number \\p a to half precision in round-down mode.\n* \\param[in] a - float. Is only being read. \n* \n* \\returns half\n* - \\p a converted to half. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_HOSTDEVICE_FP16_DECL__ __half __float2half_rd(const float a);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Converts float number to half precision in round-up mode\n* and returns \\p half with converted value.\n* \n* \\details Converts float number \\p a to half precision in round-up mode.\n* \\param[in] a - float. Is only being read. \n* \n* \\returns half\n* - \\p a converted to half. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_HOSTDEVICE_FP16_DECL__ __half __float2half_ru(const float a);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Converts \\p half number to float.\n* \n* \\details Converts half number \\p a to float.\n* \\param[in] a - float. Is only being read. \n* \n* \\returns float\n* - \\p a converted to float. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_HOSTDEVICE_FP16_DECL__ float __half2float(const __half a);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Converts input to half precision in round-to-nearest-even mode and\n* populates both halves of \\p half2 with converted value.\n*\n* \\details Converts input \\p a to half precision in round-to-nearest-even mode and\n* populates both halves of \\p half2 with converted value.\n* \\param[in] a - float. Is only being read. \n*\n* \\returns half2\n* - The \\p half2 value with both halves equal to the converted half\n* precision number.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_HOSTDEVICE_FP16_DECL__ __half2 __float2half2_rn(const float a);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Converts both input floats to half precision in round-to-nearest-even\n* mode and returns \\p half2 with converted values.\n*\n* \\details Converts both input floats to half precision in round-to-nearest-even mode\n* and combines the results into one \\p half2 number. Low 16 bits of the return\n* value correspond to the input \\p a, high 16 bits correspond to the input \\p\n* b.\n* \\param[in] a - float. Is only being read. \n* \\param[in] b - float. Is only being read. \n* \n* \\returns half2\n* - The \\p half2 value with corresponding halves equal to the\n* converted input floats.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_HOSTDEVICE_FP16_DECL__ __half2 __floats2half2_rn(const float a, const float b);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Converts low 16 bits of \\p half2 to float and returns the result\n* \n* \\details Converts low 16 bits of \\p half2 input \\p a to 32-bit floating-point number\n* and returns the result.\n* \\param[in] a - half2. Is only being read. \n* \n* \\returns float\n* - The low 16 bits of \\p a converted to float.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_HOSTDEVICE_FP16_DECL__ float __low2float(const __half2 a);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Converts high 16 bits of \\p half2 to float and returns the result\n* \n* \\details Converts high 16 bits of \\p half2 input \\p a to 32-bit floating-point number\n* and returns the result.\n* \\param[in] a - half2. Is only being read. \n* \n* \\returns float\n* - The high 16 bits of \\p a converted to float.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_HOSTDEVICE_FP16_DECL__ float __high2float(const __half2 a);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a half to a signed short integer in round-towards-zero mode.\n*\n* \\details Convert the half-precision floating-point value \\p h to a signed short\n* integer in round-towards-zero mode. NaN inputs are converted to 0.\n* \\param[in] h - half. Is only being read.\n*\n* \\returns short int\n* - \\p h converted to a signed short integer.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_HOSTDEVICE_FP16_DECL__ short int __half2short_rz(const __half h);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a half to an unsigned short integer in round-towards-zero\n* mode.\n*\n* \\details Convert the half-precision floating-point value \\p h to an unsigned short\n* integer in round-towards-zero mode. NaN inputs are converted to 0.\n* \\param[in] h - half. Is only being read.\n*\n* \\returns unsigned short int\n* - \\p h converted to an unsigned short integer.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_HOSTDEVICE_FP16_DECL__ unsigned short int __half2ushort_rz(const __half h);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a half to a signed integer in round-towards-zero mode.\n*\n* \\details Convert the half-precision floating-point value \\p h to a signed integer in\n* round-towards-zero mode. NaN inputs are converted to 0.\n* \\param[in] h - half. Is only being read.\n*\n* \\returns int\n* - \\p h converted to a signed integer.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_HOSTDEVICE_FP16_DECL__ int __half2int_rz(const __half h);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a half to an unsigned integer in round-towards-zero mode.\n*\n* \\details Convert the half-precision floating-point value \\p h to an unsigned integer\n* in round-towards-zero mode. NaN inputs are converted to 0.\n* \\param[in] h - half. Is only being read.\n*\n* \\returns unsigned int\n* - \\p h converted to an unsigned integer.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_HOSTDEVICE_FP16_DECL__ unsigned int __half2uint_rz(const __half h);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a half to a signed 64-bit integer in round-towards-zero mode.\n*\n* \\details Convert the half-precision floating-point value \\p h to a signed 64-bit\n* integer in round-towards-zero mode. NaN inputs return a long long int with hex value of 0x8000000000000000.\n* \\param[in] h - half. Is only being read.\n*\n* \\returns long long int\n* - \\p h converted to a signed 64-bit integer.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_HOSTDEVICE_FP16_DECL__ long long int __half2ll_rz(const __half h);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a half to an unsigned 64-bit integer in round-towards-zero\n* mode.\n*\n* \\details Convert the half-precision floating-point value \\p h to an unsigned 64-bit\n* integer in round-towards-zero mode. NaN inputs return 0x8000000000000000.\n* \\param[in] h - half. Is only being read.\n*\n* \\returns unsigned long long int\n* - \\p h converted to an unsigned 64-bit integer.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_HOSTDEVICE_FP16_DECL__ unsigned long long int __half2ull_rz(const __half h);\n\n#if defined(__CUDACC__)\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Converts both components of float2 number to half precision in\n* round-to-nearest-even mode and returns \\p half2 with converted values.\n* \n* \\details Converts both components of float2 to half precision in round-to-nearest\n* mode and combines the results into one \\p half2 number. Low 16 bits of the\n* return value correspond to \\p a.x and high 16 bits of the return value\n* correspond to \\p a.y.\n* \\param[in] a - float2. Is only being read. \n*  \n* \\returns half2\n* - The \\p half2 which has corresponding halves equal to the\n* converted float2 components.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_HOSTDEVICE_FP16_DECL__ __half2 __float22half2_rn(const float2 a);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Converts both halves of \\p half2 to float2 and returns the result.\n* \n* \\details Converts both halves of \\p half2 input \\p a to float2 and returns the\n* result.\n* \\param[in] a - half2. Is only being read. \n* \n* \\returns float2\n* - \\p a converted to float2.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_HOSTDEVICE_FP16_DECL__ float2 __half22float2(const __half2 a);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a half to a signed integer in round-to-nearest-even mode.\n* \n* \\details Convert the half-precision floating-point value \\p h to a signed integer in\n* round-to-nearest-even mode. NaN inputs are converted to 0.\n* \\param[in] h - half. Is only being read. \n* \n* \\returns int\n* - \\p h converted to a signed integer. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ int __half2int_rn(const __half h);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a half to a signed integer in round-down mode.\n* \n* \\details Convert the half-precision floating-point value \\p h to a signed integer in\n* round-down mode. NaN inputs are converted to 0.\n* \\param[in] h - half. Is only being read. \n* \n* \\returns int\n* - \\p h converted to a signed integer. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ int __half2int_rd(const __half h);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a half to a signed integer in round-up mode.\n* \n* \\details Convert the half-precision floating-point value \\p h to a signed integer in\n* round-up mode. NaN inputs are converted to 0.\n* \\param[in] h - half. Is only being read. \n* \n* \\returns int\n* - \\p h converted to a signed integer. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ int __half2int_ru(const __half h);\n\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a signed integer to a half in round-to-nearest-even mode.\n* \n* \\details Convert the signed integer value \\p i to a half-precision floating-point\n* value in round-to-nearest-even mode.\n* \\param[in] i - int. Is only being read. \n* \n* \\returns half\n* - \\p i converted to half. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_HOSTDEVICE_FP16_DECL__ __half __int2half_rn(const int i);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a signed integer to a half in round-towards-zero mode.\n* \n* \\details Convert the signed integer value \\p i to a half-precision floating-point\n* value in round-towards-zero mode.\n* \\param[in] i - int. Is only being read. \n* \n* \\returns half\n* - \\p i converted to half. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __int2half_rz(const int i);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a signed integer to a half in round-down mode.\n* \n* \\details Convert the signed integer value \\p i to a half-precision floating-point\n* value in round-down mode.\n* \\param[in] i - int. Is only being read. \n* \n* \\returns half\n* - \\p i converted to half. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __int2half_rd(const int i);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a signed integer to a half in round-up mode.\n* \n* \\details Convert the signed integer value \\p i to a half-precision floating-point\n* value in round-up mode.\n* \\param[in] i - int. Is only being read. \n* \n* \\returns half\n* - \\p i converted to half. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __int2half_ru(const int i);\n\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a half to a signed short integer in round-to-nearest-even\n* mode.\n* \n* \\details Convert the half-precision floating-point value \\p h to a signed short\n* integer in round-to-nearest-even mode. NaN inputs are converted to 0.\n* \\param[in] h - half. Is only being read. \n* \n* \\returns short int\n* - \\p h converted to a signed short integer. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ short int __half2short_rn(const __half h);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a half to a signed short integer in round-down mode.\n* \n* \\details Convert the half-precision floating-point value \\p h to a signed short\n* integer in round-down mode. NaN inputs are converted to 0.\n* \\param[in] h - half. Is only being read. \n* \n* \\returns short int\n* - \\p h converted to a signed short integer. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ short int __half2short_rd(const __half h);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a half to a signed short integer in round-up mode.\n* \n* \\details Convert the half-precision floating-point value \\p h to a signed short\n* integer in round-up mode. NaN inputs are converted to 0.\n* \\param[in] h - half. Is only being read. \n* \n* \\returns short int\n* - \\p h converted to a signed short integer. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ short int __half2short_ru(const __half h);\n\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a signed short integer to a half in round-to-nearest-even\n* mode.\n* \n* \\details Convert the signed short integer value \\p i to a half-precision floating-point\n* value in round-to-nearest-even mode.\n* \\param[in] i - short int. Is only being read. \n* \n* \\returns half\n* - \\p i converted to half. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_HOSTDEVICE_FP16_DECL__ __half __short2half_rn(const short int i);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a signed short integer to a half in round-towards-zero mode.\n* \n* \\details Convert the signed short integer value \\p i to a half-precision floating-point\n* value in round-towards-zero mode.\n* \\param[in] i - short int. Is only being read. \n* \n* \\returns half\n* - \\p i converted to half. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __short2half_rz(const short int i);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a signed short integer to a half in round-down mode.\n* \n* \\details Convert the signed short integer value \\p i to a half-precision floating-point\n* value in round-down mode.\n* \\param[in] i - short int. Is only being read. \n* \n* \\returns half\n* - \\p i converted to half. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __short2half_rd(const short int i);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a signed short integer to a half in round-up mode.\n* \n* \\details Convert the signed short integer value \\p i to a half-precision floating-point\n* value in round-up mode.\n* \\param[in] i - short int. Is only being read. \n* \n* \\returns half\n* - \\p i converted to half. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __short2half_ru(const short int i);\n\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a half to an unsigned integer in round-to-nearest-even mode.\n* \n* \\details Convert the half-precision floating-point value \\p h to an unsigned integer\n* in round-to-nearest-even mode. NaN inputs are converted to 0.\n* \\param[in] h - half. Is only being read. \n* \n* \\returns unsigned int\n* - \\p h converted to an unsigned integer. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ unsigned int __half2uint_rn(const __half h);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a half to an unsigned integer in round-down mode.\n*\n* \\details Convert the half-precision floating-point value \\p h to an unsigned integer\n* in round-down mode. NaN inputs are converted to 0.\n* \\param[in] h - half. Is only being read. \n*\n* \\returns unsigned int\n* - \\p h converted to an unsigned integer. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ unsigned int __half2uint_rd(const __half h);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a half to an unsigned integer in round-up mode.\n*\n* \\details Convert the half-precision floating-point value \\p h to an unsigned integer\n* in round-up mode. NaN inputs are converted to 0.\n* \\param[in] h - half. Is only being read. \n*\n* \\returns unsigned int\n* - \\p h converted to an unsigned integer. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ unsigned int __half2uint_ru(const __half h);\n\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert an unsigned integer to a half in round-to-nearest-even mode.\n* \n* \\details Convert the unsigned integer value \\p i to a half-precision floating-point\n* value in round-to-nearest-even mode.\n* \\param[in] i - unsigned int. Is only being read. \n* \n* \\returns half\n* - \\p i converted to half. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_HOSTDEVICE_FP16_DECL__ __half __uint2half_rn(const unsigned int i);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert an unsigned integer to a half in round-towards-zero mode.\n* \n* \\details Convert the unsigned integer value \\p i to a half-precision floating-point\n* value in round-towards-zero mode.\n* \\param[in] i - unsigned int. Is only being read. \n* \n* \\returns half\n* - \\p i converted to half.  \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __uint2half_rz(const unsigned int i);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert an unsigned integer to a half in round-down mode.\n* \n* \\details Convert the unsigned integer value \\p i to a half-precision floating-point\n* value in round-down mode.\n* \\param[in] i - unsigned int. Is only being read. \n* \n* \\returns half\n* - \\p i converted to half. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __uint2half_rd(const unsigned int i);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert an unsigned integer to a half in round-up mode.\n* \n* \\details Convert the unsigned integer value \\p i to a half-precision floating-point\n* value in round-up mode.\n* \\param[in] i - unsigned int. Is only being read. \n* \n* \\returns half\n* - \\p i converted to half. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __uint2half_ru(const unsigned int i);\n\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a half to an unsigned short integer in round-to-nearest-even\n* mode.\n* \n* \\details Convert the half-precision floating-point value \\p h to an unsigned short\n* integer in round-to-nearest-even mode. NaN inputs are converted to 0.\n* \\param[in] h - half. Is only being read. \n* \n* \\returns unsigned short int\n* - \\p h converted to an unsigned short integer. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ unsigned short int __half2ushort_rn(const __half h);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a half to an unsigned short integer in round-down mode.\n* \n* \\details Convert the half-precision floating-point value \\p h to an unsigned short\n* integer in round-down mode. NaN inputs are converted to 0.\n* \\param[in] h - half. Is only being read. \n* \n* \\returns unsigned short int\n* - \\p h converted to an unsigned short integer. \n*/\n__CUDA_FP16_DECL__ unsigned short int __half2ushort_rd(const __half h);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a half to an unsigned short integer in round-up mode.\n* \n* \\details Convert the half-precision floating-point value \\p h to an unsigned short\n* integer in round-up mode. NaN inputs are converted to 0.\n* \\param[in] h - half. Is only being read. \n* \n* \\returns unsigned short int\n* - \\p h converted to an unsigned short integer. \n*/\n__CUDA_FP16_DECL__ unsigned short int __half2ushort_ru(const __half h);\n\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert an unsigned short integer to a half in round-to-nearest-even\n* mode.\n* \n* \\details Convert the unsigned short integer value \\p i to a half-precision floating-point\n* value in round-to-nearest-even mode.\n* \\param[in] i - unsigned short int. Is only being read. \n* \n* \\returns half\n* - \\p i converted to half. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_HOSTDEVICE_FP16_DECL__ __half __ushort2half_rn(const unsigned short int i);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert an unsigned short integer to a half in round-towards-zero\n* mode.\n* \n* \\details Convert the unsigned short integer value \\p i to a half-precision floating-point\n* value in round-towards-zero mode.\n* \\param[in] i - unsigned short int. Is only being read. \n* \n* \\returns half\n* - \\p i converted to half. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __ushort2half_rz(const unsigned short int i);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert an unsigned short integer to a half in round-down mode.\n* \n* \\details Convert the unsigned short integer value \\p i to a half-precision floating-point\n* value in round-down mode.\n* \\param[in] i - unsigned short int. Is only being read. \n* \n* \\returns half\n* - \\p i converted to half. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __ushort2half_rd(const unsigned short int i);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert an unsigned short integer to a half in round-up mode.\n* \n* \\details Convert the unsigned short integer value \\p i to a half-precision floating-point\n* value in round-up mode.\n* \\param[in] i - unsigned short int. Is only being read. \n* \n* \\returns half\n* - \\p i converted to half. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __ushort2half_ru(const unsigned short int i);\n\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a half to an unsigned 64-bit integer in round-to-nearest-even\n* mode.\n* \n* \\details Convert the half-precision floating-point value \\p h to an unsigned 64-bit\n* integer in round-to-nearest-even mode. NaN inputs return 0x8000000000000000.\n* \\param[in] h - half. Is only being read. \n* \n* \\returns unsigned long long int\n* - \\p h converted to an unsigned 64-bit integer. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ unsigned long long int __half2ull_rn(const __half h);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a half to an unsigned 64-bit integer in round-down mode.\n* \n* \\details Convert the half-precision floating-point value \\p h to an unsigned 64-bit\n* integer in round-down mode. NaN inputs return 0x8000000000000000.\n* \\param[in] h - half. Is only being read. \n* \n* \\returns unsigned long long int\n* - \\p h converted to an unsigned 64-bit integer. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ unsigned long long int __half2ull_rd(const __half h);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a half to an unsigned 64-bit integer in round-up mode.\n* \n* \\details Convert the half-precision floating-point value \\p h to an unsigned 64-bit\n* integer in round-up mode. NaN inputs return 0x8000000000000000.\n* \\param[in] h - half. Is only being read. \n* \n* \\returns unsigned long long int\n* - \\p h converted to an unsigned 64-bit integer. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ unsigned long long int __half2ull_ru(const __half h);\n\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert an unsigned 64-bit integer to a half in round-to-nearest-even\n* mode.\n* \n* \\details Convert the unsigned 64-bit integer value \\p i to a half-precision floating-point\n* value in round-to-nearest-even mode.\n* \\param[in] i - unsigned long long int. Is only being read. \n* \n* \\returns half\n* - \\p i converted to half. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_HOSTDEVICE_FP16_DECL__ __half __ull2half_rn(const unsigned long long int i);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert an unsigned 64-bit integer to a half in round-towards-zero\n* mode.\n* \n* \\details Convert the unsigned 64-bit integer value \\p i to a half-precision floating-point\n* value in round-towards-zero mode.\n* \\param[in] i - unsigned long long int. Is only being read. \n* \n* \\returns half\n* - \\p i converted to half. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __ull2half_rz(const unsigned long long int i);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert an unsigned 64-bit integer to a half in round-down mode.\n* \n* \\details Convert the unsigned 64-bit integer value \\p i to a half-precision floating-point\n* value in round-down mode.\n* \\param[in] i - unsigned long long int. Is only being read. \n* \n* \\returns half\n* - \\p i converted to half.  \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __ull2half_rd(const unsigned long long int i);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert an unsigned 64-bit integer to a half in round-up mode.\n* \n* \\details Convert the unsigned 64-bit integer value \\p i to a half-precision floating-point\n* value in round-up mode.\n* \\param[in] i - unsigned long long int. Is only being read. \n* \n* \\returns half\n* - \\p i converted to half. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __ull2half_ru(const unsigned long long int i);\n\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a half to a signed 64-bit integer in round-to-nearest-even\n* mode.\n* \n* \\details Convert the half-precision floating-point value \\p h to a signed 64-bit\n* integer in round-to-nearest-even mode. NaN inputs return a long long int with hex value of 0x8000000000000000.\n* \\param[in] h - half. Is only being read. \n* \n* \\returns long long int\n* - \\p h converted to a signed 64-bit integer. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ long long int __half2ll_rn(const __half h);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a half to a signed 64-bit integer in round-down mode.\n* \n* \\details Convert the half-precision floating-point value \\p h to a signed 64-bit\n* integer in round-down mode. NaN inputs return a long long int with hex value of 0x8000000000000000.\n* \\param[in] h - half. Is only being read. \n* \n* \\returns long long int\n* - \\p h converted to a signed 64-bit integer. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ long long int __half2ll_rd(const __half h);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a half to a signed 64-bit integer in round-up mode.\n* \n* \\details Convert the half-precision floating-point value \\p h to a signed 64-bit\n* integer in round-up mode. NaN inputs return a long long int with hex value of 0x8000000000000000.\n* \\param[in] h - half. Is only being read. \n* \n* \\returns long long int\n* - \\p h converted to a signed 64-bit integer. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ long long int __half2ll_ru(const __half h);\n\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a signed 64-bit integer to a half in round-to-nearest-even\n* mode.\n* \n* \\details Convert the signed 64-bit integer value \\p i to a half-precision floating-point\n* value in round-to-nearest-even mode.\n* \\param[in] i - long long int. Is only being read. \n* \n* \\returns half\n* - \\p i converted to half. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_HOSTDEVICE_FP16_DECL__ __half __ll2half_rn(const long long int i);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a signed 64-bit integer to a half in round-towards-zero mode.\n* \n* \\details Convert the signed 64-bit integer value \\p i to a half-precision floating-point\n* value in round-towards-zero mode.\n* \\param[in] i - long long int. Is only being read. \n* \n* \\returns half\n* - \\p i converted to half. \n*/\n__CUDA_FP16_DECL__ __half __ll2half_rz(const long long int i);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a signed 64-bit integer to a half in round-down mode.\n* \n* \\details Convert the signed 64-bit integer value \\p i to a half-precision floating-point\n* value in round-down mode.\n* \\param[in] i - long long int. Is only being read. \n* \n* \\returns half\n* - \\p i converted to half. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __ll2half_rd(const long long int i);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Convert a signed 64-bit integer to a half in round-up mode.\n* \n* \\details Convert the signed 64-bit integer value \\p i to a half-precision floating-point\n* value in round-up mode.\n* \\param[in] i - long long int. Is only being read. \n* \n* \\returns half\n* - \\p i converted to half. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __ll2half_ru(const long long int i);\n\n/**\n* \\ingroup CUDA_MATH__HALF_FUNCTIONS\n* \\brief Truncate input argument to the integral part.\n* \n* \\details Round \\p h to the nearest integer value that does not exceed \\p h in\n* magnitude.\n* \\param[in] h - half. Is only being read. \n* \n* \\returns half\n* - The truncated integer value. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half htrunc(const __half h);\n/**\n* \\ingroup CUDA_MATH__HALF_FUNCTIONS\n* \\brief Calculate ceiling of the input argument.\n* \n* \\details Compute the smallest integer value not less than \\p h.\n* \\param[in] h - half. Is only being read. \n* \n* \\returns half\n* - The smallest integer value not less than \\p h. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half hceil(const __half h);\n/**\n* \\ingroup CUDA_MATH__HALF_FUNCTIONS\n* \\brief Calculate the largest integer less than or equal to \\p h.\n* \n* \\details Calculate the largest integer value which is less than or equal to \\p h.\n* \\param[in] h - half. Is only being read. \n* \n* \\returns half\n* - The largest integer value which is less than or equal to \\p h. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half hfloor(const __half h);\n/**\n* \\ingroup CUDA_MATH__HALF_FUNCTIONS\n* \\brief Round input to nearest integer value in half-precision floating-point\n* number.\n* \n* \\details Round \\p h to the nearest integer value in half-precision floating-point\n* format, with halfway cases rounded to the nearest even integer value.\n* \\param[in] h - half. Is only being read. \n* \n* \\returns half\n* - The nearest integer to \\p h. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half hrint(const __half h);\n\n/**\n* \\ingroup CUDA_MATH__HALF2_FUNCTIONS\n* \\brief Truncate \\p half2 vector input argument to the integral part.\n* \n* \\details Round each component of vector \\p h to the nearest integer value that does\n* not exceed \\p h in magnitude.\n* \\param[in] h - half2. Is only being read. \n* \n* \\returns half2\n* - The truncated \\p h. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 h2trunc(const __half2 h);\n/**\n* \\ingroup CUDA_MATH__HALF2_FUNCTIONS\n* \\brief Calculate \\p half2 vector ceiling of the input argument.\n* \n* \\details For each component of vector \\p h compute the smallest integer value not less\n* than \\p h.\n* \\param[in] h - half2. Is only being read. \n* \n* \\returns half2\n* - The vector of smallest integers not less than \\p h. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 h2ceil(const __half2 h);\n/**\n* \\ingroup CUDA_MATH__HALF2_FUNCTIONS\n* \\brief Calculate the largest integer less than or equal to \\p h.\n* \n* \\details For each component of vector \\p h calculate the largest integer value which\n* is less than or equal to \\p h.\n* \\param[in] h - half2. Is only being read. \n* \n* \\returns half2\n* - The vector of largest integers which is less than or equal to \\p h. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 h2floor(const __half2 h);\n/**\n* \\ingroup CUDA_MATH__HALF2_FUNCTIONS\n* \\brief Round input to nearest integer value in half-precision floating-point\n* number.\n* \n* \\details Round each component of \\p half2 vector \\p h to the nearest integer value in\n* half-precision floating-point format, with halfway cases rounded to the\n* nearest even integer value.\n* \\param[in] h - half2. Is only being read. \n* \n* \\returns half2\n* - The vector of rounded integer values. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 h2rint(const __half2 h);\n\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Returns \\p half2 with both halves equal to the input value.\n* \n* \\details Returns \\p half2 number with both halves equal to the input \\p a \\p half\n* number.\n* \\param[in] a - half. Is only being read. \n* \n* \\returns half2\n* - The vector which has both its halves equal to the input \\p a. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __half2half2(const __half a);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Swaps both halves of the \\p half2 input.\n* \n* \\details Swaps both halves of the \\p half2 input and returns a new \\p half2 number\n* with swapped halves.\n* \\param[in] a - half2. Is only being read. \n* \n* \\returns half2\n* - \\p a with its halves being swapped. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __lowhigh2highlow(const __half2 a);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Extracts low 16 bits from each of the two \\p half2 inputs and combines\n* into one \\p half2 number. \n* \n* \\details Extracts low 16 bits from each of the two \\p half2 inputs and combines into\n* one \\p half2 number. Low 16 bits from input \\p a is stored in low 16 bits of\n* the return value, low 16 bits from input \\p b is stored in high 16 bits of\n* the return value. \n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n* \n* \\returns half2\n* - The low 16 bits of \\p a and of \\p b. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __lows2half2(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Extracts high 16 bits from each of the two \\p half2 inputs and\n* combines into one \\p half2 number.\n* \n* \\details Extracts high 16 bits from each of the two \\p half2 inputs and combines into\n* one \\p half2 number. High 16 bits from input \\p a is stored in low 16 bits of\n* the return value, high 16 bits from input \\p b is stored in high 16 bits of\n* the return value.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n* \n* \\returns half2\n* - The high 16 bits of \\p a and of \\p b. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __highs2half2(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Returns high 16 bits of \\p half2 input.\n*\n* \\details Returns high 16 bits of \\p half2 input \\p a.\n* \\param[in] a - half2. Is only being read. \n*\n* \\returns half\n* - The high 16 bits of the input. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __high2half(const __half2 a);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Returns low 16 bits of \\p half2 input.\n*\n* \\details Returns low 16 bits of \\p half2 input \\p a.\n* \\param[in] a - half2. Is only being read. \n*\n* \\returns half\n* - Returns \\p half which contains low 16 bits of the input \\p a. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __low2half(const __half2 a);\n/**\n* \\ingroup CUDA_MATH__HALF_COMPARISON\n* \\brief Checks if the input \\p half number is infinite.\n* \n* \\details Checks if the input \\p half number \\p a is infinite. \n* \\param[in] a - half. Is only being read. \n* \n* \\returns int \n* - -1 iff \\p a is equal to negative infinity, \n* - 1 iff \\p a is equal to positive infinity, \n* - 0 otherwise. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ int __hisinf(const __half a);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Combines two \\p half numbers into one \\p half2 number.\n* \n* \\details Combines two input \\p half number \\p a and \\p b into one \\p half2 number.\n* Input \\p a is stored in low 16 bits of the return value, input \\p b is stored\n* in high 16 bits of the return value.\n* \\param[in] a - half. Is only being read. \n* \\param[in] b - half. Is only being read. \n* \n* \\returns half2\n* - The half2 with one half equal to \\p a and the other to \\p b. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __halves2half2(const __half a, const __half b);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Extracts low 16 bits from \\p half2 input.\n* \n* \\details Extracts low 16 bits from \\p half2 input \\p a and returns a new \\p half2\n* number which has both halves equal to the extracted bits.\n* \\param[in] a - half2. Is only being read. \n* \n* \\returns half2\n* - The half2 with both halves equal to the low 16 bits of the input. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __low2half2(const __half2 a);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Extracts high 16 bits from \\p half2 input.\n* \n* \\details Extracts high 16 bits from \\p half2 input \\p a and returns a new \\p half2\n* number which has both halves equal to the extracted bits.\n* \\param[in] a - half2. Is only being read. \n* \n* \\returns half2\n* - The half2 with both halves equal to the high 16 bits of the input. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __high2half2(const __half2 a);\n\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Reinterprets bits in a \\p half as a signed short integer.\n* \n* \\details Reinterprets the bits in the half-precision floating-point number \\p h\n* as a signed short integer. \n* \\param[in] h - half. Is only being read. \n* \n* \\returns short int\n* - The reinterpreted value. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ short int __half_as_short(const __half h);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Reinterprets bits in a \\p half as an unsigned short integer.\n* \n* \\details Reinterprets the bits in the half-precision floating-point \\p h\n* as an unsigned short number.\n* \\param[in] h - half. Is only being read. \n* \n* \\returns unsigned short int\n* - The reinterpreted value.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ unsigned short int __half_as_ushort(const __half h);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Reinterprets bits in a signed short integer as a \\p half.\n* \n* \\details Reinterprets the bits in the signed short integer \\p i as a\n* half-precision floating-point number.\n* \\param[in] i - short int. Is only being read. \n* \n* \\returns half\n* - The reinterpreted value.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __short_as_half(const short int i);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Reinterprets bits in an unsigned short integer as a \\p half.\n* \n* \\details Reinterprets the bits in the unsigned short integer \\p i as a\n* half-precision floating-point number.\n* \\param[in] i - unsigned short int. Is only being read. \n* \n* \\returns half\n* - The reinterpreted value.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __ushort_as_half(const unsigned short int i);\n/**\n* \\ingroup CUDA_MATH__HALF_COMPARISON\n* \\brief Calculates \\p half maximum of two input values.\n*\n* \\details Calculates \\p half max(\\p a, \\p b)\n* defined as (\\p a > \\p b) ? \\p a : \\p b.\n* - If either of inputs is NaN, the other input is returned.\n* - If both inputs are NaNs, then canonical NaN is returned.\n* - If values of both inputs are 0.0, then +0.0 > -0.0\n* \\param[in] a - half. Is only being read.\n* \\param[in] b - half. Is only being read.\n*\n* \\returns half\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __hmax(const __half a, const __half b);\n/**\n* \\ingroup CUDA_MATH__HALF_COMPARISON\n* \\brief Calculates \\p half minimum of two input values.\n*\n* \\details Calculates \\p half min(\\p a, \\p b)\n* defined as (\\p a < \\p b) ? \\p a : \\p b.\n* - If either of inputs is NaN, the other input is returned.\n* - If both inputs are NaNs, then canonical NaN is returned.\n* - If values of both inputs are 0.0, then +0.0 > -0.0\n* \\param[in] a - half. Is only being read.\n* \\param[in] b - half. Is only being read.\n*\n* \\returns half\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __hmin(const __half a, const __half b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Calculates \\p half2 vector maximum of two inputs.\n*\n* \\details Calculates \\p half2 vector max(\\p a, \\p b).\n* Elementwise \\p half operation is defined as\n* (\\p a > \\p b) ? \\p a : \\p b.\n* - If either of inputs is NaN, the other input is returned.\n* - If both inputs are NaNs, then canonical NaN is returned.\n* - If values of both inputs are 0.0, then +0.0 > -0.0\n* \\param[in] a - half2. Is only being read.\n* \\param[in] b - half2. Is only being read.\n*\n* \\returns half2\n* - The result of elementwise maximum of vectors \\p a  and \\p b\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __hmax2(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Calculates \\p half2 vector minimum of two inputs.\n*\n* \\details Calculates \\p half2 vector min(\\p a, \\p b).\n* Elementwise \\p half operation is defined as\n* (\\p a < \\p b) ? \\p a : \\p b.\n* - If either of inputs is NaN, the other input is returned.\n* - If both inputs are NaNs, then canonical NaN is returned.\n* - If values of both inputs are 0.0, then +0.0 > -0.0\n* \\param[in] a - half2. Is only being read.\n* \\param[in] b - half2. Is only being read.\n*\n* \\returns half2\n* - The result of elementwise minimum of vectors \\p a  and \\p b\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __hmin2(const __half2 a, const __half2 b);\n\n#if !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 300)\n#if !defined warpSize && !defined __local_warpSize\n#define warpSize    32\n#define __local_warpSize\n#endif\n\n#if defined(_WIN32)\n# define __DEPRECATED__(msg) __declspec(deprecated(msg))\n#elif (defined(__GNUC__) && (__GNUC__ < 4 || (__GNUC__ == 4 && __GNUC_MINOR__ < 5 && !defined(__clang__))))\n# define __DEPRECATED__(msg) __attribute__((deprecated))\n#else\n# define __DEPRECATED__(msg) __attribute__((deprecated(msg)))\n#endif\n\n#if !defined(__CUDA_ARCH__) || __CUDA_ARCH__ < 700\n#define __WSB_DEPRECATION_MESSAGE(x) __CUDA_FP16_STRINGIFY(x) \"() is deprecated in favor of \" __CUDA_FP16_STRINGIFY(x) \"_sync() and may be removed in a future release (Use -Wno-deprecated-declarations to suppress this warning).\"\n\n__CUDA_FP16_DECL__ __DEPRECATED__(__WSB_DEPRECATION_MESSAGE(__shfl)) __half2 __shfl(const __half2 var, const int delta, const int width = warpSize);\n__CUDA_FP16_DECL__ __DEPRECATED__(__WSB_DEPRECATION_MESSAGE(__shfl_up)) __half2 __shfl_up(const __half2 var, const unsigned int delta, const int width = warpSize);\n__CUDA_FP16_DECL__ __DEPRECATED__(__WSB_DEPRECATION_MESSAGE(__shfl_down))__half2 __shfl_down(const __half2 var, const unsigned int delta, const int width = warpSize);\n__CUDA_FP16_DECL__ __DEPRECATED__(__WSB_DEPRECATION_MESSAGE(__shfl_xor)) __half2 __shfl_xor(const __half2 var, const int delta, const int width = warpSize);\n__CUDA_FP16_DECL__ __DEPRECATED__(__WSB_DEPRECATION_MESSAGE(__shfl)) __half __shfl(const __half var, const int delta, const int width = warpSize);\n__CUDA_FP16_DECL__ __DEPRECATED__(__WSB_DEPRECATION_MESSAGE(__shfl_up)) __half __shfl_up(const __half var, const unsigned int delta, const int width = warpSize);\n__CUDA_FP16_DECL__ __DEPRECATED__(__WSB_DEPRECATION_MESSAGE(__shfl_down)) __half __shfl_down(const __half var, const unsigned int delta, const int width = warpSize);\n__CUDA_FP16_DECL__ __DEPRECATED__(__WSB_DEPRECATION_MESSAGE(__shfl_xor)) __half __shfl_xor(const __half var, const int delta, const int width = warpSize);\n#endif\n\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Exchange a variable between threads within a warp. Direct copy from indexed thread. \n* \n* \\details Returns the value of var held by the thread whose ID is given by delta. \n* If width is less than warpSize then each subsection of the warp behaves as a separate \n* entity with a starting logical thread ID of 0. If delta is outside the range [0:width-1], \n* the value returned corresponds to the value of var held by the delta modulo width (i.e. \n* within the same subsection). width must have a value which is a power of 2; \n* results are undefined if width is not a power of 2, or is a number greater than \n* warpSize. \n* \\param[in] mask - unsigned int. Is only being read. \n* \\param[in] var - half2. Is only being read. \n* \\param[in] delta - int. Is only being read. \n* \\param[in] width - int. Is only being read. \n* \n* \\returns Returns the 4-byte word referenced by var from the source thread ID as half2. \n* If the source thread ID is out of range or the source thread has exited, the calling thread's own var is returned. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior not reentrant, not thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __shfl_sync(const unsigned mask, const __half2 var, const int delta, const int width = warpSize);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Exchange a variable between threads within a warp. Copy from a thread with lower ID relative to the caller. \n* \n* \\details Calculates a source thread ID by subtracting delta from the caller's lane ID. \n* The value of var held by the resulting lane ID is returned: in effect, var is shifted up \n* the warp by delta threads. If width is less than warpSize then each subsection of the warp \n* behaves as a separate entity with a starting logical thread ID of 0. The source thread index \n* will not wrap around the value of width, so effectively the lower delta threads will be unchanged. \n* width must have a value which is a power of 2; results are undefined if width is not a power of 2, \n* or is a number greater than warpSize. \n* \\param[in] mask - unsigned int. Is only being read. \n* \\param[in] var - half2. Is only being read. \n* \\param[in] delta - int. Is only being read. \n* \\param[in] width - int. Is only being read. \n* \n* \\returns Returns the 4-byte word referenced by var from the source thread ID as half2. \n* If the source thread ID is out of range or the source thread has exited, the calling thread's own var is returned. \n* \\note_ref_guide_warp_shuffle\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior not reentrant, not thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __shfl_up_sync(const unsigned mask, const __half2 var, const unsigned int delta, const int width = warpSize);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Exchange a variable between threads within a warp. Copy from a thread with higher ID relative to the caller. \n* \n* \\details Calculates a source thread ID by adding delta to the caller's thread ID. \n* The value of var held by the resulting thread ID is returned: this has the effect \n* of shifting var down the warp by delta threads. If width is less than warpSize then \n* each subsection of the warp behaves as a separate entity with a starting logical \n* thread ID of 0. As for __shfl_up_sync(), the ID number of the source thread \n* will not wrap around the value of width and so the upper delta threads \n* will remain unchanged. \n* \\param[in] mask - unsigned int. Is only being read. \n* \\param[in] var - half2. Is only being read. \n* \\param[in] delta - int. Is only being read. \n* \\param[in] width - int. Is only being read. \n* \n* \\returns Returns the 4-byte word referenced by var from the source thread ID as half2. \n* If the source thread ID is out of range or the source thread has exited, the calling thread's own var is returned. \n* \\note_ref_guide_warp_shuffle\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior not reentrant, not thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __shfl_down_sync(const unsigned mask, const __half2 var, const unsigned int delta, const int width = warpSize);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Exchange a variable between threads within a warp. Copy from a thread based on bitwise XOR of own thread ID. \n* \n* \\details Calculates a source thread ID by performing a bitwise XOR of the caller's thread ID with mask: \n* the value of var held by the resulting thread ID is returned. If width is less than warpSize then each \n* group of width consecutive threads are able to access elements from earlier groups of threads, \n* however if they attempt to access elements from later groups of threads their own value of var \n* will be returned. This mode implements a butterfly addressing pattern such as is used in tree \n* reduction and broadcast. \n* \\param[in] mask - unsigned int. Is only being read. \n* \\param[in] var - half2. Is only being read. \n* \\param[in] delta - int. Is only being read. \n* \\param[in] width - int. Is only being read. \n* \n* \\returns Returns the 4-byte word referenced by var from the source thread ID as half2. \n* If the source thread ID is out of range or the source thread has exited, the calling thread's own var is returned. \n* \\note_ref_guide_warp_shuffle\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior not reentrant, not thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __shfl_xor_sync(const unsigned mask, const __half2 var, const int delta, const int width = warpSize);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Exchange a variable between threads within a warp. Direct copy from indexed thread. \n* \n* \\details Returns the value of var held by the thread whose ID is given by delta. \n* If width is less than warpSize then each subsection of the warp behaves as a separate \n* entity with a starting logical thread ID of 0. If delta is outside the range [0:width-1], \n* the value returned corresponds to the value of var held by the delta modulo width (i.e. \n* within the same subsection). width must have a value which is a power of 2; \n* results are undefined if width is not a power of 2, or is a number greater than \n* warpSize. \n* \\param[in] mask - unsigned int. Is only being read. \n* \\param[in] var - half. Is only being read. \n* \\param[in] delta - int. Is only being read. \n* \\param[in] width - int. Is only being read. \n* \n* \\returns Returns the 2-byte word referenced by var from the source thread ID as half. \n* If the source thread ID is out of range or the source thread has exited, the calling thread's own var is returned. \n* \\note_ref_guide_warp_shuffle\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior not reentrant, not thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __shfl_sync(const unsigned mask, const __half var, const int delta, const int width = warpSize);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Exchange a variable between threads within a warp. Copy from a thread with lower ID relative to the caller. \n* \\details Calculates a source thread ID by subtracting delta from the caller's lane ID. \n* The value of var held by the resulting lane ID is returned: in effect, var is shifted up \n* the warp by delta threads. If width is less than warpSize then each subsection of the warp \n* behaves as a separate entity with a starting logical thread ID of 0. The source thread index \n* will not wrap around the value of width, so effectively the lower delta threads will be unchanged. \n* width must have a value which is a power of 2; results are undefined if width is not a power of 2, \n* or is a number greater than warpSize. \n* \\param[in] mask - unsigned int. Is only being read. \n* \\param[in] var - half. Is only being read. \n* \\param[in] delta - int. Is only being read. \n* \\param[in] width - int. Is only being read. \n* \n* \\returns Returns the 2-byte word referenced by var from the source thread ID as half. \n* If the source thread ID is out of range or the source thread has exited, the calling thread's own var is returned. \n* \\note_ref_guide_warp_shuffle\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior not reentrant, not thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __shfl_up_sync(const unsigned mask, const __half var, const unsigned int delta, const int width = warpSize);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Exchange a variable between threads within a warp. Copy from a thread with higher ID relative to the caller. \n* \n* \\details Calculates a source thread ID by adding delta to the caller's thread ID. \n* The value of var held by the resulting thread ID is returned: this has the effect \n* of shifting var down the warp by delta threads. If width is less than warpSize then \n* each subsection of the warp behaves as a separate entity with a starting logical \n* thread ID of 0. As for __shfl_up_sync(), the ID number of the source thread \n* will not wrap around the value of width and so the upper delta threads \n* will remain unchanged. \n* \\param[in] mask - unsigned int. Is only being read. \n* \\param[in] var - half. Is only being read. \n* \\param[in] delta - int. Is only being read. \n* \\param[in] width - int. Is only being read. \n* \n* \\returns Returns the 2-byte word referenced by var from the source thread ID as half. \n* If the source thread ID is out of range or the source thread has exited, the calling thread's own var is returned. \n* \\note_ref_guide_warp_shuffle\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior not reentrant, not thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __shfl_down_sync(const unsigned mask, const __half var, const unsigned int delta, const int width = warpSize);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Exchange a variable between threads within a warp. Copy from a thread based on bitwise XOR of own thread ID. \n* \n* \\details Calculates a source thread ID by performing a bitwise XOR of the caller's thread ID with mask: \n* the value of var held by the resulting thread ID is returned. If width is less than warpSize then each \n* group of width consecutive threads are able to access elements from earlier groups of threads, \n* however if they attempt to access elements from later groups of threads their own value of var \n* will be returned. This mode implements a butterfly addressing pattern such as is used in tree \n* reduction and broadcast. \n* \\param[in] mask - unsigned int. Is only being read. \n* \\param[in] var - half. Is only being read. \n* \\param[in] delta - int. Is only being read. \n* \\param[in] width - int. Is only being read. \n* \n* \\returns Returns the 2-byte word referenced by var from the source thread ID as half. \n* If the source thread ID is out of range or the source thread has exited, the calling thread's own var is returned. \n* \\note_ref_guide_warp_shuffle\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior not reentrant, not thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __shfl_xor_sync(const unsigned mask, const __half var, const int delta, const int width = warpSize);\n\n#if defined(__local_warpSize)\n#undef warpSize\n#undef __local_warpSize\n#endif\n#endif /*!defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 300) */\n\n#if defined(__cplusplus) && ( !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 320) )\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Generates a `ld.global.nc` load instruction.\n* \\param[in] ptr - memory location\n* \\returns The value pointed by `ptr`\n*/\n__CUDA_FP16_DECL__ __half2 __ldg(const  __half2 *const ptr);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Generates a `ld.global.nc` load instruction.\n* \\param[in] ptr - memory location\n* \\returns The value pointed by `ptr`\n*/\n__CUDA_FP16_DECL__ __half __ldg(const __half *const ptr);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Generates a `ld.global.cg` load instruction.\n* \\param[in] ptr - memory location\n* \\returns The value pointed by `ptr`\n*/\n__CUDA_FP16_DECL__ __half2 __ldcg(const  __half2 *const ptr);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Generates a `ld.global.cg` load instruction.\n* \\param[in] ptr - memory location\n* \\returns The value pointed by `ptr`\n*/\n__CUDA_FP16_DECL__ __half __ldcg(const __half *const ptr);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Generates a `ld.global.ca` load instruction.\n* \\param[in] ptr - memory location\n* \\returns The value pointed by `ptr`\n*/\n__CUDA_FP16_DECL__ __half2 __ldca(const  __half2 *const ptr);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Generates a `ld.global.ca` load instruction.\n* \\param[in] ptr - memory location\n* \\returns The value pointed by `ptr`\n*/\n__CUDA_FP16_DECL__ __half __ldca(const __half *const ptr);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Generates a `ld.global.cs` load instruction.\n* \\param[in] ptr - memory location\n* \\returns The value pointed by `ptr`\n*/\n__CUDA_FP16_DECL__ __half2 __ldcs(const  __half2 *const ptr);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Generates a `ld.global.cs` load instruction.\n* \\param[in] ptr - memory location\n* \\returns The value pointed by `ptr`\n*/\n__CUDA_FP16_DECL__ __half __ldcs(const __half *const ptr);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Generates a `ld.global.lu` load instruction.\n* \\param[in] ptr - memory location\n* \\returns The value pointed by `ptr`\n*/\n__CUDA_FP16_DECL__ __half2 __ldlu(const  __half2 *const ptr);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Generates a `ld.global.lu` load instruction.\n* \\param[in] ptr - memory location\n* \\returns The value pointed by `ptr`\n*/\n__CUDA_FP16_DECL__ __half __ldlu(const __half *const ptr);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Generates a `ld.global.cv` load instruction.\n* \\param[in] ptr - memory location\n* \\returns The value pointed by `ptr`\n*/\n__CUDA_FP16_DECL__ __half2 __ldcv(const  __half2 *const ptr);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Generates a `ld.global.cv` load instruction.\n* \\param[in] ptr - memory location\n* \\returns The value pointed by `ptr`\n*/\n__CUDA_FP16_DECL__ __half __ldcv(const __half *const ptr);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Generates a `st.global.wb` store instruction.\n* \\param[out] ptr - memory location\n* \\param[in] value - the value to be stored\n*/\n__CUDA_FP16_DECL__ void __stwb(__half2 *const ptr, const __half2 value);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Generates a `st.global.wb` store instruction.\n* \\param[out] ptr - memory location\n* \\param[in] value - the value to be stored\n*/\n__CUDA_FP16_DECL__ void __stwb(__half *const ptr, const __half value);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Generates a `st.global.cg` store instruction.\n* \\param[out] ptr - memory location\n* \\param[in] value - the value to be stored\n*/\n__CUDA_FP16_DECL__ void __stcg(__half2 *const ptr, const __half2 value);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Generates a `st.global.cg` store instruction.\n* \\param[out] ptr - memory location\n* \\param[in] value - the value to be stored\n*/\n__CUDA_FP16_DECL__ void __stcg(__half *const ptr, const __half value);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Generates a `st.global.cs` store instruction.\n* \\param[out] ptr - memory location\n* \\param[in] value - the value to be stored\n*/\n__CUDA_FP16_DECL__ void __stcs(__half2 *const ptr, const __half2 value);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Generates a `st.global.cs` store instruction.\n* \\param[out] ptr - memory location\n* \\param[in] value - the value to be stored\n*/\n__CUDA_FP16_DECL__ void __stcs(__half *const ptr, const __half value);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Generates a `st.global.wt` store instruction.\n* \\param[out] ptr - memory location\n* \\param[in] value - the value to be stored\n*/\n__CUDA_FP16_DECL__ void __stwt(__half2 *const ptr, const __half2 value);\n/**\n* \\ingroup CUDA_MATH__HALF_MISC\n* \\brief Generates a `st.global.wt` store instruction.\n* \\param[out] ptr - memory location\n* \\param[in] value - the value to be stored\n*/\n__CUDA_FP16_DECL__ void __stwt(__half *const ptr, const __half value);\n#endif /*defined(__cplusplus) && ( !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 320) )*/\n\n#if !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 530)\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs half2 vector if-equal comparison.\n* \n* \\details Performs \\p half2 vector if-equal comparison of inputs \\p a and \\p b.\n* The corresponding \\p half results are set to 1.0 for true, or 0.0 for false.\n* NaN inputs generate false results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n* \n* \\returns half2\n* - The vector result of if-equal comparison of vectors \\p a and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __heq2(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector not-equal comparison.\n* \n* \\details Performs \\p half2 vector not-equal comparison of inputs \\p a and \\p b.\n* The corresponding \\p half results are set to 1.0 for true, or 0.0 for false.\n* NaN inputs generate false results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n* \n* \\returns half2\n* - The vector result of not-equal comparison of vectors \\p a and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __hne2(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector less-equal comparison.\n*\n* \\details Performs \\p half2 vector less-equal comparison of inputs \\p a and \\p b.\n* The corresponding \\p half results are set to 1.0 for true, or 0.0 for false.\n* NaN inputs generate false results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns half2\n* - The \\p half2 result of less-equal comparison of vectors \\p a and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __hle2(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector greater-equal comparison.\n*\n* \\details Performs \\p half2 vector greater-equal comparison of inputs \\p a and \\p b.\n* The corresponding \\p half results are set to 1.0 for true, or 0.0 for false.\n* NaN inputs generate false results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns half2\n* - The vector result of greater-equal comparison of vectors \\p a and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __hge2(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector less-than comparison.\n*\n* \\details Performs \\p half2 vector less-than comparison of inputs \\p a and \\p b.\n* The corresponding \\p half results are set to 1.0 for true, or 0.0 for false.\n* NaN inputs generate false results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns half2\n* - The half2 vector result of less-than comparison of vectors \\p a and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __hlt2(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector greater-than comparison.\n* \n* \\details Performs \\p half2 vector greater-than comparison of inputs \\p a and \\p b.\n* The corresponding \\p half results are set to 1.0 for true, or 0.0 for false.\n* NaN inputs generate false results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n* \n* \\returns half2\n* - The vector result of greater-than comparison of vectors \\p a and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __hgt2(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector unordered if-equal comparison.\n* \n* \\details Performs \\p half2 vector if-equal comparison of inputs \\p a and \\p b.\n* The corresponding \\p half results are set to 1.0 for true, or 0.0 for false.\n* NaN inputs generate true results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n* \n* \\returns half2\n* - The vector result of unordered if-equal comparison of vectors \\p a and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __hequ2(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector unordered not-equal comparison.\n*\n* \\details Performs \\p half2 vector not-equal comparison of inputs \\p a and \\p b.\n* The corresponding \\p half results are set to 1.0 for true, or 0.0 for false.\n* NaN inputs generate true results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns half2\n* - The vector result of unordered not-equal comparison of vectors \\p a and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __hneu2(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector unordered less-equal comparison.\n*\n* Performs \\p half2 vector less-equal comparison of inputs \\p a and \\p b.\n* The corresponding \\p half results are set to 1.0 for true, or 0.0 for false.\n* NaN inputs generate true results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns half2\n* - The vector result of unordered less-equal comparison of vectors \\p a and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __hleu2(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector unordered greater-equal comparison.\n*\n* \\details Performs \\p half2 vector greater-equal comparison of inputs \\p a and \\p b.\n* The corresponding \\p half results are set to 1.0 for true, or 0.0 for false.\n* NaN inputs generate true results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns half2\n* - The \\p half2 vector result of unordered greater-equal comparison of vectors \\p a and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __hgeu2(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector unordered less-than comparison.\n*\n* \\details Performs \\p half2 vector less-than comparison of inputs \\p a and \\p b.\n* The corresponding \\p half results are set to 1.0 for true, or 0.0 for false.\n* NaN inputs generate true results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns half2\n* - The vector result of unordered less-than comparison of vectors \\p a and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __hltu2(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector unordered greater-than comparison.\n*\n* \\details Performs \\p half2 vector greater-than comparison of inputs \\p a and \\p b.\n* The corresponding \\p half results are set to 1.0 for true, or 0.0 for false.\n* NaN inputs generate true results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns half2\n* - The \\p half2 vector result of unordered greater-than comparison of vectors \\p a and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __hgtu2(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs half2 vector if-equal comparison.\n* \n* \\details Performs \\p half2 vector if-equal comparison of inputs \\p a and \\p b.\n* The corresponding \\p unsigned bits are set to 0xFFFF for true, or 0x0 for false.\n* NaN inputs generate false results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n* \n* \\returns unsigned int\n* - The vector mask result of if-equal comparison of vectors \\p a and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ unsigned __heq2_mask(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector not-equal comparison.\n* \n* \\details Performs \\p half2 vector not-equal comparison of inputs \\p a and \\p b.\n* The corresponding \\p unsigned bits are set to 0xFFFF for true, or 0x0 for false.\n* NaN inputs generate false results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n* \n* \\returns unsigned int\n* - The vector mask result of not-equal comparison of vectors \\p a and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ unsigned __hne2_mask(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector less-equal comparison.\n*\n* \\details Performs \\p half2 vector less-equal comparison of inputs \\p a and \\p b.\n* The corresponding \\p unsigned bits are set to 0xFFFF for true, or 0x0 for false.\n* NaN inputs generate false results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns unsigned int\n* - The vector mask result of less-equal comparison of vectors \\p a and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ unsigned __hle2_mask(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector greater-equal comparison.\n*\n* \\details Performs \\p half2 vector greater-equal comparison of inputs \\p a and \\p b.\n* The corresponding \\p unsigned bits are set to 0xFFFF for true, or 0x0 for false.\n* NaN inputs generate false results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns unsigned int\n* - The vector mask result of greater-equal comparison of vectors \\p a and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ unsigned __hge2_mask(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector less-than comparison.\n*\n* \\details Performs \\p half2 vector less-than comparison of inputs \\p a and \\p b.\n* The corresponding \\p unsigned bits are set to 0xFFFF for true, or 0x0 for false.\n* NaN inputs generate false results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns unsigned int\n* - The vector mask result of less-than comparison of vectors \\p a and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ unsigned __hlt2_mask(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector greater-than comparison.\n* \n* \\details Performs \\p half2 vector greater-than comparison of inputs \\p a and \\p b.\n* The corresponding \\p unsigned bits are set to 0xFFFF for true, or 0x0 for false.\n* NaN inputs generate false results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n* \n* \\returns unsigned int\n* - The vector mask result of greater-than comparison of vectors \\p a and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ unsigned __hgt2_mask(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector unordered if-equal comparison.\n* \n* \\details Performs \\p half2 vector if-equal comparison of inputs \\p a and \\p b.\n* The corresponding \\p unsigned bits are set to 0xFFFF for true, or 0x0 for false.\n* NaN inputs generate true results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n* \n* \\returns unsigned int\n* - The vector mask result of unordered if-equal comparison of vectors \\p a and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ unsigned __hequ2_mask(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector unordered not-equal comparison.\n*\n* \\details Performs \\p half2 vector not-equal comparison of inputs \\p a and \\p b.\n* The corresponding \\p unsigned bits are set to 0xFFFF for true, or 0x0 for false.\n* NaN inputs generate true results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns unsigned int\n* - The vector mask result of unordered not-equal comparison of vectors \\p a and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ unsigned __hneu2_mask(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector unordered less-equal comparison.\n*\n* Performs \\p half2 vector less-equal comparison of inputs \\p a and \\p b.\n* The corresponding \\p unsigned bits are set to 0xFFFF for true, or 0x0 for false.\n* NaN inputs generate true results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns unsigned int\n* - The vector mask result of unordered less-equal comparison of vectors \\p a and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ unsigned __hleu2_mask(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector unordered greater-equal comparison.\n*\n* \\details Performs \\p half2 vector greater-equal comparison of inputs \\p a and \\p b.\n* The corresponding \\p unsigned bits are set to 0xFFFF for true, or 0x0 for false.\n* NaN inputs generate true results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns unsigned int\n* - The vector mask result of unordered greater-equal comparison of vectors \\p a and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ unsigned __hgeu2_mask(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector unordered less-than comparison.\n*\n* \\details Performs \\p half2 vector less-than comparison of inputs \\p a and \\p b.\n* The corresponding \\p unsigned bits are set to 0xFFFF for true, or 0x0 for false.\n* NaN inputs generate true results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns unsigned int\n* - The vector mask result of unordered less-than comparison of vectors \\p a and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ unsigned __hltu2_mask(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector unordered greater-than comparison.\n*\n* \\details Performs \\p half2 vector greater-than comparison of inputs \\p a and \\p b.\n* The corresponding \\p unsigned bits are set to 0xFFFF for true, or 0x0 for false.\n* NaN inputs generate true results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns unsigned int\n* - The vector mask result of unordered greater-than comparison of vectors \\p a and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ unsigned __hgtu2_mask(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Determine whether \\p half2 argument is a NaN.\n*\n* \\details Determine whether each half of input \\p half2 number \\p a is a NaN.\n* \\param[in] a - half2. Is only being read. \n*\n* \\returns half2\n* - The half2 with the corresponding \\p half results set to\n* 1.0 for NaN, 0.0 otherwise. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __hisnan2(const __half2 a);\n/**\n* \\ingroup CUDA_MATH__HALF2_ARITHMETIC\n* \\brief Performs \\p half2 vector addition in round-to-nearest-even mode.\n*\n* \\details Performs \\p half2 vector add of inputs \\p a and \\p b, in round-to-nearest\n* mode.\n* \\internal\n* \\req DEEPLEARN-SRM_REQ-95\n* \\endinternal\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns half2\n* - The sum of vectors \\p a and \\p b. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __hadd2(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_ARITHMETIC\n* \\brief Performs \\p half2 vector subtraction in round-to-nearest-even mode.\n*\n* \\details Subtracts \\p half2 input vector \\p b from input vector \\p a in\n* round-to-nearest-even mode.\n* \\internal\n* \\req DEEPLEARN-SRM_REQ-104\n* \\endinternal\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns half2\n* - The subtraction of vector \\p b from \\p a. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __hsub2(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_ARITHMETIC\n* \\brief Performs \\p half2 vector multiplication in round-to-nearest-even mode.\n*\n* \\details Performs \\p half2 vector multiplication of inputs \\p a and \\p b, in\n* round-to-nearest-even mode.\n* \\internal\n* \\req DEEPLEARN-SRM_REQ-102\n* \\endinternal\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns half2\n* - The result of elementwise multiplying the vectors \\p a and \\p b. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __hmul2(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_ARITHMETIC\n* \\brief Performs \\p half2 vector addition in round-to-nearest-even mode.\n*\n* \\details Performs \\p half2 vector add of inputs \\p a and \\p b, in round-to-nearest\n* mode. Prevents floating-point contractions of mul+add into fma.\n* \\internal\n* \\req DEEPLEARN-SRM_REQ-95\n* \\endinternal\n* \\param[in] a - half2. Is only being read.\n* \\param[in] b - half2. Is only being read.\n*\n* \\returns half2\n* - The sum of vectors \\p a and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __hadd2_rn(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_ARITHMETIC\n* \\brief Performs \\p half2 vector subtraction in round-to-nearest-even mode.\n*\n* \\details Subtracts \\p half2 input vector \\p b from input vector \\p a in\n* round-to-nearest-even mode. Prevents floating-point contractions of mul+sub\n* into fma.\n* \\internal\n* \\req DEEPLEARN-SRM_REQ-104\n* \\endinternal\n* \\param[in] a - half2. Is only being read.\n* \\param[in] b - half2. Is only being read.\n*\n* \\returns half2\n* - The subtraction of vector \\p b from \\p a.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __hsub2_rn(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_ARITHMETIC\n* \\brief Performs \\p half2 vector multiplication in round-to-nearest-even mode.\n*\n* \\details Performs \\p half2 vector multiplication of inputs \\p a and \\p b, in\n* round-to-nearest-even mode. Prevents floating-point contractions of\n* mul+add or sub into fma.\n* \\internal\n* \\req DEEPLEARN-SRM_REQ-102\n* \\endinternal\n* \\param[in] a - half2. Is only being read.\n* \\param[in] b - half2. Is only being read.\n*\n* \\returns half2\n* - The result of elementwise multiplying the vectors \\p a and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __hmul2_rn(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_ARITHMETIC\n* \\brief Performs \\p half2 vector division in round-to-nearest-even mode.\n*\n* \\details Divides \\p half2 input vector \\p a by input vector \\p b in round-to-nearest\n* mode.\n* \\internal\n* \\req DEEPLEARN-SRM_REQ-103\n* \\endinternal\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns half2\n* - The elementwise division of \\p a with \\p b. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __h2div(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_ARITHMETIC\n* \\brief Calculates the absolute value of both halves of the input \\p half2 number and\n* returns the result.\n*\n* \\details Calculates the absolute value of both halves of the input \\p half2 number and\n* returns the result.\n* \\param[in] a - half2. Is only being read. \n*\n* \\returns half2\n* - Returns \\p a with the absolute value of both halves. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __habs2(const __half2 a);\n/**\n* \\ingroup CUDA_MATH__HALF2_ARITHMETIC\n* \\brief Performs \\p half2 vector addition in round-to-nearest-even mode, with\n* saturation to [0.0, 1.0].\n*\n* \\details Performs \\p half2 vector add of inputs \\p a and \\p b, in round-to-nearest\n* mode, and clamps the results to range [0.0, 1.0]. NaN results are flushed to\n* +0.0.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns half2\n* - The sum of \\p a and \\p b, with respect to saturation. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __hadd2_sat(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_ARITHMETIC\n* \\brief Performs \\p half2 vector subtraction in round-to-nearest-even mode,\n* with saturation to [0.0, 1.0].\n*\n* \\details Subtracts \\p half2 input vector \\p b from input vector \\p a in\n* round-to-nearest-even mode, and clamps the results to range [0.0, 1.0]. NaN\n* results are flushed to +0.0.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns half2\n* - The subtraction of vector \\p b from \\p a, with respect to saturation.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __hsub2_sat(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_ARITHMETIC\n* \\brief Performs \\p half2 vector multiplication in round-to-nearest-even mode,\n* with saturation to [0.0, 1.0].\n*\n* \\details Performs \\p half2 vector multiplication of inputs \\p a and \\p b, in\n* round-to-nearest-even mode, and clamps the results to range [0.0, 1.0]. NaN\n* results are flushed to +0.0.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns half2\n* - The result of elementwise multiplication of vectors \\p a and \\p b, \n* with respect to saturation. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __hmul2_sat(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_ARITHMETIC\n* \\brief Performs \\p half2 vector fused multiply-add in round-to-nearest-even\n* mode.\n*\n* \\details Performs \\p half2 vector multiply on inputs \\p a and \\p b,\n* then performs a \\p half2 vector add of the result with \\p c,\n* rounding the result once in round-to-nearest-even mode.\n* \\internal\n* \\req DEEPLEARN-SRM_REQ-105\n* \\endinternal\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n* \\param[in] c - half2. Is only being read. \n*\n* \\returns half2\n* - The result of elementwise fused multiply-add operation on vectors \\p a, \\p b, and \\p c. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __hfma2(const __half2 a, const __half2 b, const __half2 c);\n/**\n* \\ingroup CUDA_MATH__HALF2_ARITHMETIC\n* \\brief Performs \\p half2 vector fused multiply-add in round-to-nearest-even\n* mode, with saturation to [0.0, 1.0].\n*\n* \\details Performs \\p half2 vector multiply on inputs \\p a and \\p b,\n* then performs a \\p half2 vector add of the result with \\p c,\n* rounding the result once in round-to-nearest-even mode, and clamps the\n* results to range [0.0, 1.0]. NaN results are flushed to +0.0.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n* \\param[in] c - half2. Is only being read. \n*\n* \\returns half2\n* - The result of elementwise fused multiply-add operation on vectors \\p a, \\p b, and \\p c, \n* with respect to saturation. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __hfma2_sat(const __half2 a, const __half2 b, const __half2 c);\n/**\n* \\ingroup CUDA_MATH__HALF2_ARITHMETIC\n* \\brief Negates both halves of the input \\p half2 number and returns the\n* result.\n*\n* \\details Negates both halves of the input \\p half2 number \\p a and returns the result.\n* \\internal\n* \\req DEEPLEARN-SRM_REQ-101\n* \\endinternal\n* \\param[in] a - half2. Is only being read. \n*\n* \\returns half2\n* - Returns \\p a with both halves negated. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __hneg2(const __half2 a);\n/**\n* \\ingroup CUDA_MATH__HALF_ARITHMETIC\n* \\brief Calculates the absolute value of input \\p half number and returns the result.\n*\n* \\details Calculates the absolute value of input \\p half number and returns the result.\n* \\param[in] a - half. Is only being read. \n*\n* \\returns half\n* - The absolute value of \\p a.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __habs(const __half a);\n/**\n* \\ingroup CUDA_MATH__HALF_ARITHMETIC\n* \\brief Performs \\p half addition in round-to-nearest-even mode.\n*\n* \\details Performs \\p half addition of inputs \\p a and \\p b, in round-to-nearest-even\n* mode.\n* \\internal\n* \\req DEEPLEARN-SRM_REQ-94\n* \\endinternal\n* \\param[in] a - half. Is only being read. \n* \\param[in] b - half. Is only being read. \n*\n* \\returns half\n* - The sum of \\p a and \\p b. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __hadd(const __half a, const __half b);\n/**\n* \\ingroup CUDA_MATH__HALF_ARITHMETIC\n* \\brief Performs \\p half subtraction in round-to-nearest-even mode.\n*\n* \\details Subtracts \\p half input \\p b from input \\p a in round-to-nearest\n* mode.\n* \\internal\n* \\req DEEPLEARN-SRM_REQ-97\n* \\endinternal\n* \\param[in] a - half. Is only being read. \n* \\param[in] b - half. Is only being read. \n*\n* \\returns half\n* - The result of subtracting \\p b from \\p a. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __hsub(const __half a, const __half b);\n/**\n* \\ingroup CUDA_MATH__HALF_ARITHMETIC\n* \\brief Performs \\p half multiplication in round-to-nearest-even mode.\n*\n* \\details Performs \\p half multiplication of inputs \\p a and \\p b, in round-to-nearest\n* mode.\n* \\internal\n* \\req DEEPLEARN-SRM_REQ-99\n* \\endinternal\n* \\param[in] a - half. Is only being read. \n* \\param[in] b - half. Is only being read. \n*\n* \\returns half\n* - The result of multiplying \\p a and \\p b. \n*/\n__CUDA_FP16_DECL__ __half __hmul(const __half a, const __half b);\n/**\n* \\ingroup CUDA_MATH__HALF_ARITHMETIC\n* \\brief Performs \\p half addition in round-to-nearest-even mode.\n*\n* \\details Performs \\p half addition of inputs \\p a and \\p b, in round-to-nearest-even\n* mode. Prevents floating-point contractions of mul+add into fma.\n* \\internal\n* \\req DEEPLEARN-SRM_REQ-94\n* \\endinternal\n* \\param[in] a - half. Is only being read.\n* \\param[in] b - half. Is only being read.\n*\n* \\returns half\n* - The sum of \\p a and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __hadd_rn(const __half a, const __half b);\n/**\n* \\ingroup CUDA_MATH__HALF_ARITHMETIC\n* \\brief Performs \\p half subtraction in round-to-nearest-even mode.\n*\n* \\details Subtracts \\p half input \\p b from input \\p a in round-to-nearest\n* mode. Prevents floating-point contractions of mul+sub into fma.\n* \\internal\n* \\req DEEPLEARN-SRM_REQ-97\n* \\endinternal\n* \\param[in] a - half. Is only being read.\n* \\param[in] b - half. Is only being read.\n*\n* \\returns half\n* - The result of subtracting \\p b from \\p a.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __hsub_rn(const __half a, const __half b);\n/**\n* \\ingroup CUDA_MATH__HALF_ARITHMETIC\n* \\brief Performs \\p half multiplication in round-to-nearest-even mode.\n*\n* \\details Performs \\p half multiplication of inputs \\p a and \\p b, in round-to-nearest\n* mode. Prevents floating-point contractions of mul+add or sub into fma.\n* \\internal\n* \\req DEEPLEARN-SRM_REQ-99\n* \\endinternal\n* \\param[in] a - half. Is only being read.\n* \\param[in] b - half. Is only being read.\n*\n* \\returns half\n* - The result of multiplying \\p a and \\p b.\n*/\n__CUDA_FP16_DECL__ __half __hmul_rn(const __half a, const __half b);\n/**\n* \\ingroup CUDA_MATH__HALF_ARITHMETIC\n* \\brief Performs \\p half division in round-to-nearest-even mode.\n* \n* \\details Divides \\p half input \\p a by input \\p b in round-to-nearest\n* mode.\n* \\internal\n* \\req DEEPLEARN-SRM_REQ-98\n* \\endinternal\n* \\param[in] a - half. Is only being read. \n* \\param[in] b - half. Is only being read. \n* \n* \\returns half\n* - The result of dividing \\p a by \\p b. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__  __half __hdiv(const __half a, const __half b);\n/**\n* \\ingroup CUDA_MATH__HALF_ARITHMETIC\n* \\brief Performs \\p half addition in round-to-nearest-even mode, with\n* saturation to [0.0, 1.0].\n*\n* \\details Performs \\p half add of inputs \\p a and \\p b, in round-to-nearest-even mode,\n* and clamps the result to range [0.0, 1.0]. NaN results are flushed to +0.0.\n* \\param[in] a - half. Is only being read. \n* \\param[in] b - half. Is only being read. \n*\n* \\returns half\n* - The sum of \\p a and \\p b, with respect to saturation.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __hadd_sat(const __half a, const __half b);\n/**\n* \\ingroup CUDA_MATH__HALF_ARITHMETIC\n* \\brief Performs \\p half subtraction in round-to-nearest-even mode, with\n* saturation to [0.0, 1.0].\n*\n* \\details Subtracts \\p half input \\p b from input \\p a in round-to-nearest\n* mode,\n* and clamps the result to range [0.0, 1.0]. NaN results are flushed to +0.0.\n* \\param[in] a - half. Is only being read. \n* \\param[in] b - half. Is only being read. \n*\n* \\returns half\n* - The result of subtraction of \\p b from \\p a, with respect to saturation.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __hsub_sat(const __half a, const __half b);\n/**\n* \\ingroup CUDA_MATH__HALF_ARITHMETIC\n* \\brief Performs \\p half multiplication in round-to-nearest-even mode, with\n* saturation to [0.0, 1.0].\n*\n* \\details Performs \\p half multiplication of inputs \\p a and \\p b, in round-to-nearest\n* mode, and clamps the result to range [0.0, 1.0]. NaN results are flushed to\n* +0.0.\n* \\param[in] a - half. Is only being read. \n* \\param[in] b - half. Is only being read. \n*\n* \\returns half\n* - The result of multiplying \\p a and \\p b, with respect to saturation.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __hmul_sat(const __half a, const __half b);\n/**\n* \\ingroup CUDA_MATH__HALF_ARITHMETIC\n* \\brief Performs \\p half fused multiply-add in round-to-nearest-even mode.\n*\n* \\details Performs \\p half multiply on inputs \\p a and \\p b,\n* then performs a \\p half add of the result with \\p c,\n* rounding the result once in round-to-nearest-even mode.\n* \\internal\n* \\req DEEPLEARN-SRM_REQ-96\n* \\endinternal\n* \\param[in] a - half. Is only being read. \n* \\param[in] b - half. Is only being read. \n* \\param[in] c - half. Is only being read. \n*\n* \\returns half\n* - The result of fused multiply-add operation on \\p\n* a, \\p b, and \\p c. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __hfma(const __half a, const __half b, const __half c);\n/**\n* \\ingroup CUDA_MATH__HALF_ARITHMETIC\n* \\brief Performs \\p half fused multiply-add in round-to-nearest-even mode,\n* with saturation to [0.0, 1.0].\n*\n* \\details Performs \\p half multiply on inputs \\p a and \\p b,\n* then performs a \\p half add of the result with \\p c,\n* rounding the result once in round-to-nearest-even mode, and clamps the result\n* to range [0.0, 1.0]. NaN results are flushed to +0.0.\n* \\param[in] a - half. Is only being read. \n* \\param[in] b - half. Is only being read. \n* \\param[in] c - half. Is only being read. \n*\n* \\returns half\n* - The result of fused multiply-add operation on \\p\n* a, \\p b, and \\p c, with respect to saturation. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __hfma_sat(const __half a, const __half b, const __half c);\n/**\n* \\ingroup CUDA_MATH__HALF_ARITHMETIC\n* \\brief Negates input \\p half number and returns the result.\n*\n* \\details Negates input \\p half number and returns the result.\n* \\internal\n* \\req DEEPLEARN-SRM_REQ-100\n* \\endinternal\n* \\param[in] a - half. Is only being read. \n*\n* \\returns half\n* - minus a\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __hneg(const __half a);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector if-equal comparison and returns boolean true\n* iff both \\p half results are true, boolean false otherwise.\n*\n* \\details Performs \\p half2 vector if-equal comparison of inputs \\p a and \\p b.\n* The bool result is set to true only if both \\p half if-equal comparisons\n* evaluate to true, or false otherwise.\n* NaN inputs generate false results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns bool\n* - true if both \\p half results of if-equal comparison\n* of vectors \\p a and \\p b are true;\n* - false otherwise.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ bool __hbeq2(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector not-equal comparison and returns boolean\n* true iff both \\p half results are true, boolean false otherwise.\n*\n* \\details Performs \\p half2 vector not-equal comparison of inputs \\p a and \\p b.\n* The bool result is set to true only if both \\p half not-equal comparisons\n* evaluate to true, or false otherwise.\n* NaN inputs generate false results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns bool\n* - true if both \\p half results of not-equal comparison\n* of vectors \\p a and \\p b are true, \n* - false otherwise. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ bool __hbne2(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector less-equal comparison and returns boolean\n* true iff both \\p half results are true, boolean false otherwise.\n*\n* \\details Performs \\p half2 vector less-equal comparison of inputs \\p a and \\p b.\n* The bool result is set to true only if both \\p half less-equal comparisons\n* evaluate to true, or false otherwise.\n* NaN inputs generate false results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns bool\n* - true if both \\p half results of less-equal comparison\n* of vectors \\p a and \\p b are true; \n* - false otherwise. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ bool __hble2(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector greater-equal comparison and returns boolean\n* true iff both \\p half results are true, boolean false otherwise.\n*\n* \\details Performs \\p half2 vector greater-equal comparison of inputs \\p a and \\p b.\n* The bool result is set to true only if both \\p half greater-equal comparisons\n* evaluate to true, or false otherwise.\n* NaN inputs generate false results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns bool\n* - true if both \\p half results of greater-equal\n* comparison of vectors \\p a and \\p b are true; \n* - false otherwise. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ bool __hbge2(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector less-than comparison and returns boolean\n* true iff both \\p half results are true, boolean false otherwise.\n*\n* \\details Performs \\p half2 vector less-than comparison of inputs \\p a and \\p b.\n* The bool result is set to true only if both \\p half less-than comparisons\n* evaluate to true, or false otherwise.\n* NaN inputs generate false results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns bool\n* - true if both \\p half results of less-than comparison\n* of vectors \\p a and \\p b are true; \n* - false otherwise. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ bool __hblt2(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector greater-than comparison and returns boolean\n* true iff both \\p half results are true, boolean false otherwise.\n*\n* \\details Performs \\p half2 vector greater-than comparison of inputs \\p a and \\p b.\n* The bool result is set to true only if both \\p half greater-than comparisons\n* evaluate to true, or false otherwise.\n* NaN inputs generate false results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n* \n* \\returns bool \n* - true if both \\p half results of greater-than\n* comparison of vectors \\p a and \\p b are true; \n* - false otherwise. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ bool __hbgt2(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector unordered if-equal comparison and returns\n* boolean true iff both \\p half results are true, boolean false otherwise.\n*\n* \\details Performs \\p half2 vector if-equal comparison of inputs \\p a and \\p b.\n* The bool result is set to true only if both \\p half if-equal comparisons\n* evaluate to true, or false otherwise.\n* NaN inputs generate true results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns bool\n* - true if both \\p half results of unordered if-equal\n* comparison of vectors \\p a and \\p b are true; \n* - false otherwise. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ bool __hbequ2(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector unordered not-equal comparison and returns\n* boolean true iff both \\p half results are true, boolean false otherwise.\n*\n* \\details Performs \\p half2 vector not-equal comparison of inputs \\p a and \\p b.\n* The bool result is set to true only if both \\p half not-equal comparisons\n* evaluate to true, or false otherwise.\n* NaN inputs generate true results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns bool\n* - true if both \\p half results of unordered not-equal\n* comparison of vectors \\p a and \\p b are true;\n* - false otherwise. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ bool __hbneu2(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector unordered less-equal comparison and returns\n* boolean true iff both \\p half results are true, boolean false otherwise.\n*\n* \\details Performs \\p half2 vector less-equal comparison of inputs \\p a and \\p b.\n* The bool result is set to true only if both \\p half less-equal comparisons\n* evaluate to true, or false otherwise.\n* NaN inputs generate true results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns bool\n* - true if both \\p half results of unordered less-equal\n* comparison of vectors \\p a and \\p b are true; \n* - false otherwise. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ bool __hbleu2(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector unordered greater-equal comparison and\n* returns boolean true iff both \\p half results are true, boolean false\n* otherwise.\n*\n* \\details Performs \\p half2 vector greater-equal comparison of inputs \\p a and \\p b.\n* The bool result is set to true only if both \\p half greater-equal comparisons\n* evaluate to true, or false otherwise.\n* NaN inputs generate true results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns bool\n* - true if both \\p half results of unordered\n* greater-equal comparison of vectors \\p a and \\p b are true; \n* - false otherwise. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ bool __hbgeu2(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector unordered less-than comparison and returns\n* boolean true iff both \\p half results are true, boolean false otherwise.\n*\n* \\details Performs \\p half2 vector less-than comparison of inputs \\p a and \\p b.\n* The bool result is set to true only if both \\p half less-than comparisons\n* evaluate to true, or false otherwise.\n* NaN inputs generate true results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns bool\n* - true if both \\p half results of unordered less-than comparison of \n* vectors \\p a and \\p b are true; \n* - false otherwise. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ bool __hbltu2(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Performs \\p half2 vector unordered greater-than comparison and\n* returns boolean true iff both \\p half results are true, boolean false\n* otherwise.\n*\n* \\details Performs \\p half2 vector greater-than comparison of inputs \\p a and \\p b.\n* The bool result is set to true only if both \\p half greater-than comparisons\n* evaluate to true, or false otherwise.\n* NaN inputs generate true results.\n* \\param[in] a - half2. Is only being read. \n* \\param[in] b - half2. Is only being read. \n*\n* \\returns bool\n* - true if both \\p half results of unordered\n* greater-than comparison of vectors \\p a and \\p b are true;\n* - false otherwise. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ bool __hbgtu2(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF_COMPARISON\n* \\brief Performs \\p half if-equal comparison.\n*\n* \\details Performs \\p half if-equal comparison of inputs \\p a and \\p b.\n* NaN inputs generate false results.\n* \\param[in] a - half. Is only being read. \n* \\param[in] b - half. Is only being read. \n*\n* \\returns bool\n* - The boolean result of if-equal comparison of \\p a and \\p b. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ bool __heq(const __half a, const __half b);\n/**\n* \\ingroup CUDA_MATH__HALF_COMPARISON\n* \\brief Performs \\p half not-equal comparison.\n*\n* \\details Performs \\p half not-equal comparison of inputs \\p a and \\p b.\n* NaN inputs generate false results.\n* \\param[in] a - half. Is only being read. \n* \\param[in] b - half. Is only being read. \n*\n* \\returns bool\n* - The boolean result of not-equal comparison of \\p a and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ bool __hne(const __half a, const __half b);\n/**\n* \\ingroup CUDA_MATH__HALF_COMPARISON\n* \\brief Performs \\p half less-equal comparison.\n*\n* \\details Performs \\p half less-equal comparison of inputs \\p a and \\p b.\n* NaN inputs generate false results.\n* \\param[in] a - half. Is only being read. \n* \\param[in] b - half. Is only being read. \n*\n* \\returns bool\n* - The boolean result of less-equal comparison of \\p a and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ bool __hle(const __half a, const __half b);\n/**\n* \\ingroup CUDA_MATH__HALF_COMPARISON\n* \\brief Performs \\p half greater-equal comparison.\n*\n* \\details Performs \\p half greater-equal comparison of inputs \\p a and \\p b.\n* NaN inputs generate false results.\n* \\param[in] a - half. Is only being read. \n* \\param[in] b - half. Is only being read. \n*\n* \\returns bool\n* - The boolean result of greater-equal comparison of \\p a and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ bool __hge(const __half a, const __half b);\n/**\n* \\ingroup CUDA_MATH__HALF_COMPARISON\n* \\brief Performs \\p half less-than comparison.\n*\n* \\details Performs \\p half less-than comparison of inputs \\p a and \\p b.\n* NaN inputs generate false results.\n* \\param[in] a - half. Is only being read. \n* \\param[in] b - half. Is only being read. \n*\n* \\returns bool\n* - The boolean result of less-than comparison of \\p a and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ bool __hlt(const __half a, const __half b);\n/**\n* \\ingroup CUDA_MATH__HALF_COMPARISON\n* \\brief Performs \\p half greater-than comparison.\n*\n* \\details Performs \\p half greater-than comparison of inputs \\p a and \\p b.\n* NaN inputs generate false results.\n* \\param[in] a - half. Is only being read. \n* \\param[in] b - half. Is only being read. \n*\n* \\returns bool\n* - The boolean result of greater-than comparison of \\p a and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ bool __hgt(const __half a, const __half b);\n/**\n* \\ingroup CUDA_MATH__HALF_COMPARISON\n* \\brief Performs \\p half unordered if-equal comparison.\n*\n* \\details Performs \\p half if-equal comparison of inputs \\p a and \\p b.\n* NaN inputs generate true results.\n* \\param[in] a - half. Is only being read. \n* \\param[in] b - half. Is only being read. \n*\n* \\returns bool\n* - The boolean result of unordered if-equal comparison of \\p a and\n* \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ bool __hequ(const __half a, const __half b);\n/**\n* \\ingroup CUDA_MATH__HALF_COMPARISON\n* \\brief Performs \\p half unordered not-equal comparison.\n*\n* \\details Performs \\p half not-equal comparison of inputs \\p a and \\p b.\n* NaN inputs generate true results.\n* \\param[in] a - half. Is only being read. \n* \\param[in] b - half. Is only being read. \n*\n* \\returns bool\n* - The boolean result of unordered not-equal comparison of \\p a and\n* \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ bool __hneu(const __half a, const __half b);\n/**\n* \\ingroup CUDA_MATH__HALF_COMPARISON\n* \\brief Performs \\p half unordered less-equal comparison.\n*\n* \\details Performs \\p half less-equal comparison of inputs \\p a and \\p b.\n* NaN inputs generate true results.\n* \\param[in] a - half. Is only being read. \n* \\param[in] b - half. Is only being read. \n*\n* \\returns bool\n* - The boolean result of unordered less-equal comparison of \\p a and\n* \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ bool __hleu(const __half a, const __half b);\n/**\n* \\ingroup CUDA_MATH__HALF_COMPARISON\n* \\brief Performs \\p half unordered greater-equal comparison.\n*\n* \\details Performs \\p half greater-equal comparison of inputs \\p a and \\p b.\n* NaN inputs generate true results.\n* \\param[in] a - half. Is only being read. \n* \\param[in] b - half. Is only being read. \n*\n* \\returns bool\n* - The boolean result of unordered greater-equal comparison of \\p a\n* and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ bool __hgeu(const __half a, const __half b);\n/**\n* \\ingroup CUDA_MATH__HALF_COMPARISON\n* \\brief Performs \\p half unordered less-than comparison.\n*\n* \\details Performs \\p half less-than comparison of inputs \\p a and \\p b.\n* NaN inputs generate true results.\n* \\param[in] a - half. Is only being read. \n* \\param[in] b - half. Is only being read. \n*\n* \\returns bool\n* - The boolean result of unordered less-than comparison of \\p a and\n* \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ bool __hltu(const __half a, const __half b);\n/**\n* \\ingroup CUDA_MATH__HALF_COMPARISON\n* \\brief Performs \\p half unordered greater-than comparison.\n*\n* \\details Performs \\p half greater-than comparison of inputs \\p a and \\p b.\n* NaN inputs generate true results.\n* \\param[in] a - half. Is only being read. \n* \\param[in] b - half. Is only being read. \n*\n* \\returns bool\n* - The boolean result of unordered greater-than comparison of \\p a\n* and \\p b.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ bool __hgtu(const __half a, const __half b);\n/**\n* \\ingroup CUDA_MATH__HALF_COMPARISON\n* \\brief Determine whether \\p half argument is a NaN.\n*\n* \\details Determine whether \\p half value \\p a is a NaN.\n* \\param[in] a - half. Is only being read. \n*\n* \\returns bool\n* - true iff argument is NaN. \n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ bool __hisnan(const __half a);\n#if !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 800)\n/**\n* \\ingroup CUDA_MATH__HALF_COMPARISON\n* \\brief Calculates \\p half maximum of two input values, NaNs pass through.\n*\n* \\details Calculates \\p half max(\\p a, \\p b)\n* defined as (\\p a > \\p b) ? \\p a : \\p b.\n* - If either of inputs is NaN, then canonical NaN is returned.\n* - If values of both inputs are 0.0, then +0.0 > -0.0\n* \\param[in] a - half. Is only being read.\n* \\param[in] b - half. Is only being read.\n*\n* \\returns half\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __hmax_nan(const __half a, const __half b);\n/**\n* \\ingroup CUDA_MATH__HALF_COMPARISON\n* \\brief Calculates \\p half minimum of two input values, NaNs pass through.\n*\n* \\details Calculates \\p half min(\\p a, \\p b)\n* defined as (\\p a < \\p b) ? \\p a : \\p b.\n* - If either of inputs is NaN, then canonical NaN is returned.\n* - If values of both inputs are 0.0, then +0.0 > -0.0\n* \\param[in] a - half. Is only being read.\n* \\param[in] b - half. Is only being read.\n*\n* \\returns half\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __hmin_nan(const __half a, const __half b);\n/**\n* \\ingroup CUDA_MATH__HALF_ARITHMETIC\n* \\brief Performs \\p half fused multiply-add in round-to-nearest-even mode with relu saturation.\n*\n* \\details Performs \\p half multiply on inputs \\p a and \\p b,\n* then performs a \\p half add of the result with \\p c,\n* rounding the result once in round-to-nearest-even mode.\n* Then negative result is clamped to 0.\n* NaN result is converted to canonical NaN.\n* \\param[in] a - half. Is only being read.\n* \\param[in] b - half. Is only being read.\n* \\param[in] c - half. Is only being read.\n*\n* \\returns half\n* - The result of fused multiply-add operation on \\p\n* a, \\p b, and \\p c with relu saturation.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half __hfma_relu(const __half a, const __half b, const __half c);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Calculates \\p half2 vector maximum of two inputs, NaNs pass through.\n*\n* \\details Calculates \\p half2 vector max(\\p a, \\p b).\n* Elementwise \\p half operation is defined as\n* (\\p a > \\p b) ? \\p a : \\p b.\n* - If either of inputs is NaN, then canonical NaN is returned.\n* - If values of both inputs are 0.0, then +0.0 > -0.0\n* \\param[in] a - half2. Is only being read.\n* \\param[in] b - half2. Is only being read.\n*\n* \\returns half2\n* - The result of elementwise maximum of vectors \\p a  and \\p b, with NaNs pass through\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __hmax2_nan(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_COMPARISON\n* \\brief Calculates \\p half2 vector minimum of two inputs, NaNs pass through.\n*\n* \\details Calculates \\p half2 vector min(\\p a, \\p b).\n* Elementwise \\p half operation is defined as\n* (\\p a < \\p b) ? \\p a : \\p b.\n* - If either of inputs is NaN, then canonical NaN is returned.\n* - If values of both inputs are 0.0, then +0.0 > -0.0\n* \\param[in] a - half2. Is only being read.\n* \\param[in] b - half2. Is only being read.\n*\n* \\returns half2\n* - The result of elementwise minimum of vectors \\p a  and \\p b, with NaNs pass through\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __hmin2_nan(const __half2 a, const __half2 b);\n/**\n* \\ingroup CUDA_MATH__HALF2_ARITHMETIC\n* \\brief Performs \\p half2 vector fused multiply-add in round-to-nearest-even\n* mode with relu saturation.\n*\n* \\details Performs \\p half2 vector multiply on inputs \\p a and \\p b,\n* then performs a \\p half2 vector add of the result with \\p c,\n* rounding the result once in round-to-nearest-even mode.\n* Then negative result is clamped to 0.\n* NaN result is converted to canonical NaN.\n* \\param[in] a - half2. Is only being read.\n* \\param[in] b - half2. Is only being read.\n* \\param[in] c - half2. Is only being read.\n*\n* \\returns half2\n* - The result of elementwise fused multiply-add operation on vectors \\p a, \\p b, and \\p c with relu saturation.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __hfma2_relu(const __half2 a, const __half2 b, const __half2 c);\n#endif /* !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 800) */\n/**\n* \\ingroup CUDA_MATH__HALF2_ARITHMETIC\n* \\brief Performs fast complex multiply-accumulate\n*\n* \\details Interprets vector \\p half2 input pairs \\p a, \\p b, and \\p c as\n* complex numbers in \\p half precision and performs\n* complex multiply-accumulate operation: a*b + c\n* \\param[in] a - half2. Is only being read.\n* \\param[in] b - half2. Is only being read.\n* \\param[in] c - half2. Is only being read.\n*\n* \\returns half2\n* - The result of complex multiply-accumulate operation on complex numbers \\p a, \\p b, and \\p c\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 __hcmadd(const __half2 a, const __half2 b, const __half2 c);\n/**\n* \\ingroup CUDA_MATH__HALF_FUNCTIONS\n* \\brief Calculates \\p half square root in round-to-nearest-even mode.\n*\n* \\details Calculates \\p half square root of input \\p a in round-to-nearest-even mode.\n* \\param[in] a - half. Is only being read. \n*\n* \\returns half\n* - The square root of \\p a.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half hsqrt(const __half a);\n/**\n* \\ingroup CUDA_MATH__HALF_FUNCTIONS\n* \\brief Calculates \\p half reciprocal square root in round-to-nearest-even\n* mode.\n*\n* \\details Calculates \\p half reciprocal square root of input \\p a in round-to-nearest\n* mode.\n* \\param[in] a - half. Is only being read. \n*\n* \\returns half\n* - The reciprocal square root of \\p a.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half hrsqrt(const __half a);\n/**\n* \\ingroup CUDA_MATH__HALF_FUNCTIONS\n* \\brief Calculates \\p half reciprocal in round-to-nearest-even mode.\n*\n* \\details Calculates \\p half reciprocal of input \\p a in round-to-nearest-even mode.\n* \\param[in] a - half. Is only being read. \n*\n* \\returns half\n* - The reciprocal of \\p a.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half hrcp(const __half a);\n/**\n* \\ingroup CUDA_MATH__HALF_FUNCTIONS\n* \\brief Calculates \\p half natural logarithm in round-to-nearest-even mode.\n*\n* \\details Calculates \\p half natural logarithm of input \\p a in round-to-nearest-even\n* mode.\n* \\param[in] a - half. Is only being read. \n*\n* \\returns half\n* - The natural logarithm of \\p a.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half hlog(const __half a);\n/**\n* \\ingroup CUDA_MATH__HALF_FUNCTIONS\n* \\brief Calculates \\p half binary logarithm in round-to-nearest-even mode.\n*\n* \\details Calculates \\p half binary logarithm of input \\p a in round-to-nearest-even\n* mode.\n* \\param[in] a - half. Is only being read. \n*\n* \\returns half\n* - The binary logarithm of \\p a.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half hlog2(const __half a);\n/**\n* \\ingroup CUDA_MATH__HALF_FUNCTIONS\n* \\brief Calculates \\p half decimal logarithm in round-to-nearest-even mode.\n*\n* \\details Calculates \\p half decimal logarithm of input \\p a in round-to-nearest-even\n* mode.\n* \\param[in] a - half. Is only being read. \n*\n* \\returns half\n* - The decimal logarithm of \\p a.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half hlog10(const __half a);\n/**\n* \\ingroup CUDA_MATH__HALF_FUNCTIONS\n* \\brief Calculates \\p half natural exponential function in round-to-nearest\n* mode.\n*\n* \\details Calculates \\p half natural exponential function of input \\p a in\n* round-to-nearest-even mode.\n* \\param[in] a - half. Is only being read. \n*\n* \\returns half\n* - The natural exponential function on \\p a.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half hexp(const __half a);\n/**\n* \\ingroup CUDA_MATH__HALF_FUNCTIONS\n* \\brief Calculates \\p half binary exponential function in round-to-nearest\n* mode.\n*\n* \\details Calculates \\p half binary exponential function of input \\p a in\n* round-to-nearest-even mode.\n* \\param[in] a - half. Is only being read. \n*\n* \\returns half\n* - The binary exponential function on \\p a.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half hexp2(const __half a);\n/**\n* \\ingroup CUDA_MATH__HALF_FUNCTIONS\n* \\brief Calculates \\p half decimal exponential function in round-to-nearest\n* mode.\n*\n* \\details Calculates \\p half decimal exponential function of input \\p a in\n* round-to-nearest-even mode.\n* \\param[in] a - half. Is only being read. \n*\n* \\returns half\n* - The decimal exponential function on \\p a.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half hexp10(const __half a);\n/**\n* \\ingroup CUDA_MATH__HALF_FUNCTIONS\n* \\brief Calculates \\p half cosine in round-to-nearest-even mode.\n*\n* \\details Calculates \\p half cosine of input \\p a in round-to-nearest-even mode.\n* \\param[in] a - half. Is only being read. \n*\n* \\returns half\n* - The cosine of \\p a.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half hcos(const __half a);\n/**\n* \\ingroup CUDA_MATH__HALF_FUNCTIONS\n* \\brief Calculates \\p half sine in round-to-nearest-even mode.\n*\n* \\details Calculates \\p half sine of input \\p a in round-to-nearest-even mode.\n* \\param[in] a - half. Is only being read. \n*\n* \\returns half\n* - The sine of \\p a.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half hsin(const __half a);\n/**\n* \\ingroup CUDA_MATH__HALF2_FUNCTIONS\n* \\brief Calculates \\p half2 vector square root in round-to-nearest-even mode.\n*\n* \\details Calculates \\p half2 square root of input vector \\p a in round-to-nearest\n* mode.\n* \\param[in] a - half2. Is only being read. \n*\n* \\returns half2\n* - The elementwise square root on vector \\p a.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 h2sqrt(const __half2 a);\n/**\n* \\ingroup CUDA_MATH__HALF2_FUNCTIONS\n* \\brief Calculates \\p half2 vector reciprocal square root in round-to-nearest\n* mode.\n*\n* \\details Calculates \\p half2 reciprocal square root of input vector \\p a in\n* round-to-nearest-even mode.\n* \\param[in] a - half2. Is only being read. \n*\n* \\returns half2\n* - The elementwise reciprocal square root on vector \\p a.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 h2rsqrt(const __half2 a);\n/**\n* \\ingroup CUDA_MATH__HALF2_FUNCTIONS\n* \\brief Calculates \\p half2 vector reciprocal in round-to-nearest-even mode.\n*\n* \\details Calculates \\p half2 reciprocal of input vector \\p a in round-to-nearest-even\n* mode.\n* \\param[in] a - half2. Is only being read. \n*\n* \\returns half2\n* - The elementwise reciprocal on vector \\p a.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 h2rcp(const __half2 a);\n/**\n* \\ingroup CUDA_MATH__HALF2_FUNCTIONS\n* \\brief Calculates \\p half2 vector natural logarithm in round-to-nearest-even\n* mode.\n*\n* \\details Calculates \\p half2 natural logarithm of input vector \\p a in\n* round-to-nearest-even mode.\n* \\param[in] a - half2. Is only being read. \n*\n* \\returns half2\n* - The elementwise natural logarithm on vector \\p a.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 h2log(const __half2 a);\n/**\n* \\ingroup CUDA_MATH__HALF2_FUNCTIONS\n* \\brief Calculates \\p half2 vector binary logarithm in round-to-nearest-even\n* mode.\n*\n* \\details Calculates \\p half2 binary logarithm of input vector \\p a in round-to-nearest\n* mode.\n* \\param[in] a - half2. Is only being read. \n*\n* \\returns half2\n* - The elementwise binary logarithm on vector \\p a.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 h2log2(const __half2 a);\n/**\n* \\ingroup CUDA_MATH__HALF2_FUNCTIONS\n* \\brief Calculates \\p half2 vector decimal logarithm in round-to-nearest-even\n* mode.\n*\n* \\details Calculates \\p half2 decimal logarithm of input vector \\p a in\n* round-to-nearest-even mode.\n* \\param[in] a - half2. Is only being read. \n*\n* \\returns half2\n* - The elementwise decimal logarithm on vector \\p a.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 h2log10(const __half2 a);\n/**\n* \\ingroup CUDA_MATH__HALF2_FUNCTIONS\n* \\brief Calculates \\p half2 vector exponential function in round-to-nearest\n* mode.\n*\n* \\details Calculates \\p half2 exponential function of input vector \\p a in\n* round-to-nearest-even mode.\n* \\param[in] a - half2. Is only being read. \n*\n* \\returns half2\n* - The elementwise exponential function on vector \\p a.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 h2exp(const __half2 a);\n/**\n* \\ingroup CUDA_MATH__HALF2_FUNCTIONS\n* \\brief Calculates \\p half2 vector binary exponential function in\n* round-to-nearest-even mode.\n*\n* \\details Calculates \\p half2 binary exponential function of input vector \\p a in\n* round-to-nearest-even mode.\n* \\param[in] a - half2. Is only being read. \n*\n* \\returns half2\n* - The elementwise binary exponential function on vector \\p a.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 h2exp2(const __half2 a);\n/**\n* \\ingroup CUDA_MATH__HALF2_FUNCTIONS\n* \\brief Calculates \\p half2 vector decimal exponential function in\n* round-to-nearest-even mode.\n* \n* \\details Calculates \\p half2 decimal exponential function of input vector \\p a in\n* round-to-nearest-even mode.\n* \\param[in] a - half2. Is only being read. \n* \n* \\returns half2\n* - The elementwise decimal exponential function on vector \\p a.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 h2exp10(const __half2 a);\n/**\n* \\ingroup CUDA_MATH__HALF2_FUNCTIONS\n* \\brief Calculates \\p half2 vector cosine in round-to-nearest-even mode.\n* \n* \\details Calculates \\p half2 cosine of input vector \\p a in round-to-nearest-even\n* mode.\n* \\param[in] a - half2. Is only being read. \n* \n* \\returns half2\n* - The elementwise cosine on vector \\p a.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 h2cos(const __half2 a);\n/**\n* \\ingroup CUDA_MATH__HALF2_FUNCTIONS\n* \\brief Calculates \\p half2 vector sine in round-to-nearest-even mode.\n* \n* \\details Calculates \\p half2 sine of input vector \\p a in round-to-nearest-even mode.\n* \\param[in] a - half2. Is only being read. \n* \n* \\returns half2\n* - The elementwise sine on vector \\p a.\n* \\internal\n* \\exception-guarantee no-throw guarantee\n* \\behavior reentrant, thread safe\n* \\endinternal\n*/\n__CUDA_FP16_DECL__ __half2 h2sin(const __half2 a);\n\n#endif /*if !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 530)*/\n\n#if !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 600)\n\n/**\n* \\ingroup CUDA_MATH__HALF2_ARITHMETIC\n* \\brief Vector add \\p val to the value stored at \\p address in global or shared memory, and writes this\n* value back to \\p address. The atomicity of the add operation is guaranteed separately for each of the\n* two __half elements; the entire __half2 is not guaranteed to be atomic as a single 32-bit access.\n* \n* \\details The location of \\p address must be in global or shared memory. This operation has undefined\n* behavior otherwise. This operation is only supported by devices of compute capability 6.x and higher.\n* \n* \\param[in] address - half2*. An address in global or shared memory.\n* \\param[in] val - half2. The value to be added.\n* \n* \\returns half2\n* - The old value read from \\p address.\n*\n* \\note_ref_guide_atomic\n*/\n__CUDA_FP16_DECL__ __half2 atomicAdd(__half2 *const address, const __half2 val);\n\n#endif /*if !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 600)*/\n\n#if !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 700)\n\n/**\n* \\ingroup CUDA_MATH__HALF_ARITHMETIC\n* \\brief Adds \\p val to the value stored at \\p address in global or shared memory, and writes this value\n* back to \\p address. This operation is performed in one atomic operation.\n* \n* \\details The location of \\p address must be in global or shared memory. This operation has undefined\n* behavior otherwise. This operation is only supported by devices of compute capability 7.x and higher.\n* \n* \\param[in] address - half*. An address in global or shared memory.\n* \\param[in] val - half. The value to be added.\n* \n* \\returns half\n* - The old value read from \\p address.\n* \n* \\note_ref_guide_atomic\n*/\n__CUDA_FP16_DECL__ __half atomicAdd(__half *const address, const __half val);\n\n#endif /*if !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 700)*/\n\n#endif /* defined(__CUDACC__) */\n\n#undef __CUDA_FP16_DECL__\n#undef __CUDA_HOSTDEVICE_FP16_DECL__\n\n#endif /* defined(__cplusplus) */\n\n/* Note the .hpp file is included even for host-side compilation, to capture the \"half\" & \"half2\" definitions */\n#include \"cuda_fp16.hpp\"\n#undef ___CUDA_FP16_STRINGIFY_INNERMOST\n#undef __CUDA_FP16_STRINGIFY\n\n#endif /* end of include guard: __CUDA_FP16_H__ */\n", "cuda_fp16.hpp": "#define cudaDeviceSynchronize() cudaSuccess\n/*\n* Copyright 1993-2022 NVIDIA Corporation.  All rights reserved.\n*\n* NOTICE TO LICENSEE:\n*\n* This source code and/or documentation (\"Licensed Deliverables\") are\n* subject to NVIDIA intellectual property rights under U.S. and\n* international Copyright laws.\n*\n* These Licensed Deliverables contained herein is PROPRIETARY and\n* CONFIDENTIAL to NVIDIA and is being provided under the terms and\n* conditions of a form of NVIDIA software license agreement by and\n* between NVIDIA and Licensee (\"License Agreement\") or electronically\n* accepted by Licensee.  Notwithstanding any terms or conditions to\n* the contrary in the License Agreement, reproduction or disclosure\n* of the Licensed Deliverables to any third party without the express\n* written consent of NVIDIA is prohibited.\n*\n* NOTWITHSTANDING ANY TERMS OR CONDITIONS TO THE CONTRARY IN THE\n* LICENSE AGREEMENT, NVIDIA MAKES NO REPRESENTATION ABOUT THE\n* SUITABILITY OF THESE LICENSED DELIVERABLES FOR ANY PURPOSE.  IT IS\n* PROVIDED \"AS IS\" WITHOUT EXPRESS OR IMPLIED WARRANTY OF ANY KIND.\n* NVIDIA DISCLAIMS ALL WARRANTIES WITH REGARD TO THESE LICENSED\n* DELIVERABLES, INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY,\n* NONINFRINGEMENT, AND FITNESS FOR A PARTICULAR PURPOSE.\n* NOTWITHSTANDING ANY TERMS OR CONDITIONS TO THE CONTRARY IN THE\n* LICENSE AGREEMENT, IN NO EVENT SHALL NVIDIA BE LIABLE FOR ANY\n* SPECIAL, INDIRECT, INCIDENTAL, OR CONSEQUENTIAL DAMAGES, OR ANY\n* DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS,\n* WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS\n* ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE\n* OF THESE LICENSED DELIVERABLES.\n*\n* U.S. Government End Users.  These Licensed Deliverables are a\n* \"commercial item\" as that term is defined at 48 C.F.R. 2.101 (OCT\n* 1995), consisting of \"commercial computer software\" and \"commercial\n* computer software documentation\" as such terms are used in 48\n* C.F.R. 12.212 (SEPT 1995) and is provided to the U.S. Government\n* only as a commercial end item.  Consistent with 48 C.F.R.12.212 and\n* 48 C.F.R. 227.7202-1 through 227.7202-4 (JUNE 1995), all\n* U.S. Government End Users acquire the Licensed Deliverables with\n* only those rights set forth herein.\n*\n* Any use of the Licensed Deliverables in individual and commercial\n* software must include, in the user documentation and internal\n* comments to the code, the above Disclaimer and U.S. Government End\n* Users Notice.\n*/\n\n#if !defined(__CUDA_FP16_HPP__)\n#define __CUDA_FP16_HPP__\n\n#if !defined(__CUDA_FP16_H__)\n#error \"Do not include this file directly. Instead, include cuda_fp16.h.\"\n#endif\n\n#if !defined(_MSC_VER) && __cplusplus >= 201103L\n#   define __CPP_VERSION_AT_LEAST_11_FP16\n#elif _MSC_FULL_VER >= 190024210 && _MSVC_LANG >= 201103L\n#   define __CPP_VERSION_AT_LEAST_11_FP16\n#endif\n\n// implicitly provided by NVRTC\n#if !defined(__CUDACC_RTC__)\n#include <nv/target>\n#endif  /* !defined(__CUDACC_RTC__) */\n\n\n#if !defined(IF_DEVICE_OR_CUDACC)\n#if defined(__CUDACC__)\n    #define IF_DEVICE_OR_CUDACC(d, c, f) NV_IF_ELSE_TARGET(NV_IS_DEVICE, d, c)\n#else\n    #define IF_DEVICE_OR_CUDACC(d, c, f) NV_IF_ELSE_TARGET(NV_IS_DEVICE, d, f)\n#endif\n#endif\n/* C++11 header for std::move. \n * In RTC mode, std::move is provided implicitly; don't include the header\n */\n#if defined(__CPP_VERSION_AT_LEAST_11_FP16) && !defined(__CUDACC_RTC__)\n#include <utility>\n#endif /* __cplusplus >= 201103L && !defined(__CUDACC_RTC__) */\n\n/* C++ header for std::memcpy (used for type punning in host-side implementations).\n * When compiling as a CUDA source file memcpy is provided implicitly.\n * !defined(__CUDACC__) implies !defined(__CUDACC_RTC__).\n */\n#if defined(__cplusplus) && !defined(__CUDACC__)\n#include <cstring>\n#endif /* defined(__cplusplus) && !defined(__CUDACC__) */\n\n\n/* Set up function decorations */\n#if defined(__CUDACC__) || defined(_NVHPC_CUDA)\n#define __CUDA_FP16_DECL__ static __device__ __inline__\n#define __CUDA_HOSTDEVICE_FP16_DECL__ static __host__ __device__ __inline__\n#define __VECTOR_FUNCTIONS_DECL__ static __inline__ __host__ __device__\n#define __CUDA_HOSTDEVICE__ __host__ __device__\n#else /* !defined(__CUDACC__) */\n#if defined(__GNUC__)\n#define __CUDA_HOSTDEVICE_FP16_DECL__ static __attribute__ ((unused))\n#else\n#define __CUDA_HOSTDEVICE_FP16_DECL__ static\n#endif /* defined(__GNUC__) */\n#define __CUDA_HOSTDEVICE__\n#endif /* defined(__CUDACC_) */\n\n/* Set up structure-alignment attribute */\n#if defined(__CUDACC__)\n#define __CUDA_ALIGN__(align) __align__(align)\n#else\n/* Define alignment macro based on compiler type (cannot assume C11 \"_Alignas\" is available) */\n#if __cplusplus >= 201103L\n#define __CUDA_ALIGN__(n) alignas(n)    /* C++11 kindly gives us a keyword for this */\n#else /* !defined(__CPP_VERSION_AT_LEAST_11_FP16)*/\n#if defined(__GNUC__)\n#define __CUDA_ALIGN__(n) __attribute__ ((aligned(n)))\n#elif defined(_MSC_VER)\n#define __CUDA_ALIGN__(n) __declspec(align(n))\n#else\n#define __CUDA_ALIGN__(n)\n#endif /* defined(__GNUC__) */\n#endif /* defined(__CPP_VERSION_AT_LEAST_11_FP16) */\n#endif /* defined(__CUDACC__) */\n\n/* Macros to allow half & half2 to be used by inline assembly */\n#define __HALF_TO_US(var) *(reinterpret_cast<unsigned short *>(&(var)))\n#define __HALF_TO_CUS(var) *(reinterpret_cast<const unsigned short *>(&(var)))\n#define __HALF2_TO_UI(var) *(reinterpret_cast<unsigned int *>(&(var)))\n#define __HALF2_TO_CUI(var) *(reinterpret_cast<const unsigned int *>(&(var)))\n\n/* Macros for half & half2 binary arithmetic */\n#define __BINARY_OP_HALF_MACRO(name) /* do */ {\\\n   __half val; \\\n   asm( \"{\" __CUDA_FP16_STRINGIFY(name) \".f16 %0,%1,%2;\\n}\" \\\n        :\"=h\"(__HALF_TO_US(val)) : \"h\"(__HALF_TO_CUS(a)),\"h\"(__HALF_TO_CUS(b))); \\\n   return val; \\\n} /* while(0) */\n#define __BINARY_OP_HALF2_MACRO(name) /* do */ {\\\n   __half2 val; \\\n   asm( \"{\" __CUDA_FP16_STRINGIFY(name) \".f16x2 %0,%1,%2;\\n}\" \\\n        :\"=r\"(__HALF2_TO_UI(val)) : \"r\"(__HALF2_TO_CUI(a)),\"r\"(__HALF2_TO_CUI(b))); \\\n   return val; \\\n} /* while(0) */\n#define __TERNARY_OP_HALF_MACRO(name) /* do */ {\\\n   __half val; \\\n   asm( \"{\" __CUDA_FP16_STRINGIFY(name) \".f16 %0,%1,%2,%3;\\n}\" \\\n        :\"=h\"(__HALF_TO_US(val)) : \"h\"(__HALF_TO_CUS(a)),\"h\"(__HALF_TO_CUS(b)),\"h\"(__HALF_TO_CUS(c))); \\\n   return val; \\\n} /* while(0) */\n#define __TERNARY_OP_HALF2_MACRO(name) /* do */ {\\\n   __half2 val; \\\n   asm( \"{\" __CUDA_FP16_STRINGIFY(name) \".f16x2 %0,%1,%2,%3;\\n}\" \\\n        :\"=r\"(__HALF2_TO_UI(val)) : \"r\"(__HALF2_TO_CUI(a)),\"r\"(__HALF2_TO_CUI(b)),\"r\"(__HALF2_TO_CUI(c))); \\\n   return val; \\\n} /* while(0) */\n\n/**\n* Types which allow static initialization of \"half\" and \"half2\" until\n* these become an actual builtin. Note this initialization is as a\n* bitfield representation of \"half\", and not a conversion from short->half.\n* Such a representation will be deprecated in a future version of CUDA. \n* (Note these are visible to non-nvcc compilers, including C-only compilation)\n*/\ntypedef struct __CUDA_ALIGN__(2) {\n    unsigned short x;\n} __half_raw;\n\ntypedef struct __CUDA_ALIGN__(4) {\n    unsigned short x;\n    unsigned short y;\n} __half2_raw;\n\n/* All other definitions in this file are only visible to C++ compilers */\n#if defined(__cplusplus)\n\n/* Hide GCC member initialization list warnings because of host/device in-function init requirement */\n#if defined(__GNUC__)\n#if __GNUC__ > 4 || (__GNUC__ == 4 && __GNUC_MINOR__ >= 6)\n_Pragma(\"GCC diagnostic push\")\n_Pragma(\"GCC diagnostic ignored \\\"-Wstrict-aliasing\\\"\")\n_Pragma(\"GCC diagnostic ignored \\\"-Weffc++\\\"\")\n#endif /* __GNUC__ > 4 || (__GNUC__ == 4 && __GNUC_MINOR__ >= 6) */\n#endif /* defined(__GNUC__) */\n\n/* class' : multiple assignment operators specified\n   The class has multiple assignment operators of a single type. This warning is informational */\n#if defined(_MSC_VER) && _MSC_VER >= 1500\n_Pragma(\"warning( push )\")\n_Pragma(\"warning( disable:4522 )\")\n#endif /* defined(__GNUC__) */\n\nstruct __CUDA_ALIGN__(2) __half {\nprotected:\n    unsigned short __x;\n\npublic:\n#if defined(__CPP_VERSION_AT_LEAST_11_FP16)\n    __half() = default;\n#else\n    __CUDA_HOSTDEVICE__ __half() { }\n#endif /* defined(__CPP_VERSION_AT_LEAST_11_FP16) */\n\n    /* Convert to/from __half_raw */\n    __CUDA_HOSTDEVICE__ __half(const __half_raw &hr) : __x(hr.x) { }\n    __CUDA_HOSTDEVICE__ __half &operator=(const __half_raw &hr) { __x = hr.x; return *this; }\n    __CUDA_HOSTDEVICE__ volatile __half &operator=(const __half_raw &hr) volatile { __x = hr.x; return *this; }\n    __CUDA_HOSTDEVICE__ volatile __half &operator=(const volatile __half_raw &hr) volatile { __x = hr.x; return *this; }\n    __CUDA_HOSTDEVICE__ operator __half_raw() const { __half_raw ret; ret.x = __x; return ret; }\n    __CUDA_HOSTDEVICE__ operator __half_raw() const volatile { __half_raw ret; ret.x = __x; return ret; }\n\n#if !defined(__CUDA_NO_HALF_CONVERSIONS__)\n\n    /* Construct from float/double */\n    __CUDA_HOSTDEVICE__ __half(const float f) { __x = __float2half(f).__x;  }\n    __CUDA_HOSTDEVICE__ __half(const double f) { __x = __double2half(f).__x;  }\n\n    __CUDA_HOSTDEVICE__ operator float() const { return __half2float(*this); }\n    __CUDA_HOSTDEVICE__ __half &operator=(const float f) { __x = __float2half(f).__x; return *this; }\n\n    /* We omit \"cast to double\" operator, so as to not be ambiguous about up-cast */\n    __CUDA_HOSTDEVICE__ __half &operator=(const double f) { __x = __double2half(f).__x; return *this; }\n\n/* Member functions only available to nvcc compilation so far */\n#if defined(__CUDACC__)\n    /* Allow automatic construction from types supported natively in hardware */\n    /* Note we do avoid constructor init-list because of special host/device compilation rules */\n    __CUDA_HOSTDEVICE__ __half(const short val) { __x = __short2half_rn(val).__x;  }\n    __CUDA_HOSTDEVICE__ __half(const unsigned short val) { __x = __ushort2half_rn(val).__x;  }\n    __CUDA_HOSTDEVICE__ __half(const int val) { __x = __int2half_rn(val).__x;  }\n    __CUDA_HOSTDEVICE__ __half(const unsigned int val) { __x = __uint2half_rn(val).__x;  }\n    __CUDA_HOSTDEVICE__ __half(const long long val) { __x = __ll2half_rn(val).__x;  }\n    __CUDA_HOSTDEVICE__ __half(const unsigned long long val) { __x = __ull2half_rn(val).__x; }\n\n    /* Allow automatic casts to supported builtin types, matching all that are permitted with float */\n    __CUDA_HOSTDEVICE__ operator short() const { return __half2short_rz(*this); }\n    __CUDA_HOSTDEVICE__ __half &operator=(const short val) { __x = __short2half_rn(val).__x; return *this; }\n\n    __CUDA_HOSTDEVICE__ operator unsigned short() const { return __half2ushort_rz(*this); }\n    __CUDA_HOSTDEVICE__ __half &operator=(const unsigned short val) { __x = __ushort2half_rn(val).__x; return *this; }\n\n    __CUDA_HOSTDEVICE__ operator int() const { return __half2int_rz(*this); }\n    __CUDA_HOSTDEVICE__ __half &operator=(const int val) { __x = __int2half_rn(val).__x; return *this; }\n\n    __CUDA_HOSTDEVICE__ operator unsigned int() const { return __half2uint_rz(*this); }\n    __CUDA_HOSTDEVICE__ __half &operator=(const unsigned int val) { __x = __uint2half_rn(val).__x; return *this; }\n\n    __CUDA_HOSTDEVICE__ operator long long() const { return __half2ll_rz(*this); }\n    __CUDA_HOSTDEVICE__ __half &operator=(const long long val) { __x = __ll2half_rn(val).__x; return *this; }\n\n    __CUDA_HOSTDEVICE__ operator unsigned long long() const { return __half2ull_rz(*this); }\n    __CUDA_HOSTDEVICE__ __half &operator=(const unsigned long long val) { __x = __ull2half_rn(val).__x; return *this; }\n\n    /* Boolean conversion - note both 0 and -0 must return false */\n    __CUDA_HOSTDEVICE__ operator bool() const { return (__x & 0x7FFFU) != 0U; }\n#endif /* defined(__CUDACC__) */\n#endif /* !defined(__CUDA_NO_HALF_CONVERSIONS__) */\n};\n\n/* Global-space operator functions are only available to nvcc compilation */\n#if defined(__CUDACC__)\n\n/* Arithmetic FP16 operations only supported on arch >= 5.3 */\n#if !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 530) || defined(_NVHPC_CUDA)\n#if !defined(__CUDA_NO_HALF_OPERATORS__)\n/* Some basic arithmetic operations expected of a builtin */\n__device__ __forceinline__ __half operator+(const __half &lh, const __half &rh) { return __hadd(lh, rh); }\n__device__ __forceinline__ __half operator-(const __half &lh, const __half &rh) { return __hsub(lh, rh); }\n__device__ __forceinline__ __half operator*(const __half &lh, const __half &rh) { return __hmul(lh, rh); }\n__device__ __forceinline__ __half operator/(const __half &lh, const __half &rh) { return __hdiv(lh, rh); }\n\n__device__ __forceinline__ __half &operator+=(__half &lh, const __half &rh) { lh = __hadd(lh, rh); return lh; }\n__device__ __forceinline__ __half &operator-=(__half &lh, const __half &rh) { lh = __hsub(lh, rh); return lh; }\n__device__ __forceinline__ __half &operator*=(__half &lh, const __half &rh) { lh = __hmul(lh, rh); return lh; }\n__device__ __forceinline__ __half &operator/=(__half &lh, const __half &rh) { lh = __hdiv(lh, rh); return lh; }\n\n/* Note for increment and decrement we use the raw value 0x3C00U equating to half(1.0F), to avoid the extra conversion */\n__device__ __forceinline__ __half &operator++(__half &h)      { __half_raw one; one.x = 0x3C00U; h += one; return h; }\n__device__ __forceinline__ __half &operator--(__half &h)      { __half_raw one; one.x = 0x3C00U; h -= one; return h; }\n__device__ __forceinline__ __half  operator++(__half &h, const int ignored)\n{\n    // ignored on purpose. Parameter only needed to distinguish the function declaration from other types of operators.\n    static_cast<void>(ignored);\n\n    const __half ret = h;\n    __half_raw one;\n    one.x = 0x3C00U;\n    h += one;\n    return ret;\n}\n__device__ __forceinline__ __half  operator--(__half &h, const int ignored)\n{\n    // ignored on purpose. Parameter only needed to distinguish the function declaration from other types of operators.\n    static_cast<void>(ignored);\n\n    const __half ret = h;\n    __half_raw one;\n    one.x = 0x3C00U;\n    h -= one;\n    return ret;\n}\n\n/* Unary plus and inverse operators */\n__device__ __forceinline__ __half operator+(const __half &h) { return h; }\n__device__ __forceinline__ __half operator-(const __half &h) { return __hneg(h); }\n\n/* Some basic comparison operations to make it look like a builtin */\n__device__ __forceinline__ bool operator==(const __half &lh, const __half &rh) { return __heq(lh, rh); }\n__device__ __forceinline__ bool operator!=(const __half &lh, const __half &rh) { return __hneu(lh, rh); }\n__device__ __forceinline__ bool operator> (const __half &lh, const __half &rh) { return __hgt(lh, rh); }\n__device__ __forceinline__ bool operator< (const __half &lh, const __half &rh) { return __hlt(lh, rh); }\n__device__ __forceinline__ bool operator>=(const __half &lh, const __half &rh) { return __hge(lh, rh); }\n__device__ __forceinline__ bool operator<=(const __half &lh, const __half &rh) { return __hle(lh, rh); }\n#endif /* !defined(__CUDA_NO_HALF_OPERATORS__) */\n#endif /* !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 530) || defined(_NVHPC_CUDA) */\n#endif /* defined(__CUDACC__) */\n\n/* __half2 is visible to non-nvcc host compilers */\nstruct __CUDA_ALIGN__(4) __half2 {\n    __half x;\n    __half y;\n\n    // All construct/copy/assign/move\npublic:\n#if defined(__CPP_VERSION_AT_LEAST_11_FP16)\n    __half2() = default;\n    __CUDA_HOSTDEVICE__ __half2(const __half2 &&src) { __HALF2_TO_UI(*this) = std::move(__HALF2_TO_CUI(src)); }\n    __CUDA_HOSTDEVICE__ __half2 &operator=(const __half2 &&src) { __HALF2_TO_UI(*this) = std::move(__HALF2_TO_CUI(src)); return *this; }\n#else\n    __CUDA_HOSTDEVICE__ __half2() { }\n#endif /* defined(__CPP_VERSION_AT_LEAST_11_FP16) */\n    __CUDA_HOSTDEVICE__ __half2(const __half &a, const __half &b) : x(a), y(b) { }\n    __CUDA_HOSTDEVICE__ __half2(const __half2 &src) { __HALF2_TO_UI(*this) = __HALF2_TO_CUI(src); }\n    __CUDA_HOSTDEVICE__ __half2 &operator=(const __half2 &src) { __HALF2_TO_UI(*this) = __HALF2_TO_CUI(src); return *this; }\n\n    /* Convert to/from __half2_raw */\n    __CUDA_HOSTDEVICE__ __half2(const __half2_raw &h2r ) { __HALF2_TO_UI(*this) = __HALF2_TO_CUI(h2r); }\n    __CUDA_HOSTDEVICE__ __half2 &operator=(const __half2_raw &h2r) { __HALF2_TO_UI(*this) = __HALF2_TO_CUI(h2r); return *this; }\n    __CUDA_HOSTDEVICE__ operator __half2_raw() const { __half2_raw ret; ret.x = 0U; ret.y = 0U; __HALF2_TO_UI(ret) = __HALF2_TO_CUI(*this); return ret; }\n};\n\n/* Global-space operator functions are only available to nvcc compilation */\n#if defined(__CUDACC__)\n\n/* Arithmetic FP16x2 operations only supported on arch >= 5.3 */\n#if (!defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 530) || defined(_NVHPC_CUDA)) && !defined(__CUDA_NO_HALF2_OPERATORS__)\n\n__device__ __forceinline__ __half2 operator+(const __half2 &lh, const __half2 &rh) { return __hadd2(lh, rh); }\n__device__ __forceinline__ __half2 operator-(const __half2 &lh, const __half2 &rh) { return __hsub2(lh, rh); }\n__device__ __forceinline__ __half2 operator*(const __half2 &lh, const __half2 &rh) { return __hmul2(lh, rh); }\n__device__ __forceinline__ __half2 operator/(const __half2 &lh, const __half2 &rh) { return __h2div(lh, rh); }\n\n__device__ __forceinline__ __half2& operator+=(__half2 &lh, const __half2 &rh) { lh = __hadd2(lh, rh); return lh; }\n__device__ __forceinline__ __half2& operator-=(__half2 &lh, const __half2 &rh) { lh = __hsub2(lh, rh); return lh; }\n__device__ __forceinline__ __half2& operator*=(__half2 &lh, const __half2 &rh) { lh = __hmul2(lh, rh); return lh; }\n__device__ __forceinline__ __half2& operator/=(__half2 &lh, const __half2 &rh) { lh = __h2div(lh, rh); return lh; }\n\n__device__ __forceinline__ __half2 &operator++(__half2 &h)      { __half2_raw one; one.x = 0x3C00U; one.y = 0x3C00U; h = __hadd2(h, one); return h; }\n__device__ __forceinline__ __half2 &operator--(__half2 &h)      { __half2_raw one; one.x = 0x3C00U; one.y = 0x3C00U; h = __hsub2(h, one); return h; }\n__device__ __forceinline__ __half2  operator++(__half2 &h, const int ignored)\n{\n    // ignored on purpose. Parameter only needed to distinguish the function declaration from other types of operators.\n    static_cast<void>(ignored);\n\n    const __half2 ret = h;\n    __half2_raw one;\n    one.x = 0x3C00U;\n    one.y = 0x3C00U;\n    h = __hadd2(h, one);\n    return ret;\n}\n__device__ __forceinline__ __half2  operator--(__half2 &h, const int ignored)\n{\n    // ignored on purpose. Parameter only needed to distinguish the function declaration from other types of operators.\n    static_cast<void>(ignored);\n\n    const __half2 ret = h;\n    __half2_raw one;\n    one.x = 0x3C00U;\n    one.y = 0x3C00U;\n    h = __hsub2(h, one);\n    return ret;\n}\n\n__device__ __forceinline__ __half2 operator+(const __half2 &h) { return h; }\n__device__ __forceinline__ __half2 operator-(const __half2 &h) { return __hneg2(h); }\n\n__device__ __forceinline__ bool operator==(const __half2 &lh, const __half2 &rh) { return __hbeq2(lh, rh); }\n__device__ __forceinline__ bool operator!=(const __half2 &lh, const __half2 &rh) { return __hbneu2(lh, rh); }\n__device__ __forceinline__ bool operator>(const __half2 &lh, const __half2 &rh) { return __hbgt2(lh, rh); }\n__device__ __forceinline__ bool operator<(const __half2 &lh, const __half2 &rh) { return __hblt2(lh, rh); }\n__device__ __forceinline__ bool operator>=(const __half2 &lh, const __half2 &rh) { return __hbge2(lh, rh); }\n__device__ __forceinline__ bool operator<=(const __half2 &lh, const __half2 &rh) { return __hble2(lh, rh); }\n\n#endif /* (!defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 530) || defined(_NVHPC_CUDA)) && !defined(__CUDA_NO_HALF2_OPERATORS__) */\n#endif /* defined(__CUDACC__) */\n\n/* Restore warning for multiple assignment operators */\n#if defined(_MSC_VER) && _MSC_VER >= 1500\n_Pragma(\"warning( pop )\")\n#endif /* defined(_MSC_VER) && _MSC_VER >= 1500 */\n\n/* Restore -Weffc++ warnings from here on */\n#if defined(__GNUC__)\n#if __GNUC__ > 4 || (__GNUC__ == 4 && __GNUC_MINOR__ >= 6)\n_Pragma(\"GCC diagnostic pop\")\n#endif /* __GNUC__ > 4 || (__GNUC__ == 4 && __GNUC_MINOR__ >= 6) */\n#endif /* defined(__GNUC__) */\n\n#undef __CUDA_HOSTDEVICE__\n#undef __CUDA_ALIGN__\n\n#ifndef __CUDACC_RTC__  /* no host functions in NVRTC mode */\nstatic inline unsigned short __internal_float2half(const float f, unsigned int &sign, unsigned int &remainder)\n{\n    unsigned int x;\n    unsigned int u;\n    unsigned int result;\n#if defined(__CUDACC__)\n    (void)memcpy(&x, &f, sizeof(f));\n#else\n    (void)std::memcpy(&x, &f, sizeof(f));\n#endif\n    u = (x & 0x7fffffffU);\n    sign = ((x >> 16U) & 0x8000U);\n    // NaN/+Inf/-Inf\n    if (u >= 0x7f800000U) {\n        remainder = 0U;\n        result = ((u == 0x7f800000U) ? (sign | 0x7c00U) : 0x7fffU);\n    } else if (u > 0x477fefffU) { // Overflows\n        remainder = 0x80000000U;\n        result = (sign | 0x7bffU);\n    } else if (u >= 0x38800000U) { // Normal numbers\n        remainder = u << 19U;\n        u -= 0x38000000U;\n        result = (sign | (u >> 13U));\n    } else if (u < 0x33000001U) { // +0/-0\n        remainder = u;\n        result = sign;\n    } else { // Denormal numbers\n        const unsigned int exponent = u >> 23U;\n        const unsigned int shift = 0x7eU - exponent;\n        unsigned int mantissa = (u & 0x7fffffU);\n        mantissa |= 0x800000U;\n        remainder = mantissa << (32U - shift);\n        result = (sign | (mantissa >> shift));\n        result &= 0x0000FFFFU;\n    }\n    return static_cast<unsigned short>(result);\n}\n#endif  /* #if !defined(__CUDACC_RTC__) */\n\n__CUDA_HOSTDEVICE_FP16_DECL__ __half __double2half(const double a)\n{\nIF_DEVICE_OR_CUDACC(\n    __half val;\n    asm(\"{  cvt.rn.f16.f64 %0, %1;}\\n\" : \"=h\"(__HALF_TO_US(val)) : \"d\"(a));\n    return val;\n,\n    __half result;\n    // Perform rounding to 11 bits of precision, convert value\n    // to float and call existing float to half conversion.\n    // By pre-rounding to 11 bits we avoid additional rounding\n    // in float to half conversion.\n    unsigned long long int absa;\n    unsigned long long int ua;\n    (void)memcpy(&ua, &a, sizeof(a));\n    absa = (ua & 0x7fffffffffffffffULL);\n    if ((absa >= 0x40f0000000000000ULL) || (absa <= 0x3e60000000000000ULL))\n    {\n        // |a| >= 2^16 or NaN or |a| <= 2^(-25)\n        // double-rounding is not a problem\n        result = __float2half(static_cast<float>(a));\n    }\n    else\n    {\n        // here 2^(-25) < |a| < 2^16\n        // prepare shifter value such that a + shifter\n        // done in double precision performs round-to-nearest-even\n        // and (a + shifter) - shifter results in a rounded to\n        // 11 bits of precision. Shifter needs to have exponent of\n        // a plus 53 - 11 = 42 and a leading bit in mantissa to guard\n        // against negative values.\n        // So need to have |a| capped to avoid overflow in exponent.\n        // For inputs that are smaller than half precision minnorm\n        // we prepare fixed shifter exponent.\n        unsigned long long shifterBits;\n        if (absa >= 0x3f10000000000000ULL)\n        {   // Here if |a| >= 2^(-14)\n            // add 42 to exponent bits\n            shifterBits  = (ua & 0x7ff0000000000000ULL) + 0x02A0000000000000ULL;\n        }\n        else\n        {   // 2^(-25) < |a| < 2^(-14), potentially results in denormal\n            // set exponent bits to 42 - 14 + bias\n            shifterBits = 0x41B0000000000000ULL;\n        }\n        // set leading mantissa bit to protect against negative inputs\n        shifterBits |= 0x0008000000000000ULL;\n        double shifter;\n        (void)memcpy(&shifter, &shifterBits, sizeof(shifterBits));\n        double aShiftRound = a + shifter;\n\n        // Prevent the compiler from optimizing away a + shifter - shifter\n        // by doing intermediate memcopy and harmless bitwize operation\n        unsigned long long int aShiftRoundBits;\n        (void)memcpy(&aShiftRoundBits, &aShiftRound, sizeof(aShiftRound));\n\n        // the value is positive, so this operation doesn't change anything\n        aShiftRoundBits &= 0x7fffffffffffffffULL;\n\n        (void)memcpy(&aShiftRound, &aShiftRoundBits, sizeof(aShiftRound));\n\n        result = __float2half(static_cast<float>(aShiftRound - shifter));\n    }\n\n    return result;\n,\n    __half result;\n    /*\n    // Perform rounding to 11 bits of precision, convert value\n    // to float and call existing float to half conversion.\n    // By pre-rounding to 11 bits we avoid additional rounding\n    // in float to half conversion.\n    */\n    unsigned long long int absa;\n    unsigned long long int ua;\n    (void)std::memcpy(&ua, &a, sizeof(a));\n    absa = (ua & 0x7fffffffffffffffULL);\n    if ((absa >= 0x40f0000000000000ULL) || (absa <= 0x3e60000000000000ULL))\n    {\n        /*\n        // |a| >= 2^16 or NaN or |a| <= 2^(-25)\n        // double-rounding is not a problem\n        */\n        result = __float2half(static_cast<float>(a));\n    }\n    else\n    {\n        /*\n        // here 2^(-25) < |a| < 2^16\n        // prepare shifter value such that a + shifter\n        // done in double precision performs round-to-nearest-even\n        // and (a + shifter) - shifter results in a rounded to\n        // 11 bits of precision. Shifter needs to have exponent of\n        // a plus 53 - 11 = 42 and a leading bit in mantissa to guard\n        // against negative values.\n        // So need to have |a| capped to avoid overflow in exponent.\n        // For inputs that are smaller than half precision minnorm\n        // we prepare fixed shifter exponent.\n        */\n        unsigned long long shifterBits;\n        if (absa >= 0x3f10000000000000ULL)\n        {\n            /*\n            // Here if |a| >= 2^(-14)\n            // add 42 to exponent bits\n            */\n            shifterBits  = (ua & 0x7ff0000000000000ULL) + 0x02A0000000000000ULL;\n        }\n        else\n        {\n            /*\n            // 2^(-25) < |a| < 2^(-14), potentially results in denormal\n            // set exponent bits to 42 - 14 + bias\n            */\n            shifterBits = 0x41B0000000000000ULL;\n        }\n        // set leading mantissa bit to protect against negative inputs\n        shifterBits |= 0x0008000000000000ULL;\n        double shifter;\n        (void)std::memcpy(&shifter, &shifterBits, sizeof(shifterBits));\n        double aShiftRound = a + shifter;\n\n        /*\n        // Prevent the compiler from optimizing away a + shifter - shifter\n        // by doing intermediate memcopy and harmless bitwize operation\n        */\n        unsigned long long int aShiftRoundBits;\n        (void)std::memcpy(&aShiftRoundBits, &aShiftRound, sizeof(aShiftRound));\n\n        // the value is positive, so this operation doesn't change anything\n        aShiftRoundBits &= 0x7fffffffffffffffULL;\n\n        (void)std::memcpy(&aShiftRound, &aShiftRoundBits, sizeof(aShiftRound));\n\n        result = __float2half(static_cast<float>(aShiftRound - shifter));\n    }\n\n    return result;\n)\n}\n\n__CUDA_HOSTDEVICE_FP16_DECL__ __half __float2half(const float a)\n{\n    __half val;\nNV_IF_ELSE_TARGET(NV_IS_DEVICE,\n    asm(\"{  cvt.rn.f16.f32 %0, %1;}\\n\" : \"=h\"(__HALF_TO_US(val)) : \"f\"(a));\n,\n    __half_raw r;\n    unsigned int sign = 0U;\n    unsigned int remainder = 0U;\n    r.x = __internal_float2half(a, sign, remainder);\n    if ((remainder > 0x80000000U) || ((remainder == 0x80000000U) && ((r.x & 0x1U) != 0U))) {\n        r.x++;\n    }\n    val = r;\n)\n    return val;\n}\n__CUDA_HOSTDEVICE_FP16_DECL__ __half __float2half_rn(const float a)\n{\n    __half val;\nNV_IF_ELSE_TARGET(NV_IS_DEVICE,\n    asm(\"{  cvt.rn.f16.f32 %0, %1;}\\n\" : \"=h\"(__HALF_TO_US(val)) : \"f\"(a));\n,\n    __half_raw r;\n    unsigned int sign = 0U;\n    unsigned int remainder = 0U;\n    r.x = __internal_float2half(a, sign, remainder);\n    if ((remainder > 0x80000000U) || ((remainder == 0x80000000U) && ((r.x & 0x1U) != 0U))) {\n        r.x++;\n    }\n    val = r;\n)\n    return val;\n}\n__CUDA_HOSTDEVICE_FP16_DECL__ __half __float2half_rz(const float a)\n{\n    __half val;\nNV_IF_ELSE_TARGET(NV_IS_DEVICE,\n    asm(\"{  cvt.rz.f16.f32 %0, %1;}\\n\" : \"=h\"(__HALF_TO_US(val)) : \"f\"(a));\n,\n    __half_raw r;\n    unsigned int sign = 0U;\n    unsigned int remainder = 0U;\n    r.x = __internal_float2half(a, sign, remainder);\n    val = r;\n)\n    return val;\n}\n__CUDA_HOSTDEVICE_FP16_DECL__ __half __float2half_rd(const float a)\n{\n    __half val;\nNV_IF_ELSE_TARGET(NV_IS_DEVICE,\n    asm(\"{  cvt.rm.f16.f32 %0, %1;}\\n\" : \"=h\"(__HALF_TO_US(val)) : \"f\"(a));\n,\n    __half_raw r;\n    unsigned int sign = 0U;\n    unsigned int remainder = 0U;\n    r.x = __internal_float2half(a, sign, remainder);\n    if ((remainder != 0U) && (sign != 0U)) {\n        r.x++;\n    }\n    val = r;\n)\n    return val;\n}\n__CUDA_HOSTDEVICE_FP16_DECL__ __half __float2half_ru(const float a)\n{\n    __half val;\nNV_IF_ELSE_TARGET(NV_IS_DEVICE,\n    asm(\"{  cvt.rp.f16.f32 %0, %1;}\\n\" : \"=h\"(__HALF_TO_US(val)) : \"f\"(a));\n,\n    __half_raw r;\n    unsigned int sign = 0U;\n    unsigned int remainder = 0U;\n    r.x = __internal_float2half(a, sign, remainder);\n    if ((remainder != 0U) && (sign == 0U)) {\n        r.x++;\n    }\n    val = r;\n)\n    return val;\n}\n__CUDA_HOSTDEVICE_FP16_DECL__ __half2 __float2half2_rn(const float a)\n{\n    __half2 val;\nNV_IF_ELSE_TARGET(NV_IS_DEVICE,\n    asm(\"{.reg .f16 low;\\n\"\n        \"  cvt.rn.f16.f32 low, %1;\\n\"\n        \"  mov.b32 %0, {low,low};}\\n\" : \"=r\"(__HALF2_TO_UI(val)) : \"f\"(a));\n,\n    val = __half2(__float2half_rn(a), __float2half_rn(a));\n)\n    return val;\n}\n\n#if defined(__CUDACC__) || defined(_NVHPC_CUDA)\n__CUDA_FP16_DECL__ __half2 __internal_device_float2_to_half2_rn(const float a, const float b) {\n    __half2 val;\nNV_IF_ELSE_TARGET(NV_PROVIDES_SM_80,\n    asm(\"{ cvt.rn.f16x2.f32 %0, %2, %1; }\\n\"\n        : \"=r\"(__HALF2_TO_UI(val)) : \"f\"(a), \"f\"(b));\n,\n    asm(\"{.reg .f16 low,high;\\n\"\n        \"  cvt.rn.f16.f32 low, %1;\\n\"\n        \"  cvt.rn.f16.f32 high, %2;\\n\"\n        \"  mov.b32 %0, {low,high};}\\n\" : \"=r\"(__HALF2_TO_UI(val)) : \"f\"(a), \"f\"(b));\n)\n    return val;\n}\n\n#endif /* defined(__CUDACC__) || defined(_NVHPC_CUDA) */\n\n__CUDA_HOSTDEVICE_FP16_DECL__ __half2 __floats2half2_rn(const float a, const float b)\n{\n    __half2 val;\nNV_IF_ELSE_TARGET(NV_IS_DEVICE,\n   val = __internal_device_float2_to_half2_rn(a,b);\n,\n    val = __half2(__float2half_rn(a), __float2half_rn(b));\n)\n    return val;\n}\n\n#ifndef __CUDACC_RTC__  /* no host functions in NVRTC mode */\nstatic inline float __internal_half2float(const unsigned short h)\n{\n    unsigned int sign = ((static_cast<unsigned int>(h) >> 15U) & 1U);\n    unsigned int exponent = ((static_cast<unsigned int>(h) >> 10U) & 0x1fU);\n    unsigned int mantissa = ((static_cast<unsigned int>(h) & 0x3ffU) << 13U);\n    float f;\n    if (exponent == 0x1fU) { /* NaN or Inf */\n        /* discard sign of a NaN */\n        sign = ((mantissa != 0U) ? (sign >> 1U) : sign);\n        mantissa = ((mantissa != 0U) ? 0x7fffffU : 0U);\n        exponent = 0xffU;\n    } else if (exponent == 0U) { /* Denorm or Zero */\n        if (mantissa != 0U) {\n            unsigned int msb;\n            exponent = 0x71U;\n            do {\n                msb = (mantissa & 0x400000U);\n                mantissa <<= 1U; /* normalize */\n                --exponent;\n            } while (msb == 0U);\n            mantissa &= 0x7fffffU; /* 1.mantissa is implicit */\n        }\n    } else {\n        exponent += 0x70U;\n    }\n    const unsigned int u = ((sign << 31U) | (exponent << 23U) | mantissa);\n#if defined(__CUDACC__)\n    (void)memcpy(&f, &u, sizeof(u));\n#else\n    (void)std::memcpy(&f, &u, sizeof(u));\n#endif\n    return f;\n}\n#endif  /* !defined(__CUDACC_RTC__) */\n\n__CUDA_HOSTDEVICE_FP16_DECL__ float __half2float(const __half a)\n{\n    float val;\nNV_IF_ELSE_TARGET(NV_IS_DEVICE,\n    asm(\"{  cvt.f32.f16 %0, %1;}\\n\" : \"=f\"(val) : \"h\"(__HALF_TO_CUS(a)));\n,\n    val = __internal_half2float(static_cast<__half_raw>(a).x);\n)\n    return val;\n}\n__CUDA_HOSTDEVICE_FP16_DECL__ float __low2float(const __half2 a)\n{\n    float val;\nNV_IF_ELSE_TARGET(NV_IS_DEVICE,\n    asm(\"{.reg .f16 low,high;\\n\"\n        \"  mov.b32 {low,high},%1;\\n\"\n        \"  cvt.f32.f16 %0, low;}\\n\" : \"=f\"(val) : \"r\"(__HALF2_TO_CUI(a)));\n,\n    val = __internal_half2float(static_cast<__half2_raw>(a).x);\n)\n    return val;\n}\n__CUDA_HOSTDEVICE_FP16_DECL__ float __high2float(const __half2 a)\n{\n    float val;\nNV_IF_ELSE_TARGET(NV_IS_DEVICE,\n    asm(\"{.reg .f16 low,high;\\n\"\n        \"  mov.b32 {low,high},%1;\\n\"\n        \"  cvt.f32.f16 %0, high;}\\n\" : \"=f\"(val) : \"r\"(__HALF2_TO_CUI(a)));\n,\n    val = __internal_half2float(static_cast<__half2_raw>(a).y);\n)\n    return val;\n}\n__CUDA_HOSTDEVICE_FP16_DECL__ short int __half2short_rz(const __half h)\n{\n    short int i;\nNV_IF_ELSE_TARGET(NV_IS_DEVICE,\n    asm(\"cvt.rzi.s16.f16 %0, %1;\" : \"=h\"(i) : \"h\"(__HALF_TO_CUS(h)));\n,\n    const float f = __half2float(h);\n    const short int max_val = (short int)0x7fffU;\n    const short int min_val = (short int)0x8000U;\n    const unsigned short bits = static_cast<unsigned short>(static_cast<__half_raw>(h).x << 1U);\n    // saturation fixup\n    if (bits > (unsigned short)0xF800U) {\n        // NaN\n        i = 0;\n    } else if (f > static_cast<float>(max_val)) {\n        // saturate maximum\n        i = max_val;\n    } else if (f < static_cast<float>(min_val)) {\n        // saturate minimum\n        i = min_val;\n    } else {\n        // normal value, conversion is well-defined\n        i = static_cast<short int>(f);\n    }\n)\n    return i;\n}\n__CUDA_HOSTDEVICE_FP16_DECL__ unsigned short int __half2ushort_rz(const __half h)\n{\n    unsigned short int i;\nNV_IF_ELSE_TARGET(NV_IS_DEVICE,\n    asm(\"cvt.rzi.u16.f16 %0, %1;\" : \"=h\"(i) : \"h\"(__HALF_TO_CUS(h)));\n,\n    const float f = __half2float(h);\n    const unsigned short int max_val = 0xffffU;\n    const unsigned short int min_val = 0U;\n    const unsigned short bits = static_cast<unsigned short>(static_cast<__half_raw>(h).x << 1U);\n    // saturation fixup\n    if (bits > (unsigned short)0xF800U) {\n        // NaN\n        i = 0U;\n    } else if (f > static_cast<float>(max_val)) {\n        // saturate maximum\n        i = max_val;\n    } else if (f < static_cast<float>(min_val)) {\n        // saturate minimum\n        i = min_val;\n    } else {\n        // normal value, conversion is well-defined\n        i = static_cast<unsigned short int>(f);\n    }\n)\n    return i;\n}\n__CUDA_HOSTDEVICE_FP16_DECL__ int __half2int_rz(const __half h)\n{\n    int i;\nNV_IF_ELSE_TARGET(NV_IS_DEVICE,\n    asm(\"cvt.rzi.s32.f16 %0, %1;\" : \"=r\"(i) : \"h\"(__HALF_TO_CUS(h)));\n,\n    const float f = __half2float(h);\n    const int max_val = (int)0x7fffffffU;\n    const int min_val = (int)0x80000000U;\n    const unsigned short bits = static_cast<unsigned short>(static_cast<__half_raw>(h).x << 1U);\n    // saturation fixup\n    if (bits > (unsigned short)0xF800U) {\n        // NaN\n        i = 0;\n    } else if (f > static_cast<float>(max_val)) {\n        // saturate maximum\n        i = max_val;\n    } else if (f < static_cast<float>(min_val)) {\n        // saturate minimum\n        i = min_val;\n    } else {\n        // normal value, conversion is well-defined\n        i = static_cast<int>(f);\n    }\n)\n    return i;\n}\n__CUDA_HOSTDEVICE_FP16_DECL__ unsigned int __half2uint_rz(const __half h)\n{\n    unsigned int i;\nNV_IF_ELSE_TARGET(NV_IS_DEVICE,\n    asm(\"cvt.rzi.u32.f16 %0, %1;\" : \"=r\"(i) : \"h\"(__HALF_TO_CUS(h)));\n,\n    const float f = __half2float(h);\n    const unsigned int max_val = 0xffffffffU;\n    const unsigned int min_val = 0U;\n    const unsigned short bits = static_cast<unsigned short>(static_cast<__half_raw>(h).x << 1U);\n    // saturation fixup\n    if (bits > (unsigned short)0xF800U) {\n        // NaN\n        i = 0U;\n    } else if (f > static_cast<float>(max_val)) {\n        // saturate maximum\n        i = max_val;\n    } else if (f < static_cast<float>(min_val)) {\n        // saturate minimum\n        i = min_val;\n    } else {\n        // normal value, conversion is well-defined\n        i = static_cast<unsigned int>(f);\n    }\n)\n    return i;\n}\n__CUDA_HOSTDEVICE_FP16_DECL__ long long int __half2ll_rz(const __half h)\n{\n    long long int i;\nNV_IF_ELSE_TARGET(NV_IS_DEVICE,\n    asm(\"cvt.rzi.s64.f16 %0, %1;\" : \"=l\"(i) : \"h\"(__HALF_TO_CUS(h)));\n,\n    const float f = __half2float(h);\n    const long long int max_val = (long long int)0x7fffffffffffffffULL;\n    const long long int min_val = (long long int)0x8000000000000000ULL;\n    const unsigned short bits = static_cast<unsigned short>(static_cast<__half_raw>(h).x << 1U);\n    // saturation fixup\n    if (bits > (unsigned short)0xF800U) {\n        // NaN\n        i = min_val;\n    } else if (f > static_cast<float>(max_val)) {\n        // saturate maximum\n        i = max_val;\n    } else if (f < static_cast<float>(min_val)) {\n        // saturate minimum\n        i = min_val;\n    } else {\n        // normal value, conversion is well-defined\n        i = static_cast<long long int>(f);\n    }\n)\n    return i;\n}\n__CUDA_HOSTDEVICE_FP16_DECL__ unsigned long long int __half2ull_rz(const __half h)\n{\n    unsigned long long int i;\nNV_IF_ELSE_TARGET(NV_IS_DEVICE,\n    asm(\"cvt.rzi.u64.f16 %0, %1;\" : \"=l\"(i) : \"h\"(__HALF_TO_CUS(h)));\n,\n    const float f = __half2float(h);\n    const unsigned long long int max_val = 0xffffffffffffffffULL;\n    const unsigned long long int min_val = 0ULL;\n    const unsigned short bits = static_cast<unsigned short>(static_cast<__half_raw>(h).x << 1U);\n    // saturation fixup\n    if (bits > (unsigned short)0xF800U) {\n        // NaN\n        i = 0x8000000000000000ULL;\n    } else if (f > static_cast<float>(max_val)) {\n        // saturate maximum\n        i = max_val;\n    } else if (f < static_cast<float>(min_val)) {\n        // saturate minimum\n        i = min_val;\n    } else {\n        // normal value, conversion is well-defined\n        i = static_cast<unsigned long long int>(f);\n    }\n)\n    return i;\n}\n\n/* Intrinsic functions only available to nvcc compilers */\n#if defined(__CUDACC__)\n\n/* CUDA vector-types compatible vector creation function (note returns __half2, not half2) */\n__VECTOR_FUNCTIONS_DECL__ __half2 make_half2(const __half x, const __half y)\n{\n    __half2 t; t.x = x; t.y = y; return t;\n}\n#undef __VECTOR_FUNCTIONS_DECL__\n\n\n/* Definitions of intrinsics */\n__CUDA_HOSTDEVICE_FP16_DECL__ __half2 __float22half2_rn(const float2 a)\n{\n    const __half2 val = __floats2half2_rn(a.x, a.y);\n    return val;\n}\n__CUDA_HOSTDEVICE_FP16_DECL__ float2 __half22float2(const __half2 a)\n{\n    float hi_float;\n    float lo_float;\nNV_IF_ELSE_TARGET(NV_IS_DEVICE,\n    asm(\"{.reg .f16 low,high;\\n\"\n        \"  mov.b32 {low,high},%1;\\n\"\n        \"  cvt.f32.f16 %0, low;}\\n\" : \"=f\"(lo_float) : \"r\"(__HALF2_TO_CUI(a)));\n\n    asm(\"{.reg .f16 low,high;\\n\"\n        \"  mov.b32 {low,high},%1;\\n\"\n        \"  cvt.f32.f16 %0, high;}\\n\" : \"=f\"(hi_float) : \"r\"(__HALF2_TO_CUI(a)));\n,\n    lo_float = __internal_half2float(((__half2_raw)a).x);\n    hi_float = __internal_half2float(((__half2_raw)a).y);\n)\n    return make_float2(lo_float, hi_float);\n}\n__CUDA_FP16_DECL__ int __half2int_rn(const __half h)\n{\n    int i;\n    asm(\"cvt.rni.s32.f16 %0, %1;\" : \"=r\"(i) : \"h\"(__HALF_TO_CUS(h)));\n    return i;\n}\n__CUDA_FP16_DECL__ int __half2int_rd(const __half h)\n{\n    int i;\n    asm(\"cvt.rmi.s32.f16 %0, %1;\" : \"=r\"(i) : \"h\"(__HALF_TO_CUS(h)));\n    return i;\n}\n__CUDA_FP16_DECL__ int __half2int_ru(const __half h)\n{\n    int i;\n    asm(\"cvt.rpi.s32.f16 %0, %1;\" : \"=r\"(i) : \"h\"(__HALF_TO_CUS(h)));\n    return i;\n}\n__CUDA_HOSTDEVICE_FP16_DECL__ __half __int2half_rn(const int i)\n{\n    __half h;\nNV_IF_ELSE_TARGET(NV_IS_DEVICE,\n    asm(\"cvt.rn.f16.s32 %0, %1;\" : \"=h\"(__HALF_TO_US(h)) : \"r\"(i));\n,\n    // double-rounding is not a problem here: if integer\n    // has more than 24 bits, it is already too large to\n    // be represented in half precision, and result will\n    // be infinity.\n    const float  f = static_cast<float>(i);\n                 h = __float2half_rn(f);\n)\n    return h;\n}\n__CUDA_FP16_DECL__ __half __int2half_rz(const int i)\n{\n    __half h;\n    asm(\"cvt.rz.f16.s32 %0, %1;\" : \"=h\"(__HALF_TO_US(h)) : \"r\"(i));\n    return h;\n}\n__CUDA_FP16_DECL__ __half __int2half_rd(const int i)\n{\n    __half h;\n    asm(\"cvt.rm.f16.s32 %0, %1;\" : \"=h\"(__HALF_TO_US(h)) : \"r\"(i));\n    return h;\n}\n__CUDA_FP16_DECL__ __half __int2half_ru(const int i)\n{\n    __half h;\n    asm(\"cvt.rp.f16.s32 %0, %1;\" : \"=h\"(__HALF_TO_US(h)) : \"r\"(i));\n    return h;\n}\n\n__CUDA_FP16_DECL__ short int __half2short_rn(const __half h)\n{\n    short int i;\n    asm(\"cvt.rni.s16.f16 %0, %1;\" : \"=h\"(i) : \"h\"(__HALF_TO_CUS(h)));\n    return i;\n}\n__CUDA_FP16_DECL__ short int __half2short_rd(const __half h)\n{\n    short int i;\n    asm(\"cvt.rmi.s16.f16 %0, %1;\" : \"=h\"(i) : \"h\"(__HALF_TO_CUS(h)));\n    return i;\n}\n__CUDA_FP16_DECL__ short int __half2short_ru(const __half h)\n{\n    short int i;\n    asm(\"cvt.rpi.s16.f16 %0, %1;\" : \"=h\"(i) : \"h\"(__HALF_TO_CUS(h)));\n    return i;\n}\n__CUDA_HOSTDEVICE_FP16_DECL__ __half __short2half_rn(const short int i)\n{\n    __half h;\nNV_IF_ELSE_TARGET(NV_IS_DEVICE,\n    asm(\"cvt.rn.f16.s16 %0, %1;\" : \"=h\"(__HALF_TO_US(h)) : \"h\"(i));\n,\n    const float  f = static_cast<float>(i);\n                 h = __float2half_rn(f);\n)\n    return h;\n}\n__CUDA_FP16_DECL__ __half __short2half_rz(const short int i)\n{\n    __half h;\n    asm(\"cvt.rz.f16.s16 %0, %1;\" : \"=h\"(__HALF_TO_US(h)) : \"h\"(i));\n    return h;\n}\n__CUDA_FP16_DECL__ __half __short2half_rd(const short int i)\n{\n    __half h;\n    asm(\"cvt.rm.f16.s16 %0, %1;\" : \"=h\"(__HALF_TO_US(h)) : \"h\"(i));\n    return h;\n}\n__CUDA_FP16_DECL__ __half __short2half_ru(const short int i)\n{\n    __half h;\n    asm(\"cvt.rp.f16.s16 %0, %1;\" : \"=h\"(__HALF_TO_US(h)) : \"h\"(i));\n    return h;\n}\n\n__CUDA_FP16_DECL__ unsigned int __half2uint_rn(const __half h)\n{\n    unsigned int i;\n    asm(\"cvt.rni.u32.f16 %0, %1;\" : \"=r\"(i) : \"h\"(__HALF_TO_CUS(h)));\n    return i;\n}\n__CUDA_FP16_DECL__ unsigned int __half2uint_rd(const __half h)\n{\n    unsigned int i;\n    asm(\"cvt.rmi.u32.f16 %0, %1;\" : \"=r\"(i) : \"h\"(__HALF_TO_CUS(h)));\n    return i;\n}\n__CUDA_FP16_DECL__ unsigned int __half2uint_ru(const __half h)\n{\n    unsigned int i;\n    asm(\"cvt.rpi.u32.f16 %0, %1;\" : \"=r\"(i) : \"h\"(__HALF_TO_CUS(h)));\n    return i;\n}\n__CUDA_HOSTDEVICE_FP16_DECL__ __half __uint2half_rn(const unsigned int i)\n{\n    __half h;\nNV_IF_ELSE_TARGET(NV_IS_DEVICE,\n    asm(\"cvt.rn.f16.u32 %0, %1;\" : \"=h\"(__HALF_TO_US(h)) : \"r\"(i));\n,\n    // double-rounding is not a problem here: if integer\n    // has more than 24 bits, it is already too large to\n    // be represented in half precision, and result will\n    // be infinity.\n    const float  f = static_cast<float>(i);\n                 h = __float2half_rn(f);\n)\n    return h;\n}\n__CUDA_FP16_DECL__ __half __uint2half_rz(const unsigned int i)\n{\n    __half h;\n    asm(\"cvt.rz.f16.u32 %0, %1;\" : \"=h\"(__HALF_TO_US(h)) : \"r\"(i));\n    return h;\n}\n__CUDA_FP16_DECL__ __half __uint2half_rd(const unsigned int i)\n{\n    __half h;\n    asm(\"cvt.rm.f16.u32 %0, %1;\" : \"=h\"(__HALF_TO_US(h)) : \"r\"(i));\n    return h;\n}\n__CUDA_FP16_DECL__ __half __uint2half_ru(const unsigned int i)\n{\n    __half h;\n    asm(\"cvt.rp.f16.u32 %0, %1;\" : \"=h\"(__HALF_TO_US(h)) : \"r\"(i));\n    return h;\n}\n\n__CUDA_FP16_DECL__ unsigned short int __half2ushort_rn(const __half h)\n{\n    unsigned short int i;\n    asm(\"cvt.rni.u16.f16 %0, %1;\" : \"=h\"(i) : \"h\"(__HALF_TO_CUS(h)));\n    return i;\n}\n__CUDA_FP16_DECL__ unsigned short int __half2ushort_rd(const __half h)\n{\n    unsigned short int i;\n    asm(\"cvt.rmi.u16.f16 %0, %1;\" : \"=h\"(i) : \"h\"(__HALF_TO_CUS(h)));\n    return i;\n}\n__CUDA_FP16_DECL__ unsigned short int __half2ushort_ru(const __half h)\n{\n    unsigned short int i;\n    asm(\"cvt.rpi.u16.f16 %0, %1;\" : \"=h\"(i) : \"h\"(__HALF_TO_CUS(h)));\n    return i;\n}\n__CUDA_HOSTDEVICE_FP16_DECL__ __half __ushort2half_rn(const unsigned short int i)\n{\n    __half h;\nNV_IF_ELSE_TARGET(NV_IS_DEVICE,\n    asm(\"cvt.rn.f16.u16 %0, %1;\" : \"=h\"(__HALF_TO_US(h)) : \"h\"(i));\n,\n    const float  f = static_cast<float>(i);\n                 h = __float2half_rn(f);\n)\n    return h;\n}\n__CUDA_FP16_DECL__ __half __ushort2half_rz(const unsigned short int i)\n{\n    __half h;\n    asm(\"cvt.rz.f16.u16 %0, %1;\" : \"=h\"(__HALF_TO_US(h)) : \"h\"(i));\n    return h;\n}\n__CUDA_FP16_DECL__ __half __ushort2half_rd(const unsigned short int i)\n{\n    __half h;\n    asm(\"cvt.rm.f16.u16 %0, %1;\" : \"=h\"(__HALF_TO_US(h)) : \"h\"(i));\n    return h;\n}\n__CUDA_FP16_DECL__ __half __ushort2half_ru(const unsigned short int i)\n{\n    __half h;\n    asm(\"cvt.rp.f16.u16 %0, %1;\" : \"=h\"(__HALF_TO_US(h)) : \"h\"(i));\n    return h;\n}\n\n__CUDA_FP16_DECL__ unsigned long long int __half2ull_rn(const __half h)\n{\n    unsigned long long int i;\n    asm(\"cvt.rni.u64.f16 %0, %1;\" : \"=l\"(i) : \"h\"(__HALF_TO_CUS(h)));\n    return i;\n}\n__CUDA_FP16_DECL__ unsigned long long int __half2ull_rd(const __half h)\n{\n    unsigned long long int i;\n    asm(\"cvt.rmi.u64.f16 %0, %1;\" : \"=l\"(i) : \"h\"(__HALF_TO_CUS(h)));\n    return i;\n}\n__CUDA_FP16_DECL__ unsigned long long int __half2ull_ru(const __half h)\n{\n    unsigned long long int i;\n    asm(\"cvt.rpi.u64.f16 %0, %1;\" : \"=l\"(i) : \"h\"(__HALF_TO_CUS(h)));\n    return i;\n}\n__CUDA_HOSTDEVICE_FP16_DECL__ __half __ull2half_rn(const unsigned long long int i)\n{\n    __half h;\nNV_IF_ELSE_TARGET(NV_IS_DEVICE,\n    asm(\"cvt.rn.f16.u64 %0, %1;\" : \"=h\"(__HALF_TO_US(h)) : \"l\"(i));\n,\n    // double-rounding is not a problem here: if integer\n    // has more than 24 bits, it is already too large to\n    // be represented in half precision, and result will\n    // be infinity.\n    const float  f = static_cast<float>(i);\n                 h = __float2half_rn(f);\n)\n    return h;\n}\n__CUDA_FP16_DECL__ __half __ull2half_rz(const unsigned long long int i)\n{\n    __half h;\n    asm(\"cvt.rz.f16.u64 %0, %1;\" : \"=h\"(__HALF_TO_US(h)) : \"l\"(i));\n    return h;\n}\n__CUDA_FP16_DECL__ __half __ull2half_rd(const unsigned long long int i)\n{\n    __half h;\n    asm(\"cvt.rm.f16.u64 %0, %1;\" : \"=h\"(__HALF_TO_US(h)) : \"l\"(i));\n    return h;\n}\n__CUDA_FP16_DECL__ __half __ull2half_ru(const unsigned long long int i)\n{\n    __half h;\n    asm(\"cvt.rp.f16.u64 %0, %1;\" : \"=h\"(__HALF_TO_US(h)) : \"l\"(i));\n    return h;\n}\n\n__CUDA_FP16_DECL__ long long int __half2ll_rn(const __half h)\n{\n    long long int i;\n    asm(\"cvt.rni.s64.f16 %0, %1;\" : \"=l\"(i) : \"h\"(__HALF_TO_CUS(h)));\n    return i;\n}\n__CUDA_FP16_DECL__ long long int __half2ll_rd(const __half h)\n{\n    long long int i;\n    asm(\"cvt.rmi.s64.f16 %0, %1;\" : \"=l\"(i) : \"h\"(__HALF_TO_CUS(h)));\n    return i;\n}\n__CUDA_FP16_DECL__ long long int __half2ll_ru(const __half h)\n{\n    long long int i;\n    asm(\"cvt.rpi.s64.f16 %0, %1;\" : \"=l\"(i) : \"h\"(__HALF_TO_CUS(h)));\n    return i;\n}\n__CUDA_HOSTDEVICE_FP16_DECL__ __half __ll2half_rn(const long long int i)\n{\n    __half h;\nNV_IF_ELSE_TARGET(NV_IS_DEVICE,\n    asm(\"cvt.rn.f16.s64 %0, %1;\" : \"=h\"(__HALF_TO_US(h)) : \"l\"(i));\n,\n    // double-rounding is not a problem here: if integer\n    // has more than 24 bits, it is already too large to\n    // be represented in half precision, and result will\n    // be infinity.\n    const float  f = static_cast<float>(i);\n                 h = __float2half_rn(f);\n)\n    return h;\n}\n__CUDA_FP16_DECL__ __half __ll2half_rz(const long long int i)\n{\n    __half h;\n    asm(\"cvt.rz.f16.s64 %0, %1;\" : \"=h\"(__HALF_TO_US(h)) : \"l\"(i));\n    return h;\n}\n__CUDA_FP16_DECL__ __half __ll2half_rd(const long long int i)\n{\n    __half h;\n    asm(\"cvt.rm.f16.s64 %0, %1;\" : \"=h\"(__HALF_TO_US(h)) : \"l\"(i));\n    return h;\n}\n__CUDA_FP16_DECL__ __half __ll2half_ru(const long long int i)\n{\n    __half h;\n    asm(\"cvt.rp.f16.s64 %0, %1;\" : \"=h\"(__HALF_TO_US(h)) : \"l\"(i));\n    return h;\n}\n\n__CUDA_FP16_DECL__ __half htrunc(const __half h)\n{\n    __half r;\n    asm(\"cvt.rzi.f16.f16 %0, %1;\" : \"=h\"(__HALF_TO_US(r)) : \"h\"(__HALF_TO_CUS(h)));\n    return r;\n}\n__CUDA_FP16_DECL__ __half hceil(const __half h)\n{\n    __half r;\n    asm(\"cvt.rpi.f16.f16 %0, %1;\" : \"=h\"(__HALF_TO_US(r)) : \"h\"(__HALF_TO_CUS(h)));\n    return r;\n}\n__CUDA_FP16_DECL__ __half hfloor(const __half h)\n{\n    __half r;\n    asm(\"cvt.rmi.f16.f16 %0, %1;\" : \"=h\"(__HALF_TO_US(r)) : \"h\"(__HALF_TO_CUS(h)));\n    return r;\n}\n__CUDA_FP16_DECL__ __half hrint(const __half h)\n{\n    __half r;\n    asm(\"cvt.rni.f16.f16 %0, %1;\" : \"=h\"(__HALF_TO_US(r)) : \"h\"(__HALF_TO_CUS(h)));\n    return r;\n}\n\n__CUDA_FP16_DECL__ __half2 h2trunc(const __half2 h)\n{\n    __half2 val;\n    asm(\"{.reg .f16 low,high;\\n\"\n        \"  mov.b32 {low,high}, %1;\\n\"\n        \"  cvt.rzi.f16.f16 low, low;\\n\"\n        \"  cvt.rzi.f16.f16 high, high;\\n\"\n        \"  mov.b32 %0, {low,high};}\\n\" : \"=r\"(__HALF2_TO_UI(val)) : \"r\"(__HALF2_TO_CUI(h)));\n    return val;\n}\n__CUDA_FP16_DECL__ __half2 h2ceil(const __half2 h)\n{\n    __half2 val;\n    asm(\"{.reg .f16 low,high;\\n\"\n        \"  mov.b32 {low,high}, %1;\\n\"\n        \"  cvt.rpi.f16.f16 low, low;\\n\"\n        \"  cvt.rpi.f16.f16 high, high;\\n\"\n        \"  mov.b32 %0, {low,high};}\\n\" : \"=r\"(__HALF2_TO_UI(val)) : \"r\"(__HALF2_TO_CUI(h)));\n    return val;\n}\n__CUDA_FP16_DECL__ __half2 h2floor(const __half2 h)\n{\n    __half2 val;\n    asm(\"{.reg .f16 low,high;\\n\"\n        \"  mov.b32 {low,high}, %1;\\n\"\n        \"  cvt.rmi.f16.f16 low, low;\\n\"\n        \"  cvt.rmi.f16.f16 high, high;\\n\"\n        \"  mov.b32 %0, {low,high};}\\n\" : \"=r\"(__HALF2_TO_UI(val)) : \"r\"(__HALF2_TO_CUI(h)));\n    return val;\n}\n__CUDA_FP16_DECL__ __half2 h2rint(const __half2 h)\n{\n    __half2 val;\n    asm(\"{.reg .f16 low,high;\\n\"\n        \"  mov.b32 {low,high}, %1;\\n\"\n        \"  cvt.rni.f16.f16 low, low;\\n\"\n        \"  cvt.rni.f16.f16 high, high;\\n\"\n        \"  mov.b32 %0, {low,high};}\\n\" : \"=r\"(__HALF2_TO_UI(val)) : \"r\"(__HALF2_TO_CUI(h)));\n    return val;\n}\n__CUDA_FP16_DECL__ __half2 __lows2half2(const __half2 a, const __half2 b)\n{\n    __half2 val;\n    asm(\"{.reg .f16 alow,ahigh,blow,bhigh;\\n\"\n        \"  mov.b32 {alow,ahigh}, %1;\\n\"\n        \"  mov.b32 {blow,bhigh}, %2;\\n\"\n        \"  mov.b32 %0, {alow,blow};}\\n\" : \"=r\"(__HALF2_TO_UI(val)) : \"r\"(__HALF2_TO_CUI(a)), \"r\"(__HALF2_TO_CUI(b)));\n    return val;\n}\n__CUDA_FP16_DECL__ __half2 __highs2half2(const __half2 a, const __half2 b)\n{\n    __half2 val;\n    asm(\"{.reg .f16 alow,ahigh,blow,bhigh;\\n\"\n        \"  mov.b32 {alow,ahigh}, %1;\\n\"\n        \"  mov.b32 {blow,bhigh}, %2;\\n\"\n        \"  mov.b32 %0, {ahigh,bhigh};}\\n\" : \"=r\"(__HALF2_TO_UI(val)) : \"r\"(__HALF2_TO_CUI(a)), \"r\"(__HALF2_TO_CUI(b)));\n    return val;\n}\n__CUDA_FP16_DECL__ __half __low2half(const __half2 a)\n{\n    __half ret;\n    asm(\"{.reg .f16 low,high;\\n\"\n        \" mov.b32 {low,high}, %1;\\n\"\n        \" mov.b16 %0, low;}\" : \"=h\"(__HALF_TO_US(ret)) : \"r\"(__HALF2_TO_CUI(a)));\n    return ret;\n}\n__CUDA_FP16_DECL__ int __hisinf(const __half a)\n{\n    int retval;\n    if (__HALF_TO_CUS(a) == 0xFC00U) {\n        retval = -1;\n    } else if (__HALF_TO_CUS(a) == 0x7C00U) {\n        retval = 1;\n    } else {\n        retval = 0;\n    }\n    return retval;\n}\n__CUDA_FP16_DECL__ __half2 __low2half2(const __half2 a)\n{\n    __half2 val;\n    asm(\"{.reg .f16 low,high;\\n\"\n        \"  mov.b32 {low,high}, %1;\\n\"\n        \"  mov.b32 %0, {low,low};}\\n\" : \"=r\"(__HALF2_TO_UI(val)) : \"r\"(__HALF2_TO_CUI(a)));\n    return val;\n}\n__CUDA_FP16_DECL__ __half2 __high2half2(const __half2 a)\n{\n    __half2 val;\n    asm(\"{.reg .f16 low,high;\\n\"\n        \"  mov.b32 {low,high}, %1;\\n\"\n        \"  mov.b32 %0, {high,high};}\\n\" : \"=r\"(__HALF2_TO_UI(val)) : \"r\"(__HALF2_TO_CUI(a)));\n    return val;\n}\n__CUDA_FP16_DECL__ __half __high2half(const __half2 a)\n{\n    __half ret;\n    asm(\"{.reg .f16 low,high;\\n\"\n        \" mov.b32 {low,high}, %1;\\n\"\n        \" mov.b16 %0, high;}\" : \"=h\"(__HALF_TO_US(ret)) : \"r\"(__HALF2_TO_CUI(a)));\n    return ret;\n}\n__CUDA_FP16_DECL__ __half2 __halves2half2(const __half a, const __half b)\n{\n    __half2 val;\n    asm(\"{  mov.b32 %0, {%1,%2};}\\n\"\n        : \"=r\"(__HALF2_TO_UI(val)) : \"h\"(__HALF_TO_CUS(a)), \"h\"(__HALF_TO_CUS(b)));\n    return val;\n}\n__CUDA_FP16_DECL__ __half2 __half2half2(const __half a)\n{\n    __half2 val;\n    asm(\"{  mov.b32 %0, {%1,%1};}\\n\"\n        : \"=r\"(__HALF2_TO_UI(val)) : \"h\"(__HALF_TO_CUS(a)));\n    return val;\n}\n__CUDA_FP16_DECL__ __half2 __lowhigh2highlow(const __half2 a)\n{\n    __half2 val;\n    asm(\"{.reg .f16 low,high;\\n\"\n        \"  mov.b32 {low,high}, %1;\\n\"\n        \"  mov.b32 %0, {high,low};}\\n\" : \"=r\"(__HALF2_TO_UI(val)) : \"r\"(__HALF2_TO_CUI(a)));\n    return val;\n}\n__CUDA_FP16_DECL__ short int __half_as_short(const __half h)\n{\n    return static_cast<short int>(__HALF_TO_CUS(h));\n}\n__CUDA_FP16_DECL__ unsigned short int __half_as_ushort(const __half h)\n{\n    return __HALF_TO_CUS(h);\n}\n__CUDA_FP16_DECL__ __half __short_as_half(const short int i)\n{\n    __half h;\n    __HALF_TO_US(h) = static_cast<unsigned short int>(i);\n    return h;\n}\n__CUDA_FP16_DECL__ __half __ushort_as_half(const unsigned short int i)\n{\n    __half h;\n    __HALF_TO_US(h) = i;\n    return h;\n}\n\n/******************************************************************************\n*                             __half arithmetic                             *\n******************************************************************************/\n__CUDA_FP16_DECL__ __half __hmax(const __half a, const __half b)\n{\n#if !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 800)\n    __BINARY_OP_HALF_MACRO(max)\n#else\n    const float fa = __half2float(a);\n    const float fb = __half2float(b);\n    float fr;\n    asm(\"{max.f32 %0,%1,%2;\\n}\"\n        :\"=f\"(fr) : \"f\"(fa), \"f\"(fb));\n    const __half hr = __float2half(fr);\n    return hr;\n#endif\n}\n__CUDA_FP16_DECL__ __half __hmin(const __half a, const __half b)\n{\n#if !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 800)\n    __BINARY_OP_HALF_MACRO(min)\n#else\n    const float fa = __half2float(a);\n    const float fb = __half2float(b);\n    float fr;\n    asm(\"{min.f32 %0,%1,%2;\\n}\"\n        :\"=f\"(fr) : \"f\"(fa), \"f\"(fb));\n    const __half hr = __float2half(fr);\n    return hr;\n#endif\n}\n\n/******************************************************************************\n*                            __half2 arithmetic                             *\n******************************************************************************/\n__CUDA_FP16_DECL__ __half2 __hmax2(const __half2 a, const __half2 b)\n{\n#if !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 800)\n    __BINARY_OP_HALF2_MACRO(max)\n#else\n    const float2 fa = __half22float2(a);\n    const float2 fb = __half22float2(b);\n    float2 fr;\n    asm(\"{max.f32 %0,%1,%2;\\n}\"\n        :\"=f\"(fr.x) : \"f\"(fa.x), \"f\"(fb.x));\n    asm(\"{max.f32 %0,%1,%2;\\n}\"\n        :\"=f\"(fr.y) : \"f\"(fa.y), \"f\"(fb.y));\n    const __half2 hr = __float22half2_rn(fr);\n    return hr;\n#endif\n}\n__CUDA_FP16_DECL__ __half2 __hmin2(const __half2 a, const __half2 b)\n{\n#if !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 800)\n    __BINARY_OP_HALF2_MACRO(min)\n#else\n    const float2 fa = __half22float2(a);\n    const float2 fb = __half22float2(b);\n    float2 fr;\n    asm(\"{min.f32 %0,%1,%2;\\n}\"\n        :\"=f\"(fr.x) : \"f\"(fa.x), \"f\"(fb.x));\n    asm(\"{min.f32 %0,%1,%2;\\n}\"\n        :\"=f\"(fr.y) : \"f\"(fa.y), \"f\"(fb.y));\n    const __half2 hr = __float22half2_rn(fr);\n    return hr;\n#endif\n}\n\n\n#if !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 300) || defined(_NVHPC_CUDA)\n/******************************************************************************\n*                           __half, __half2 warp shuffle                     *\n******************************************************************************/\n#define __SHUFFLE_HALF2_MACRO(name) /* do */ {\\\n   __half2 r; \\\n   asm volatile (\"{\" __CUDA_FP16_STRINGIFY(name) \" %0,%1,%2,%3;\\n}\" \\\n       :\"=r\"(__HALF2_TO_UI(r)): \"r\"(__HALF2_TO_CUI(var)), \"r\"(delta), \"r\"(c)); \\\n   return r; \\\n} /* while(0) */\n\n#define __SHUFFLE_SYNC_HALF2_MACRO(name) /* do */ {\\\n   __half2 r; \\\n   asm volatile (\"{\" __CUDA_FP16_STRINGIFY(name) \" %0,%1,%2,%3,%4;\\n}\" \\\n       :\"=r\"(__HALF2_TO_UI(r)): \"r\"(__HALF2_TO_CUI(var)), \"r\"(delta), \"r\"(c), \"r\"(mask)); \\\n   return r; \\\n} /* while(0) */\n\n#if defined(_NVHPC_CUDA) || !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ < 700)\n\n__CUDA_FP16_DECL__ __half2 __shfl(const __half2 var, const int delta, const int width)\n{\n    unsigned int warp_size;\n    asm(\"{mov.u32 %0, WARP_SZ;\\n}\" : \"=r\"(warp_size));\n    const unsigned int c = ((warp_size - static_cast<unsigned>(width)) << 8U) | 0x1fU;\n    __SHUFFLE_HALF2_MACRO(shfl.idx.b32)\n}\n__CUDA_FP16_DECL__ __half2 __shfl_up(const __half2 var, const unsigned int delta, const int width)\n{\n    unsigned int warp_size;\n    asm(\"{mov.u32 %0, WARP_SZ;\\n}\" : \"=r\"(warp_size));\n    const unsigned int c = (warp_size - static_cast<unsigned>(width)) << 8U;\n    __SHUFFLE_HALF2_MACRO(shfl.up.b32)\n}\n__CUDA_FP16_DECL__ __half2 __shfl_down(const __half2 var, const unsigned int delta, const int width)\n{\n    unsigned int warp_size;\n    asm(\"{mov.u32 %0, WARP_SZ;\\n}\" : \"=r\"(warp_size));\n    const unsigned int c = ((warp_size - static_cast<unsigned>(width)) << 8U) | 0x1fU;\n    __SHUFFLE_HALF2_MACRO(shfl.down.b32)\n}\n__CUDA_FP16_DECL__ __half2 __shfl_xor(const __half2 var, const int delta, const int width)\n{\n    unsigned int warp_size;\n    asm(\"{mov.u32 %0, WARP_SZ;\\n}\" : \"=r\"(warp_size));\n    const unsigned int c = ((warp_size - static_cast<unsigned>(width)) << 8U) | 0x1fU;\n    __SHUFFLE_HALF2_MACRO(shfl.bfly.b32)\n}\n\n#endif /* defined(_NVHPC_CUDA) || !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ < 700) */\n\n__CUDA_FP16_DECL__ __half2 __shfl_sync(const unsigned mask, const __half2 var, const int delta, const int width)\n{\n    unsigned int warp_size;\n    asm(\"{mov.u32 %0, WARP_SZ;\\n}\" : \"=r\"(warp_size));\n    const unsigned int c = ((warp_size - static_cast<unsigned>(width)) << 8U) | 0x1fU;\n    __SHUFFLE_SYNC_HALF2_MACRO(shfl.sync.idx.b32)\n}\n__CUDA_FP16_DECL__ __half2 __shfl_up_sync(const unsigned mask, const __half2 var, const unsigned int delta, const int width)\n{\n    unsigned int warp_size;\n    asm(\"{mov.u32 %0, WARP_SZ;\\n}\" : \"=r\"(warp_size));\n    const unsigned int c = (warp_size - static_cast<unsigned>(width)) << 8U;\n    __SHUFFLE_SYNC_HALF2_MACRO(shfl.sync.up.b32)\n}\n__CUDA_FP16_DECL__ __half2 __shfl_down_sync(const unsigned mask, const __half2 var, const unsigned int delta, const int width)\n{\n    unsigned int warp_size;\n    asm(\"{mov.u32 %0, WARP_SZ;\\n}\" : \"=r\"(warp_size));\n    const unsigned int c = ((warp_size - static_cast<unsigned>(width)) << 8U) | 0x1fU;\n    __SHUFFLE_SYNC_HALF2_MACRO(shfl.sync.down.b32)\n}\n__CUDA_FP16_DECL__ __half2 __shfl_xor_sync(const unsigned mask, const __half2 var, const int delta, const int width)\n{\n    unsigned int warp_size;\n    asm(\"{mov.u32 %0, WARP_SZ;\\n}\" : \"=r\"(warp_size));\n    const unsigned int c = ((warp_size - static_cast<unsigned>(width)) << 8U) | 0x1fU;\n    __SHUFFLE_SYNC_HALF2_MACRO(shfl.sync.bfly.b32)\n}\n\n#undef __SHUFFLE_HALF2_MACRO\n#undef __SHUFFLE_SYNC_HALF2_MACRO\n\n#if defined(_NVHPC_CUDA) || !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ < 700)\n\n__CUDA_FP16_DECL__ __half __shfl(const __half var, const int delta, const int width)\n{\n    const __half2 temp1 = __halves2half2(var, var);\n    const __half2 temp2 = __shfl(temp1, delta, width);\n    return __low2half(temp2);\n}\n__CUDA_FP16_DECL__ __half __shfl_up(const __half var, const unsigned int delta, const int width)\n{\n    const __half2 temp1 = __halves2half2(var, var);\n    const __half2 temp2 = __shfl_up(temp1, delta, width);\n    return __low2half(temp2);\n}\n__CUDA_FP16_DECL__ __half __shfl_down(const __half var, const unsigned int delta, const int width)\n{\n    const __half2 temp1 = __halves2half2(var, var);\n    const __half2 temp2 = __shfl_down(temp1, delta, width);\n    return __low2half(temp2);\n}\n__CUDA_FP16_DECL__ __half __shfl_xor(const __half var, const int delta, const int width)\n{\n    const __half2 temp1 = __halves2half2(var, var);\n    const __half2 temp2 = __shfl_xor(temp1, delta, width);\n    return __low2half(temp2);\n}\n\n#endif /* defined(_NVHPC_CUDA) || !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ < 700) */\n\n__CUDA_FP16_DECL__ __half __shfl_sync(const unsigned mask, const __half var, const int delta, const int width)\n{\n    const __half2 temp1 = __halves2half2(var, var);\n    const __half2 temp2 = __shfl_sync(mask, temp1, delta, width);\n    return __low2half(temp2);\n}\n__CUDA_FP16_DECL__ __half __shfl_up_sync(const unsigned mask, const __half var, const unsigned int delta, const int width)\n{\n    const __half2 temp1 = __halves2half2(var, var);\n    const __half2 temp2 = __shfl_up_sync(mask, temp1, delta, width);\n    return __low2half(temp2);\n}\n__CUDA_FP16_DECL__ __half __shfl_down_sync(const unsigned mask, const __half var, const unsigned int delta, const int width)\n{\n    const __half2 temp1 = __halves2half2(var, var);\n    const __half2 temp2 = __shfl_down_sync(mask, temp1, delta, width);\n    return __low2half(temp2);\n}\n__CUDA_FP16_DECL__ __half __shfl_xor_sync(const unsigned mask, const __half var, const int delta, const int width)\n{\n    const __half2 temp1 = __halves2half2(var, var);\n    const __half2 temp2 = __shfl_xor_sync(mask, temp1, delta, width);\n    return __low2half(temp2);\n}\n\n#endif /* !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 300) || defined(_NVHPC_CUDA) */\n/******************************************************************************\n*               __half and __half2 __ldg,__ldcg,__ldca,__ldcs                *\n******************************************************************************/\n\n#if defined(__cplusplus) && (!defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 320) || defined(_NVHPC_CUDA))\n#if (defined(_MSC_VER) && defined(_WIN64)) || defined(__LP64__) || defined(__CUDACC_RTC__)\n#define __LDG_PTR   \"l\"\n#else\n#define __LDG_PTR   \"r\"\n#endif /*(defined(_MSC_VER) && defined(_WIN64)) || defined(__LP64__) || defined(__CUDACC_RTC__)*/\n__CUDA_FP16_DECL__ __half2 __ldg(const  __half2 *const ptr)\n{\n    __half2 ret;\n    asm (\"ld.global.nc.b32 %0, [%1];\"  : \"=r\"(__HALF2_TO_UI(ret)) : __LDG_PTR(ptr));\n    return ret;\n}\n__CUDA_FP16_DECL__ __half __ldg(const __half *const ptr)\n{\n    __half ret;\n    asm (\"ld.global.nc.b16 %0, [%1];\"  : \"=h\"(__HALF_TO_US(ret)) : __LDG_PTR(ptr));\n    return ret;\n}\n__CUDA_FP16_DECL__ __half2 __ldcg(const  __half2 *const ptr)\n{\n    __half2 ret;\n    asm (\"ld.global.cg.b32 %0, [%1];\"  : \"=r\"(__HALF2_TO_UI(ret)) : __LDG_PTR(ptr));\n    return ret;\n}\n__CUDA_FP16_DECL__ __half __ldcg(const __half *const ptr)\n{\n    __half ret;\n    asm (\"ld.global.cg.b16 %0, [%1];\"  : \"=h\"(__HALF_TO_US(ret)) : __LDG_PTR(ptr));\n    return ret;\n}\n__CUDA_FP16_DECL__ __half2 __ldca(const  __half2 *const ptr)\n{\n    __half2 ret;\n    asm (\"ld.global.ca.b32 %0, [%1];\"  : \"=r\"(__HALF2_TO_UI(ret)) : __LDG_PTR(ptr));\n    return ret;\n}\n__CUDA_FP16_DECL__ __half __ldca(const __half *const ptr)\n{\n    __half ret;\n    asm (\"ld.global.ca.b16 %0, [%1];\"  : \"=h\"(__HALF_TO_US(ret)) : __LDG_PTR(ptr));\n    return ret;\n}\n__CUDA_FP16_DECL__ __half2 __ldcs(const  __half2 *const ptr)\n{\n    __half2 ret;\n    asm (\"ld.global.cs.b32 %0, [%1];\"  : \"=r\"(__HALF2_TO_UI(ret)) : __LDG_PTR(ptr));\n    return ret;\n}\n__CUDA_FP16_DECL__ __half __ldcs(const __half *const ptr)\n{\n    __half ret;\n    asm (\"ld.global.cs.b16 %0, [%1];\"  : \"=h\"(__HALF_TO_US(ret)) : __LDG_PTR(ptr));\n    return ret;\n}\n__CUDA_FP16_DECL__ __half2 __ldlu(const  __half2 *const ptr)\n{\n    __half2 ret;\n    asm (\"ld.global.lu.b32 %0, [%1];\"  : \"=r\"(__HALF2_TO_UI(ret)) : __LDG_PTR(ptr) : \"memory\");\n    return ret;\n}\n__CUDA_FP16_DECL__ __half __ldlu(const __half *const ptr)\n{\n    __half ret;\n    asm (\"ld.global.lu.b16 %0, [%1];\"  : \"=h\"(__HALF_TO_US(ret)) : __LDG_PTR(ptr) : \"memory\");\n    return ret;\n}\n__CUDA_FP16_DECL__ __half2 __ldcv(const  __half2 *const ptr)\n{\n    __half2 ret;\n    asm (\"ld.global.cv.b32 %0, [%1];\"  : \"=r\"(__HALF2_TO_UI(ret)) : __LDG_PTR(ptr) : \"memory\");\n    return ret;\n}\n__CUDA_FP16_DECL__ __half __ldcv(const __half *const ptr)\n{\n    __half ret;\n    asm (\"ld.global.cv.b16 %0, [%1];\"  : \"=h\"(__HALF_TO_US(ret)) : __LDG_PTR(ptr) : \"memory\");\n    return ret;\n}\n__CUDA_FP16_DECL__ void __stwb(__half2 *const ptr, const __half2 value)\n{\n    asm (\"st.global.wb.b32 [%0], %1;\"  :: __LDG_PTR(ptr), \"r\"(__HALF2_TO_CUI(value)) : \"memory\");\n}\n__CUDA_FP16_DECL__ void __stwb(__half *const ptr, const __half value)\n{\n    asm (\"st.global.wb.b16 [%0], %1;\"  :: __LDG_PTR(ptr),  \"h\"(__HALF_TO_CUS(value)) : \"memory\");\n}\n__CUDA_FP16_DECL__ void __stcg(__half2 *const ptr, const __half2 value)\n{\n    asm (\"st.global.cg.b32 [%0], %1;\"  :: __LDG_PTR(ptr), \"r\"(__HALF2_TO_CUI(value)) : \"memory\");\n}\n__CUDA_FP16_DECL__ void __stcg(__half *const ptr, const __half value)\n{\n    asm (\"st.global.cg.b16 [%0], %1;\"  :: __LDG_PTR(ptr),  \"h\"(__HALF_TO_CUS(value)) : \"memory\");\n}\n__CUDA_FP16_DECL__ void __stcs(__half2 *const ptr, const __half2 value)\n{\n    asm (\"st.global.cs.b32 [%0], %1;\"  :: __LDG_PTR(ptr), \"r\"(__HALF2_TO_CUI(value)) : \"memory\");\n}\n__CUDA_FP16_DECL__ void __stcs(__half *const ptr, const __half value)\n{\n    asm (\"st.global.cs.b16 [%0], %1;\"  :: __LDG_PTR(ptr),  \"h\"(__HALF_TO_CUS(value)) : \"memory\");\n}\n__CUDA_FP16_DECL__ void __stwt(__half2 *const ptr, const __half2 value)\n{\n    asm (\"st.global.wt.b32 [%0], %1;\"  :: __LDG_PTR(ptr), \"r\"(__HALF2_TO_CUI(value)) : \"memory\");\n}\n__CUDA_FP16_DECL__ void __stwt(__half *const ptr, const __half value)\n{\n    asm (\"st.global.wt.b16 [%0], %1;\"  :: __LDG_PTR(ptr),  \"h\"(__HALF_TO_CUS(value)) : \"memory\");\n}\n#undef __LDG_PTR\n#endif /* defined(__cplusplus) && (!defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 320) || defined(_NVHPC_CUDA)) */\n#if !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 530) || defined(_NVHPC_CUDA)\n/******************************************************************************\n*                             __half2 comparison                             *\n******************************************************************************/\n#define __COMPARISON_OP_HALF2_MACRO(name) /* do */ {\\\n   __half2 val; \\\n   asm( \"{ \" __CUDA_FP16_STRINGIFY(name) \".f16x2.f16x2 %0,%1,%2;\\n}\" \\\n        :\"=r\"(__HALF2_TO_UI(val)) : \"r\"(__HALF2_TO_CUI(a)),\"r\"(__HALF2_TO_CUI(b))); \\\n   return val; \\\n} /* while(0) */\n__CUDA_FP16_DECL__ __half2 __heq2(const __half2 a, const __half2 b)\n{\n    __COMPARISON_OP_HALF2_MACRO(set.eq)\n}\n__CUDA_FP16_DECL__ __half2 __hne2(const __half2 a, const __half2 b)\n{\n    __COMPARISON_OP_HALF2_MACRO(set.ne)\n}\n__CUDA_FP16_DECL__ __half2 __hle2(const __half2 a, const __half2 b)\n{\n    __COMPARISON_OP_HALF2_MACRO(set.le)\n}\n__CUDA_FP16_DECL__ __half2 __hge2(const __half2 a, const __half2 b)\n{\n    __COMPARISON_OP_HALF2_MACRO(set.ge)\n}\n__CUDA_FP16_DECL__ __half2 __hlt2(const __half2 a, const __half2 b)\n{\n    __COMPARISON_OP_HALF2_MACRO(set.lt)\n}\n__CUDA_FP16_DECL__ __half2 __hgt2(const __half2 a, const __half2 b)\n{\n    __COMPARISON_OP_HALF2_MACRO(set.gt)\n}\n__CUDA_FP16_DECL__ __half2 __hequ2(const __half2 a, const __half2 b)\n{\n    __COMPARISON_OP_HALF2_MACRO(set.equ)\n}\n__CUDA_FP16_DECL__ __half2 __hneu2(const __half2 a, const __half2 b)\n{\n    __COMPARISON_OP_HALF2_MACRO(set.neu)\n}\n__CUDA_FP16_DECL__ __half2 __hleu2(const __half2 a, const __half2 b)\n{\n    __COMPARISON_OP_HALF2_MACRO(set.leu)\n}\n__CUDA_FP16_DECL__ __half2 __hgeu2(const __half2 a, const __half2 b)\n{\n    __COMPARISON_OP_HALF2_MACRO(set.geu)\n}\n__CUDA_FP16_DECL__ __half2 __hltu2(const __half2 a, const __half2 b)\n{\n    __COMPARISON_OP_HALF2_MACRO(set.ltu)\n}\n__CUDA_FP16_DECL__ __half2 __hgtu2(const __half2 a, const __half2 b)\n{\n    __COMPARISON_OP_HALF2_MACRO(set.gtu)\n}\n#undef __COMPARISON_OP_HALF2_MACRO\n/******************************************************************************\n*                 __half2 comparison with mask output                        *\n******************************************************************************/\n#define __COMPARISON_OP_HALF2_MACRO_MASK(name) /* do */ {\\\n   unsigned val; \\\n   asm( \"{ \" __CUDA_FP16_STRINGIFY(name) \".u32.f16x2 %0,%1,%2;\\n}\" \\\n        :\"=r\"(__HALF2_TO_UI(val)) : \"r\"(__HALF2_TO_CUI(a)),\"r\"(__HALF2_TO_CUI(b))); \\\n   return val; \\\n} /* while(0) */\n__CUDA_FP16_DECL__ unsigned __heq2_mask(const __half2 a, const __half2 b)\n{\n    __COMPARISON_OP_HALF2_MACRO_MASK(set.eq)\n}\n__CUDA_FP16_DECL__ unsigned __hne2_mask(const __half2 a, const __half2 b)\n{\n    __COMPARISON_OP_HALF2_MACRO_MASK(set.ne)\n}\n__CUDA_FP16_DECL__ unsigned __hle2_mask(const __half2 a, const __half2 b)\n{\n    __COMPARISON_OP_HALF2_MACRO_MASK(set.le)\n}\n__CUDA_FP16_DECL__ unsigned __hge2_mask(const __half2 a, const __half2 b)\n{\n    __COMPARISON_OP_HALF2_MACRO_MASK(set.ge)\n}\n__CUDA_FP16_DECL__ unsigned __hlt2_mask(const __half2 a, const __half2 b)\n{\n    __COMPARISON_OP_HALF2_MACRO_MASK(set.lt)\n}\n__CUDA_FP16_DECL__ unsigned __hgt2_mask(const __half2 a, const __half2 b)\n{\n    __COMPARISON_OP_HALF2_MACRO_MASK(set.gt)\n}\n__CUDA_FP16_DECL__ unsigned __hequ2_mask(const __half2 a, const __half2 b)\n{\n    __COMPARISON_OP_HALF2_MACRO_MASK(set.equ)\n}\n__CUDA_FP16_DECL__ unsigned __hneu2_mask(const __half2 a, const __half2 b)\n{\n    __COMPARISON_OP_HALF2_MACRO_MASK(set.neu)\n}\n__CUDA_FP16_DECL__ unsigned __hleu2_mask(const __half2 a, const __half2 b)\n{\n    __COMPARISON_OP_HALF2_MACRO_MASK(set.leu)\n}\n__CUDA_FP16_DECL__ unsigned __hgeu2_mask(const __half2 a, const __half2 b)\n{\n    __COMPARISON_OP_HALF2_MACRO_MASK(set.geu)\n}\n__CUDA_FP16_DECL__ unsigned __hltu2_mask(const __half2 a, const __half2 b)\n{\n    __COMPARISON_OP_HALF2_MACRO_MASK(set.ltu)\n}\n__CUDA_FP16_DECL__ unsigned __hgtu2_mask(const __half2 a, const __half2 b)\n{\n    __COMPARISON_OP_HALF2_MACRO_MASK(set.gtu)\n}\n#undef __COMPARISON_OP_HALF2_MACRO_MASK\n#define __BOOL_COMPARISON_OP_HALF2_MACRO(name) /* do */ {\\\n   __half2 val; \\\n   bool retval; \\\n   asm( \"{ \" __CUDA_FP16_STRINGIFY(name) \".f16x2.f16x2 %0,%1,%2;\\n}\" \\\n        :\"=r\"(__HALF2_TO_UI(val)) : \"r\"(__HALF2_TO_CUI(a)),\"r\"(__HALF2_TO_CUI(b))); \\\n   if (__HALF2_TO_CUI(val) == 0x3C003C00U) {\\\n      retval = true; \\\n   } else { \\\n      retval = false; \\\n   }\\\n   return retval;\\\n} /* while(0) */\n__CUDA_FP16_DECL__ bool __hbeq2(const __half2 a, const __half2 b)\n{\n    __BOOL_COMPARISON_OP_HALF2_MACRO(set.eq)\n}\n__CUDA_FP16_DECL__ bool __hbne2(const __half2 a, const __half2 b)\n{\n    __BOOL_COMPARISON_OP_HALF2_MACRO(set.ne)\n}\n__CUDA_FP16_DECL__ bool __hble2(const __half2 a, const __half2 b)\n{\n    __BOOL_COMPARISON_OP_HALF2_MACRO(set.le)\n}\n__CUDA_FP16_DECL__ bool __hbge2(const __half2 a, const __half2 b)\n{\n    __BOOL_COMPARISON_OP_HALF2_MACRO(set.ge)\n}\n__CUDA_FP16_DECL__ bool __hblt2(const __half2 a, const __half2 b)\n{\n    __BOOL_COMPARISON_OP_HALF2_MACRO(set.lt)\n}\n__CUDA_FP16_DECL__ bool __hbgt2(const __half2 a, const __half2 b)\n{\n    __BOOL_COMPARISON_OP_HALF2_MACRO(set.gt)\n}\n__CUDA_FP16_DECL__ bool __hbequ2(const __half2 a, const __half2 b)\n{\n    __BOOL_COMPARISON_OP_HALF2_MACRO(set.equ)\n}\n__CUDA_FP16_DECL__ bool __hbneu2(const __half2 a, const __half2 b)\n{\n    __BOOL_COMPARISON_OP_HALF2_MACRO(set.neu)\n}\n__CUDA_FP16_DECL__ bool __hbleu2(const __half2 a, const __half2 b)\n{\n    __BOOL_COMPARISON_OP_HALF2_MACRO(set.leu)\n}\n__CUDA_FP16_DECL__ bool __hbgeu2(const __half2 a, const __half2 b)\n{\n    __BOOL_COMPARISON_OP_HALF2_MACRO(set.geu)\n}\n__CUDA_FP16_DECL__ bool __hbltu2(const __half2 a, const __half2 b)\n{\n    __BOOL_COMPARISON_OP_HALF2_MACRO(set.ltu)\n}\n__CUDA_FP16_DECL__ bool __hbgtu2(const __half2 a, const __half2 b)\n{\n    __BOOL_COMPARISON_OP_HALF2_MACRO(set.gtu)\n}\n#undef __BOOL_COMPARISON_OP_HALF2_MACRO\n/******************************************************************************\n*                             __half comparison                              *\n******************************************************************************/\n#define __COMPARISON_OP_HALF_MACRO(name) /* do */ {\\\n   unsigned short val; \\\n   asm( \"{ .reg .pred __$temp3;\\n\" \\\n        \"  setp.\" __CUDA_FP16_STRINGIFY(name) \".f16  __$temp3, %1, %2;\\n\" \\\n        \"  selp.u16 %0, 1, 0, __$temp3;}\" \\\n        : \"=h\"(val) : \"h\"(__HALF_TO_CUS(a)), \"h\"(__HALF_TO_CUS(b))); \\\n   return (val != 0U) ? true : false; \\\n} /* while(0) */\n__CUDA_FP16_DECL__ bool __heq(const __half a, const __half b)\n{\n    __COMPARISON_OP_HALF_MACRO(eq)\n}\n__CUDA_FP16_DECL__ bool __hne(const __half a, const __half b)\n{\n    __COMPARISON_OP_HALF_MACRO(ne)\n}\n__CUDA_FP16_DECL__ bool __hle(const __half a, const __half b)\n{\n    __COMPARISON_OP_HALF_MACRO(le)\n}\n__CUDA_FP16_DECL__ bool __hge(const __half a, const __half b)\n{\n    __COMPARISON_OP_HALF_MACRO(ge)\n}\n__CUDA_FP16_DECL__ bool __hlt(const __half a, const __half b)\n{\n    __COMPARISON_OP_HALF_MACRO(lt)\n}\n__CUDA_FP16_DECL__ bool __hgt(const __half a, const __half b)\n{\n    __COMPARISON_OP_HALF_MACRO(gt)\n}\n__CUDA_FP16_DECL__ bool __hequ(const __half a, const __half b)\n{\n    __COMPARISON_OP_HALF_MACRO(equ)\n}\n__CUDA_FP16_DECL__ bool __hneu(const __half a, const __half b)\n{\n    __COMPARISON_OP_HALF_MACRO(neu)\n}\n__CUDA_FP16_DECL__ bool __hleu(const __half a, const __half b)\n{\n    __COMPARISON_OP_HALF_MACRO(leu)\n}\n__CUDA_FP16_DECL__ bool __hgeu(const __half a, const __half b)\n{\n    __COMPARISON_OP_HALF_MACRO(geu)\n}\n__CUDA_FP16_DECL__ bool __hltu(const __half a, const __half b)\n{\n    __COMPARISON_OP_HALF_MACRO(ltu)\n}\n__CUDA_FP16_DECL__ bool __hgtu(const __half a, const __half b)\n{\n    __COMPARISON_OP_HALF_MACRO(gtu)\n}\n#undef __COMPARISON_OP_HALF_MACRO\n/******************************************************************************\n*                            __half2 arithmetic                             *\n******************************************************************************/\n__CUDA_FP16_DECL__ __half2 __hadd2(const __half2 a, const __half2 b)\n{\n    __BINARY_OP_HALF2_MACRO(add)\n}\n__CUDA_FP16_DECL__ __half2 __hsub2(const __half2 a, const __half2 b)\n{\n    __BINARY_OP_HALF2_MACRO(sub)\n}\n__CUDA_FP16_DECL__ __half2 __hmul2(const __half2 a, const __half2 b)\n{\n    __BINARY_OP_HALF2_MACRO(mul)\n}\n__CUDA_FP16_DECL__ __half2 __hadd2_sat(const __half2 a, const __half2 b)\n{\n    __BINARY_OP_HALF2_MACRO(add.sat)\n}\n__CUDA_FP16_DECL__ __half2 __hsub2_sat(const __half2 a, const __half2 b)\n{\n    __BINARY_OP_HALF2_MACRO(sub.sat)\n}\n__CUDA_FP16_DECL__ __half2 __hmul2_sat(const __half2 a, const __half2 b)\n{\n    __BINARY_OP_HALF2_MACRO(mul.sat)\n}\n__CUDA_FP16_DECL__ __half2 __hadd2_rn(const __half2 a, const __half2 b)\n{\n    __BINARY_OP_HALF2_MACRO(add.rn)\n}\n__CUDA_FP16_DECL__ __half2 __hsub2_rn(const __half2 a, const __half2 b)\n{\n    __BINARY_OP_HALF2_MACRO(sub.rn)\n}\n__CUDA_FP16_DECL__ __half2 __hmul2_rn(const __half2 a, const __half2 b)\n{\n    __BINARY_OP_HALF2_MACRO(mul.rn)\n}\n__CUDA_FP16_DECL__ __half2 __hfma2(const __half2 a, const __half2 b, const __half2 c)\n{\n    __TERNARY_OP_HALF2_MACRO(fma.rn)\n}\n__CUDA_FP16_DECL__ __half2 __hfma2_sat(const __half2 a, const __half2 b, const __half2 c)\n{\n    __TERNARY_OP_HALF2_MACRO(fma.rn.sat)\n}\n__CUDA_FP16_DECL__ __half2 __h2div(const __half2 a, const __half2 b) {\n    __half ha = __low2half(a);\n    __half hb = __low2half(b);\n\n    const __half v1 = __hdiv(ha, hb);\n\n    ha = __high2half(a);\n    hb = __high2half(b);\n\n    const __half v2 = __hdiv(ha, hb);\n\n    return __halves2half2(v1, v2);\n}\n/******************************************************************************\n*                             __half arithmetic                             *\n******************************************************************************/\n__CUDA_FP16_DECL__ __half __hadd(const __half a, const __half b)\n{\n    __BINARY_OP_HALF_MACRO(add)\n}\n__CUDA_FP16_DECL__ __half __hsub(const __half a, const __half b)\n{\n    __BINARY_OP_HALF_MACRO(sub)\n}\n__CUDA_FP16_DECL__ __half __hmul(const __half a, const __half b)\n{\n    __BINARY_OP_HALF_MACRO(mul)\n}\n__CUDA_FP16_DECL__ __half __hadd_sat(const __half a, const __half b)\n{\n    __BINARY_OP_HALF_MACRO(add.sat)\n}\n__CUDA_FP16_DECL__ __half __hsub_sat(const __half a, const __half b)\n{\n    __BINARY_OP_HALF_MACRO(sub.sat)\n}\n__CUDA_FP16_DECL__ __half __hmul_sat(const __half a, const __half b)\n{\n    __BINARY_OP_HALF_MACRO(mul.sat)\n}\n__CUDA_FP16_DECL__ __half __hadd_rn(const __half a, const __half b)\n{\n    __BINARY_OP_HALF_MACRO(add.rn)\n}\n__CUDA_FP16_DECL__ __half __hsub_rn(const __half a, const __half b)\n{\n    __BINARY_OP_HALF_MACRO(sub.rn)\n}\n__CUDA_FP16_DECL__ __half __hmul_rn(const __half a, const __half b)\n{\n    __BINARY_OP_HALF_MACRO(mul.rn)\n}\n__CUDA_FP16_DECL__ __half __hfma(const __half a, const __half b, const __half c)\n{\n    __TERNARY_OP_HALF_MACRO(fma.rn)\n}\n__CUDA_FP16_DECL__ __half __hfma_sat(const __half a, const __half b, const __half c)\n{\n    __TERNARY_OP_HALF_MACRO(fma.rn.sat)\n}\n__CUDA_FP16_DECL__ __half __hdiv(const __half a, const __half b) {\n    __half v;\n    __half abs;\n    __half den;\n    __HALF_TO_US(den) = 0x008FU;\n\n    float rcp;\n    const float fa = __half2float(a);\n    const float fb = __half2float(b);\n\n    asm(\"{rcp.approx.ftz.f32 %0, %1;\\n}\" :\"=f\"(rcp) : \"f\"(fb));\n\n    float fv = rcp * fa;\n\n    v = __float2half(fv);\n    abs = __habs(v);\n    if (__hlt(abs, den) && __hlt(__float2half(0.0f), abs))  {\n        const float err = __fmaf_rn(-fb, fv, fa);\n        fv = __fmaf_rn(rcp, err, fv);\n        v = __float2half(fv);\n    }\n    return v;\n}\n\n/******************************************************************************\n*                             __half2 functions                  *\n******************************************************************************/\n#define __SPEC_CASE2(i,r, spc, ulp) \\\n   \"{.reg.b32 spc, ulp, p;\\n\"\\\n   \"  mov.b32 spc,\" __CUDA_FP16_STRINGIFY(spc) \";\\n\"\\\n   \"  mov.b32 ulp,\" __CUDA_FP16_STRINGIFY(ulp) \";\\n\"\\\n   \"  set.eq.f16x2.f16x2 p,\" __CUDA_FP16_STRINGIFY(i) \", spc;\\n\"\\\n   \"  fma.rn.f16x2 \" __CUDA_FP16_STRINGIFY(r) \",p,ulp,\" __CUDA_FP16_STRINGIFY(r) \";\\n}\\n\"\n#define __SPEC_CASE(i,r, spc, ulp) \\\n   \"{.reg.b16 spc, ulp, p;\\n\"\\\n   \"  mov.b16 spc,\" __CUDA_FP16_STRINGIFY(spc) \";\\n\"\\\n   \"  mov.b16 ulp,\" __CUDA_FP16_STRINGIFY(ulp) \";\\n\"\\\n   \"  set.eq.f16.f16 p,\" __CUDA_FP16_STRINGIFY(i) \", spc;\\n\"\\\n   \"  fma.rn.f16 \" __CUDA_FP16_STRINGIFY(r) \",p,ulp,\" __CUDA_FP16_STRINGIFY(r) \";\\n}\\n\"\n#define __APPROX_FCAST(fun) /* do */ {\\\n   __half val;\\\n   asm(\"{.reg.b32         f;        \\n\"\\\n                \" .reg.b16         r;        \\n\"\\\n                \"  mov.b16         r,%1;     \\n\"\\\n                \"  cvt.f32.f16     f,r;      \\n\"\\\n                \"  \" __CUDA_FP16_STRINGIFY(fun) \".approx.ftz.f32   f,f;  \\n\"\\\n                \"  cvt.rn.f16.f32      r,f;  \\n\"\\\n                \"  mov.b16         %0,r;     \\n\"\\\n                \"}\": \"=h\"(__HALF_TO_US(val)) : \"h\"(__HALF_TO_CUS(a)));\\\n   return val;\\\n} /* while(0) */\n#define __APPROX_FCAST2(fun) /* do */ {\\\n   __half2 val;\\\n   asm(\"{.reg.b16         hl, hu;         \\n\"\\\n                \" .reg.b32         fl, fu;         \\n\"\\\n                \"  mov.b32         {hl, hu}, %1;   \\n\"\\\n                \"  cvt.f32.f16     fl, hl;         \\n\"\\\n                \"  cvt.f32.f16     fu, hu;         \\n\"\\\n                \"  \" __CUDA_FP16_STRINGIFY(fun) \".approx.ftz.f32   fl, fl;     \\n\"\\\n                \"  \" __CUDA_FP16_STRINGIFY(fun) \".approx.ftz.f32   fu, fu;     \\n\"\\\n                \"  cvt.rn.f16.f32      hl, fl;     \\n\"\\\n                \"  cvt.rn.f16.f32      hu, fu;     \\n\"\\\n                \"  mov.b32         %0, {hl, hu};   \\n\"\\\n                \"}\":\"=r\"(__HALF2_TO_UI(val)) : \"r\"(__HALF2_TO_CUI(a)));       \\\n   return val;\\\n} /* while(0) */\nstatic __device__ __forceinline__ float __float_simpl_sinf(float a);\nstatic __device__ __forceinline__ float __float_simpl_cosf(float a);\n__CUDA_FP16_DECL__ __half hsin(const __half a) {\n    const float sl = __float_simpl_sinf(__half2float(a));\n    __half r = __float2half_rn(sl);\n    asm(\"{\\n\\t\"\n        \"  .reg.b16 i,r,t;     \\n\\t\"\n        \"  mov.b16 r, %0;      \\n\\t\"\n        \"  mov.b16 i, %1;      \\n\\t\"\n        \"  and.b16 t, r, 0x8000U; \\n\\t\"\n        \"  abs.f16 r, r;   \\n\\t\"\n        \"  abs.f16 i, i;   \\n\\t\"\n        __SPEC_CASE(i, r, 0X32B3U, 0x0800U)\n        __SPEC_CASE(i, r, 0X5CB0U, 0x9000U)\n        \"  or.b16  r,r,t;      \\n\\t\"\n        \"  mov.b16 %0, r;      \\n\"\n        \"}\\n\" : \"+h\"(__HALF_TO_US(r)) : \"h\"(__HALF_TO_CUS(a)));\n    return r;\n}\n__CUDA_FP16_DECL__ __half2 h2sin(const __half2 a) {\n    const float sl = __float_simpl_sinf(__half2float(a.x));\n    const float sh = __float_simpl_sinf(__half2float(a.y));\n    __half2 r = __floats2half2_rn(sl, sh);\n    asm(\"{\\n\\t\"\n        \"  .reg.b32 i,r,t;             \\n\\t\"\n        \"  mov.b32 r, %0;              \\n\\t\"\n        \"  mov.b32 i, %1;              \\n\\t\"\n        \"  and.b32 t, r, 0x80008000U;   \\n\\t\"\n        \"  abs.f16x2 r, r;   \\n\\t\"\n        \"  abs.f16x2 i, i;   \\n\\t\"\n        __SPEC_CASE2(i, r, 0X32B332B3U, 0x08000800U)\n        __SPEC_CASE2(i, r, 0X5CB05CB0U, 0x90009000U)\n        \"  or.b32  r, r, t;            \\n\\t\"\n        \"  mov.b32 %0, r;              \\n\"\n        \"}\\n\" : \"+r\"(__HALF2_TO_UI(r)) : \"r\"(__HALF2_TO_CUI(a)));\n    return r;\n}\n__CUDA_FP16_DECL__ __half hcos(const __half a) {\n    const float cl = __float_simpl_cosf(__half2float(a));\n    __half r = __float2half_rn(cl);\n    asm(\"{\\n\\t\"\n        \"  .reg.b16 i,r;        \\n\\t\"\n        \"  mov.b16 r, %0;       \\n\\t\"\n        \"  mov.b16 i, %1;       \\n\\t\"\n        \"  abs.f16 i, i;        \\n\\t\"\n        __SPEC_CASE(i, r, 0X2B7CU, 0x1000U)\n        \"  mov.b16 %0, r;       \\n\"\n        \"}\\n\" : \"+h\"(__HALF_TO_US(r)) : \"h\"(__HALF_TO_CUS(a)));\n    return r;\n}\n__CUDA_FP16_DECL__ __half2 h2cos(const __half2 a) {\n    const float cl = __float_simpl_cosf(__half2float(a.x));\n    const float ch = __float_simpl_cosf(__half2float(a.y));\n    __half2 r = __floats2half2_rn(cl, ch);\n    asm(\"{\\n\\t\"\n        \"  .reg.b32 i,r;   \\n\\t\"\n        \"  mov.b32 r, %0;  \\n\\t\"\n        \"  mov.b32 i, %1;  \\n\\t\"\n        \"  abs.f16x2 i, i; \\n\\t\"\n        __SPEC_CASE2(i, r, 0X2B7C2B7CU, 0x10001000U)\n        \"  mov.b32 %0, r;  \\n\"\n        \"}\\n\" : \"+r\"(__HALF2_TO_UI(r)) : \"r\"(__HALF2_TO_CUI(a)));\n    return r;\n}\nstatic __device__ __forceinline__ float __internal_trig_reduction_kernel(const float a, unsigned int *const quadrant)\n{\n    const float ar = __fmaf_rn(a, 0.636619772F, 12582912.0F);\n    const unsigned q = __float_as_uint(ar);\n    const float j = __fsub_rn(ar, 12582912.0F);\n    float t = __fmaf_rn(j, -1.5707962512969971e+000F, a);\n    t = __fmaf_rn(j, -7.5497894158615964e-008F, t);\n    *quadrant = q;\n    return t;\n}\nstatic __device__ __forceinline__ float __internal_sin_cos_kernel(const float x, const unsigned int i)\n{\n    float z;\n    const float x2 = x*x;\n    float a8;\n    float a6;\n    float a4;\n    float a2;\n    float a1;\n    float a0;\n\n    if ((i & 1U) != 0U) {\n        // cos\n        a8 =  2.44331571e-5F;\n        a6 = -1.38873163e-3F;\n        a4 =  4.16666457e-2F;\n        a2 = -5.00000000e-1F;\n        a1 = x2;\n        a0 = 1.0F;\n    }\n    else {\n        // sin\n        a8 = -1.95152959e-4F;\n        a6 =  8.33216087e-3F;\n        a4 = -1.66666546e-1F;\n        a2 = 0.0F;\n        a1 = x;\n        a0 = x;\n    }\n\n    z = __fmaf_rn(a8, x2, a6);\n    z = __fmaf_rn(z, x2, a4);\n    z = __fmaf_rn(z, x2, a2);\n    z = __fmaf_rn(z, a1, a0);\n\n    if ((i & 2U) != 0U) {\n        z = -z;\n    }\n    return z;\n}\nstatic __device__ __forceinline__ float __float_simpl_sinf(float a)\n{\n    float z;\n    unsigned i;\n    a = __internal_trig_reduction_kernel(a, &i);\n    z = __internal_sin_cos_kernel(a, i);\n    return z;\n}\nstatic __device__ __forceinline__ float __float_simpl_cosf(float a)\n{\n    float z;\n    unsigned i;\n    a = __internal_trig_reduction_kernel(a, &i);\n    z = __internal_sin_cos_kernel(a, (i & 0x3U) + 1U);\n    return z;\n}\n\n__CUDA_FP16_DECL__ __half hexp(const __half a) {\n    __half val;\n    asm(\"{.reg.b32         f, C, nZ;       \\n\"\n        \" .reg.b16         h,r;            \\n\"\n        \"  mov.b16         h,%1;           \\n\"\n        \"  cvt.f32.f16     f,h;            \\n\"\n        \"  mov.b32         C, 0x3fb8aa3bU; \\n\"\n        \"  mov.b32         nZ, 0x80000000U;\\n\"\n        \"  fma.rn.f32      f,f,C,nZ;       \\n\"\n        \"  ex2.approx.ftz.f32  f,f;        \\n\"\n        \"  cvt.rn.f16.f32      r,f;        \\n\"\n        __SPEC_CASE(h, r, 0X1F79U, 0x9400U)\n        __SPEC_CASE(h, r, 0X25CFU, 0x9400U)\n        __SPEC_CASE(h, r, 0XC13BU, 0x0400U)\n        __SPEC_CASE(h, r, 0XC1EFU, 0x0200U)\n        \"  mov.b16         %0,r;           \\n\"\n        \"}\": \"=h\"(__HALF_TO_US(val)) : \"h\"(__HALF_TO_CUS(a)));\n    return val;\n}\n__CUDA_FP16_DECL__ __half2 h2exp(const __half2 a) {\n    __half2 val;\n    asm(\"{.reg.b16         hl, hu;         \\n\"\n        \" .reg.b32         h,r,fl,fu,C,nZ; \\n\"\n        \"  mov.b32         {hl, hu}, %1;   \\n\"\n        \"  mov.b32         h, %1;          \\n\"\n        \"  cvt.f32.f16     fl, hl;         \\n\"\n        \"  cvt.f32.f16     fu, hu;         \\n\"\n        \"  mov.b32         C, 0x3fb8aa3bU; \\n\"\n        \"  mov.b32         nZ, 0x80000000U;\\n\"\n        \"  fma.rn.f32      fl,fl,C,nZ;     \\n\"\n        \"  fma.rn.f32      fu,fu,C,nZ;     \\n\"\n        \"  ex2.approx.ftz.f32  fl, fl;     \\n\"\n        \"  ex2.approx.ftz.f32  fu, fu;     \\n\"\n        \"  cvt.rn.f16.f32      hl, fl;     \\n\"\n        \"  cvt.rn.f16.f32      hu, fu;     \\n\"\n        \"  mov.b32         r, {hl, hu};    \\n\"\n        __SPEC_CASE2(h, r, 0X1F791F79U, 0x94009400U)\n        __SPEC_CASE2(h, r, 0X25CF25CFU, 0x94009400U)\n        __SPEC_CASE2(h, r, 0XC13BC13BU, 0x04000400U)\n        __SPEC_CASE2(h, r, 0XC1EFC1EFU, 0x02000200U)\n        \"  mov.b32         %0, r;  \\n\"\n        \"}\":\"=r\"(__HALF2_TO_UI(val)) : \"r\"(__HALF2_TO_CUI(a)));\n    return val;\n}\n__CUDA_FP16_DECL__ __half hexp2(const __half a) {\n    __half val;\n    asm(\"{.reg.b32         f, ULP;         \\n\"\n        \" .reg.b16         r;              \\n\"\n        \"  mov.b16         r,%1;           \\n\"\n        \"  cvt.f32.f16     f,r;            \\n\"\n        \"  ex2.approx.ftz.f32      f,f;    \\n\"\n        \"  mov.b32         ULP, 0x33800000U;\\n\"\n        \"  fma.rn.f32      f,f,ULP,f;      \\n\"\n        \"  cvt.rn.f16.f32      r,f;        \\n\"\n        \"  mov.b16         %0,r;           \\n\"\n        \"}\": \"=h\"(__HALF_TO_US(val)) : \"h\"(__HALF_TO_CUS(a)));\n    return val;\n}\n__CUDA_FP16_DECL__ __half2 h2exp2(const __half2 a) {\n    __half2 val;\n    asm(\"{.reg.b16         hl, hu;         \\n\"\n        \" .reg.b32         fl, fu, ULP;    \\n\"\n        \"  mov.b32         {hl, hu}, %1;   \\n\"\n        \"  cvt.f32.f16     fl, hl;         \\n\"\n        \"  cvt.f32.f16     fu, hu;         \\n\"\n        \"  ex2.approx.ftz.f32  fl, fl;     \\n\"\n        \"  ex2.approx.ftz.f32  fu, fu;     \\n\"\n        \"  mov.b32         ULP, 0x33800000U;\\n\"\n        \"  fma.rn.f32      fl,fl,ULP,fl;   \\n\"\n        \"  fma.rn.f32      fu,fu,ULP,fu;   \\n\"\n        \"  cvt.rn.f16.f32      hl, fl;     \\n\"\n        \"  cvt.rn.f16.f32      hu, fu;     \\n\"\n        \"  mov.b32         %0, {hl, hu};   \\n\"\n        \"}\":\"=r\"(__HALF2_TO_UI(val)) : \"r\"(__HALF2_TO_CUI(a)));\n    return val;\n}\n__CUDA_FP16_DECL__ __half hexp10(const __half a) {\n    __half val;\n    asm(\"{.reg.b16         h,r;            \\n\"\n        \" .reg.b32         f, C, nZ;       \\n\"\n        \"  mov.b16         h, %1;          \\n\"\n        \"  cvt.f32.f16     f, h;           \\n\"\n        \"  mov.b32         C, 0x40549A78U; \\n\"\n        \"  mov.b32         nZ, 0x80000000U;\\n\"\n        \"  fma.rn.f32      f,f,C,nZ;       \\n\"\n        \"  ex2.approx.ftz.f32  f, f;       \\n\"\n        \"  cvt.rn.f16.f32      r, f;       \\n\"\n        __SPEC_CASE(h, r, 0x34DEU, 0x9800U)\n        __SPEC_CASE(h, r, 0x9766U, 0x9000U)\n        __SPEC_CASE(h, r, 0x9972U, 0x1000U)\n        __SPEC_CASE(h, r, 0xA5C4U, 0x1000U)\n        __SPEC_CASE(h, r, 0xBF0AU, 0x8100U)\n        \"  mov.b16         %0, r;          \\n\"\n        \"}\":\"=h\"(__HALF_TO_US(val)) : \"h\"(__HALF_TO_CUS(a)));\n    return val;\n}\n__CUDA_FP16_DECL__ __half2 h2exp10(const __half2 a) {\n    __half2 val;\n    asm(\"{.reg.b16         hl, hu;         \\n\"\n        \" .reg.b32         h,r,fl,fu,C,nZ; \\n\"\n        \"  mov.b32         {hl, hu}, %1;   \\n\"\n        \"  mov.b32         h, %1;          \\n\"\n        \"  cvt.f32.f16     fl, hl;         \\n\"\n        \"  cvt.f32.f16     fu, hu;         \\n\"\n        \"  mov.b32         C, 0x40549A78U; \\n\"\n        \"  mov.b32         nZ, 0x80000000U;\\n\"\n        \"  fma.rn.f32      fl,fl,C,nZ;     \\n\"\n        \"  fma.rn.f32      fu,fu,C,nZ;     \\n\"\n        \"  ex2.approx.ftz.f32  fl, fl;     \\n\"\n        \"  ex2.approx.ftz.f32  fu, fu;     \\n\"\n        \"  cvt.rn.f16.f32      hl, fl;     \\n\"\n        \"  cvt.rn.f16.f32      hu, fu;     \\n\"\n        \"  mov.b32         r, {hl, hu};    \\n\"\n        __SPEC_CASE2(h, r, 0x34DE34DEU, 0x98009800U)\n        __SPEC_CASE2(h, r, 0x97669766U, 0x90009000U)\n        __SPEC_CASE2(h, r, 0x99729972U, 0x10001000U)\n        __SPEC_CASE2(h, r, 0xA5C4A5C4U, 0x10001000U)\n        __SPEC_CASE2(h, r, 0xBF0ABF0AU, 0x81008100U)\n        \"  mov.b32         %0, r;  \\n\"\n        \"}\":\"=r\"(__HALF2_TO_UI(val)) : \"r\"(__HALF2_TO_CUI(a)));\n    return val;\n}\n__CUDA_FP16_DECL__ __half hlog2(const __half a) {\n    __half val;\n    asm(\"{.reg.b16         h, r;           \\n\"\n        \" .reg.b32         f;              \\n\"\n        \"  mov.b16         h, %1;          \\n\"\n        \"  cvt.f32.f16     f, h;           \\n\"\n        \"  lg2.approx.ftz.f32  f, f;       \\n\"\n        \"  cvt.rn.f16.f32      r, f;       \\n\"\n        __SPEC_CASE(r, r, 0xA2E2U, 0x8080U)\n        __SPEC_CASE(r, r, 0xBF46U, 0x9400U)\n        \"  mov.b16         %0, r;          \\n\"\n        \"}\":\"=h\"(__HALF_TO_US(val)) : \"h\"(__HALF_TO_CUS(a)));\n    return val;\n}\n__CUDA_FP16_DECL__ __half2 h2log2(const __half2 a) {\n    __half2 val;\n    asm(\"{.reg.b16         hl, hu;         \\n\"\n        \" .reg.b32         fl, fu, r, p;   \\n\"\n        \"  mov.b32         {hl, hu}, %1;   \\n\"\n        \"  cvt.f32.f16     fl, hl;         \\n\"\n        \"  cvt.f32.f16     fu, hu;         \\n\"\n        \"  lg2.approx.ftz.f32  fl, fl;     \\n\"\n        \"  lg2.approx.ftz.f32  fu, fu;     \\n\"\n        \"  cvt.rn.f16.f32      hl, fl;     \\n\"\n        \"  cvt.rn.f16.f32      hu, fu;     \\n\"\n        \"  mov.b32         r, {hl, hu};    \\n\"\n        __SPEC_CASE2(r, r, 0xA2E2A2E2U, 0x80808080U)\n        __SPEC_CASE2(r, r, 0xBF46BF46U, 0x94009400U)\n        \"  mov.b32         %0, r;          \\n\"\n        \"}\":\"=r\"(__HALF2_TO_UI(val)) : \"r\"(__HALF2_TO_CUI(a)));\n    return val;\n}\n__CUDA_FP16_DECL__ __half hlog(const __half a) {\n    __half val;\n    asm(\"{.reg.b32         f, C;           \\n\"\n        \" .reg.b16         r,h;            \\n\"\n        \"  mov.b16         h,%1;           \\n\"\n        \"  cvt.f32.f16     f,h;            \\n\"\n        \"  lg2.approx.ftz.f32  f,f;        \\n\"\n        \"  mov.b32         C, 0x3f317218U;  \\n\"\n        \"  mul.f32         f,f,C;          \\n\"\n        \"  cvt.rn.f16.f32      r,f;        \\n\"\n        __SPEC_CASE(h, r, 0X160DU, 0x9C00U)\n        __SPEC_CASE(h, r, 0X3BFEU, 0x8010U)\n        __SPEC_CASE(h, r, 0X3C0BU, 0x8080U)\n        __SPEC_CASE(h, r, 0X6051U, 0x1C00U)\n        \"  mov.b16         %0,r;           \\n\"\n        \"}\": \"=h\"(__HALF_TO_US(val)) : \"h\"(__HALF_TO_CUS(a)));\n    return val;\n}\n__CUDA_FP16_DECL__ __half2 h2log(const __half2 a) {\n    __half2 val;\n    asm(\"{.reg.b16         hl, hu;             \\n\"\n        \" .reg.b32         r, fl, fu, C, h;    \\n\"\n        \"  mov.b32         {hl, hu}, %1;       \\n\"\n        \"  mov.b32         h, %1;              \\n\"\n        \"  cvt.f32.f16     fl, hl;             \\n\"\n        \"  cvt.f32.f16     fu, hu;             \\n\"\n        \"  lg2.approx.ftz.f32  fl, fl;         \\n\"\n        \"  lg2.approx.ftz.f32  fu, fu;         \\n\"\n        \"  mov.b32         C, 0x3f317218U;     \\n\"\n        \"  mul.f32         fl,fl,C;            \\n\"\n        \"  mul.f32         fu,fu,C;            \\n\"\n        \"  cvt.rn.f16.f32      hl, fl;         \\n\"\n        \"  cvt.rn.f16.f32      hu, fu;         \\n\"\n        \"  mov.b32         r, {hl, hu};        \\n\"\n        __SPEC_CASE2(h, r, 0X160D160DU, 0x9C009C00U)\n        __SPEC_CASE2(h, r, 0X3BFE3BFEU, 0x80108010U)\n        __SPEC_CASE2(h, r, 0X3C0B3C0BU, 0x80808080U)\n        __SPEC_CASE2(h, r, 0X60516051U, 0x1C001C00U)\n        \"  mov.b32         %0, r;              \\n\"\n        \"}\":\"=r\"(__HALF2_TO_UI(val)) : \"r\"(__HALF2_TO_CUI(a)));\n    return val;\n}\n__CUDA_FP16_DECL__ __half hlog10(const __half a) {\n    __half val;\n    asm(\"{.reg.b16         h, r;           \\n\"\n        \" .reg.b32         f, C;           \\n\"\n        \"  mov.b16         h, %1;          \\n\"\n        \"  cvt.f32.f16     f, h;           \\n\"\n        \"  lg2.approx.ftz.f32  f, f;       \\n\"\n        \"  mov.b32         C, 0x3E9A209BU; \\n\"\n        \"  mul.f32         f,f,C;          \\n\"\n        \"  cvt.rn.f16.f32      r, f;       \\n\"\n        __SPEC_CASE(h, r, 0x338FU, 0x1000U)\n        __SPEC_CASE(h, r, 0x33F8U, 0x9000U)\n        __SPEC_CASE(h, r, 0x57E1U, 0x9800U)\n        __SPEC_CASE(h, r, 0x719DU, 0x9C00U)\n        \"  mov.b16         %0, r;          \\n\"\n        \"}\":\"=h\"(__HALF_TO_US(val)) : \"h\"(__HALF_TO_CUS(a)));\n    return val;\n}\n__CUDA_FP16_DECL__ __half2 h2log10(const __half2 a) {\n    __half2 val;\n    asm(\"{.reg.b16         hl, hu;             \\n\"\n        \" .reg.b32         r, fl, fu, C, h;    \\n\"\n        \"  mov.b32         {hl, hu}, %1;       \\n\"\n        \"  mov.b32         h, %1;              \\n\"\n        \"  cvt.f32.f16     fl, hl;             \\n\"\n        \"  cvt.f32.f16     fu, hu;             \\n\"\n        \"  lg2.approx.ftz.f32  fl, fl;         \\n\"\n        \"  lg2.approx.ftz.f32  fu, fu;         \\n\"\n        \"  mov.b32         C, 0x3E9A209BU;     \\n\"\n        \"  mul.f32         fl,fl,C;            \\n\"\n        \"  mul.f32         fu,fu,C;            \\n\"\n        \"  cvt.rn.f16.f32      hl, fl;         \\n\"\n        \"  cvt.rn.f16.f32      hu, fu;         \\n\"\n        \"  mov.b32         r, {hl, hu};        \\n\"\n        __SPEC_CASE2(h, r, 0x338F338FU, 0x10001000U)\n        __SPEC_CASE2(h, r, 0x33F833F8U, 0x90009000U)\n        __SPEC_CASE2(h, r, 0x57E157E1U, 0x98009800U)\n        __SPEC_CASE2(h, r, 0x719D719DU, 0x9C009C00U)\n        \"  mov.b32         %0, r;              \\n\"\n        \"}\":\"=r\"(__HALF2_TO_UI(val)) : \"r\"(__HALF2_TO_CUI(a)));\n    return val;\n}\n#undef __SPEC_CASE2\n#undef __SPEC_CASE\n__CUDA_FP16_DECL__ __half2 h2rcp(const __half2 a) {\n    __APPROX_FCAST2(rcp)\n}\n__CUDA_FP16_DECL__ __half hrcp(const __half a) {\n    __APPROX_FCAST(rcp)\n}\n__CUDA_FP16_DECL__ __half2 h2rsqrt(const __half2 a) {\n    __APPROX_FCAST2(rsqrt)\n}\n__CUDA_FP16_DECL__ __half hrsqrt(const __half a) {\n    __APPROX_FCAST(rsqrt)\n}\n__CUDA_FP16_DECL__ __half2 h2sqrt(const __half2 a) {\n    __APPROX_FCAST2(sqrt)\n}\n__CUDA_FP16_DECL__ __half hsqrt(const __half a) {\n    __APPROX_FCAST(sqrt)\n}\n#undef __APPROX_FCAST\n#undef __APPROX_FCAST2\n__CUDA_FP16_DECL__ __half2 __hisnan2(const __half2 a)\n{\n    __half2 r;\n    asm(\"{set.nan.f16x2.f16x2 %0,%1,%2;\\n}\"\n        :\"=r\"(__HALF2_TO_UI(r)) : \"r\"(__HALF2_TO_CUI(a)), \"r\"(__HALF2_TO_CUI(a)));\n    return r;\n}\n__CUDA_FP16_DECL__ bool __hisnan(const __half a)\n{\n    __half r;\n    asm(\"{set.nan.f16.f16 %0,%1,%2;\\n}\"\n        :\"=h\"(__HALF_TO_US(r)) : \"h\"(__HALF_TO_CUS(a)), \"h\"(__HALF_TO_CUS(a)));\n    return __HALF_TO_CUS(r) != 0U;\n}\n__CUDA_FP16_DECL__ __half2 __hneg2(const __half2 a)\n{\n    __half2 r;\n    asm(\"{neg.f16x2 %0,%1;\\n}\"\n        :\"=r\"(__HALF2_TO_UI(r)) : \"r\"(__HALF2_TO_CUI(a)));\n    return r;\n}\n__CUDA_FP16_DECL__ __half __hneg(const __half a)\n{\n    __half r;\n    asm(\"{neg.f16 %0,%1;\\n}\"\n        :\"=h\"(__HALF_TO_US(r)) : \"h\"(__HALF_TO_CUS(a)));\n    return r;\n}\n__CUDA_FP16_DECL__ __half2 __habs2(const __half2 a)\n{\n    __half2 r;\n    asm(\"{abs.f16x2 %0,%1;\\n}\"\n        :\"=r\"(__HALF2_TO_UI(r)) : \"r\"(__HALF2_TO_CUI(a)));\n    return r;\n}\n__CUDA_FP16_DECL__ __half __habs(const __half a)\n{\n    __half r;\n    asm(\"{abs.f16 %0,%1;\\n}\"\n        :\"=h\"(__HALF_TO_US(r)) : \"h\"(__HALF_TO_CUS(a)));\n    return r;\n}\n\n__CUDA_FP16_DECL__ __half2 __hcmadd(const __half2 a, const __half2 b, const __half2 c)\n{\n    // fast version of complex multiply-accumulate\n    // (a.re, a.im) * (b.re, b.im) + (c.re, c.im)\n    // acc.re = (c.re + a.re*b.re) - a.im*b.im\n    // acc.im = (c.im + a.re*b.im) + a.im*b.re\n    __half real_tmp =  __hfma(a.x, b.x, c.x);\n    __half img_tmp  =  __hfma(a.x, b.y, c.y);\n    real_tmp = __hfma(__hneg(a.y), b.y, real_tmp);\n    img_tmp  = __hfma(a.y,         b.x, img_tmp);\n    return make_half2(real_tmp, img_tmp);\n}\n\n#endif /* !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 530) || defined(_NVHPC_CUDA) */\n\n#if defined(_NVHPC_CUDA) || !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 800)\n__CUDA_FP16_DECL__ __half __hmax_nan(const __half a, const __half b)\n{\n    __BINARY_OP_HALF_MACRO(max.NaN)\n}\n__CUDA_FP16_DECL__ __half __hmin_nan(const __half a, const __half b)\n{\n    __BINARY_OP_HALF_MACRO(min.NaN)\n}\n__CUDA_FP16_DECL__ __half __hfma_relu(const __half a, const __half b, const __half c)\n{\n    __TERNARY_OP_HALF_MACRO(fma.rn.relu)\n}\n\n__CUDA_FP16_DECL__ __half2 __hmax2_nan(const __half2 a, const __half2 b)\n{\n    __BINARY_OP_HALF2_MACRO(max.NaN)\n}\n__CUDA_FP16_DECL__ __half2 __hmin2_nan(const __half2 a, const __half2 b)\n{\n    __BINARY_OP_HALF2_MACRO(min.NaN)\n}\n__CUDA_FP16_DECL__ __half2 __hfma2_relu(const __half2 a, const __half2 b, const __half2 c)\n{\n    __TERNARY_OP_HALF2_MACRO(fma.rn.relu)\n}\n#endif /*defined(_NVHPC_CUDA) || !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 800)*/\n\n/* Define __PTR for atomicAdd prototypes below, undef after done */\n#if (defined(_MSC_VER) && defined(_WIN64)) || defined(__LP64__) || defined(__CUDACC_RTC__)\n#define __PTR   \"l\"\n#else\n#define __PTR   \"r\"\n#endif /*(defined(_MSC_VER) && defined(_WIN64)) || defined(__LP64__) || defined(__CUDACC_RTC__)*/\n\n#if defined(_NVHPC_CUDA) || !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 600)\n\n__CUDA_FP16_DECL__  __half2 atomicAdd(__half2 *const address, const __half2 val) {\n    __half2 r;\n    asm volatile (\"{ atom.add.noftz.f16x2 %0,[%1],%2; }\\n\"\n                  : \"=r\"(__HALF2_TO_UI(r)) : __PTR(address), \"r\"(__HALF2_TO_CUI(val))\n                  : \"memory\");\n   return r;\n}\n\n#endif /*defined(_NVHPC_CUDA) || !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 600)*/\n\n#if defined(_NVHPC_CUDA) || !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 700)\n\n__CUDA_FP16_DECL__  __half atomicAdd(__half *const address, const __half val) {\n    __half r;\n    asm volatile (\"{ atom.add.noftz.f16 %0,[%1],%2; }\\n\"\n                  : \"=h\"(__HALF_TO_US(r))\n                  : __PTR(address), \"h\"(__HALF_TO_CUS(val))\n                  : \"memory\");\n   return r;\n}\n\n#endif /*defined(_NVHPC_CUDA) || !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 700)*/\n\n#undef __PTR\n\n#undef __CUDA_FP16_DECL__\n#endif /* defined(__CUDACC__) */\n#endif /* defined(__cplusplus) */\n\n#undef __TERNARY_OP_HALF2_MACRO\n#undef __TERNARY_OP_HALF_MACRO\n#undef __BINARY_OP_HALF2_MACRO\n#undef __BINARY_OP_HALF_MACRO\n\n#undef __CUDA_HOSTDEVICE_FP16_DECL__\n#undef __CUDA_FP16_DECL__\n\n#undef __HALF_TO_US\n#undef __HALF_TO_CUS\n#undef __HALF2_TO_UI\n#undef __HALF2_TO_CUI\n\n/* Define first-class types \"half\" and \"half2\", unless user specifies otherwise via \"#define CUDA_NO_HALF\" */\n/* C cannot ever have these types defined here, because __half and __half2 are C++ classes */\n#if defined(__cplusplus) && !defined(CUDA_NO_HALF)\ntypedef __half half;\ntypedef __half2 half2;\n// for consistency with __nv_bfloat16\ntypedef __half      __nv_half;\ntypedef __half2     __nv_half2;\ntypedef __half_raw  __nv_half_raw;\ntypedef __half2_raw __nv_half2_raw;\ntypedef __half        nv_half;\ntypedef __half2       nv_half2;\n#endif /* defined(__cplusplus) && !defined(CUDA_NO_HALF) */\n\n#if defined(__CPP_VERSION_AT_LEAST_11_FP16)\n#undef __CPP_VERSION_AT_LEAST_11_FP16\n#endif /* defined(__CPP_VERSION_AT_LEAST_11_FP16) */\n\n#endif /* end of include guard: __CUDA_FP16_HPP__ */\n", "cupy/cuda_workaround.h": "#ifndef _JITIFY_INCLUDE_GUARD_297548E68FAE3341\n#define _JITIFY_INCLUDE_GUARD_297548E68FAE3341\n#define cudaDeviceSynchronize() cudaSuccess\n#ifdef __CUDACC_RTC__\n\n/*\n * Note: Every time this file is updated, build_num in jitify.pyx should be bumped.\n */\n\n// for using CCCL with Jitify\n#include <cuda/std/type_traits>\n#include <cuda/std/limits>\n#include <cuda/std/utility>\n#include <cuda/std/tuple>\n\nnamespace std {\n    // <type_traits>\n    // TODO(leofang): expose all APIs patched by Jitify for parity\n    using cuda::std::add_const;\n    using cuda::std::add_cv;\n    using cuda::std::add_lvalue_reference;\n    using cuda::std::add_volatile;\n    using cuda::std::alignment_of;\n    using cuda::std::conditional;\n    using cuda::std::enable_if;\n    using cuda::std::false_type;\n    using cuda::std::integral_constant;\n    using cuda::std::is_array;\n    using cuda::std::is_base_of;\n    using cuda::std::is_convertible;\n    using cuda::std::is_enum;\n    using cuda::std::is_floating_point;\n    using cuda::std::is_function;\n    using cuda::std::is_integral;\n    using cuda::std::is_lvalue_reference;\n    using cuda::std::is_same;\n    using cuda::std::is_signed;\n    using cuda::std::is_pointer;\n    using cuda::std::is_unsigned;\n    using cuda::std::is_volatile;\n    using cuda::std::make_signed;\n    using cuda::std::make_unsigned;\n    using cuda::std::remove_cv;\n    using cuda::std::remove_reference;\n    using cuda::std::remove_pointer;\n    using cuda::std::result_of;\n    using cuda::std::true_type;\n\n    // <limits>\n    using cuda::std::numeric_limits;\n\n    // <utility>\n    using cuda::std::declval;\n#if __cplusplus >= 201402L\n    using cuda::std::index_sequence;\n    using cuda::std::integer_sequence;\n    using cuda::std::make_index_sequence;\n    using cuda::std::make_integer_sequence;\n#endif\n    using cuda::std::make_pair;\n    using cuda::std::pair;\n\n    // <tuple>\n    using cuda::std::get;\n    using cuda::std::tuple;\n    using cuda::std::make_tuple;\n}\n\n#endif\n\n#endif // _JITIFY_INCLUDE_GUARD_297548E68FAE3341\n", "cxx_atomic.h": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------------------------------------------------------===//\n//\n// Part of libcu++, the C++ Standard Library for your entire system,\n// under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX_CXX_ATOMIC_H\n#define _LIBCUDACXX_CXX_ATOMIC_H\n\ntemplate <typename _Tp, int _Sco>\nstruct __cxx_atomic_base_impl {\n  using __underlying_t = _Tp;\n  using __temporary_t = __cxx_atomic_base_impl<_Tp, _Sco>;\n  using __wrap_t = __cxx_atomic_base_impl<_Tp, _Sco>;\n\n  static constexpr int __sco = _Sco;\n\n#if !defined(_LIBCUDACXX_COMPILER_GCC) || (__GNUC__ >= 5)\n  static_assert(is_trivially_copyable<_Tp>::value,\n    \"std::atomic<Tp> requires that 'Tp' be a trivially copyable type\");\n#endif\n\n  constexpr\n  __cxx_atomic_base_impl() noexcept = default;\n  constexpr\n  __cxx_atomic_base_impl(__cxx_atomic_base_impl &&) noexcept = default;\n  _LIBCUDACXX_INLINE_VISIBILITY constexpr explicit\n  __cxx_atomic_base_impl(_Tp value) noexcept : __a_value(value) {}\n\n  __cxx_atomic_base_impl& operator=(const __cxx_atomic_base_impl &) noexcept = default;\n\n  _ALIGNAS(sizeof(_Tp)) _Tp __a_value;\n};\n\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr\n_Tp* __cxx_get_underlying_atomic(__cxx_atomic_base_impl<_Tp, _Sco> * __a) noexcept {\n  return &__a->__a_value;\n}\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr\nvolatile _Tp* __cxx_get_underlying_atomic(__cxx_atomic_base_impl<_Tp, _Sco> volatile* __a) noexcept {\n  return &__a->__a_value;\n}\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr\nconst _Tp* __cxx_get_underlying_atomic(__cxx_atomic_base_impl<_Tp, _Sco> const* __a) noexcept {\n  return &__a->__a_value;\n}\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr\nconst volatile _Tp* __cxx_get_underlying_atomic(__cxx_atomic_base_impl<_Tp, _Sco> const volatile* __a) noexcept {\n  return &__a->__a_value;\n}\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr\n__cxx_atomic_base_impl<_Tp, _Sco>* __cxx_atomic_unwrap(__cxx_atomic_base_impl<_Tp, _Sco>* __a) noexcept {\n  return __a;\n}\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr\nvolatile __cxx_atomic_base_impl<_Tp, _Sco>* __cxx_atomic_unwrap(__cxx_atomic_base_impl<_Tp, _Sco> volatile* __a) noexcept {\n  return __a;\n}\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr\nconst __cxx_atomic_base_impl<_Tp, _Sco>* __cxx_atomic_unwrap(__cxx_atomic_base_impl<_Tp, _Sco> const* __a) noexcept {\n  return __a;\n}\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr\nconst volatile __cxx_atomic_base_impl<_Tp, _Sco>* __cxx_atomic_unwrap(__cxx_atomic_base_impl<_Tp, _Sco> const volatile* __a) noexcept {\n  return __a;\n}\n\ntemplate <typename _Tp, int _Sco>\nstruct __cxx_atomic_ref_base_impl {\n  using __underlying_t = _Tp;\n  using __temporary_t = _Tp;\n  using __wrap_t = _Tp;\n\n  static constexpr int __sco = _Sco;\n\n#if !defined(_LIBCUDACXX_COMPILER_GCC) || (__GNUC__ >= 5)\n  static_assert(is_trivially_copyable<_Tp>::value,\n    \"std::atomic_ref<Tp> requires that 'Tp' be a trivially copyable type\");\n#endif\n\n  constexpr\n  __cxx_atomic_ref_base_impl() noexcept = delete;\n  constexpr\n  __cxx_atomic_ref_base_impl(__cxx_atomic_ref_base_impl &&) noexcept = default;\n  constexpr\n  __cxx_atomic_ref_base_impl(const __cxx_atomic_ref_base_impl &) noexcept = default;\n  _LIBCUDACXX_INLINE_VISIBILITY constexpr explicit\n  __cxx_atomic_ref_base_impl(_Tp& value) noexcept : __a_value(&value) {}\n\n  _Tp* __a_value;\n};\n\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr\n_Tp* __cxx_get_underlying_atomic(__cxx_atomic_ref_base_impl<_Tp, _Sco>* __a) noexcept {\n  return __a->__a_value;\n}\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr\nvolatile _Tp* __cxx_get_underlying_atomic(__cxx_atomic_ref_base_impl<_Tp, _Sco> volatile* __a) noexcept {\n  return __a->__a_value;\n}\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr\nconst _Tp* __cxx_get_underlying_atomic(__cxx_atomic_ref_base_impl<_Tp, _Sco> const* __a) noexcept {\n  return __a->__a_value;\n}\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr\nconst volatile _Tp* __cxx_get_underlying_atomic(__cxx_atomic_ref_base_impl<_Tp, _Sco> const volatile* __a) noexcept {\n  return __a->__a_value;\n}\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr\n_Tp* __cxx_atomic_unwrap(__cxx_atomic_ref_base_impl<_Tp, _Sco>* __a) noexcept {\n  return __cxx_get_underlying_atomic(__a);\n}\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr\nvolatile _Tp* __cxx_atomic_unwrap(__cxx_atomic_ref_base_impl<_Tp, _Sco> volatile* __a) noexcept {\n  return __cxx_get_underlying_atomic(__a);\n}\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr\nconst _Tp* __cxx_atomic_unwrap(__cxx_atomic_ref_base_impl<_Tp, _Sco> const* __a) noexcept {\n  return __cxx_get_underlying_atomic(__a);\n}\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr\nconst volatile _Tp* __cxx_atomic_unwrap(__cxx_atomic_ref_base_impl<_Tp, _Sco> const volatile* __a) noexcept {\n  return __cxx_get_underlying_atomic(__a);\n}\n\ntemplate <typename _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr\n_Tp* __cxx_get_underlying_atomic(_Tp* __a) noexcept {\n  return __a;\n}\n\ntemplate <typename _Tp, typename _Up>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr\nauto __cxx_atomic_wrap_to_base(_Tp*, _Up __val) noexcept -> typename _Tp::__wrap_t {\n  return typename _Tp::__wrap_t(__val);\n}\ntemplate <typename _Tp>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr\nauto __cxx_atomic_base_temporary(_Tp*) noexcept -> typename _Tp::__temporary_t {\n  return typename _Tp::__temporary_t();\n}\n\ntemplate <typename _Tp>\nusing __cxx_atomic_underlying_t = typename _Tp::__underlying_t;\n\n#endif //_LIBCUDACXX_CXX_ATOMIC_H\n", "detail/__config": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of libcu++, the C++ Standard Library for your entire system,\n// under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef __cuda_std__\n#define __cuda_std__\n\n#define _LIBCUDACXX_CUDA_API_VERSION 2002000\n\n#define _LIBCUDACXX_CUDA_API_VERSION_MAJOR \\\n  (_LIBCUDACXX_CUDA_API_VERSION / 1000000)\n\n#define _LIBCUDACXX_CUDA_API_VERSION_MINOR \\\n  (_LIBCUDACXX_CUDA_API_VERSION / 1000 % 1000)\n\n#define _LIBCUDACXX_CUDA_API_VERSION_PATCH \\\n  (_LIBCUDACXX_CUDA_API_VERSION % 1000)\n\n#ifndef _LIBCUDACXX_CUDA_ABI_VERSION_LATEST\n#  define _LIBCUDACXX_CUDA_ABI_VERSION_LATEST 4\n#endif\n\n#ifdef _LIBCUDACXX_CUDA_ABI_VERSION\n#  if _LIBCUDACXX_CUDA_ABI_VERSION != 2 && _LIBCUDACXX_CUDA_ABI_VERSION != 3 && _LIBCUDACXX_CUDA_ABI_VERSION != 4\n#    error Unsupported libcu++ ABI version requested. Please define _LIBCUDACXX_CUDA_ABI_VERSION to either 2 or 3.\n#  endif\n#else\n#  define _LIBCUDACXX_CUDA_ABI_VERSION _LIBCUDACXX_CUDA_ABI_VERSION_LATEST\n#endif\n\n#ifdef _LIBCUDACXX_PIPELINE_ASSUMED_ABI_VERSION\n#  if _LIBCUDACXX_PIPELINE_ASSUMED_ABI_VERSION != _LIBCUDACXX_CUDA_ABI_VERSION\n#    error cuda_pipeline.h has assumed a different libcu++ ABI version than provided by this library. To fix this, please include a libcu++ header before including cuda_pipeline.h, or upgrade to a version of the toolkit this version of libcu++ shipped in.\n#  endif\n#endif\n\n#include \"libcxx/include/__config\"\n\n#endif //__cuda_std__\n", "detail/__pragma_pop": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of libcu++, the C++ Standard Library for your entire system,\n// under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#include \"libcxx/include/__pragma_pop\"\n\n", "detail/__pragma_push": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of libcu++, the C++ Standard Library for your entire system,\n// under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#include \"libcxx/include/__pragma_push\"\n#include \"libcxx/include/__undef_macros\"\n\n", "detail/libcxx/include/barrier": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===--------------------------- barrier ----------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX_BARRIER\n#define _LIBCUDACXX_BARRIER\n\n/*\n    barrier synopsis\n\nnamespace std\n{\n\n  template<class CompletionFunction = see below>\n  class barrier\n  {\n  public:\n    using arrival_token = see below;\n\n    constexpr explicit barrier(ptrdiff_t phase_count,\n                               CompletionFunction f = CompletionFunction());\n    ~barrier();\n\n    barrier(const barrier&) = delete;\n    barrier& operator=(const barrier&) = delete;\n\n    [[nodiscard]] arrival_token arrive(ptrdiff_t update = 1);\n    void wait(arrival_token&& arrival) const;\n\n    void arrive_and_wait();\n    void arrive_and_drop();\n\n  private:\n    CompletionFunction __completion; // exposition only\n  };\n\n}\n\n*/\n\n#ifndef __cuda_std__\n#include <__config>\n#ifndef _LIBCUDACXX_HAS_NO_TREE_BARRIER\n#  include <thread>\n#  include <vector>\n#endif\n#else\n#ifndef _LIBCUDACXX_COMPILER_NVRTC\n#include <new>\n#endif // _LIBCUDACXX_COMPILER_NVRTC\n#endif // __cuda_std__\n\n#include \"__assert\" // all public C++ headers provide the assertion handler\n#include \"__debug\"\n#include \"atomic\"\n#include \"chrono\"\n#include \"cstddef\"\n\n#ifndef __cuda_std__\n#include <__pragma_push>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n#ifdef _LIBCUDACXX_HAS_NO_THREADS\n# error <barrier> is not supported on this single threaded system\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\nstruct __empty_completion\n{\n    inline _LIBCUDACXX_INLINE_VISIBILITY\n    void operator()() noexcept { }\n};\n\n#ifndef _LIBCUDACXX_HAS_NO_TREE_BARRIER\n\ntemplate<class _CompletionF = __empty_completion, int _Sco = 0>\nclass alignas(64) __barrier_base {\n\n    ptrdiff_t                       __expected;\n    __atomic_base<ptrdiff_t, _Sco>  __expected_adjustment;\n    _CompletionF                    __completion;\n\n    using __phase_t = uint8_t;\n    __atomic_base<__phase_t, _Sco>  __phase;\n\n    struct alignas(64) __state_t\n    {\n        struct {\n            __atomic_base<__phase_t, _Sco> __phase = ATOMIC_VAR_INIT(0);\n        } __tickets[64];\n    };\n    ::std::vector<__state_t>   __state;\n\n    inline _LIBCUDACXX_INLINE_VISIBILITY\n    bool __arrive(__phase_t const __old_phase)\n    {\n        __phase_t const __half_step = __old_phase + 1, __full_step = __old_phase + 2;\n#ifndef _LIBCUDACXX_HAS_NO_THREAD_FAVORITE_BARRIER_INDEX\n        ptrdiff_t __current = __libcpp_thread_favorite_barrier_index,\n#else\n        ptrdiff_t __current = 0,\n#endif\n                  __current_expected = __expected,\n                  __last_node = (__current_expected >> 1);\n        for(size_t __round = 0;; ++__round) {\n            _LIBCUDACXX_ASSERT(__round <= 63, \"\");\n            if(__current_expected == 1)\n                return true;\n            for(;;++__current) {\n#ifndef _LIBCUDACXX_HAS_NO_THREAD_FAVORITE_BARRIER_INDEX\n                if(0 == __round) {\n                    if(__current >= __current_expected)\n                        __current = 0;\n                    __libcpp_thread_favorite_barrier_index = __current;\n                }\n#endif\n                _LIBCUDACXX_ASSERT(__current <= __last_node, \"\");\n                __phase_t expect = __old_phase;\n                if(__current == __last_node && (__current_expected & 1))\n                {\n                    if(__state[__current].__tickets[__round].__phase.compare_exchange_strong(expect, __full_step, memory_order_acq_rel))\n                        break;    // I'm 1 in 1, go to next __round\n                    _LIBCUDACXX_ASSERT(expect == __full_step, \"\");\n                }\n                else if(__state[__current].__tickets[__round].__phase.compare_exchange_strong(expect, __half_step, memory_order_acq_rel))\n                {\n                    return false; // I'm 1 in 2, done with arrival\n                }\n                else if(expect == __half_step)\n                {\n                    if(__state[__current].__tickets[__round].__phase.compare_exchange_strong(expect, __full_step, memory_order_acq_rel))\n                        break;    // I'm 2 in 2, go to next __round\n                    _LIBCUDACXX_ASSERT(expect == __full_step, \"\");\n                }\n                _LIBCUDACXX_ASSERT(__round == 0 && expect == __full_step, \"\");\n            }\n            __current_expected = (__current_expected >> 1) + (__current_expected & 1);\n            __current &= ~( 1 << __round );\n            __last_node &= ~( 1 << __round );\n        }\n    }\n\npublic:\n    using arrival_token = __phase_t;\n\n    inline _LIBCUDACXX_INLINE_VISIBILITY\n    __barrier_base(ptrdiff_t __expected, _CompletionF __completion = _CompletionF())\n            : __expected(__expected), __expected_adjustment(0), __completion(__completion),\n              __phase(0), __state((__expected+1) >> 1)\n    {\n        _LIBCUDACXX_ASSERT(__expected >= 0, \"\");\n    }\n\n    inline _LIBCUDACXX_INLINE_VISIBILITY\n    ~__barrier_base() = default;\n\n    __barrier_base(__barrier_base const&) = delete;\n    __barrier_base& operator=(__barrier_base const&) = delete;\n\n     _LIBCUDACXX_NODISCARD_ATTRIBUTE inline _LIBCUDACXX_INLINE_VISIBILITY\n    arrival_token arrive(ptrdiff_t update = 1)\n    {\n        _LIBCUDACXX_ASSERT(update > 0, \"\");\n        auto __old_phase = __phase.load(memory_order_relaxed);\n        for(; update; --update)\n            if(__arrive(__old_phase)) {\n                __completion();\n                __expected += __expected_adjustment.load(memory_order_relaxed);\n                __expected_adjustment.store(0, memory_order_relaxed);\n                __phase.store(__old_phase + 2, memory_order_release);\n            }\n        return __old_phase;\n    }\n    inline _LIBCUDACXX_INLINE_VISIBILITY\n    void wait(arrival_token&& __old_phase) const\n    {\n        __libcpp_thread_poll_with_backoff([=]() -> bool {\n            return __phase.load(memory_order_acquire) != __old_phase;\n        });\n    }\n    inline _LIBCUDACXX_INLINE_VISIBILITY\n    void arrive_and_wait()\n    {\n        wait(arrive());\n    }\n    inline _LIBCUDACXX_INLINE_VISIBILITY\n    void arrive_and_drop()\n    {\n        __expected_adjustment.fetch_sub(1, memory_order_relaxed);\n        (void)arrive();\n    }\n};\n\n#else\n\n# if _LIBCUDACXX_CUDA_ABI_VERSION < 3\n#  define _LIBCUDACXX_BARRIER_ALIGNMENTS alignas(64)\n# else\n#  define _LIBCUDACXX_BARRIER_ALIGNMENTS\n# endif\n\ntemplate<class _Barrier>\nclass __barrier_poll_tester_phase {\n    _Barrier const* __this;\n    typename _Barrier::arrival_token __phase;\n\npublic:\n    _LIBCUDACXX_INLINE_VISIBILITY\n    __barrier_poll_tester_phase(_Barrier const* __this_,\n            typename _Barrier::arrival_token&& __phase_)\n        : __this(__this_)\n        , __phase(_CUDA_VSTD::move(__phase_))\n    {}\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool operator()() const\n    {\n        return __this->__try_wait(__phase);\n    }\n};\n\ntemplate<class _Barrier>\nclass __barrier_poll_tester_parity {\n    _Barrier const* __this;\n    bool __parity;\n\npublic:\n    _LIBCUDACXX_INLINE_VISIBILITY\n    __barrier_poll_tester_parity(_Barrier const* __this_, bool __parity_)\n        : __this(__this_)\n        , __parity(__parity_)\n    {}\n\n    inline _LIBCUDACXX_INLINE_VISIBILITY\n    bool operator()() const\n    {\n        return __this->__try_wait_parity(__parity);\n    }\n};\n\ntemplate<class _Barrier>\n_LIBCUDACXX_INLINE_VISIBILITY\nbool __call_try_wait(const _Barrier& __b, typename _Barrier::arrival_token&& __phase)\n{\n    return __b.__try_wait(_CUDA_VSTD::move(__phase));\n}\n\ntemplate<class _Barrier>\n_LIBCUDACXX_INLINE_VISIBILITY\nbool __call_try_wait_parity(const _Barrier& __b, bool __parity)\n{\n    return __b.__try_wait_parity(__parity);\n}\n\n\ntemplate<class _CompletionF, int _Sco = 0>\nclass __barrier_base {\n\n    _LIBCUDACXX_BARRIER_ALIGNMENTS __atomic_base<ptrdiff_t, _Sco> __expected, __arrived;\n    _LIBCUDACXX_BARRIER_ALIGNMENTS _CompletionF                   __completion;\n    _LIBCUDACXX_BARRIER_ALIGNMENTS __atomic_base<bool, _Sco>      __phase;\n\npublic:\n    using arrival_token = bool;\n\nprivate:\n    template<typename _Barrier>\n    friend class __barrier_poll_tester_phase;\n    template<typename _Barrier>\n    friend class __barrier_poll_tester_parity;\n    template<typename _Barrier>\n    _LIBCUDACXX_INLINE_VISIBILITY\n    friend bool __call_try_wait(const _Barrier& __b,\n    typename _Barrier::arrival_token&& __phase);\n    template<typename _Barrier>\n    _LIBCUDACXX_INLINE_VISIBILITY\n    friend bool __call_try_wait_parity(const _Barrier& __b, bool __parity);\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool __try_wait(arrival_token __old) const\n    {\n        return __phase.load(memory_order_acquire) != __old;\n    }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool __try_wait_parity(bool __parity) const\n    {\n        return __try_wait(__parity);\n    }\n\npublic:\n    __barrier_base() = default;\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    __barrier_base(ptrdiff_t __expected, _CompletionF __completion = _CompletionF())\n        : __phase(false), __expected(__expected), __arrived(__expected), __completion(__completion)\n    {\n    }\n\n    ~__barrier_base() = default;\n\n    __barrier_base(__barrier_base const&) = delete;\n    __barrier_base& operator=(__barrier_base const&) = delete;\n\n    _LIBCUDACXX_NODISCARD_ATTRIBUTE _LIBCUDACXX_INLINE_VISIBILITY\n    arrival_token arrive(ptrdiff_t __update = 1)\n    {\n        auto const __old_phase = __phase.load(memory_order_relaxed);\n        auto const __result = __arrived.fetch_sub(__update, memory_order_acq_rel) - __update;\n        auto const __new_expected = __expected.load(memory_order_relaxed);\n\n#if (_LIBCUDACXX_DEBUG_LEVEL >= 2)\n        _LIBCUDACXX_DEBUG_ASSERT(__result >= 0);\n#endif\n\n        if(0 == __result) {\n            __completion();\n            __arrived.store(__new_expected, memory_order_relaxed);\n            __phase.store(!__old_phase, memory_order_release);\n            __cxx_atomic_notify_all(&__phase.__a_);\n        }\n        return __old_phase;\n    }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    void wait(arrival_token&& __old_phase) const\n    {\n        __phase.wait(__old_phase, memory_order_acquire);\n    }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    void arrive_and_wait()\n    {\n        wait(arrive());\n    }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    void arrive_and_drop()\n    {\n        __expected.fetch_sub(1, memory_order_relaxed);\n        (void)arrive();\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    static constexpr ptrdiff_t max() noexcept\n    {\n        return numeric_limits<ptrdiff_t>::max();\n    }\n};\n\ntemplate<int _Sco>\nclass __barrier_base<__empty_completion, _Sco> {\n\n    static constexpr uint64_t __expected_unit = 1ull;\n    static constexpr uint64_t __arrived_unit = 1ull << 32;\n    static constexpr uint64_t __expected_mask = __arrived_unit - 1;\n    static constexpr uint64_t __phase_bit = 1ull << 63;\n    static constexpr uint64_t __arrived_mask = (__phase_bit - 1) & ~__expected_mask;\n\n    _LIBCUDACXX_BARRIER_ALIGNMENTS __atomic_base<uint64_t, _Sco> __phase_arrived_expected;\n\npublic:\n    using arrival_token = uint64_t;\n\nprivate:\n    template<typename _Barrier>\n    friend class __barrier_poll_tester_phase;\n    template<typename _Barrier>\n    friend class __barrier_poll_tester_parity;\n    template<typename _Barrier>\n    _LIBCUDACXX_INLINE_VISIBILITY\n    friend bool __call_try_wait(const _Barrier& __b,\n    typename _Barrier::arrival_token&& __phase);\n    template<typename _Barrier>\n    _LIBCUDACXX_INLINE_VISIBILITY\n    friend bool __call_try_wait_parity(const _Barrier& __b, bool __parity);\n\n    static _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    uint64_t __init(ptrdiff_t __count) noexcept\n    {\n#if (_LIBCUDACXX_DEBUG_LEVEL >= 2)\n        _LIBCUDACXX_DEBUG_ASSERT(__count >= 0);\n#endif\n\n        return (((1u << 31) - __count) << 32)\n              | ((1u << 31) - __count);\n    }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool __try_wait_phase(uint64_t __phase) const\n    {\n        uint64_t const __current = __phase_arrived_expected.load(memory_order_acquire);\n        return ((__current & __phase_bit) != __phase);\n    }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool __try_wait(arrival_token __old) const\n    {\n\t\treturn __try_wait_phase(__old & __phase_bit);\n    }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    bool __try_wait_parity(bool __parity) const\n    {\n        return __try_wait_phase(__parity ? __phase_bit : 0);\n    }\n\npublic:\n    __barrier_base() = default;\n\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    __barrier_base(ptrdiff_t __count, __empty_completion = __empty_completion())\n        : __phase_arrived_expected(__init(__count)) {\n#if (_LIBCUDACXX_DEBUG_LEVEL >= 2)\n        _LIBCUDACXX_DEBUG_ASSERT(__count >= 0);\n#endif\n    }\n\n    ~__barrier_base() = default;\n\n    __barrier_base(__barrier_base const&) = delete;\n    __barrier_base& operator=(__barrier_base const&) = delete;\n\n    _LIBCUDACXX_NODISCARD_ATTRIBUTE inline _LIBCUDACXX_INLINE_VISIBILITY\n    arrival_token arrive(ptrdiff_t __update = 1)\n    {\n        auto const __inc = __arrived_unit * __update;\n        auto const __old = __phase_arrived_expected.fetch_add(__inc, memory_order_acq_rel);\n        if((__old ^ (__old + __inc)) & __phase_bit) {\n            __phase_arrived_expected.fetch_add((__old & __expected_mask) << 32, memory_order_relaxed);\n            __phase_arrived_expected.notify_all();\n        }\n        return __old & __phase_bit;\n    }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    void wait(arrival_token&& __phase) const\n    {\n\t\t__libcpp_thread_poll_with_backoff(__barrier_poll_tester_phase<__barrier_base>(this, _CUDA_VSTD::move(__phase)));\n    }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    void wait_parity(bool __parity) const\n    {\n        __libcpp_thread_poll_with_backoff(__barrier_poll_tester_parity<__barrier_base>(this, __parity));\n    }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    void arrive_and_wait()\n    {\n        wait(arrive());\n    }\n    _LIBCUDACXX_INLINE_VISIBILITY\n    void arrive_and_drop()\n    {\n        __phase_arrived_expected.fetch_add(__expected_unit, memory_order_relaxed);\n        (void)arrive();\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    static constexpr ptrdiff_t max() noexcept\n    {\n        return numeric_limits<int32_t>::max();\n    }\n};\n\n#endif //_LIBCUDACXX_HAS_NO_TREE_BARRIER\n\ntemplate<class _CompletionF = __empty_completion>\nclass barrier : public __barrier_base<_CompletionF> {\npublic:\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    barrier(ptrdiff_t __count, _CompletionF __completion = _CompletionF())\n        : __barrier_base<_CompletionF>(__count, __completion) {\n    }\n};\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#ifndef __cuda_std__\n#include <__pragma_pop>\n#else\n#include \"__cuda/barrier.h\"\n#endif // __cuda_std__\n\n#endif //_LIBCUDACXX_BARRIER\n", "detail/libcxx/include/functional": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===------------------------ functional ----------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX_FUNCTIONAL\n#define _LIBCUDACXX_FUNCTIONAL\n\n/*\n    functional synopsis\n\nnamespace std\n{\n\ntemplate <class Arg, class Result>\nstruct unary_function\n{\n    typedef Arg    argument_type;\n    typedef Result result_type;\n};\n\ntemplate <class Arg1, class Arg2, class Result>\nstruct binary_function\n{\n    typedef Arg1   first_argument_type;\n    typedef Arg2   second_argument_type;\n    typedef Result result_type;\n};\n\ntemplate <class T>\nclass reference_wrapper\n    : public unary_function<T1, R> // if wrapping a unary functor\n    : public binary_function<T1, T2, R> // if wraping a binary functor\n{\npublic:\n    // types\n    typedef T type;\n    typedef see below result_type; // Not always defined\n\n    // construct/copy/destroy\n    reference_wrapper(T&) noexcept;\n    reference_wrapper(T&&) = delete; // do not bind to temps\n    reference_wrapper(const reference_wrapper<T>& x) noexcept;\n\n    // assignment\n    reference_wrapper& operator=(const reference_wrapper<T>& x) noexcept;\n\n    // access\n    operator T& () const noexcept;\n    T& get() const noexcept;\n\n    // invoke\n    template <class... ArgTypes>\n      typename result_of<T&(ArgTypes&&...)>::type\n          operator() (ArgTypes&&...) const;\n};\n\ntemplate <class T> reference_wrapper<T> ref(T& t) noexcept;\ntemplate <class T> void ref(const T&& t) = delete;\ntemplate <class T> reference_wrapper<T> ref(reference_wrapper<T>t) noexcept;\n\ntemplate <class T> reference_wrapper<const T> cref(const T& t) noexcept;\ntemplate <class T> void cref(const T&& t) = delete;\ntemplate <class T> reference_wrapper<const T> cref(reference_wrapper<T> t) noexcept;\n\ntemplate <class T> struct unwrap_reference;                                       // since C++20\ntemplate <class T> struct unwrap_ref_decay : unwrap_reference<decay_t<T>> { };    // since C++20\ntemplate <class T> using unwrap_reference_t = typename unwrap_reference<T>::type; // since C++20\ntemplate <class T> using unwrap_ref_decay_t = typename unwrap_ref_decay<T>::type; // since C++20\n\ntemplate <class T> // <class T=void> in C++14\nstruct plus : binary_function<T, T, T>\n{\n    T operator()(const T& x, const T& y) const;\n};\n\ntemplate <class T> // <class T=void> in C++14\nstruct minus : binary_function<T, T, T>\n{\n    T operator()(const T& x, const T& y) const;\n};\n\ntemplate <class T> // <class T=void> in C++14\nstruct multiplies : binary_function<T, T, T>\n{\n    T operator()(const T& x, const T& y) const;\n};\n\ntemplate <class T> // <class T=void> in C++14\nstruct divides : binary_function<T, T, T>\n{\n    T operator()(const T& x, const T& y) const;\n};\n\ntemplate <class T> // <class T=void> in C++14\nstruct modulus : binary_function<T, T, T>\n{\n    T operator()(const T& x, const T& y) const;\n};\n\ntemplate <class T> // <class T=void> in C++14\nstruct negate : unary_function<T, T>\n{\n    T operator()(const T& x) const;\n};\n\ntemplate <class T> // <class T=void> in C++14\nstruct equal_to : binary_function<T, T, bool>\n{\n    bool operator()(const T& x, const T& y) const;\n};\n\ntemplate <class T> // <class T=void> in C++14\nstruct not_equal_to : binary_function<T, T, bool>\n{\n    bool operator()(const T& x, const T& y) const;\n};\n\ntemplate <class T> // <class T=void> in C++14\nstruct greater : binary_function<T, T, bool>\n{\n    bool operator()(const T& x, const T& y) const;\n};\n\ntemplate <class T> // <class T=void> in C++14\nstruct less : binary_function<T, T, bool>\n{\n    bool operator()(const T& x, const T& y) const;\n};\n\ntemplate <class T> // <class T=void> in C++14\nstruct greater_equal : binary_function<T, T, bool>\n{\n    bool operator()(const T& x, const T& y) const;\n};\n\ntemplate <class T> // <class T=void> in C++14\nstruct less_equal : binary_function<T, T, bool>\n{\n    bool operator()(const T& x, const T& y) const;\n};\n\ntemplate <class T> // <class T=void> in C++14\nstruct logical_and : binary_function<T, T, bool>\n{\n    bool operator()(const T& x, const T& y) const;\n};\n\ntemplate <class T> // <class T=void> in C++14\nstruct logical_or : binary_function<T, T, bool>\n{\n    bool operator()(const T& x, const T& y) const;\n};\n\ntemplate <class T> // <class T=void> in C++14\nstruct logical_not : unary_function<T, bool>\n{\n    bool operator()(const T& x) const;\n};\n\ntemplate <class T> // <class T=void> in C++14\nstruct bit_and : unary_function<T, bool>\n{\n    bool operator()(const T& x, const T& y) const;\n};\n\ntemplate <class T> // <class T=void> in C++14\nstruct bit_or : unary_function<T, bool>\n{\n    bool operator()(const T& x, const T& y) const;\n};\n\ntemplate <class T> // <class T=void> in C++14\nstruct bit_xor : unary_function<T, bool>\n{\n    bool operator()(const T& x, const T& y) const;\n};\n\ntemplate <class T=void> // C++14\nstruct bit_xor : unary_function<T, bool>\n{\n    bool operator()(const T& x) const;\n};\n\ntemplate <class Predicate>\nclass unary_negate // deprecated in C++17\n    : public unary_function<typename Predicate::argument_type, bool>\n{\npublic:\n    explicit unary_negate(const Predicate& pred);\n    bool operator()(const typename Predicate::argument_type& x) const;\n};\n\ntemplate <class Predicate> // deprecated in C++17\nunary_negate<Predicate> not1(const Predicate& pred);\n\ntemplate <class Predicate>\nclass binary_negate // deprecated in C++17\n    : public binary_function<typename Predicate::first_argument_type,\n                             typename Predicate::second_argument_type,\n                             bool>\n{\npublic:\n    explicit binary_negate(const Predicate& pred);\n    bool operator()(const typename Predicate::first_argument_type& x,\n                    const typename Predicate::second_argument_type& y) const;\n};\n\ntemplate <class Predicate> // deprecated in C++17\nbinary_negate<Predicate> not2(const Predicate& pred);\n\ntemplate <class F> unspecified not_fn(F&& f); // C++17\n\ntemplate<class T> struct is_bind_expression;\ntemplate<class T> struct is_placeholder;\n\n    // See C++14 20.9.9, Function object binders\ntemplate <class T> inline constexpr bool is_bind_expression_v\n  = is_bind_expression<T>::value; // C++17\ntemplate <class T> inline constexpr int is_placeholder_v\n  = is_placeholder<T>::value; // C++17\n\n\ntemplate<class Fn, class... BoundArgs>\n  unspecified bind(Fn&&, BoundArgs&&...);\ntemplate<class R, class Fn, class... BoundArgs>\n  unspecified bind(Fn&&, BoundArgs&&...);\n\ntemplate<class F, class... Args>\n invoke_result_t<F, Args...> invoke(F&& f, Args&&... args) // C++17\n    noexcept(is_nothrow_invocable_v<F, Args...>);\n\nnamespace placeholders {\n  // M is the implementation-defined number of placeholders\n  extern unspecified _1;\n  extern unspecified _2;\n  .\n  .\n  .\n  extern unspecified _Mp;\n}\n\ntemplate <class Operation>\nclass binder1st     // deprecated in C++11, removed in C++17\n    : public unary_function<typename Operation::second_argument_type,\n                            typename Operation::result_type>\n{\nprotected:\n    Operation                               op;\n    typename Operation::first_argument_type value;\npublic:\n    binder1st(const Operation& x, const typename Operation::first_argument_type y);\n    typename Operation::result_type operator()(      typename Operation::second_argument_type& x) const;\n    typename Operation::result_type operator()(const typename Operation::second_argument_type& x) const;\n};\n\ntemplate <class Operation, class T>\nbinder1st<Operation> bind1st(const Operation& op, const T& x);  // deprecated in C++11, removed in C++17\n\ntemplate <class Operation>\nclass binder2nd     // deprecated in C++11, removed in C++17\n    : public unary_function<typename Operation::first_argument_type,\n                            typename Operation::result_type>\n{\nprotected:\n    Operation                                op;\n    typename Operation::second_argument_type value;\npublic:\n    binder2nd(const Operation& x, const typename Operation::second_argument_type y);\n    typename Operation::result_type operator()(      typename Operation::first_argument_type& x) const;\n    typename Operation::result_type operator()(const typename Operation::first_argument_type& x) const;\n};\n\ntemplate <class Operation, class T>\nbinder2nd<Operation> bind2nd(const Operation& op, const T& x);  // deprecated in C++11, removed in C++17\n\ntemplate <class Arg, class Result>      // deprecated in C++11, removed in C++17\nclass pointer_to_unary_function : public unary_function<Arg, Result>\n{\npublic:\n    explicit pointer_to_unary_function(Result (*f)(Arg));\n    Result operator()(Arg x) const;\n};\n\ntemplate <class Arg, class Result>\npointer_to_unary_function<Arg,Result> ptr_fun(Result (*f)(Arg));      // deprecated in C++11, removed in C++17\n\ntemplate <class Arg1, class Arg2, class Result>      // deprecated in C++11, removed in C++17\nclass pointer_to_binary_function : public binary_function<Arg1, Arg2, Result>\n{\npublic:\n    explicit pointer_to_binary_function(Result (*f)(Arg1, Arg2));\n    Result operator()(Arg1 x, Arg2 y) const;\n};\n\ntemplate <class Arg1, class Arg2, class Result>\npointer_to_binary_function<Arg1,Arg2,Result> ptr_fun(Result (*f)(Arg1,Arg2));      // deprecated in C++11, removed in C++17\n\ntemplate<class S, class T>      // deprecated in C++11, removed in C++17\nclass mem_fun_t : public unary_function<T*, S>\n{\npublic:\n    explicit mem_fun_t(S (T::*p)());\n    S operator()(T* p) const;\n};\n\ntemplate<class S, class T, class A>\nclass mem_fun1_t : public binary_function<T*, A, S>      // deprecated in C++11, removed in C++17\n{\npublic:\n    explicit mem_fun1_t(S (T::*p)(A));\n    S operator()(T* p, A x) const;\n};\n\ntemplate<class S, class T>          mem_fun_t<S,T>    mem_fun(S (T::*f)());      // deprecated in C++11, removed in C++17\ntemplate<class S, class T, class A> mem_fun1_t<S,T,A> mem_fun(S (T::*f)(A));     // deprecated in C++11, removed in C++17\n\ntemplate<class S, class T>\nclass mem_fun_ref_t : public unary_function<T, S>      // deprecated in C++11, removed in C++17\n{\npublic:\n    explicit mem_fun_ref_t(S (T::*p)());\n    S operator()(T& p) const;\n};\n\ntemplate<class S, class T, class A>\nclass mem_fun1_ref_t : public binary_function<T, A, S>      // deprecated in C++11, removed in C++17\n{\npublic:\n    explicit mem_fun1_ref_t(S (T::*p)(A));\n    S operator()(T& p, A x) const;\n};\n\ntemplate<class S, class T>          mem_fun_ref_t<S,T>    mem_fun_ref(S (T::*f)());      // deprecated in C++11, removed in C++17\ntemplate<class S, class T, class A> mem_fun1_ref_t<S,T,A> mem_fun_ref(S (T::*f)(A));     // deprecated in C++11, removed in C++17\n\ntemplate <class S, class T>\nclass const_mem_fun_t : public unary_function<const T*, S>      // deprecated in C++11, removed in C++17\n{\npublic:\n    explicit const_mem_fun_t(S (T::*p)() const);\n    S operator()(const T* p) const;\n};\n\ntemplate <class S, class T, class A>\nclass const_mem_fun1_t : public binary_function<const T*, A, S>      // deprecated in C++11, removed in C++17\n{\npublic:\n    explicit const_mem_fun1_t(S (T::*p)(A) const);\n    S operator()(const T* p, A x) const;\n};\n\ntemplate <class S, class T>          const_mem_fun_t<S,T>    mem_fun(S (T::*f)() const);      // deprecated in C++11, removed in C++17\ntemplate <class S, class T, class A> const_mem_fun1_t<S,T,A> mem_fun(S (T::*f)(A) const);     // deprecated in C++11, removed in C++17\n\ntemplate <class S, class T>\nclass const_mem_fun_ref_t : public unary_function<T, S>      // deprecated in C++11, removed in C++17\n{\npublic:\n    explicit const_mem_fun_ref_t(S (T::*p)() const);\n    S operator()(const T& p) const;\n};\n\ntemplate <class S, class T, class A>\nclass const_mem_fun1_ref_t : public binary_function<T, A, S>      // deprecated in C++11, removed in C++17\n{\npublic:\n    explicit const_mem_fun1_ref_t(S (T::*p)(A) const);\n    S operator()(const T& p, A x) const;\n};\n\ntemplate <class S, class T>          const_mem_fun_ref_t<S,T>    mem_fun_ref(S (T::*f)() const);   // deprecated in C++11, removed in C++17\ntemplate <class S, class T, class A> const_mem_fun1_ref_t<S,T,A> mem_fun_ref(S (T::*f)(A) const);  // deprecated in C++11, removed in C++17\n\ntemplate<class R, class T> unspecified mem_fn(R T::*);\n\nclass bad_function_call\n    : public exception\n{\n};\n\ntemplate<class> class function; // undefined\n\ntemplate<class R, class... ArgTypes>\nclass function<R(ArgTypes...)>\n  : public unary_function<T1, R>      // iff sizeof...(ArgTypes) == 1 and\n                                      // ArgTypes contains T1\n  : public binary_function<T1, T2, R> // iff sizeof...(ArgTypes) == 2 and\n                                      // ArgTypes contains T1 and T2\n{\npublic:\n    typedef R result_type;\n\n    // construct/copy/destroy:\n    function() noexcept;\n    function(nullptr_t) noexcept;\n    function(const function&);\n    function(function&&) noexcept;\n    template<class F>\n      function(F);\n    template<Allocator Alloc>\n      function(allocator_arg_t, const Alloc&) noexcept;            // removed in C++17\n    template<Allocator Alloc>\n      function(allocator_arg_t, const Alloc&, nullptr_t) noexcept; // removed in C++17\n    template<Allocator Alloc>\n      function(allocator_arg_t, const Alloc&, const function&);    // removed in C++17\n    template<Allocator Alloc>\n      function(allocator_arg_t, const Alloc&, function&&);         // removed in C++17\n    template<class F, Allocator Alloc>\n      function(allocator_arg_t, const Alloc&, F);                  // removed in C++17\n\n    function& operator=(const function&);\n    function& operator=(function&&) noexcept;\n    function& operator=(nullptr_t) noexcept;\n    template<class F>\n      function& operator=(F&&);\n    template<class F>\n      function& operator=(reference_wrapper<F>) noexcept;\n\n    ~function();\n\n    // function modifiers:\n    void swap(function&) noexcept;\n    template<class F, class Alloc>\n      void assign(F&&, const Alloc&);                 // Removed in C++17\n\n    // function capacity:\n    explicit operator bool() const noexcept;\n\n    // function invocation:\n    R operator()(ArgTypes...) const;\n\n    // function target access:\n    const std::type_info& target_type() const noexcept;\n    template <typename T>       T* target() noexcept;\n    template <typename T> const T* target() const noexcept;\n};\n\n// Deduction guides\ntemplate<class R, class ...Args>\nfunction(R(*)(Args...)) -> function<R(Args...)>; // since C++17\n\ntemplate<class F>\nfunction(F) -> function<see-below>; // since C++17\n\n// Null pointer comparisons:\ntemplate <class R, class ... ArgTypes>\n  bool operator==(const function<R(ArgTypes...)>&, nullptr_t) noexcept;\n\ntemplate <class R, class ... ArgTypes>\n  bool operator==(nullptr_t, const function<R(ArgTypes...)>&) noexcept;\n\ntemplate <class R, class ... ArgTypes>\n  bool operator!=(const function<R(ArgTypes...)>&, nullptr_t) noexcept;\n\ntemplate <class  R, class ... ArgTypes>\n  bool operator!=(nullptr_t, const function<R(ArgTypes...)>&) noexcept;\n\n// specialized algorithms:\ntemplate <class  R, class ... ArgTypes>\n  void swap(function<R(ArgTypes...)>&, function<R(ArgTypes...)>&) noexcept;\n\ntemplate <class T> struct hash;\n\ntemplate <> struct hash<bool>;\ntemplate <> struct hash<char>;\ntemplate <> struct hash<signed char>;\ntemplate <> struct hash<unsigned char>;\ntemplate <> struct hash<char16_t>;\ntemplate <> struct hash<char32_t>;\ntemplate <> struct hash<wchar_t>;\ntemplate <> struct hash<short>;\ntemplate <> struct hash<unsigned short>;\ntemplate <> struct hash<int>;\ntemplate <> struct hash<unsigned int>;\ntemplate <> struct hash<long>;\ntemplate <> struct hash<long long>;\ntemplate <> struct hash<unsigned long>;\ntemplate <> struct hash<unsigned long long>;\n\ntemplate <> struct hash<float>;\ntemplate <> struct hash<double>;\ntemplate <> struct hash<long double>;\n\ntemplate<class T> struct hash<T*>;\ntemplate <> struct hash<nullptr_t>;  // C++17\n\n}  // std\n\nPOLICY:  For non-variadic implementations, the number of arguments is limited\n         to 3.  It is hoped that the need for non-variadic implementations\n         will be minimal.\n\n*/\n\n#ifndef __cuda_std__\n#include <__config>\n#else\n#ifndef _LIBCUDACXX_COMPILER_NVRTC\n#include <errno.h>\n#endif // _LIBCUDACXX_COMPILER_NVRTC\n#endif // __cuda_std__\n\n\n#include \"__functional_base\"\n#include \"__functional/binary_function.h\"\n#include \"__functional/binary_negate.h\"\n#include \"__functional/bind_back.h\"\n#include \"__functional/bind_front.h\"\n#include \"__functional/bind.h\"\n#include \"__functional/binder1st.h\"\n#include \"__functional/binder2nd.h\"\n#include \"__functional/compose.h\"\n#include \"__functional/default_searcher.h\"\n#include \"__functional/function.h\"\n#include \"__functional/hash.h\"\n#include \"__functional/identity.h\"\n#include \"__functional/invoke.h\"\n#include \"__functional/is_transparent.h\"\n#include \"__functional/mem_fn.h\"\n#include \"__functional/mem_fun_ref.h\"\n#include \"__functional/not_fn.h\"\n#include \"__functional/operations.h\"\n#include \"__functional/perfect_forward.h\"\n#include \"__functional/pointer_to_binary_function.h\"\n#include \"__functional/pointer_to_unary_function.h\"\n#include \"__functional/reference_wrapper.h\"\n#include \"__functional/unary_function.h\"\n#include \"__functional/unary_negate.h\"\n#include \"__functional/unwrap_ref.h\"\n#include \"__functional/weak_result_type.h\"\n\n#include \"chrono\"\n#include \"climits\"\n#include \"iterator\"\n#include \"type_traits\"\n#include \"version\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n#endif  // _LIBCUDACXX_FUNCTIONAL\n", "detail/libcxx/include/limits": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===---------------------------- limits ----------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX_LIMITS\n#define _LIBCUDACXX_LIMITS\n\n/*\n    limits synopsis\n\nnamespace std\n{\n\ntemplate<class T>\nclass numeric_limits\n{\npublic:\n    static constexpr bool is_specialized = false;\n    static constexpr T min() noexcept;\n    static constexpr T max() noexcept;\n    static constexpr T lowest() noexcept;\n\n    static constexpr int  digits = 0;\n    static constexpr int  digits10 = 0;\n    static constexpr int  max_digits10 = 0;\n    static constexpr bool is_signed = false;\n    static constexpr bool is_integer = false;\n    static constexpr bool is_exact = false;\n    static constexpr int  radix = 0;\n    static constexpr T epsilon() noexcept;\n    static constexpr T round_error() noexcept;\n\n    static constexpr int  min_exponent = 0;\n    static constexpr int  min_exponent10 = 0;\n    static constexpr int  max_exponent = 0;\n    static constexpr int  max_exponent10 = 0;\n\n    static constexpr bool has_infinity = false;\n    static constexpr bool has_quiet_NaN = false;\n    static constexpr bool has_signaling_NaN = false;\n    static constexpr float_denorm_style has_denorm = denorm_absent;\n    static constexpr bool has_denorm_loss = false;\n    static constexpr T infinity() noexcept;\n    static constexpr T quiet_NaN() noexcept;\n    static constexpr T signaling_NaN() noexcept;\n    static constexpr T denorm_min() noexcept;\n\n    static constexpr bool is_iec559 = false;\n    static constexpr bool is_bounded = false;\n    static constexpr bool is_modulo = false;\n\n    static constexpr bool traps = false;\n    static constexpr bool tinyness_before = false;\n    static constexpr float_round_style round_style = round_toward_zero;\n};\n\nenum float_round_style\n{\n    round_indeterminate       = -1,\n    round_toward_zero         =  0,\n    round_to_nearest          =  1,\n    round_toward_infinity     =  2,\n    round_toward_neg_infinity =  3\n};\n\nenum float_denorm_style\n{\n    denorm_indeterminate = -1,\n    denorm_absent = 0,\n    denorm_present = 1\n};\n\ntemplate<> class numeric_limits<cv bool>;\n\ntemplate<> class numeric_limits<cv char>;\ntemplate<> class numeric_limits<cv signed char>;\ntemplate<> class numeric_limits<cv unsigned char>;\ntemplate<> class numeric_limits<cv wchar_t>;\ntemplate<> class numeric_limits<cv char8_t>; // C++20\ntemplate<> class numeric_limits<cv char16_t>;\ntemplate<> class numeric_limits<cv char32_t>;\n\ntemplate<> class numeric_limits<cv short>;\ntemplate<> class numeric_limits<cv int>;\ntemplate<> class numeric_limits<cv long>;\ntemplate<> class numeric_limits<cv long long>;\ntemplate<> class numeric_limits<cv unsigned short>;\ntemplate<> class numeric_limits<cv unsigned int>;\ntemplate<> class numeric_limits<cv unsigned long>;\ntemplate<> class numeric_limits<cv unsigned long long>;\n\ntemplate<> class numeric_limits<cv float>;\ntemplate<> class numeric_limits<cv double>;\ntemplate<> class numeric_limits<cv long double>;\n\n}  // std\n\n*/\n\n#ifndef __cuda_std__\n#include <__config>\n#else\n#ifdef _LIBCUDACXX_COMPILER_NVRTC\n#include \"climits\"\n#endif // _LIBCUDACXX_COMPILER_NVRTC\n#endif // __cuda_std__\n\n#include \"__assert\" // all public C++ headers provide the assertion handler\n#include \"type_traits\"\n#include \"version\"\n\n#ifndef __cuda_std__\n#include <__pragma_push>\n#endif //__cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n#if defined(_LIBCUDACXX_COMPILER_MSVC)\n#include \"support/win32/limits_msvc_win32.h\"\n#endif // _LIBCUDACXX_MSVCRT\n\n#if defined(_LIBCUDACXX_COMPILER_IBM)\n#include \"support/ibm/limits.h\"\n#endif // _LIBCUDACXX_COMPILER_IBM\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\nenum float_round_style\n{\n    round_indeterminate       = -1,\n    round_toward_zero         =  0,\n    round_to_nearest          =  1,\n    round_toward_infinity     =  2,\n    round_toward_neg_infinity =  3\n};\n\nenum float_denorm_style\n{\n    denorm_indeterminate = -1,\n    denorm_absent = 0,\n    denorm_present = 1\n};\n\ntemplate <class _Tp, bool = is_arithmetic<_Tp>::value>\nclass __libcpp_numeric_limits\n{\nprotected:\n    typedef _Tp type;\n\n    static constexpr bool is_specialized = false;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type min() noexcept {return type();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type max() noexcept {return type();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type lowest() noexcept {return type();}\n\n    static constexpr int  digits = 0;\n    static constexpr int  digits10 = 0;\n    static constexpr int  max_digits10 = 0;\n    static constexpr bool is_signed = false;\n    static constexpr bool is_integer = false;\n    static constexpr bool is_exact = false;\n    static constexpr int  radix = 0;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type epsilon() noexcept {return type();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type round_error() noexcept {return type();}\n\n    static constexpr int  min_exponent = 0;\n    static constexpr int  min_exponent10 = 0;\n    static constexpr int  max_exponent = 0;\n    static constexpr int  max_exponent10 = 0;\n\n    static constexpr bool has_infinity = false;\n    static constexpr bool has_quiet_NaN = false;\n    static constexpr bool has_signaling_NaN = false;\n    static constexpr float_denorm_style has_denorm = denorm_absent;\n    static constexpr bool has_denorm_loss = false;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type infinity() noexcept {return type();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type quiet_NaN() noexcept {return type();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type signaling_NaN() noexcept {return type();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type denorm_min() noexcept {return type();}\n\n    static constexpr bool is_iec559 = false;\n    static constexpr bool is_bounded = false;\n    static constexpr bool is_modulo = false;\n\n    static constexpr bool traps = false;\n    static constexpr bool tinyness_before = false;\n    static constexpr float_round_style round_style = round_toward_zero;\n};\n\ntemplate <class _Tp, int __digits, bool _IsSigned>\nstruct __libcpp_compute_min\n{\n    static constexpr _Tp value = static_cast<_Tp>(_Tp(1) << __digits);\n};\n\ntemplate <class _Tp, int __digits>\nstruct __libcpp_compute_min<_Tp, __digits, false>\n{\n    static constexpr _Tp value = _Tp(0);\n};\n\ntemplate <class _Tp>\nclass __libcpp_numeric_limits<_Tp, true>\n{\nprotected:\n    typedef _Tp type;\n\n    static constexpr bool is_specialized = true;\n\n    static constexpr bool is_signed = type(-1) < type(0);\n    static constexpr int  digits = static_cast<int>(sizeof(type) * __CHAR_BIT__ - is_signed);\n    static constexpr int  digits10 = digits * 3 / 10;\n    static constexpr int  max_digits10 = 0;\n    static constexpr type __min = __libcpp_compute_min<type, digits, is_signed>::value;\n    static constexpr type __max = is_signed ? type(type(~0) ^ __min) : type(~0);\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type min() noexcept {return __min;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type max() noexcept {return __max;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type lowest() noexcept {return min();}\n\n    static constexpr bool is_integer = true;\n    static constexpr bool is_exact = true;\n    static constexpr int  radix = 2;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type epsilon() noexcept {return type(0);}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type round_error() noexcept {return type(0);}\n\n    static constexpr int  min_exponent = 0;\n    static constexpr int  min_exponent10 = 0;\n    static constexpr int  max_exponent = 0;\n    static constexpr int  max_exponent10 = 0;\n\n    static constexpr bool has_infinity = false;\n    static constexpr bool has_quiet_NaN = false;\n    static constexpr bool has_signaling_NaN = false;\n    static constexpr float_denorm_style has_denorm = denorm_absent;\n    static constexpr bool has_denorm_loss = false;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type infinity() noexcept {return type(0);}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type quiet_NaN() noexcept {return type(0);}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type signaling_NaN() noexcept {return type(0);}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type denorm_min() noexcept {return type(0);}\n\n    static constexpr bool is_iec559 = false;\n    static constexpr bool is_bounded = true;\n    static constexpr bool is_modulo = !_CUDA_VSTD::is_signed<_Tp>::value;\n\n#if defined(__i386__) || defined(__x86_64__) || defined(__pnacl__) || \\\n    defined(__wasm__)\n    static constexpr bool traps = true;\n#else\n    static constexpr bool traps = false;\n#endif\n    static constexpr bool tinyness_before = false;\n    static constexpr float_round_style round_style = round_toward_zero;\n};\n\ntemplate <>\nclass __libcpp_numeric_limits<bool, true>\n{\nprotected:\n    typedef bool type;\n\n    static constexpr bool is_specialized = true;\n\n    static constexpr bool is_signed = false;\n    static constexpr int  digits = 1;\n    static constexpr int  digits10 = 0;\n    static constexpr int  max_digits10 = 0;\n    static constexpr type __min = false;\n    static constexpr type __max = true;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type min() noexcept {return __min;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type max() noexcept {return __max;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type lowest() noexcept {return min();}\n\n    static constexpr bool is_integer = true;\n    static constexpr bool is_exact = true;\n    static constexpr int  radix = 2;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type epsilon() noexcept {return type(0);}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type round_error() noexcept {return type(0);}\n\n    static constexpr int  min_exponent = 0;\n    static constexpr int  min_exponent10 = 0;\n    static constexpr int  max_exponent = 0;\n    static constexpr int  max_exponent10 = 0;\n\n    static constexpr bool has_infinity = false;\n    static constexpr bool has_quiet_NaN = false;\n    static constexpr bool has_signaling_NaN = false;\n    static constexpr float_denorm_style has_denorm = denorm_absent;\n    static constexpr bool has_denorm_loss = false;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type infinity() noexcept {return type(0);}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type quiet_NaN() noexcept {return type(0);}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type signaling_NaN() noexcept {return type(0);}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type denorm_min() noexcept {return type(0);}\n\n    static constexpr bool is_iec559 = false;\n    static constexpr bool is_bounded = true;\n    static constexpr bool is_modulo = false;\n\n    static constexpr bool traps = false;\n    static constexpr bool tinyness_before = false;\n    static constexpr float_round_style round_style = round_toward_zero;\n};\n\ntemplate <>\nclass __libcpp_numeric_limits<float, true>\n{\nprotected:\n    typedef float type;\n\n    static constexpr bool is_specialized = true;\n\n    static constexpr bool is_signed = true;\n    static constexpr int  digits = __FLT_MANT_DIG__;\n    static constexpr int  digits10 = __FLT_DIG__;\n    static constexpr int  max_digits10 = 2+(digits * 30103l)/100000l;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type min() noexcept {return __FLT_MIN__;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type max() noexcept {return __FLT_MAX__;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type lowest() noexcept {return -max();}\n\n    static constexpr bool is_integer = false;\n    static constexpr bool is_exact = false;\n    static constexpr int  radix = __FLT_RADIX__;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type epsilon() noexcept {return __FLT_EPSILON__;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type round_error() noexcept {return 0.5F;}\n\n    static constexpr int  min_exponent = __FLT_MIN_EXP__;\n    static constexpr int  min_exponent10 = __FLT_MIN_10_EXP__;\n    static constexpr int  max_exponent = __FLT_MAX_EXP__;\n    static constexpr int  max_exponent10 = __FLT_MAX_10_EXP__;\n\n    static constexpr bool has_infinity = true;\n    static constexpr bool has_quiet_NaN = true;\n    static constexpr bool has_signaling_NaN = true;\n    static constexpr float_denorm_style has_denorm = denorm_present;\n    static constexpr bool has_denorm_loss = false;\n#ifdef _LIBCUDACXX_COMPILER_NVRTC\n    _LIBCUDACXX_INLINE_VISIBILITY static type infinity() noexcept {return __builtin_huge_valf();}\n    _LIBCUDACXX_INLINE_VISIBILITY static type quiet_NaN() noexcept {return __builtin_nanf(\"\");}\n    _LIBCUDACXX_INLINE_VISIBILITY static type signaling_NaN() noexcept {return __builtin_nansf(\"\");}\n#else\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type infinity() noexcept {return __builtin_huge_valf();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type quiet_NaN() noexcept {return __builtin_nanf(\"\");}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type signaling_NaN() noexcept {return __builtin_nansf(\"\");}\n#endif\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type denorm_min() noexcept {return __FLT_DENORM_MIN__;}\n\n    static constexpr bool is_iec559 = true;\n    static constexpr bool is_bounded = true;\n    static constexpr bool is_modulo = false;\n\n    static constexpr bool traps = false;\n    static constexpr bool tinyness_before = false;\n    static constexpr float_round_style round_style = round_to_nearest;\n};\n\ntemplate <>\nclass __libcpp_numeric_limits<double, true>\n{\nprotected:\n    typedef double type;\n\n    static constexpr bool is_specialized = true;\n\n    static constexpr bool is_signed = true;\n    static constexpr int  digits = __DBL_MANT_DIG__;\n    static constexpr int  digits10 = __DBL_DIG__;\n    static constexpr int  max_digits10 = 2+(digits * 30103l)/100000l;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type min() noexcept {return __DBL_MIN__;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type max() noexcept {return __DBL_MAX__;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type lowest() noexcept {return -max();}\n\n    static constexpr bool is_integer = false;\n    static constexpr bool is_exact = false;\n    static constexpr int  radix = __FLT_RADIX__;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type epsilon() noexcept {return __DBL_EPSILON__;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type round_error() noexcept {return 0.5;}\n\n    static constexpr int  min_exponent = __DBL_MIN_EXP__;\n    static constexpr int  min_exponent10 = __DBL_MIN_10_EXP__;\n    static constexpr int  max_exponent = __DBL_MAX_EXP__;\n    static constexpr int  max_exponent10 = __DBL_MAX_10_EXP__;\n\n    static constexpr bool has_infinity = true;\n    static constexpr bool has_quiet_NaN = true;\n    static constexpr bool has_signaling_NaN = true;\n    static constexpr float_denorm_style has_denorm = denorm_present;\n    static constexpr bool has_denorm_loss = false;\n#ifdef _LIBCUDACXX_COMPILER_NVRTC\n    _LIBCUDACXX_INLINE_VISIBILITY static type infinity() noexcept {return __builtin_huge_val();}\n    _LIBCUDACXX_INLINE_VISIBILITY static type quiet_NaN() noexcept {return __builtin_nan(\"\");}\n    _LIBCUDACXX_INLINE_VISIBILITY static type signaling_NaN() noexcept {return __builtin_nans(\"\");}\n#else\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type infinity() noexcept {return __builtin_huge_val();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type quiet_NaN() noexcept {return __builtin_nan(\"\");}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type signaling_NaN() noexcept {return __builtin_nans(\"\");}\n#endif\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type denorm_min() noexcept {return __DBL_DENORM_MIN__;}\n\n    static constexpr bool is_iec559 = true;\n    static constexpr bool is_bounded = true;\n    static constexpr bool is_modulo = false;\n\n    static constexpr bool traps = false;\n    static constexpr bool tinyness_before = false;\n    static constexpr float_round_style round_style = round_to_nearest;\n};\n\ntemplate <>\nclass __libcpp_numeric_limits<long double, true>\n{\n#ifndef _LIBCUDACXX_HAS_NO_LONG_DOUBLE\nprotected:\n    typedef long double type;\n\n    static constexpr bool is_specialized = true;\n\n    static constexpr bool is_signed = true;\n    static constexpr int  digits = __LDBL_MANT_DIG__;\n    static constexpr int  digits10 = __LDBL_DIG__;\n    static constexpr int  max_digits10 = 2+(digits * 30103l)/100000l;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type min() noexcept {return __LDBL_MIN__;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type max() noexcept {return __LDBL_MAX__;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type lowest() noexcept {return -max();}\n\n    static constexpr bool is_integer = false;\n    static constexpr bool is_exact = false;\n    static constexpr int  radix = __FLT_RADIX__;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type epsilon() noexcept {return __LDBL_EPSILON__;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type round_error() noexcept {return 0.5L;}\n\n    static constexpr int  min_exponent = __LDBL_MIN_EXP__;\n    static constexpr int  min_exponent10 = __LDBL_MIN_10_EXP__;\n    static constexpr int  max_exponent = __LDBL_MAX_EXP__;\n    static constexpr int  max_exponent10 = __LDBL_MAX_10_EXP__;\n\n    static constexpr bool has_infinity = true;\n    static constexpr bool has_quiet_NaN = true;\n    static constexpr bool has_signaling_NaN = true;\n    static constexpr float_denorm_style has_denorm = denorm_present;\n    static constexpr bool has_denorm_loss = false;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type infinity() noexcept {return __builtin_huge_vall();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type quiet_NaN() noexcept {return __builtin_nanl(\"\");}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type signaling_NaN() noexcept {return __builtin_nansl(\"\");}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type denorm_min() noexcept {return __LDBL_DENORM_MIN__;}\n\n#if (defined(__ppc__) || defined(__ppc64__))\n    static constexpr bool is_iec559 = false;\n#else\n    static constexpr bool is_iec559 = true;\n#endif\n    static constexpr bool is_bounded = true;\n    static constexpr bool is_modulo = false;\n\n    static constexpr bool traps = false;\n    static constexpr bool tinyness_before = false;\n    static constexpr float_round_style round_style = round_to_nearest;\n#endif\n};\n\ntemplate <class _Tp>\nclass _LIBCUDACXX_TEMPLATE_VIS numeric_limits\n    : private __libcpp_numeric_limits<__remove_cv_t<_Tp>>\n{\n    typedef __libcpp_numeric_limits<__remove_cv_t<_Tp>> __base;\n    typedef typename __base::type type;\npublic:\n    static constexpr bool is_specialized = __base::is_specialized;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type min() noexcept {return __base::min();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type max() noexcept {return __base::max();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type lowest() noexcept {return __base::lowest();}\n\n    static constexpr int  digits = __base::digits;\n    static constexpr int  digits10 = __base::digits10;\n    static constexpr int  max_digits10 = __base::max_digits10;\n    static constexpr bool is_signed = __base::is_signed;\n    static constexpr bool is_integer = __base::is_integer;\n    static constexpr bool is_exact = __base::is_exact;\n    static constexpr int  radix = __base::radix;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type epsilon() noexcept {return __base::epsilon();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type round_error() noexcept {return __base::round_error();}\n\n    static constexpr int  min_exponent = __base::min_exponent;\n    static constexpr int  min_exponent10 = __base::min_exponent10;\n    static constexpr int  max_exponent = __base::max_exponent;\n    static constexpr int  max_exponent10 = __base::max_exponent10;\n\n    static constexpr bool has_infinity = __base::has_infinity;\n    static constexpr bool has_quiet_NaN = __base::has_quiet_NaN;\n    static constexpr bool has_signaling_NaN = __base::has_signaling_NaN;\n    static constexpr float_denorm_style has_denorm = __base::has_denorm;\n    static constexpr bool has_denorm_loss = __base::has_denorm_loss;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type infinity() noexcept {return __base::infinity();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type quiet_NaN() noexcept {return __base::quiet_NaN();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type signaling_NaN() noexcept {return __base::signaling_NaN();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type denorm_min() noexcept {return __base::denorm_min();}\n\n    static constexpr bool is_iec559 = __base::is_iec559;\n    static constexpr bool is_bounded = __base::is_bounded;\n    static constexpr bool is_modulo = __base::is_modulo;\n\n    static constexpr bool traps = __base::traps;\n    static constexpr bool tinyness_before = __base::tinyness_before;\n    static constexpr float_round_style round_style = __base::round_style;\n};\n\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::is_specialized;\ntemplate <class _Tp>\n    constexpr int numeric_limits<_Tp>::digits;\ntemplate <class _Tp>\n    constexpr int numeric_limits<_Tp>::digits10;\ntemplate <class _Tp>\n    constexpr int numeric_limits<_Tp>::max_digits10;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::is_signed;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::is_integer;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::is_exact;\ntemplate <class _Tp>\n    constexpr int numeric_limits<_Tp>::radix;\ntemplate <class _Tp>\n    constexpr int numeric_limits<_Tp>::min_exponent;\ntemplate <class _Tp>\n    constexpr int numeric_limits<_Tp>::min_exponent10;\ntemplate <class _Tp>\n    constexpr int numeric_limits<_Tp>::max_exponent;\ntemplate <class _Tp>\n    constexpr int numeric_limits<_Tp>::max_exponent10;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::has_infinity;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::has_quiet_NaN;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::has_signaling_NaN;\ntemplate <class _Tp>\n    constexpr float_denorm_style numeric_limits<_Tp>::has_denorm;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::has_denorm_loss;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::is_iec559;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::is_bounded;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::is_modulo;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::traps;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::tinyness_before;\ntemplate <class _Tp>\n    constexpr float_round_style numeric_limits<_Tp>::round_style;\n\ntemplate <class _Tp>\nclass _LIBCUDACXX_TEMPLATE_VIS numeric_limits<const _Tp>\n    : private numeric_limits<_Tp>\n{\n    typedef numeric_limits<_Tp> __base;\n    typedef _Tp type;\npublic:\n    static constexpr bool is_specialized = __base::is_specialized;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type min() noexcept {return __base::min();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type max() noexcept {return __base::max();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type lowest() noexcept {return __base::lowest();}\n\n    static constexpr int  digits = __base::digits;\n    static constexpr int  digits10 = __base::digits10;\n    static constexpr int  max_digits10 = __base::max_digits10;\n    static constexpr bool is_signed = __base::is_signed;\n    static constexpr bool is_integer = __base::is_integer;\n    static constexpr bool is_exact = __base::is_exact;\n    static constexpr int  radix = __base::radix;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type epsilon() noexcept {return __base::epsilon();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type round_error() noexcept {return __base::round_error();}\n\n    static constexpr int  min_exponent = __base::min_exponent;\n    static constexpr int  min_exponent10 = __base::min_exponent10;\n    static constexpr int  max_exponent = __base::max_exponent;\n    static constexpr int  max_exponent10 = __base::max_exponent10;\n\n    static constexpr bool has_infinity = __base::has_infinity;\n    static constexpr bool has_quiet_NaN = __base::has_quiet_NaN;\n    static constexpr bool has_signaling_NaN = __base::has_signaling_NaN;\n    static constexpr float_denorm_style has_denorm = __base::has_denorm;\n    static constexpr bool has_denorm_loss = __base::has_denorm_loss;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type infinity() noexcept {return __base::infinity();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type quiet_NaN() noexcept {return __base::quiet_NaN();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type signaling_NaN() noexcept {return __base::signaling_NaN();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type denorm_min() noexcept {return __base::denorm_min();}\n\n    static constexpr bool is_iec559 = __base::is_iec559;\n    static constexpr bool is_bounded = __base::is_bounded;\n    static constexpr bool is_modulo = __base::is_modulo;\n\n    static constexpr bool traps = __base::traps;\n    static constexpr bool tinyness_before = __base::tinyness_before;\n    static constexpr float_round_style round_style = __base::round_style;\n};\n\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::is_specialized;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const _Tp>::digits;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const _Tp>::digits10;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const _Tp>::max_digits10;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::is_signed;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::is_integer;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::is_exact;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const _Tp>::radix;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const _Tp>::min_exponent;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const _Tp>::min_exponent10;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const _Tp>::max_exponent;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const _Tp>::max_exponent10;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::has_infinity;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::has_quiet_NaN;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::has_signaling_NaN;\ntemplate <class _Tp>\n    constexpr float_denorm_style numeric_limits<const _Tp>::has_denorm;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::has_denorm_loss;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::is_iec559;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::is_bounded;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::is_modulo;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::traps;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::tinyness_before;\ntemplate <class _Tp>\n    constexpr float_round_style numeric_limits<const _Tp>::round_style;\n\ntemplate <class _Tp>\nclass _LIBCUDACXX_TEMPLATE_VIS numeric_limits<volatile _Tp>\n    : private numeric_limits<_Tp>\n{\n    typedef numeric_limits<_Tp> __base;\n    typedef _Tp type;\npublic:\n    static constexpr bool is_specialized = __base::is_specialized;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type min() noexcept {return __base::min();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type max() noexcept {return __base::max();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type lowest() noexcept {return __base::lowest();}\n\n    static constexpr int  digits = __base::digits;\n    static constexpr int  digits10 = __base::digits10;\n    static constexpr int  max_digits10 = __base::max_digits10;\n    static constexpr bool is_signed = __base::is_signed;\n    static constexpr bool is_integer = __base::is_integer;\n    static constexpr bool is_exact = __base::is_exact;\n    static constexpr int  radix = __base::radix;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type epsilon() noexcept {return __base::epsilon();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type round_error() noexcept {return __base::round_error();}\n\n    static constexpr int  min_exponent = __base::min_exponent;\n    static constexpr int  min_exponent10 = __base::min_exponent10;\n    static constexpr int  max_exponent = __base::max_exponent;\n    static constexpr int  max_exponent10 = __base::max_exponent10;\n\n    static constexpr bool has_infinity = __base::has_infinity;\n    static constexpr bool has_quiet_NaN = __base::has_quiet_NaN;\n    static constexpr bool has_signaling_NaN = __base::has_signaling_NaN;\n    static constexpr float_denorm_style has_denorm = __base::has_denorm;\n    static constexpr bool has_denorm_loss = __base::has_denorm_loss;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type infinity() noexcept {return __base::infinity();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type quiet_NaN() noexcept {return __base::quiet_NaN();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type signaling_NaN() noexcept {return __base::signaling_NaN();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type denorm_min() noexcept {return __base::denorm_min();}\n\n    static constexpr bool is_iec559 = __base::is_iec559;\n    static constexpr bool is_bounded = __base::is_bounded;\n    static constexpr bool is_modulo = __base::is_modulo;\n\n    static constexpr bool traps = __base::traps;\n    static constexpr bool tinyness_before = __base::tinyness_before;\n    static constexpr float_round_style round_style = __base::round_style;\n};\n\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::is_specialized;\ntemplate <class _Tp>\n    constexpr int numeric_limits<volatile _Tp>::digits;\ntemplate <class _Tp>\n    constexpr int numeric_limits<volatile _Tp>::digits10;\ntemplate <class _Tp>\n    constexpr int numeric_limits<volatile _Tp>::max_digits10;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::is_signed;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::is_integer;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::is_exact;\ntemplate <class _Tp>\n    constexpr int numeric_limits<volatile _Tp>::radix;\ntemplate <class _Tp>\n    constexpr int numeric_limits<volatile _Tp>::min_exponent;\ntemplate <class _Tp>\n    constexpr int numeric_limits<volatile _Tp>::min_exponent10;\ntemplate <class _Tp>\n    constexpr int numeric_limits<volatile _Tp>::max_exponent;\ntemplate <class _Tp>\n    constexpr int numeric_limits<volatile _Tp>::max_exponent10;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::has_infinity;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::has_quiet_NaN;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::has_signaling_NaN;\ntemplate <class _Tp>\n    constexpr float_denorm_style numeric_limits<volatile _Tp>::has_denorm;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::has_denorm_loss;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::is_iec559;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::is_bounded;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::is_modulo;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::traps;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::tinyness_before;\ntemplate <class _Tp>\n    constexpr float_round_style numeric_limits<volatile _Tp>::round_style;\n\ntemplate <class _Tp>\nclass _LIBCUDACXX_TEMPLATE_VIS numeric_limits<const volatile _Tp>\n    : private numeric_limits<_Tp>\n{\n    typedef numeric_limits<_Tp> __base;\n    typedef _Tp type;\npublic:\n    static constexpr bool is_specialized = __base::is_specialized;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type min() noexcept {return __base::min();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type max() noexcept {return __base::max();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type lowest() noexcept {return __base::lowest();}\n\n    static constexpr int  digits = __base::digits;\n    static constexpr int  digits10 = __base::digits10;\n    static constexpr int  max_digits10 = __base::max_digits10;\n    static constexpr bool is_signed = __base::is_signed;\n    static constexpr bool is_integer = __base::is_integer;\n    static constexpr bool is_exact = __base::is_exact;\n    static constexpr int  radix = __base::radix;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type epsilon() noexcept {return __base::epsilon();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type round_error() noexcept {return __base::round_error();}\n\n    static constexpr int  min_exponent = __base::min_exponent;\n    static constexpr int  min_exponent10 = __base::min_exponent10;\n    static constexpr int  max_exponent = __base::max_exponent;\n    static constexpr int  max_exponent10 = __base::max_exponent10;\n\n    static constexpr bool has_infinity = __base::has_infinity;\n    static constexpr bool has_quiet_NaN = __base::has_quiet_NaN;\n    static constexpr bool has_signaling_NaN = __base::has_signaling_NaN;\n    static constexpr float_denorm_style has_denorm = __base::has_denorm;\n    static constexpr bool has_denorm_loss = __base::has_denorm_loss;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type infinity() noexcept {return __base::infinity();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type quiet_NaN() noexcept {return __base::quiet_NaN();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type signaling_NaN() noexcept {return __base::signaling_NaN();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type denorm_min() noexcept {return __base::denorm_min();}\n\n    static constexpr bool is_iec559 = __base::is_iec559;\n    static constexpr bool is_bounded = __base::is_bounded;\n    static constexpr bool is_modulo = __base::is_modulo;\n\n    static constexpr bool traps = __base::traps;\n    static constexpr bool tinyness_before = __base::tinyness_before;\n    static constexpr float_round_style round_style = __base::round_style;\n};\n\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::is_specialized;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const volatile _Tp>::digits;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const volatile _Tp>::digits10;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const volatile _Tp>::max_digits10;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::is_signed;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::is_integer;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::is_exact;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const volatile _Tp>::radix;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const volatile _Tp>::min_exponent;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const volatile _Tp>::min_exponent10;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const volatile _Tp>::max_exponent;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const volatile _Tp>::max_exponent10;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::has_infinity;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::has_quiet_NaN;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::has_signaling_NaN;\ntemplate <class _Tp>\n    constexpr float_denorm_style numeric_limits<const volatile _Tp>::has_denorm;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::has_denorm_loss;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::is_iec559;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::is_bounded;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::is_modulo;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::traps;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::tinyness_before;\ntemplate <class _Tp>\n    constexpr float_round_style numeric_limits<const volatile _Tp>::round_style;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#ifndef __cuda_std__\n#include <__pragma_pop>\n#endif //__cuda_std__\n\n#endif  // _LIBCUDACXX_LIMITS\n", "detail/libcxx/include/tuple": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===--------------------------- tuple ------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX_TUPLE\n#define _LIBCUDACXX_TUPLE\n\n/*\n    tuple synopsis\n\nnamespace std\n{\n\ntemplate <class... T>\nclass tuple {\npublic:\n    explicit(see-below) constexpr tuple();\n    explicit(see-below) tuple(const T&...);  // constexpr in C++14\n    template <class... U>\n        explicit(see-below) tuple(U&&...);  // constexpr in C++14\n    tuple(const tuple&) = default;\n    tuple(tuple&&) = default;\n    template <class... U>\n        explicit(see-below) tuple(const tuple<U...>&);  // constexpr in C++14\n    template <class... U>\n        explicit(see-below) tuple(tuple<U...>&&);  // constexpr in C++14\n    template <class U1, class U2>\n        explicit(see-below) tuple(const pair<U1, U2>&); // iff sizeof...(T) == 2 // constexpr in C++14\n    template <class U1, class U2>\n        explicit(see-below) tuple(pair<U1, U2>&&); // iff sizeof...(T) == 2  // constexpr in C++14\n\n    // allocator-extended constructors\n    template <class Alloc>\n        tuple(allocator_arg_t, const Alloc& a);\n    template <class Alloc>\n        explicit(see-below) tuple(allocator_arg_t, const Alloc& a, const T&...);\n    template <class Alloc, class... U>\n        explicit(see-below) tuple(allocator_arg_t, const Alloc& a, U&&...);\n    template <class Alloc>\n        tuple(allocator_arg_t, const Alloc& a, const tuple&);\n    template <class Alloc>\n        tuple(allocator_arg_t, const Alloc& a, tuple&&);\n    template <class Alloc, class... U>\n        explicit(see-below) tuple(allocator_arg_t, const Alloc& a, const tuple<U...>&);\n    template <class Alloc, class... U>\n        explicit(see-below) tuple(allocator_arg_t, const Alloc& a, tuple<U...>&&);\n    template <class Alloc, class U1, class U2>\n        explicit(see-below) tuple(allocator_arg_t, const Alloc& a, const pair<U1, U2>&);\n    template <class Alloc, class U1, class U2>\n        explicit(see-below) tuple(allocator_arg_t, const Alloc& a, pair<U1, U2>&&);\n\n    tuple& operator=(const tuple&);\n    tuple&\n        operator=(tuple&&) noexcept(AND(is_nothrow_move_assignable<T>::value ...));\n    template <class... U>\n        tuple& operator=(const tuple<U...>&);\n    template <class... U>\n        tuple& operator=(tuple<U...>&&);\n    template <class U1, class U2>\n        tuple& operator=(const pair<U1, U2>&); // iff sizeof...(T) == 2\n    template <class U1, class U2>\n        tuple& operator=(pair<U1, U2>&&); // iff sizeof...(T) == 2\n\n    void swap(tuple&) noexcept(AND(swap(declval<T&>(), declval<T&>())...));\n};\n\ntemplate <class ...T>\ntuple(T...) -> tuple<T...>;                                         // since C++17\ntemplate <class T1, class T2>\ntuple(pair<T1, T2>) -> tuple<T1, T2>;                               // since C++17\ntemplate <class Alloc, class ...T>\ntuple(allocator_arg_t, Alloc, T...) -> tuple<T...>;                 // since C++17\ntemplate <class Alloc, class T1, class T2>\ntuple(allocator_arg_t, Alloc, pair<T1, T2>) -> tuple<T1, T2>;       // since C++17\ntemplate <class Alloc, class ...T>\ntuple(allocator_arg_t, Alloc, tuple<T...>) -> tuple<T...>;          // since C++17\n\ninline constexpr unspecified ignore;\n\ntemplate <class... T> tuple<V...>  make_tuple(T&&...); // constexpr in C++14\ntemplate <class... T> tuple<ATypes...> forward_as_tuple(T&&...) noexcept; // constexpr in C++14\ntemplate <class... T> tuple<T&...> tie(T&...) noexcept; // constexpr in C++14\ntemplate <class... Tuples> tuple<CTypes...> tuple_cat(Tuples&&... tpls); // constexpr in C++14\n\n// [tuple.apply], calling a function with a tuple of arguments:\ntemplate <class F, class Tuple>\n  constexpr decltype(auto) apply(F&& f, Tuple&& t); // C++17\ntemplate <class T, class Tuple>\n  constexpr T make_from_tuple(Tuple&& t); // C++17\n\n// 20.4.1.4, tuple helper classes:\ntemplate <class T> struct tuple_size; // undefined\ntemplate <class... T> struct tuple_size<tuple<T...>>;\ntemplate <class T>\n inline constexpr size_t tuple_size_v = tuple_size<T>::value; // C++17\ntemplate <size_t I, class T> struct tuple_element; // undefined\ntemplate <size_t I, class... T> struct tuple_element<I, tuple<T...>>;\ntemplate <size_t I, class T>\n  using tuple_element_t = typename tuple_element <I, T>::type; // C++14\n\n// 20.4.1.5, element access:\ntemplate <size_t I, class... T>\n    typename tuple_element<I, tuple<T...>>::type&\n    get(tuple<T...>&) noexcept; // constexpr in C++14\ntemplate <size_t I, class... T>\n    const typename tuple_element<I, tuple<T...>>::type&\n    get(const tuple<T...>&) noexcept; // constexpr in C++14\ntemplate <size_t I, class... T>\n    typename tuple_element<I, tuple<T...>>::type&&\n    get(tuple<T...>&&) noexcept; // constexpr in C++14\ntemplate <size_t I, class... T>\n    const typename tuple_element<I, tuple<T...>>::type&&\n    get(const tuple<T...>&&) noexcept; // constexpr in C++14\n\ntemplate <class T1, class... T>\n    constexpr T1& get(tuple<T...>&) noexcept;  // C++14\ntemplate <class T1, class... T>\n    constexpr const T1& get(const tuple<T...>&) noexcept;   // C++14\ntemplate <class T1, class... T>\n    constexpr T1&& get(tuple<T...>&&) noexcept;   // C++14\ntemplate <class T1, class... T>\n    constexpr const T1&& get(const tuple<T...>&&) noexcept;   // C++14\n\n// 20.4.1.6, relational operators:\ntemplate<class... T, class... U> bool operator==(const tuple<T...>&, const tuple<U...>&); // constexpr in C++14\ntemplate<class... T, class... U> bool operator<(const tuple<T...>&, const tuple<U...>&);  // constexpr in C++14\ntemplate<class... T, class... U> bool operator!=(const tuple<T...>&, const tuple<U...>&); // constexpr in C++14\ntemplate<class... T, class... U> bool operator>(const tuple<T...>&, const tuple<U...>&);  // constexpr in C++14\ntemplate<class... T, class... U> bool operator<=(const tuple<T...>&, const tuple<U...>&); // constexpr in C++14\ntemplate<class... T, class... U> bool operator>=(const tuple<T...>&, const tuple<U...>&); // constexpr in C++14\n\ntemplate <class... Types, class Alloc>\n  struct uses_allocator<tuple<Types...>, Alloc>;\n\ntemplate <class... Types>\n  void\n  swap(tuple<Types...>& x, tuple<Types...>& y) noexcept(noexcept(x.swap(y)));\n\n}  // std\n\n*/\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"__assert\" // all public C++ headers provide the assertion handler\n#include \"__functional_base\"\n#include \"__functional/unwrap_ref.h\"\n#include \"__fwd/array.h\"\n#include \"__type_traits/maybe_const.h\"\n#include \"__tuple_dir/apply_cv.h\"\n#include \"__tuple_dir/make_tuple_types.h\"\n#include \"__tuple_dir/sfinae_helpers.h\"\n#include \"__tuple_dir/structured_bindings.h\"\n#include \"__tuple_dir/tuple_element.h\"\n#include \"__tuple_dir/tuple_indices.h\"\n#include \"__tuple_dir/tuple_like.h\"\n#include \"__tuple_dir/tuple_size.h\"\n#include \"__tuple_dir/tuple_types.h\"\n#include \"__utility/forward.h\"\n#include \"__utility/integer_sequence.h\"\n#include \"__utility/move.h\"\n#include \"__utility/pair.h\"\n#include \"__utility/piecewise_construct.h\"\n#include \"__utility/swap.h\"\n#include \"climits\"\n#include \"cstddef\"\n#include \"cstdint\"\n#include \"utility\"\n#include \"type_traits\"\n\n// standard-mandated includes\n#include \"version\"\n\n// [tuple.syn]\n#ifndef _LIBCUDACXX_HAS_NO_SPACESHIP_OPERATOR\n#include \"compare\"\n#endif\n\n#ifndef __cuda_std__\n#include <__pragma_push>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n\n// __tuple_leaf\n\ntemplate <size_t _Ip, class _Hp,\n          bool=is_empty<_Hp>::value && !__libcpp_is_final<_Hp>::value\n         >\nclass __tuple_leaf;\n\ntemplate <size_t _Ip, class _Hp, bool _Ep>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nvoid swap(__tuple_leaf<_Ip, _Hp, _Ep>& __x, __tuple_leaf<_Ip, _Hp, _Ep>& __y)\n    noexcept(__is_nothrow_swappable<_Hp>::value)\n{\n    swap(__x.get(), __y.get());\n}\n\ntemplate <size_t _Ip, class _Hp, bool>\nclass __tuple_leaf\n{\n    _Hp __value_;\n\n    template <class _Tp>\n    _LIBCUDACXX_INLINE_VISIBILITY\n    static constexpr bool __can_bind_reference() {\n#if __has_keyword(__reference_binds_to_temporary)\n      return !__reference_binds_to_temporary(_Hp, _Tp);\n#else\n      return true;\n#endif\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY __tuple_leaf& operator=(const __tuple_leaf&);\npublic:\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr __tuple_leaf()\n             noexcept(is_nothrow_default_constructible<_Hp>::value) : __value_()\n       {static_assert(!is_reference<_Hp>::value,\n              \"Attempted to default construct a reference element in a tuple\");}\n\n    template <class _Alloc>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        __tuple_leaf(integral_constant<int, 0>, const _Alloc&)\n            : __value_()\n        {static_assert(!is_reference<_Hp>::value,\n              \"Attempted to default construct a reference element in a tuple\");}\n\n    template <class _Alloc>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        __tuple_leaf(integral_constant<int, 1>, const _Alloc& __a)\n            : __value_(allocator_arg_t(), __a)\n        {static_assert(!is_reference<_Hp>::value,\n              \"Attempted to default construct a reference element in a tuple\");}\n\n    template <class _Alloc>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        __tuple_leaf(integral_constant<int, 2>, const _Alloc& __a)\n            : __value_(__a)\n        {static_assert(!is_reference<_Hp>::value,\n              \"Attempted to default construct a reference element in a tuple\");}\n\n    template<class _Tp>\n    using __can_forward = _And<_IsNotSame<__remove_cvref_t<_Tp>, __tuple_leaf>,\n                               is_constructible<_Hp, _Tp>>;\n\n    template <class _Tp,\n              __enable_if_t<__can_forward<_Tp>::value, int> = 0>\n        _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n        explicit __tuple_leaf(_Tp&& __t) noexcept((is_nothrow_constructible<_Hp, _Tp>::value))\n            : __value_(_CUDA_VSTD::forward<_Tp>(__t))\n        {static_assert(__can_bind_reference<_Tp&&>(),\n       \"Attempted construction of reference element binds to a temporary whose lifetime has ended\");}\n\n    template <class _Tp, class _Alloc>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        explicit __tuple_leaf(integral_constant<int, 0>, const _Alloc&, _Tp&& __t)\n            : __value_(_CUDA_VSTD::forward<_Tp>(__t))\n        {static_assert(__can_bind_reference<_Tp&&>(),\n       \"Attempted construction of reference element binds to a temporary whose lifetime has ended\");}\n\n    template <class _Tp, class _Alloc>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        explicit __tuple_leaf(integral_constant<int, 1>, const _Alloc& __a, _Tp&& __t)\n            : __value_(allocator_arg_t(), __a, _CUDA_VSTD::forward<_Tp>(__t))\n        {static_assert(!is_reference<_Hp>::value,\n            \"Attempted to uses-allocator construct a reference element in a tuple\");}\n\n    template <class _Tp, class _Alloc>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        explicit __tuple_leaf(integral_constant<int, 2>, const _Alloc& __a, _Tp&& __t)\n            : __value_(_CUDA_VSTD::forward<_Tp>(__t), __a)\n        {static_assert(!is_reference<_Hp>::value,\n           \"Attempted to uses-allocator construct a reference element in a tuple\");}\n\n    __tuple_leaf(const __tuple_leaf& __t) = default;\n    __tuple_leaf(__tuple_leaf&& __t) = default;\n\n    template <class _Tp>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        __tuple_leaf&\n        operator=(_Tp&& __t) noexcept((is_nothrow_assignable<_Hp&, _Tp>::value))\n        {\n            __value_ = _CUDA_VSTD::forward<_Tp>(__t);\n            return *this;\n        }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    int swap(__tuple_leaf& __t) noexcept(__is_nothrow_swappable<__tuple_leaf>::value)\n    {\n        _CUDA_VSTD::swap(*this, __t);\n        return 0;\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11       _Hp& get()       noexcept {return __value_;}\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 const _Hp& get() const noexcept {return __value_;}\n};\n\ntemplate <size_t _Ip, class _Hp>\nclass __tuple_leaf<_Ip, _Hp, true>\n    : private _Hp\n{\n\n    _LIBCUDACXX_INLINE_VISIBILITY __tuple_leaf& operator=(const __tuple_leaf&);\npublic:\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr __tuple_leaf()\n             noexcept(is_nothrow_default_constructible<_Hp>::value) {}\n\n    template <class _Alloc>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        __tuple_leaf(integral_constant<int, 0>, const _Alloc&) {}\n\n    template <class _Alloc>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        __tuple_leaf(integral_constant<int, 1>, const _Alloc& __a)\n            : _Hp(allocator_arg_t(), __a) {}\n\n    template <class _Alloc>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        __tuple_leaf(integral_constant<int, 2>, const _Alloc& __a)\n            : _Hp(__a) {}\n\n    template<class _Tp>\n    using __can_forward = _And<_IsNotSame<__remove_cvref_t<_Tp>, __tuple_leaf>,\n                               is_constructible<_Hp, _Tp>>;\n\n    template <class _Tp,\n              __enable_if_t<__can_forward<_Tp>::value, int> = 0>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    explicit __tuple_leaf(_Tp&& __t) noexcept((is_nothrow_constructible<_Hp, _Tp>::value))\n        : _Hp(_CUDA_VSTD::forward<_Tp>(__t)) {}\n\n    template <class _Tp, class _Alloc>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        explicit __tuple_leaf(integral_constant<int, 0>, const _Alloc&, _Tp&& __t)\n            : _Hp(_CUDA_VSTD::forward<_Tp>(__t)) {}\n\n    template <class _Tp, class _Alloc>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        explicit __tuple_leaf(integral_constant<int, 1>, const _Alloc& __a, _Tp&& __t)\n            : _Hp(allocator_arg_t(), __a, _CUDA_VSTD::forward<_Tp>(__t)) {}\n\n    template <class _Tp, class _Alloc>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        explicit __tuple_leaf(integral_constant<int, 2>, const _Alloc& __a, _Tp&& __t)\n            : _Hp(_CUDA_VSTD::forward<_Tp>(__t), __a) {}\n\n    __tuple_leaf(__tuple_leaf const &) = default;\n    __tuple_leaf(__tuple_leaf &&) = default;\n\n    template <class _Tp>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        __tuple_leaf&\n        operator=(_Tp&& __t) noexcept((is_nothrow_assignable<_Hp&, _Tp>::value))\n        {\n            _Hp::operator=(_CUDA_VSTD::forward<_Tp>(__t));\n            return *this;\n        }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    int\n    swap(__tuple_leaf& __t) noexcept(__is_nothrow_swappable<__tuple_leaf>::value)\n    {\n        _CUDA_VSTD::swap(*this, __t);\n        return 0;\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11       _Hp& get()       noexcept {return static_cast<_Hp&>(*this);}\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 const _Hp& get() const noexcept {return static_cast<const _Hp&>(*this);}\n};\n\ntemplate <class ..._Tp>\n_LIBCUDACXX_INLINE_VISIBILITY\nvoid __swallow(_Tp&&...) noexcept {}\n\ntemplate <class _Tp>\nstruct __all_default_constructible;\n\ntemplate <class ..._Tp>\nstruct __all_default_constructible<__tuple_types<_Tp...>>\n    : __all<is_default_constructible<_Tp>::value...>\n{ };\n\n// __tuple_impl\n\ntemplate<class _Indx, class ..._Tp> struct __tuple_impl;\n\ntemplate<size_t ..._Indx, class ..._Tp>\nstruct _LIBCUDACXX_DECLSPEC_EMPTY_BASES __tuple_impl<__tuple_indices<_Indx...>, _Tp...>\n    : public __tuple_leaf<_Indx, _Tp>...\n{\n    _LIBCUDACXX_INLINE_VISIBILITY\n    constexpr __tuple_impl()\n        noexcept(__all<is_nothrow_default_constructible<_Tp>::value...>::value) {}\n\n\n    // Handle non-allocator, full initialization\n    template <size_t ..._Uf, class ..._Tf,\n              size_t ..._Ul, class ..._Tl, class ..._Up,\n              _EnableIf<sizeof...(_Up) == sizeof...(_Tp), bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n        explicit\n        __tuple_impl(__tuple_indices<_Uf...>, __tuple_types<_Tf...>,\n                     __tuple_indices<_Ul...>, __tuple_types<_Tl...>,\n                     _Up&&... __u)\n                     noexcept((__all<is_nothrow_constructible<_Tf, _Up>::value...>::value)) :\n            __tuple_leaf<_Uf, _Tf>(_CUDA_VSTD::forward<_Up>(__u))...\n            {}\n\n    // Handle non-allocator, partial default initialization\n    template <size_t ..._Uf, class ..._Tf,\n              size_t ..._Ul, class ..._Tl, class ..._Up,\n              _EnableIf<(sizeof...(_Up) > 0) && (sizeof...(_Up) < sizeof...(_Tp)), bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n        explicit\n        __tuple_impl(__tuple_indices<_Uf...>, __tuple_types<_Tf...>,\n                     __tuple_indices<_Ul...>, __tuple_types<_Tl...>,\n                     _Up&&... __u)\n                     noexcept((__all<is_nothrow_constructible<_Tf, _Up>::value...>::value &&\n                                 __all<is_nothrow_default_constructible<_Tl>::value...>::value)) :\n            __tuple_leaf<_Uf, _Tf>(_CUDA_VSTD::forward<_Up>(__u))...,\n            __tuple_leaf<_Ul, _Tl>()...\n            {}\n\n    // Handle non-allocator, full default initialization\n    template <size_t ..._Uf, class ..._Tf,\n              size_t ..._Ul, class ..._Tl, class ..._Up,\n              _EnableIf<sizeof...(_Up) == 0, bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n        explicit\n        __tuple_impl(__tuple_indices<_Uf...>, __tuple_types<_Tf...>,\n                     __tuple_indices<_Ul...>, __tuple_types<_Tl...>,\n                     _Up&&...)\n                     noexcept((__all<is_nothrow_default_constructible<_Tl>::value...>::value)) :\n            __tuple_leaf<_Ul, _Tl>()...\n            {}\n\n\n    // Handle allocator aware, full initialization\n    template <class _Alloc, size_t ..._Uf, class ..._Tf,\n              size_t ..._Ul, class ..._Tl, class ..._Up,\n              _EnableIf<sizeof...(_Up) == sizeof...(_Tp), bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        explicit\n        __tuple_impl(allocator_arg_t, const _Alloc& __a,\n                     __tuple_indices<_Uf...>, __tuple_types<_Tf...>,\n                     __tuple_indices<_Ul...>, __tuple_types<_Tl...>,\n                     _Up&&... __u) :\n            __tuple_leaf<_Uf, _Tf>(__uses_alloc_ctor<_Tf, _Alloc, _Up>(), __a,\n                    _CUDA_VSTD::forward<_Up>(__u))...\n            {}\n\n    // Handle allocator aware, partial default initialization\n    template <class _Alloc, size_t ..._Uf, class ..._Tf,\n              size_t ..._Ul, class ..._Tl, class ..._Up,\n              _EnableIf<(sizeof...(_Up) > 0) && (sizeof...(_Up) < sizeof...(_Tp)), bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        explicit\n        __tuple_impl(allocator_arg_t, const _Alloc& __a,\n                     __tuple_indices<_Uf...>, __tuple_types<_Tf...>,\n                     __tuple_indices<_Ul...>, __tuple_types<_Tl...>,\n                     _Up&&... __u) :\n            __tuple_leaf<_Uf, _Tf>(__uses_alloc_ctor<_Tf, _Alloc, _Up>(), __a,\n                    _CUDA_VSTD::forward<_Up>(__u))...,\n            __tuple_leaf<_Ul, _Tl>(__uses_alloc_ctor<_Tl, _Alloc>(), __a)...\n            {}\n\n    // Handle allocator aware, full default initialization\n    template <class _Alloc, size_t ..._Uf, class ..._Tf,\n              size_t ..._Ul, class ..._Tl, class ..._Up,\n              _EnableIf<sizeof...(_Up) == 0, bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        explicit\n        __tuple_impl(allocator_arg_t, const _Alloc& __a,\n                     __tuple_indices<_Uf...>, __tuple_types<_Tf...>,\n                     __tuple_indices<_Ul...>, __tuple_types<_Tl...>,\n                     _Up&&...) :\n            __tuple_leaf<_Ul, _Tl>(__uses_alloc_ctor<_Tl, _Alloc>(), __a)...\n            {}\n\n    template <class _Tuple,\n              _EnableIf<\n                    __tuple_constructible<_Tuple, tuple<_Tp...> >::value,\n                    bool\n                > = false\n             >\n        _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n        __tuple_impl(_Tuple&& __t) noexcept((__all<is_nothrow_constructible<_Tp, typename tuple_element<_Indx,\n                                       typename __make_tuple_types<_Tuple>::type>::type>::value...>::value))\n            : __tuple_leaf<_Indx, _Tp>(_CUDA_VSTD::forward<typename tuple_element<_Indx,\n                                       typename __make_tuple_types<_Tuple>::type>::type>(_CUDA_VSTD::get<_Indx>(__t)))...\n            {}\n\n    template <class _Alloc, class _Tuple,\n              _EnableIf<\n                    __tuple_constructible<_Tuple, tuple<_Tp...> >::value,\n                    bool\n                > = false\n             >\n        _LIBCUDACXX_INLINE_VISIBILITY\n        __tuple_impl(allocator_arg_t, const _Alloc& __a, _Tuple&& __t)\n            : __tuple_leaf<_Indx, _Tp>(__uses_alloc_ctor<_Tp, _Alloc, typename tuple_element<_Indx,\n                                       typename __make_tuple_types<_Tuple>::type>::type>(), __a,\n                                       _CUDA_VSTD::forward<typename tuple_element<_Indx,\n                                       typename __make_tuple_types<_Tuple>::type>::type>(_CUDA_VSTD::get<_Indx>(__t)))...\n            {}\n\n    template <class _Tuple>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        typename enable_if\n        <\n            __tuple_assignable<_Tuple, tuple<_Tp...> >::value,\n            __tuple_impl&\n        >::type\n        operator=(_Tuple&& __t) noexcept((__all<is_nothrow_assignable<_Tp&, typename tuple_element<_Indx,\n                                       typename __make_tuple_types<_Tuple>::type>::type>::value...>::value))\n        {\n            __swallow(__tuple_leaf<_Indx, _Tp>::operator=(_CUDA_VSTD::forward<typename tuple_element<_Indx,\n                                       typename __make_tuple_types<_Tuple>::type>::type>(_CUDA_VSTD::get<_Indx>(__t)))...);\n            return *this;\n        }\n\n    __tuple_impl(const __tuple_impl&) = default;\n    __tuple_impl(__tuple_impl&&) = default;\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    __tuple_impl&\n    operator=(const __tuple_impl& __t) noexcept((__all<is_nothrow_copy_assignable<_Tp>::value...>::value))\n    {\n        __swallow(__tuple_leaf<_Indx, _Tp>::operator=(static_cast<const __tuple_leaf<_Indx, _Tp>&>(__t).get())...);\n        return *this;\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    __tuple_impl&\n    operator=(__tuple_impl&& __t) noexcept((__all<is_nothrow_move_assignable<_Tp>::value...>::value))\n    {\n        __swallow(__tuple_leaf<_Indx, _Tp>::operator=(_CUDA_VSTD::forward<_Tp>(static_cast<__tuple_leaf<_Indx, _Tp>&>(__t).get()))...);\n        return *this;\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    void swap(__tuple_impl& __t)\n        noexcept(__all<__is_nothrow_swappable<_Tp>::value...>::value)\n    {\n        __swallow(__tuple_leaf<_Indx, _Tp>::swap(static_cast<__tuple_leaf<_Indx, _Tp>&>(__t))...);\n    }\n};\n\n\n\ntemplate <class ..._Tp>\nclass _LIBCUDACXX_TEMPLATE_VIS tuple\n{\n    typedef __tuple_impl<typename __make_tuple_indices<sizeof...(_Tp)>::type, _Tp...> _BaseT;\n\n    _BaseT __base_;\n\n#if defined(_LIBCUDACXX_ENABLE_TUPLE_IMPLICIT_REDUCED_ARITY_EXTENSION)\n    static constexpr bool _EnableImplicitReducedArityExtension = true;\n#else\n    static constexpr bool _EnableImplicitReducedArityExtension = false;\n#endif\n\n    template <class ..._Args>\n    struct _PackExpandsToThisTuple : false_type {};\n\n    template <class _Arg>\n    struct _PackExpandsToThisTuple<_Arg>\n        : is_same<__remove_cvref_t<_Arg>, tuple> {};\n\n    template <bool _MaybeEnable, class _Dummy = void>\n    struct _CheckArgsConstructor : __check_tuple_constructor_fail {};\n\n    template <class _Dummy>\n    struct _CheckArgsConstructor<true, _Dummy>\n    {\n        template <int&...>\n        struct __enable_implicit_default {\n            static constexpr bool value =\n                __all<__is_implicitly_default_constructible<_Tp>::value...>::value;\n        };\n\n        template <int&...>\n        struct __enable_explicit_default {\n            static constexpr bool value =\n                __all<is_default_constructible<_Tp>::value ...>::value &&\n                !__enable_implicit_default<>::value;\n        };\n\n        template <class ..._Args>\n        struct __enable_explicit {\n            static constexpr bool value =\n                __tuple_constructible<\n                    tuple<_Args...>,\n                    typename __make_tuple_types<tuple,\n                             sizeof...(_Args) < sizeof...(_Tp) ?\n                                 sizeof...(_Args) :\n                                 sizeof...(_Tp)>::type\n                >::value &&\n                !__tuple_convertible<\n                    tuple<_Args...>,\n                    typename __make_tuple_types<tuple,\n                             sizeof...(_Args) < sizeof...(_Tp) ?\n                                 sizeof...(_Args) :\n                                 sizeof...(_Tp)>::type\n                >::value &&\n                __all_default_constructible<\n                    typename __make_tuple_types<tuple, sizeof...(_Tp),\n                             sizeof...(_Args) < sizeof...(_Tp) ?\n                                 sizeof...(_Args) :\n                                 sizeof...(_Tp)>::type\n                >::value;\n        };\n\n        template <class ..._Args>\n        struct __enable_implicit {\n            static constexpr bool value =\n               __tuple_constructible<\n                    tuple<_Args...>,\n                    typename __make_tuple_types<tuple,\n                             sizeof...(_Args) < sizeof...(_Tp) ?\n                                 sizeof...(_Args) :\n                                 sizeof...(_Tp)>::type\n                >::value &&\n                __tuple_convertible<\n                    tuple<_Args...>,\n                    typename __make_tuple_types<tuple,\n                             sizeof...(_Args) < sizeof...(_Tp) ?\n                                 sizeof...(_Args) :\n                                 sizeof...(_Tp)>::type\n                >::value &&\n                __all_default_constructible<\n                    typename __make_tuple_types<tuple, sizeof...(_Tp),\n                             sizeof...(_Args) < sizeof...(_Tp) ?\n                                 sizeof...(_Args) :\n                                 sizeof...(_Tp)>::type\n                >::value;\n        };\n    };\n\n    template <bool _MaybeEnable,\n              bool = sizeof...(_Tp) == 1,\n              class _Dummy = void>\n    struct _CheckTupleLikeConstructor : __check_tuple_constructor_fail {};\n\n    template <class _Dummy>\n    struct _CheckTupleLikeConstructor<true, false, _Dummy>\n    {\n        template <class _Tuple>\n        struct __enable_implicit {\n            static constexpr bool value =\n                   __tuple_constructible<_Tuple, tuple>::value\n                && __tuple_convertible<_Tuple, tuple>::value;\n        };\n\n        template <class _Tuple>\n        struct __enable_explicit {\n            static constexpr bool value =\n                   __tuple_constructible<_Tuple, tuple>::value\n               && !__tuple_convertible<_Tuple, tuple>::value;\n        };\n    };\n\n    template <class _Dummy>\n    struct _CheckTupleLikeConstructor<true, true, _Dummy>\n    {\n        template <class _Tuple>\n        struct _PreferTupleLikeConstructorImpl :\n            _Or<\n                // Don't attempt the two checks below if the tuple we are given\n                // has the same type as this tuple.\n                _IsSame<__remove_cvref_t<_Tuple>, tuple>,\n                _Lazy<_And,\n                    _Not<is_constructible<_Tp..., _Tuple>>,\n                    _Not<is_convertible<_Tuple, _Tp...>>\n                >\n            >\n        { };\n\n        // This trait is used to disable the tuple-like constructor when\n        // the UTypes... constructor should be selected instead.\n        // See LWG issue #2549.\n        template <class _Tuple>\n        using _PreferTupleLikeConstructor = _PreferTupleLikeConstructorImpl<_Tuple>;\n\n        template <class _Tuple>\n        struct __enable_implicit {\n            static constexpr bool value =\n                _And<\n                    __tuple_constructible<_Tuple, tuple>,\n                    __tuple_convertible<_Tuple, tuple>,\n                    _PreferTupleLikeConstructor<_Tuple>\n                >::value;\n        };\n\n        template <class _Tuple>\n        struct __enable_explicit {\n            static constexpr bool value =\n                _And<\n                    __tuple_constructible<_Tuple, tuple>,\n                    _PreferTupleLikeConstructor<_Tuple>,\n                    _Not<__tuple_convertible<_Tuple, tuple>>\n                >::value;\n        };\n    };\n\n    template <size_t _Jp, class ..._Up> friend\n        _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n        typename tuple_element<_Jp, tuple<_Up...> >::type& get(tuple<_Up...>&) noexcept;\n    template <size_t _Jp, class ..._Up> friend\n        _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n        const typename tuple_element<_Jp, tuple<_Up...> >::type& get(const tuple<_Up...>&) noexcept;\n    template <size_t _Jp, class ..._Up> friend\n        _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n        typename tuple_element<_Jp, tuple<_Up...> >::type&& get(tuple<_Up...>&&) noexcept;\n    template <size_t _Jp, class ..._Up> friend\n        _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 _LIBCUDACXX_INLINE_VISIBILITY\n        const typename tuple_element<_Jp, tuple<_Up...> >::type&& get(const tuple<_Up...>&&) noexcept;\npublic:\n\n    template<bool _Dummy>\n    struct _EnableConstructor {\n        static constexpr bool __implicit_default =\n            _CheckArgsConstructor<_Dummy>::template __enable_implicit_default<>::value;\n        static constexpr bool __explicit_default =\n            _CheckArgsConstructor<_Dummy>::template __enable_explicit_default<>::value;\n\n        static constexpr bool __implicit_variadic =\n            _CheckArgsConstructor<_Dummy>::template __enable_implicit<_Tp const&...>::value;\n        static constexpr bool __explicit_variadic =\n            _CheckArgsConstructor<_Dummy>::template __enable_explicit<_Tp const&...>::value;\n    };\n\n    template <bool _Dummy = true, __enable_if_t<_EnableConstructor<_Dummy>::__implicit_default, bool> = false>\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    tuple() noexcept(__all<is_nothrow_default_constructible<_Tp>::value...>::value) {}\n\n    template <bool _Dummy = true, __enable_if_t<_EnableConstructor<_Dummy>::__explicit_default, bool> = false>\n    explicit _LIBCUDACXX_INLINE_VISIBILITY constexpr\n    tuple() noexcept(__all<is_nothrow_default_constructible<_Tp>::value...>::value) {}\n\n    tuple(tuple const&) = default;\n    tuple(tuple&&) = default;\n\n    template <class _AllocArgT>\n    struct _EnableAllocConstructor {\n        static constexpr bool __implicit =\n            _CheckArgsConstructor<_IsSame<allocator_arg_t, _AllocArgT>::value >::template __enable_implicit_default<>::value;\n        static constexpr bool __explicit =\n            _CheckArgsConstructor<_IsSame<allocator_arg_t, _AllocArgT>::value >::template __enable_explicit_default<>::value;\n    };\n\n    template <class _AllocArgT, class _Alloc,\n              __enable_if_t<_EnableAllocConstructor<_AllocArgT>::__implicit, bool> = false>\n    _LIBCUDACXX_INLINE_VISIBILITY\n    tuple(_AllocArgT, _Alloc const& __a)\n      : __base_(allocator_arg_t(), __a,\n                    __tuple_indices<>(), __tuple_types<>(),\n                    typename __make_tuple_indices<sizeof...(_Tp), 0>::type(),\n                    __tuple_types<_Tp...>()) {}\n\n    template <class _AllocArgT, class _Alloc,\n              __enable_if_t<_EnableAllocConstructor<_AllocArgT>::__explicit, bool> = false>\n    explicit _LIBCUDACXX_INLINE_VISIBILITY\n    tuple(_AllocArgT, _Alloc const& __a)\n      : __base_(allocator_arg_t(), __a,\n                    __tuple_indices<>(), __tuple_types<>(),\n                    typename __make_tuple_indices<sizeof...(_Tp), 0>::type(),\n                    __tuple_types<_Tp...>()) {}\n\n    template <bool _Dummy = true, __enable_if_t<_EnableConstructor<_Dummy>::__implicit_variadic, bool> = false>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    tuple(const _Tp& ... __t) noexcept((__all<is_nothrow_copy_constructible<_Tp>::value...>::value))\n        : __base_(typename __make_tuple_indices<sizeof...(_Tp)>::type(),\n                typename __make_tuple_types<tuple, sizeof...(_Tp)>::type(),\n                typename __make_tuple_indices<0>::type(),\n                typename __make_tuple_types<tuple, 0>::type(),\n                __t...\n               ) {}\n\n    template <bool _Dummy = true, __enable_if_t<_EnableConstructor<_Dummy>::__explicit_variadic, bool> = false>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    explicit tuple(const _Tp& ... __t) noexcept((__all<is_nothrow_copy_constructible<_Tp>::value...>::value))\n        : __base_(typename __make_tuple_indices<sizeof...(_Tp)>::type(),\n                typename __make_tuple_types<tuple, sizeof...(_Tp)>::type(),\n                typename __make_tuple_indices<0>::type(),\n                typename __make_tuple_types<tuple, 0>::type(),\n                __t...\n               ) {}\n\n    template <class _Alloc, bool _Dummy = true, __enable_if_t<_EnableConstructor<_Dummy>::__implicit_variadic, bool> = false>\n      _LIBCUDACXX_INLINE_VISIBILITY\n      tuple(allocator_arg_t, const _Alloc& __a, const _Tp& ... __t)\n        : __base_(allocator_arg_t(), __a,\n                typename __make_tuple_indices<sizeof...(_Tp)>::type(),\n                typename __make_tuple_types<tuple, sizeof...(_Tp)>::type(),\n                typename __make_tuple_indices<0>::type(),\n                typename __make_tuple_types<tuple, 0>::type(),\n                __t...\n               ) {}\n\n    template <class _Alloc, bool _Dummy = true, __enable_if_t<_EnableConstructor<_Dummy>::__explicit_variadic, bool> = false>\n      _LIBCUDACXX_INLINE_VISIBILITY\n      explicit\n      tuple(allocator_arg_t, const _Alloc& __a, const _Tp& ... __t)\n        : __base_(allocator_arg_t(), __a,\n                typename __make_tuple_indices<sizeof...(_Tp)>::type(),\n                typename __make_tuple_types<tuple, sizeof...(_Tp)>::type(),\n                typename __make_tuple_indices<0>::type(),\n                typename __make_tuple_types<tuple, 0>::type(),\n                __t...\n               ) {}\n\n#if defined(_LIBCUDACXX_NO_TUPLE_NOEXCEPT)\n    template <class ..._Vp>\n    using __base_noexcept_constructible = false_type;\n#else\n    template <class ..._Vp>\n    using __base_noexcept_constructible = is_nothrow_constructible<\n                _BaseT,\n                _Vp...\n            >;\n#endif // defined(_LIBCUDACXX_NO_TUPLE_NOEXCEPT)\n\n    template <class ..._Up>\n    struct _EnableVariadicConvertingConstructor {\n        static constexpr bool _PackIsTuple = _PackExpandsToThisTuple<_Up...>::value;\n        static constexpr bool __exact_match = sizeof...(_Up) == sizeof...(_Tp) && !_PackIsTuple;\n        static constexpr bool __less_arity  = sizeof...(_Up) < sizeof...(_Tp) && !_PackIsTuple;\n\n        static constexpr bool __implicit =\n            _CheckArgsConstructor<__exact_match>::template __enable_implicit<_Up...>::value ||\n            _CheckArgsConstructor<__less_arity && _EnableImplicitReducedArityExtension >::template __enable_implicit<_Up...>::value;\n        static constexpr bool __explicit =\n            _CheckArgsConstructor<__exact_match>::template __enable_explicit<_Up...>::value ||\n            _CheckArgsConstructor<__less_arity && !_EnableImplicitReducedArityExtension>::template __enable_implicit<_Up...>::value;\n\n        static constexpr bool __implicit_alloc =\n            _CheckArgsConstructor<__exact_match>::template __enable_implicit<_Up...>::value;\n        static constexpr bool __explicit_alloc =\n            _CheckArgsConstructor<__exact_match>::template __enable_explicit<_Up...>::value;\n    };\n    template <class ..._Up,\n              __enable_if_t<_EnableVariadicConvertingConstructor<_Up...>::__implicit, bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n        tuple(_Up&&... __u)\n            noexcept((\n                __base_noexcept_constructible<\n                    typename __make_tuple_indices<sizeof...(_Up)>::type,\n                    typename __make_tuple_types<tuple, sizeof...(_Up)>::type,\n                    typename __make_tuple_indices<sizeof...(_Tp), sizeof...(_Up)>::type,\n                    typename __make_tuple_types<tuple, sizeof...(_Tp), sizeof...(_Up)>::type,\n                    _Up...\n                >::value\n            ))\n            : __base_(typename __make_tuple_indices<sizeof...(_Up)>::type(),\n                    typename __make_tuple_types<tuple, sizeof...(_Up)>::type(),\n                    typename __make_tuple_indices<sizeof...(_Tp), sizeof...(_Up)>::type(),\n                    typename __make_tuple_types<tuple, sizeof...(_Tp), sizeof...(_Up)>::type(),\n                    _CUDA_VSTD::forward<_Up>(__u)...) {}\n\n    template <class ..._Up,\n              __enable_if_t<_EnableVariadicConvertingConstructor<_Up...>::__explicit, bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n        explicit\n        tuple(_Up&&... __u)\n            noexcept((\n                __base_noexcept_constructible<\n                    typename __make_tuple_indices<sizeof...(_Up)>::type,\n                    typename __make_tuple_types<tuple, sizeof...(_Up)>::type,\n                    typename __make_tuple_indices<sizeof...(_Tp), sizeof...(_Up)>::type,\n                    typename __make_tuple_types<tuple, sizeof...(_Tp), sizeof...(_Up)>::type,\n                    _Up...\n                >::value\n            ))\n            : __base_(typename __make_tuple_indices<sizeof...(_Up)>::type(),\n                    typename __make_tuple_types<tuple, sizeof...(_Up)>::type(),\n                    typename __make_tuple_indices<sizeof...(_Tp), sizeof...(_Up)>::type(),\n                    typename __make_tuple_types<tuple, sizeof...(_Tp), sizeof...(_Up)>::type(),\n                    _CUDA_VSTD::forward<_Up>(__u)...) {}\n\n    template <class _Alloc, class ..._Up,\n              __enable_if_t<_EnableVariadicConvertingConstructor<_Up...>::__implicit_alloc, bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        tuple(allocator_arg_t, const _Alloc& __a, _Up&&... __u)\n            : __base_(allocator_arg_t(), __a,\n                    typename __make_tuple_indices<sizeof...(_Up)>::type(),\n                    typename __make_tuple_types<tuple, sizeof...(_Up)>::type(),\n                    typename __make_tuple_indices<sizeof...(_Tp), sizeof...(_Up)>::type(),\n                    typename __make_tuple_types<tuple, sizeof...(_Tp), sizeof...(_Up)>::type(),\n                    _CUDA_VSTD::forward<_Up>(__u)...) {}\n\n    template <class _Alloc, class ..._Up,\n              __enable_if_t<_EnableVariadicConvertingConstructor<_Up...>::__explicit_alloc, bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        explicit\n        tuple(allocator_arg_t, const _Alloc& __a, _Up&&... __u)\n            : __base_(allocator_arg_t(), __a,\n                    typename __make_tuple_indices<sizeof...(_Up)>::type(),\n                    typename __make_tuple_types<tuple, sizeof...(_Up)>::type(),\n                    typename __make_tuple_indices<sizeof...(_Tp), sizeof...(_Up)>::type(),\n                    typename __make_tuple_types<tuple, sizeof...(_Tp), sizeof...(_Up)>::type(),\n                    _CUDA_VSTD::forward<_Up>(__u)...) {}\n\n\n    template <class _Tuple, bool _DisableIfLValue>\n    struct _EnableTupleLikeConstructor {\n        static constexpr bool __implicit =\n            _CheckTupleLikeConstructor<__tuple_like_with_size<_Tuple, sizeof...(_Tp)>::value\n                             && !_PackExpandsToThisTuple<_Tuple>::value\n                             && (!is_lvalue_reference<_Tuple>::value || !_DisableIfLValue)\n                         >::template __enable_implicit<_Tuple>::value;\n\n        static constexpr bool __explicit =\n            _CheckTupleLikeConstructor<__tuple_like_with_size<_Tuple, sizeof...(_Tp)>::value\n                             && !_PackExpandsToThisTuple<_Tuple>::value\n                             && (!is_lvalue_reference<_Tuple>::value || !_DisableIfLValue)\n                         >::template __enable_explicit<_Tuple>::value;\n\n        static constexpr bool __implicit_alloc =\n            _CheckTupleLikeConstructor<__tuple_like_with_size<_Tuple, sizeof...(_Tp)>::value>::template __enable_implicit<_Tuple>::value;\n\n        static constexpr bool __explicit_alloc =\n            _CheckTupleLikeConstructor<__tuple_like_with_size<_Tuple, sizeof...(_Tp)>::value>::template __enable_explicit<_Tuple>::value;\n    };\n\n    template <class _Tuple, __enable_if_t<_EnableTupleLikeConstructor<_Tuple, true>::__implicit, bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n        tuple(_Tuple&& __t) noexcept((is_nothrow_constructible<_BaseT, _Tuple>::value))\n            : __base_(_CUDA_VSTD::forward<_Tuple>(__t)) {}\n\n    template <class _Tuple, __enable_if_t<_EnableTupleLikeConstructor<const _Tuple&, false>::__implicit, bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n        tuple(const _Tuple& __t) noexcept((is_nothrow_constructible<_BaseT, const _Tuple&>::value))\n            : __base_(__t) {}\n\n    template <class _Tuple, __enable_if_t<_EnableTupleLikeConstructor<_Tuple, true>::__explicit, bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n        explicit\n        tuple(_Tuple&& __t) noexcept((is_nothrow_constructible<_BaseT, _Tuple>::value))\n            : __base_(_CUDA_VSTD::forward<_Tuple>(__t)) {}\n\n    template <class _Tuple, __enable_if_t<_EnableTupleLikeConstructor<const _Tuple&, false>::__explicit, bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n        explicit\n        tuple(const _Tuple& __t) noexcept((is_nothrow_constructible<_BaseT, const _Tuple&>::value))\n            : __base_(__t) {}\n\n    template <class _Alloc, class _Tuple,\n              __enable_if_t<_EnableTupleLikeConstructor<_Tuple, true>::__implicit_alloc, bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        tuple(allocator_arg_t, const _Alloc& __a, _Tuple&& __t)\n            : __base_(allocator_arg_t(), __a, _CUDA_VSTD::forward<_Tuple>(__t)) {}\n\n    template <class _Alloc, class _Tuple,\n              __enable_if_t<_EnableTupleLikeConstructor<_Tuple, true>::__explicit_alloc, bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        explicit\n        tuple(allocator_arg_t, const _Alloc& __a, _Tuple&& __t)\n            : __base_(allocator_arg_t(), __a, _CUDA_VSTD::forward<_Tuple>(__t)) {}\n\n    using _CanCopyAssign = __all<is_copy_assignable<_Tp>::value...>;\n    using _CanMoveAssign = __all<is_move_assignable<_Tp>::value...>;\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    tuple& operator=(__conditional_t<_CanCopyAssign::value, tuple, __nat> const& __t)\n        noexcept((__all<is_nothrow_copy_assignable<_Tp>::value...>::value))\n    {\n        __base_.operator=(__t.__base_);\n        return *this;\n    }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    tuple& operator=(__conditional_t<_CanMoveAssign::value, tuple, __nat>&& __t)\n        noexcept((__all<is_nothrow_move_assignable<_Tp>::value...>::value))\n    {\n        __base_.operator=(static_cast<_BaseT&&>(__t.__base_));\n        return *this;\n    }\n\n    template <class _Tuple,\n              __enable_if_t<__tuple_assignable<_Tuple, tuple>::value, bool> = false>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        tuple&\n        operator=(_Tuple&& __t) noexcept((is_nothrow_assignable<_BaseT&, _Tuple>::value))\n        {\n            __base_.operator=(_CUDA_VSTD::forward<_Tuple>(__t));\n            return *this;\n        }\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    void swap(tuple& __t) noexcept(__all<__is_nothrow_swappable<_Tp>::value...>::value)\n        {__base_.swap(__t.__base_);}\n};\n\ntemplate <>\nclass _LIBCUDACXX_TEMPLATE_VIS tuple<>\n{\npublic:\n    constexpr tuple() noexcept = default;\n    template <class _Alloc>\n    _LIBCUDACXX_INLINE_VISIBILITY\n        tuple(allocator_arg_t, const _Alloc&) noexcept {}\n    template <class _Alloc>\n    _LIBCUDACXX_INLINE_VISIBILITY\n        tuple(allocator_arg_t, const _Alloc&, const tuple&) noexcept {}\n    template <class _Up>\n    _LIBCUDACXX_INLINE_VISIBILITY\n        tuple(array<_Up, 0>) noexcept {}\n    template <class _Alloc, class _Up>\n    _LIBCUDACXX_INLINE_VISIBILITY\n        tuple(allocator_arg_t, const _Alloc&, array<_Up, 0>) noexcept {}\n    _LIBCUDACXX_INLINE_VISIBILITY\n    void swap(tuple&) noexcept {}\n};\n\n#ifndef _LIBCUDACXX_HAS_NO_DEDUCTION_GUIDES\ntemplate <class ..._Tp>\ntuple(_Tp...) -> tuple<_Tp...>;\ntemplate <class _Tp1, class _Tp2>\ntuple(pair<_Tp1, _Tp2>) -> tuple<_Tp1, _Tp2>;\ntemplate <class _Alloc, class ..._Tp>\ntuple(allocator_arg_t, _Alloc, _Tp...) -> tuple<_Tp...>;\ntemplate <class _Alloc, class _Tp1, class _Tp2>\ntuple(allocator_arg_t, _Alloc, pair<_Tp1, _Tp2>) -> tuple<_Tp1, _Tp2>;\ntemplate <class _Alloc, class ..._Tp>\ntuple(allocator_arg_t, _Alloc, tuple<_Tp...>) -> tuple<_Tp...>;\n#endif\n\ntemplate <class ..._Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY\n__enable_if_t<_And<__is_swappable<_Tp>...>::value, void>\nswap(tuple<_Tp...>& __t, tuple<_Tp...>& __u) noexcept(__all<__is_nothrow_swappable<_Tp>::value...>::value) {\n    __t.swap(__u);\n}\n\n// get\ntemplate <size_t _Ip, class ..._Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\ntypename tuple_element<_Ip, tuple<_Tp...> >::type&\nget(tuple<_Tp...>& __t) noexcept\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE typename tuple_element<_Ip, tuple<_Tp...> >::type type;\n    return static_cast<__tuple_leaf<_Ip, type>&>(__t.__base_).get();\n}\n\ntemplate <size_t _Ip, class ..._Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nconst typename tuple_element<_Ip, tuple<_Tp...> >::type&\nget(const tuple<_Tp...>& __t) noexcept\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE typename tuple_element<_Ip, tuple<_Tp...> >::type type;\n    return static_cast<const __tuple_leaf<_Ip, type>&>(__t.__base_).get();\n}\n\ntemplate <size_t _Ip, class ..._Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\ntypename tuple_element<_Ip, tuple<_Tp...> >::type&&\nget(tuple<_Tp...>&& __t) noexcept\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE typename tuple_element<_Ip, tuple<_Tp...> >::type type;\n    return static_cast<type&&>(\n             static_cast<__tuple_leaf<_Ip, type>&&>(__t.__base_).get());\n}\n\ntemplate <size_t _Ip, class ..._Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nconst typename tuple_element<_Ip, tuple<_Tp...> >::type&&\nget(const tuple<_Tp...>&& __t) noexcept\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE typename tuple_element<_Ip, tuple<_Tp...> >::type type;\n    return static_cast<const type&&>(\n             static_cast<const __tuple_leaf<_Ip, type>&&>(__t.__base_).get());\n}\n\n#if _LIBCUDACXX_STD_VER > 11\n\nnamespace __find_detail {\n\nstatic constexpr size_t __not_found = ~size_t(0);\nstatic constexpr size_t __ambiguous = __not_found - 1;\n\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr size_t __find_idx_return(size_t __curr_i, size_t __res, bool __matches) {\n    return !__matches ? __res :\n        (__res == __not_found ? __curr_i : __ambiguous);\n}\n\ntemplate <size_t _Nx>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr size_t __find_idx(size_t __i, const bool (&__matches)[_Nx]) {\n  return __i == _Nx ? __not_found :\n      __find_idx_return(__i, __find_idx(__i + 1, __matches), __matches[__i]);\n}\n\ntemplate <class _T1, class ..._Args>\nstruct __find_exactly_one_checked {\n    static constexpr bool __matches[sizeof...(_Args)] = {is_same<_T1, _Args>::value...};\n    static constexpr size_t value = __find_detail::__find_idx(0, __matches);\n    static_assert(value != __not_found, \"type not found in type list\" );\n    static_assert(value != __ambiguous, \"type occurs more than once in type list\");\n};\n\ntemplate <class _T1>\nstruct __find_exactly_one_checked<_T1> {\n    static_assert(!is_same<_T1, _T1>::value, \"type not in empty type list\");\n};\n\n} // namespace __find_detail;\n\ntemplate <typename _T1, typename... _Args>\nstruct __find_exactly_one_t\n    : public __find_detail::__find_exactly_one_checked<_T1, _Args...> {\n};\n\ntemplate <class _T1, class... _Args>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr _T1& get(tuple<_Args...>& __tup) noexcept\n{\n    return _CUDA_VSTD::get<__find_exactly_one_t<_T1, _Args...>::value>(__tup);\n}\n\ntemplate <class _T1, class... _Args>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr _T1 const& get(tuple<_Args...> const& __tup) noexcept\n{\n    return _CUDA_VSTD::get<__find_exactly_one_t<_T1, _Args...>::value>(__tup);\n}\n\ntemplate <class _T1, class... _Args>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr _T1&& get(tuple<_Args...>&& __tup) noexcept\n{\n    return _CUDA_VSTD::get<__find_exactly_one_t<_T1, _Args...>::value>(_CUDA_VSTD::move(__tup));\n}\n\ntemplate <class _T1, class... _Args>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr _T1 const&& get(tuple<_Args...> const&& __tup) noexcept\n{\n    return _CUDA_VSTD::get<__find_exactly_one_t<_T1, _Args...>::value>(_CUDA_VSTD::move(__tup));\n}\n\n#endif\n\n// tie\n\ntemplate <class ..._Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\ntuple<_Tp&...>\ntie(_Tp&... __t) noexcept\n{\n    return tuple<_Tp&...>(__t...);\n}\n\ntemplate <class _Up>\nstruct __ignore_t\n{\n    template <class _Tp>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    const __ignore_t& operator=(_Tp&&) const {return *this;}\n};\n\nnamespace {\n#ifdef __CUDA_ARCH__\n    static\n#endif\n    _LIBCUDACXX_INLINE_VAR constexpr __ignore_t<unsigned char> ignore = __ignore_t<unsigned char>();\n}\n\ntemplate <class... _Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\ntuple<typename __unwrap_ref_decay<_Tp>::type...>\nmake_tuple(_Tp&&... __t)\n{\n    return tuple<typename __unwrap_ref_decay<_Tp>::type...>(_CUDA_VSTD::forward<_Tp>(__t)...);\n}\n\ntemplate <class... _Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\ntuple<_Tp&&...>\nforward_as_tuple(_Tp&&... __t) noexcept\n{\n    return tuple<_Tp&&...>(_CUDA_VSTD::forward<_Tp>(__t)...);\n}\n\ntemplate <size_t _Ip>\nstruct __tuple_equal\n{\n    template <class _Tp, class _Up>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    bool operator()(const _Tp& __x, const _Up& __y)\n    {\n        return __tuple_equal<_Ip - 1>()(__x, __y) && _CUDA_VSTD::get<_Ip-1>(__x) == _CUDA_VSTD::get<_Ip-1>(__y);\n    }\n};\n\ntemplate <>\nstruct __tuple_equal<0>\n{\n    template <class _Tp, class _Up>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    bool operator()(const _Tp&, const _Up&)\n    {\n        return true;\n    }\n};\n\ntemplate <class ..._Tp, class ..._Up>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nbool\noperator==(const tuple<_Tp...>& __x, const tuple<_Up...>& __y)\n{\n    static_assert (sizeof...(_Tp) == sizeof...(_Up), \"Can't compare tuples of different sizes\");\n    return __tuple_equal<sizeof...(_Tp)>()(__x, __y);\n}\n\ntemplate <class ..._Tp, class ..._Up>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nbool\noperator!=(const tuple<_Tp...>& __x, const tuple<_Up...>& __y)\n{\n    return !(__x == __y);\n}\n\ntemplate <size_t _Ip>\nstruct __tuple_less\n{\n    template <class _Tp, class _Up>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    bool operator()(const _Tp& __x, const _Up& __y)\n    {\n        const size_t __idx = tuple_size<_Tp>::value - _Ip;\n        if (_CUDA_VSTD::get<__idx>(__x) < _CUDA_VSTD::get<__idx>(__y))\n            return true;\n        if (_CUDA_VSTD::get<__idx>(__y) < _CUDA_VSTD::get<__idx>(__x))\n            return false;\n        return __tuple_less<_Ip-1>()(__x, __y);\n    }\n};\n\ntemplate <>\nstruct __tuple_less<0>\n{\n    template <class _Tp, class _Up>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    bool operator()(const _Tp&, const _Up&)\n    {\n        return false;\n    }\n};\n\ntemplate <class ..._Tp, class ..._Up>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nbool\noperator<(const tuple<_Tp...>& __x, const tuple<_Up...>& __y)\n{\n    static_assert (sizeof...(_Tp) == sizeof...(_Up), \"Can't compare tuples of different sizes\");\n    return __tuple_less<sizeof...(_Tp)>()(__x, __y);\n}\n\ntemplate <class ..._Tp, class ..._Up>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nbool\noperator>(const tuple<_Tp...>& __x, const tuple<_Up...>& __y)\n{\n    return __y < __x;\n}\n\ntemplate <class ..._Tp, class ..._Up>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nbool\noperator>=(const tuple<_Tp...>& __x, const tuple<_Up...>& __y)\n{\n    return !(__x < __y);\n}\n\ntemplate <class ..._Tp, class ..._Up>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nbool\noperator<=(const tuple<_Tp...>& __x, const tuple<_Up...>& __y)\n{\n    return !(__y < __x);\n}\n\n// tuple_cat\n\ntemplate <class _Tp, class _Up> struct __tuple_cat_type;\n\ntemplate <class ..._Ttypes, class ..._Utypes>\nstruct __tuple_cat_type<tuple<_Ttypes...>, __tuple_types<_Utypes...> >\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE tuple<_Ttypes..., _Utypes...> type;\n};\n\ntemplate <class _ResultTuple, bool _Is_Tuple0TupleLike, class ..._Tuples>\nstruct __tuple_cat_return_1\n{\n};\n\ntemplate <class ..._Types, class _Tuple0>\nstruct __tuple_cat_return_1<tuple<_Types...>, true, _Tuple0>\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE typename __tuple_cat_type<tuple<_Types...>,\n            typename __make_tuple_types<__remove_cvref_t<_Tuple0>>::type>::type\n                                                                           type;\n};\n\ntemplate <class ..._Types, class _Tuple0, class _Tuple1, class ..._Tuples>\nstruct __tuple_cat_return_1<tuple<_Types...>, true, _Tuple0, _Tuple1, _Tuples...>\n    : public __tuple_cat_return_1<\n                 typename __tuple_cat_type<\n                     tuple<_Types...>,\n                     typename __make_tuple_types<__remove_cvref_t<_Tuple0>>::type\n                 >::type,\n                 __tuple_like<typename remove_reference<_Tuple1>::type>::value,\n                 _Tuple1, _Tuples...>\n{\n};\n\ntemplate <class ..._Tuples> struct __tuple_cat_return;\n\ntemplate <class _Tuple0, class ..._Tuples>\nstruct __tuple_cat_return<_Tuple0, _Tuples...>\n    : public __tuple_cat_return_1<tuple<>,\n         __tuple_like<typename remove_reference<_Tuple0>::type>::value, _Tuple0,\n                                                                     _Tuples...>\n{\n};\n\ntemplate <>\nstruct __tuple_cat_return<>\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE tuple<> type;\n};\n\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\ntuple<>\ntuple_cat()\n{\n    return tuple<>();\n}\n\ntemplate <class _Rp, class _Indices, class _Tuple0, class ..._Tuples>\nstruct __tuple_cat_return_ref_imp;\n\ntemplate <class ..._Types, size_t ..._I0, class _Tuple0>\nstruct __tuple_cat_return_ref_imp<tuple<_Types...>, __tuple_indices<_I0...>, _Tuple0>\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE typename remove_reference<_Tuple0>::type _T0;\n    typedef tuple<_Types..., typename __apply_cv<_Tuple0,\n                          typename tuple_element<_I0, _T0>::type>::type&&...> type;\n};\n\ntemplate <class ..._Types, size_t ..._I0, class _Tuple0, class _Tuple1, class ..._Tuples>\nstruct __tuple_cat_return_ref_imp<tuple<_Types...>, __tuple_indices<_I0...>,\n                                  _Tuple0, _Tuple1, _Tuples...>\n    : public __tuple_cat_return_ref_imp<\n         tuple<_Types..., typename __apply_cv<_Tuple0,\n               typename tuple_element<_I0,\n                  typename remove_reference<_Tuple0>::type>::type>::type&&...>,\n         typename __make_tuple_indices<tuple_size<typename\n                                 remove_reference<_Tuple1>::type>::value>::type,\n         _Tuple1, _Tuples...>\n{\n};\n\ntemplate <class _Tuple0, class ..._Tuples>\nstruct __tuple_cat_return_ref\n    : public __tuple_cat_return_ref_imp<tuple<>,\n               typename __make_tuple_indices<\n                        tuple_size<typename remove_reference<_Tuple0>::type>::value\n               >::type, _Tuple0, _Tuples...>\n{\n};\n\ntemplate <class _Types, class _I0, class _J0>\nstruct __tuple_cat;\n\ntemplate <class ..._Types, size_t ..._I0, size_t ..._J0>\nstruct __tuple_cat<tuple<_Types...>, __tuple_indices<_I0...>, __tuple_indices<_J0...> >\n{\n    template <class _Tuple0>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    typename __tuple_cat_return_ref<tuple<_Types...>&&, _Tuple0&&>::type\n    operator()(tuple<_Types...> __t, _Tuple0&& __t0)\n    {\n        (void)__t;\n        return _CUDA_VSTD::forward_as_tuple(_CUDA_VSTD::forward<_Types>(_CUDA_VSTD::get<_I0>(__t))...,\n                                            _CUDA_VSTD::get<_J0>(_CUDA_VSTD::forward<_Tuple0>(__t0))...);\n    }\n\n    template <class _Tuple0, class _Tuple1, class ..._Tuples>\n    _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    typename __tuple_cat_return_ref<tuple<_Types...>&&, _Tuple0&&, _Tuple1&&, _Tuples&&...>::type\n    operator()(tuple<_Types...> __t, _Tuple0&& __t0, _Tuple1&& __t1, _Tuples&& ...__tpls)\n    {\n        (void)__t;\n        typedef _LIBCUDACXX_NODEBUG_TYPE typename remove_reference<_Tuple0>::type _T0;\n        typedef _LIBCUDACXX_NODEBUG_TYPE typename remove_reference<_Tuple1>::type _T1;\n        return __tuple_cat<\n           tuple<_Types..., typename __apply_cv<_Tuple0, typename tuple_element<_J0, _T0>::type>::type&&...>,\n           typename __make_tuple_indices<sizeof ...(_Types) + tuple_size<_T0>::value>::type,\n           typename __make_tuple_indices<tuple_size<_T1>::value>::type>()\n                           (_CUDA_VSTD::forward_as_tuple(\n                              _CUDA_VSTD::forward<_Types>(_CUDA_VSTD::get<_I0>(__t))...,\n                              _CUDA_VSTD::get<_J0>(_CUDA_VSTD::forward<_Tuple0>(__t0))...\n                            ),\n                            _CUDA_VSTD::forward<_Tuple1>(__t1),\n                            _CUDA_VSTD::forward<_Tuples>(__tpls)...);\n    }\n};\n\ntemplate <class _Tuple0, class... _Tuples>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\ntypename __tuple_cat_return<_Tuple0, _Tuples...>::type\ntuple_cat(_Tuple0&& __t0, _Tuples&&... __tpls)\n{\n    typedef _LIBCUDACXX_NODEBUG_TYPE typename remove_reference<_Tuple0>::type _T0;\n    return __tuple_cat<tuple<>, __tuple_indices<>,\n                  typename __make_tuple_indices<tuple_size<_T0>::value>::type>()\n                  (tuple<>(), _CUDA_VSTD::forward<_Tuple0>(__t0),\n                                            _CUDA_VSTD::forward<_Tuples>(__tpls)...);\n}\n\ntemplate <class ..._Tp, class _Alloc>\nstruct _LIBCUDACXX_TEMPLATE_VIS uses_allocator<tuple<_Tp...>, _Alloc>\n    : true_type {};\n\ntemplate <class _T1, class _T2>\ntemplate <class... _Args1, class... _Args2, size_t ..._I1, size_t ..._I2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\npair<_T1, _T2>::pair(piecewise_construct_t,\n                     tuple<_Args1...>& __first_args, tuple<_Args2...>& __second_args,\n                     __tuple_indices<_I1...>, __tuple_indices<_I2...>)\n    :  first(_CUDA_VSTD::forward<_Args1>(_CUDA_VSTD::get<_I1>( __first_args))...),\n      second(_CUDA_VSTD::forward<_Args2>(_CUDA_VSTD::get<_I2>(__second_args))...)\n{\n}\n\n#if _LIBCUDACXX_STD_VER > 14\ntemplate <class _Tp>\n_LIBCUDACXX_INLINE_VAR constexpr size_t tuple_size_v = tuple_size<_Tp>::value;\n\n#define _LIBCUDACXX_NOEXCEPT_RETURN(...) noexcept(noexcept(__VA_ARGS__)) { return __VA_ARGS__; }\n\ntemplate <class _Fn, class _Tuple, size_t ..._Id>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr decltype(auto) __apply_tuple_impl(_Fn && __f, _Tuple && __t,\n                                            __tuple_indices<_Id...>)\n_LIBCUDACXX_NOEXCEPT_RETURN(\n    _CUDA_VSTD::__invoke(\n        _CUDA_VSTD::forward<_Fn>(__f),\n        _CUDA_VSTD::get<_Id>(_CUDA_VSTD::forward<_Tuple>(__t))...)\n)\n\ntemplate <class _Fn, class _Tuple>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr decltype(auto) apply(_Fn && __f, _Tuple && __t)\n_LIBCUDACXX_NOEXCEPT_RETURN(\n    _CUDA_VSTD::__apply_tuple_impl(\n        _CUDA_VSTD::forward<_Fn>(__f), _CUDA_VSTD::forward<_Tuple>(__t),\n        typename __make_tuple_indices<tuple_size_v<remove_reference_t<_Tuple>>>::type{})\n)\n\ntemplate <class _Tp, class _Tuple, size_t... _Idx>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr _Tp __make_from_tuple_impl(_Tuple&& __t, __tuple_indices<_Idx...>)\n_LIBCUDACXX_NOEXCEPT_RETURN(\n    _Tp(_CUDA_VSTD::get<_Idx>(_CUDA_VSTD::forward<_Tuple>(__t))...)\n)\n\ntemplate <class _Tp, class _Tuple>\ninline _LIBCUDACXX_INLINE_VISIBILITY\nconstexpr _Tp make_from_tuple(_Tuple&& __t)\n_LIBCUDACXX_NOEXCEPT_RETURN(\n    _CUDA_VSTD::__make_from_tuple_impl<_Tp>(_CUDA_VSTD::forward<_Tuple>(__t),\n        typename __make_tuple_indices<tuple_size_v<remove_reference_t<_Tuple>>>::type{})\n)\n\n#undef _LIBCUDACXX_NOEXCEPT_RETURN\n\n#endif // _LIBCUDACXX_STD_VER > 14\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#ifndef __cuda_std__\n#include <__pragma_pop>\n#endif //__cuda_std__\n\n#endif  // _LIBCUDACXX_TUPLE\n", "detail/libcxx/include/type_traits": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===------------------------ type_traits ---------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX_TYPE_TRAITS\n#define _LIBCUDACXX_TYPE_TRAITS\n\n/*\n    type_traits synopsis\n\nnamespace std\n{\n\n    // helper class:\n    template <class T, T v> struct integral_constant;\n    typedef integral_constant<bool, true>  true_type;   // C++11\n    typedef integral_constant<bool, false> false_type;  // C++11\n\n    template <bool B>                                   // C++14\n    using bool_constant = integral_constant<bool, B>;   // C++14\n    typedef bool_constant<true> true_type;              // C++14\n    typedef bool_constant<false> false_type;            // C++14\n\n    // helper traits\n    template <bool, class T = void> struct enable_if;\n    template <bool, class T, class F> struct conditional;\n\n    // Primary classification traits:\n    template <class T> struct is_void;\n    template <class T> struct is_null_pointer;  // C++14\n    template <class T> struct is_integral;\n    template <class T> struct is_floating_point;\n    template <class T> struct is_array;\n    template <class T> struct is_pointer;\n    template <class T> struct is_lvalue_reference;\n    template <class T> struct is_rvalue_reference;\n    template <class T> struct is_member_object_pointer;\n    template <class T> struct is_member_function_pointer;\n    template <class T> struct is_enum;\n    template <class T> struct is_union;\n    template <class T> struct is_class;\n    template <class T> struct is_function;\n\n    // Secondary classification traits:\n    template <class T> struct is_reference;\n    template <class T> struct is_arithmetic;\n    template <class T> struct is_fundamental;\n    template <class T> struct is_member_pointer;\n    template <class T> struct is_scalar;\n    template <class T> struct is_object;\n    template <class T> struct is_compound;\n\n    // Const-volatile properties and transformations:\n    template <class T> struct is_const;\n    template <class T> struct is_volatile;\n    template <class T> struct remove_const;\n    template <class T> struct remove_volatile;\n    template <class T> struct remove_cv;\n    template <class T> struct add_const;\n    template <class T> struct add_volatile;\n    template <class T> struct add_cv;\n\n    // Reference transformations:\n    template <class T> struct remove_reference;\n    template <class T> struct add_lvalue_reference;\n    template <class T> struct add_rvalue_reference;\n\n    // Pointer transformations:\n    template <class T> struct remove_pointer;\n    template <class T> struct add_pointer;\n\n    template<class T> struct type_identity;                     // C++20\n    template<class T>\n      using type_identity_t = typename type_identity<T>::type;  // C++20\n\n    // Integral properties:\n    template <class T> struct is_signed;\n    template <class T> struct is_unsigned;\n    template <class T> struct make_signed;\n    template <class T> struct make_unsigned;\n\n    // Array properties and transformations:\n    template <class T> struct rank;\n    template <class T, unsigned I = 0> struct extent;\n    template <class T> struct remove_extent;\n    template <class T> struct remove_all_extents;\n\n    template <class T> struct is_bounded_array;                 // C++20\n    template <class T> struct is_unbounded_array;               // C++20\n\n    // Member introspection:\n    template <class T> struct is_pod;\n    template <class T> struct is_trivial;\n    template <class T> struct is_trivially_copyable;\n    template <class T> struct is_standard_layout;\n    template <class T> struct is_literal_type;\n    template <class T> struct is_empty;\n    template <class T> struct is_polymorphic;\n    template <class T> struct is_abstract;\n    template <class T> struct is_final; // C++14\n    template <class T> struct is_aggregate; // C++17\n\n    template <class T, class... Args> struct is_constructible;\n    template <class T>                struct is_default_constructible;\n    template <class T>                struct is_copy_constructible;\n    template <class T>                struct is_move_constructible;\n    template <class T, class U>       struct is_assignable;\n    template <class T>                struct is_copy_assignable;\n    template <class T>                struct is_move_assignable;\n    template <class T, class U>       struct is_swappable_with;       // C++17\n    template <class T>                struct is_swappable;            // C++17\n    template <class T>                struct is_destructible;\n\n    template <class T, class... Args> struct is_trivially_constructible;\n    template <class T>                struct is_trivially_default_constructible;\n    template <class T>                struct is_trivially_copy_constructible;\n    template <class T>                struct is_trivially_move_constructible;\n    template <class T, class U>       struct is_trivially_assignable;\n    template <class T>                struct is_trivially_copy_assignable;\n    template <class T>                struct is_trivially_move_assignable;\n    template <class T>                struct is_trivially_destructible;\n\n    template <class T, class... Args> struct is_nothrow_constructible;\n    template <class T>                struct is_nothrow_default_constructible;\n    template <class T>                struct is_nothrow_copy_constructible;\n    template <class T>                struct is_nothrow_move_constructible;\n    template <class T, class U>       struct is_nothrow_assignable;\n    template <class T>                struct is_nothrow_copy_assignable;\n    template <class T>                struct is_nothrow_move_assignable;\n    template <class T, class U>       struct is_nothrow_swappable_with; // C++17\n    template <class T>                struct is_nothrow_swappable;      // C++17\n    template <class T>                struct is_nothrow_destructible;\n\n    template <class T> struct has_virtual_destructor;\n\n    template<class T> struct has_unique_object_representations;         // C++17\n\n    // Relationships between types:\n    template <class T, class U> struct is_same;\n    template <class Base, class Derived> struct is_base_of;\n\n    template <class From, class To> struct is_convertible;\n    template <typename From, typename To> struct is_nothrow_convertible;                  // C++20\n    template <typename From, typename To> inline constexpr bool is_nothrow_convertible_v; // C++20\n\n    template <class Fn, class... ArgTypes> struct is_invocable;\n    template <class R, class Fn, class... ArgTypes> struct is_invocable_r;\n\n    template <class Fn, class... ArgTypes> struct is_nothrow_invocable;\n    template <class R, class Fn, class... ArgTypes> struct is_nothrow_invocable_r;\n\n    // Alignment properties and transformations:\n    template <class T> struct alignment_of;\n    template <size_t Len, size_t Align = most_stringent_alignment_requirement>\n        struct aligned_storage;\n    template <size_t Len, class... Types> struct aligned_union;\n    template <class T> struct remove_cvref; // C++20\n\n    template <class T> struct decay;\n    template <class... T> struct common_type;\n    template <class T> struct underlying_type;\n    template <class> class result_of; // undefined\n    template <class Fn, class... ArgTypes> class result_of<Fn(ArgTypes...)>;\n    template <class Fn, class... ArgTypes> struct invoke_result;  // C++17\n\n    // const-volatile modifications:\n    template <class T>\n      using remove_const_t    = typename remove_const<T>::type;  // C++14\n    template <class T>\n      using remove_volatile_t = typename remove_volatile<T>::type;  // C++14\n    template <class T>\n      using remove_cv_t       = typename remove_cv<T>::type;  // C++14\n    template <class T>\n      using add_const_t       = typename add_const<T>::type;  // C++14\n    template <class T>\n      using add_volatile_t    = typename add_volatile<T>::type;  // C++14\n    template <class T>\n      using add_cv_t          = typename add_cv<T>::type;  // C++14\n\n    // reference modifications:\n    template <class T>\n      using remove_reference_t     = typename remove_reference<T>::type;  // C++14\n    template <class T>\n      using add_lvalue_reference_t = typename add_lvalue_reference<T>::type;  // C++14\n    template <class T>\n      using add_rvalue_reference_t = typename add_rvalue_reference<T>::type;  // C++14\n\n    // sign modifications:\n    template <class T>\n      using make_signed_t   = typename make_signed<T>::type;  // C++14\n    template <class T>\n      using make_unsigned_t = typename make_unsigned<T>::type;  // C++14\n\n    // array modifications:\n    template <class T>\n      using remove_extent_t      = typename remove_extent<T>::type;  // C++14\n    template <class T>\n      using remove_all_extents_t = typename remove_all_extents<T>::type;  // C++14\n\n    template <class T>\n      inline constexpr bool is_bounded_array_v\n        = is_bounded_array<T>::value;                                     // C++20\n      inline constexpr bool is_unbounded_array_v\n        = is_unbounded_array<T>::value;                                   // C++20\n\n    // pointer modifications:\n    template <class T>\n      using remove_pointer_t = typename remove_pointer<T>::type;  // C++14\n    template <class T>\n      using add_pointer_t    = typename add_pointer<T>::type;  // C++14\n\n    // other transformations:\n    template <size_t Len, std::size_t Align=default-alignment>\n      using aligned_storage_t = typename aligned_storage<Len,Align>::type;  // C++14\n    template <std::size_t Len, class... Types>\n      using aligned_union_t   = typename aligned_union<Len,Types...>::type;  // C++14\n    template <class T>\n      using remove_cvref_t    = typename remove_cvref<T>::type;  // C++20\n    template <class T>\n      using decay_t           = typename decay<T>::type;  // C++14\n    template <bool b, class T=void>\n      using enable_if_t       = typename enable_if<b,T>::type;  // C++14\n    template <bool b, class T, class F>\n      using conditional_t     = typename conditional<b,T,F>::type;  // C++14\n    template <class... T>\n      using common_type_t     = typename common_type<T...>::type;  // C++14\n    template <class T>\n      using underlying_type_t = typename underlying_type<T>::type;  // C++14\n    template <class T>\n      using result_of_t       = typename result_of<T>::type;  // C++14\n    template <class Fn, class... ArgTypes>\n      using invoke_result_t   = typename invoke_result<Fn, ArgTypes...>::type;  // C++17\n\n    template <class...>\n      using void_t = void;   // C++17\n\n      // See C++14 20.10.4.1, primary type categories\n      template <class T> inline constexpr bool is_void_v\n        = is_void<T>::value;                                             // C++17\n      template <class T> inline constexpr bool is_null_pointer_v\n        = is_null_pointer<T>::value;                                     // C++17\n      template <class T> inline constexpr bool is_integral_v\n        = is_integral<T>::value;                                         // C++17\n      template <class T> inline constexpr bool is_floating_point_v\n        = is_floating_point<T>::value;                                   // C++17\n      template <class T> inline constexpr bool is_array_v\n        = is_array<T>::value;                                            // C++17\n      template <class T> inline constexpr bool is_pointer_v\n        = is_pointer<T>::value;                                          // C++17\n      template <class T> inline constexpr bool is_lvalue_reference_v\n        = is_lvalue_reference<T>::value;                                 // C++17\n      template <class T> inline constexpr bool is_rvalue_reference_v\n        = is_rvalue_reference<T>::value;                                 // C++17\n      template <class T> inline constexpr bool is_member_object_pointer_v\n        = is_member_object_pointer<T>::value;                            // C++17\n      template <class T> inline constexpr bool is_member_function_pointer_v\n        = is_member_function_pointer<T>::value;                          // C++17\n      template <class T> inline constexpr bool is_enum_v\n        = is_enum<T>::value;                                             // C++17\n      template <class T> inline constexpr bool is_union_v\n        = is_union<T>::value;                                            // C++17\n      template <class T> inline constexpr bool is_class_v\n        = is_class<T>::value;                                            // C++17\n      template <class T> inline constexpr bool is_function_v\n        = is_function<T>::value;                                         // C++17\n\n      // See C++14 20.10.4.2, composite type categories\n      template <class T> inline constexpr bool is_reference_v\n        = is_reference<T>::value;                                        // C++17\n      template <class T> inline constexpr bool is_arithmetic_v\n        = is_arithmetic<T>::value;                                       // C++17\n      template <class T> inline constexpr bool is_fundamental_v\n        = is_fundamental<T>::value;                                      // C++17\n      template <class T> inline constexpr bool is_object_v\n        = is_object<T>::value;                                           // C++17\n      template <class T> inline constexpr bool is_scalar_v\n        = is_scalar<T>::value;                                           // C++17\n      template <class T> inline constexpr bool is_compound_v\n        = is_compound<T>::value;                                         // C++17\n      template <class T> inline constexpr bool is_member_pointer_v\n        = is_member_pointer<T>::value;                                   // C++17\n\n      // See C++14 20.10.4.3, type properties\n      template <class T> inline constexpr bool is_const_v\n        = is_const<T>::value;                                            // C++17\n      template <class T> inline constexpr bool is_volatile_v\n        = is_volatile<T>::value;                                         // C++17\n      template <class T> inline constexpr bool is_trivial_v\n        = is_trivial<T>::value;                                          // C++17\n      template <class T> inline constexpr bool is_trivially_copyable_v\n        = is_trivially_copyable<T>::value;                               // C++17\n      template <class T> inline constexpr bool is_standard_layout_v\n        = is_standard_layout<T>::value;                                  // C++17\n      template <class T> inline constexpr bool is_pod_v\n        = is_pod<T>::value;                                              // C++17\n      template <class T> inline constexpr bool is_literal_type_v\n        = is_literal_type<T>::value;                                     // C++17\n      template <class T> inline constexpr bool is_empty_v\n        = is_empty<T>::value;                                            // C++17\n      template <class T> inline constexpr bool is_polymorphic_v\n        = is_polymorphic<T>::value;                                      // C++17\n      template <class T> inline constexpr bool is_abstract_v\n        = is_abstract<T>::value;                                         // C++17\n      template <class T> inline constexpr bool is_final_v\n        = is_final<T>::value;                                            // C++17\n      template <class T> inline constexpr bool is_aggregate_v\n        = is_aggregate<T>::value;                                        // C++17\n      template <class T> inline constexpr bool is_signed_v\n        = is_signed<T>::value;                                           // C++17\n      template <class T> inline constexpr bool is_unsigned_v\n        = is_unsigned<T>::value;                                         // C++17\n      template <class T, class... Args> inline constexpr bool is_constructible_v\n        = is_constructible<T, Args...>::value;                           // C++17\n      template <class T> inline constexpr bool is_default_constructible_v\n        = is_default_constructible<T>::value;                            // C++17\n      template <class T> inline constexpr bool is_copy_constructible_v\n        = is_copy_constructible<T>::value;                               // C++17\n      template <class T> inline constexpr bool is_move_constructible_v\n        = is_move_constructible<T>::value;                               // C++17\n      template <class T, class U> inline constexpr bool is_assignable_v\n        = is_assignable<T, U>::value;                                    // C++17\n      template <class T> inline constexpr bool is_copy_assignable_v\n        = is_copy_assignable<T>::value;                                  // C++17\n      template <class T> inline constexpr bool is_move_assignable_v\n        = is_move_assignable<T>::value;                                  // C++17\n      template <class T, class U> inline constexpr bool is_swappable_with_v\n        = is_swappable_with<T, U>::value;                                // C++17\n      template <class T> inline constexpr bool is_swappable_v\n        = is_swappable<T>::value;                                        // C++17\n      template <class T> inline constexpr bool is_destructible_v\n        = is_destructible<T>::value;                                     // C++17\n      template <class T, class... Args> inline constexpr bool is_trivially_constructible_v\n        = is_trivially_constructible<T, Args...>::value;                 // C++17\n      template <class T> inline constexpr bool is_trivially_default_constructible_v\n        = is_trivially_default_constructible<T>::value;                  // C++17\n      template <class T> inline constexpr bool is_trivially_copy_constructible_v\n        = is_trivially_copy_constructible<T>::value;                     // C++17\n      template <class T> inline constexpr bool is_trivially_move_constructible_v\n        = is_trivially_move_constructible<T>::value;                     // C++17\n      template <class T, class U> inline constexpr bool is_trivially_assignable_v\n        = is_trivially_assignable<T, U>::value;                          // C++17\n      template <class T> inline constexpr bool is_trivially_copy_assignable_v\n        = is_trivially_copy_assignable<T>::value;                        // C++17\n      template <class T> inline constexpr bool is_trivially_move_assignable_v\n        = is_trivially_move_assignable<T>::value;                        // C++17\n      template <class T> inline constexpr bool is_trivially_destructible_v\n        = is_trivially_destructible<T>::value;                           // C++17\n      template <class T, class... Args> inline constexpr bool is_nothrow_constructible_v\n        = is_nothrow_constructible<T, Args...>::value;                   // C++17\n      template <class T> inline constexpr bool is_nothrow_default_constructible_v\n        = is_nothrow_default_constructible<T>::value;                    // C++17\n      template <class T> inline constexpr bool is_nothrow_copy_constructible_v\n        = is_nothrow_copy_constructible<T>::value;                       // C++17\n      template <class T> inline constexpr bool is_nothrow_move_constructible_v\n        = is_nothrow_move_constructible<T>::value;                       // C++17\n      template <class T, class U> inline constexpr bool is_nothrow_assignable_v\n        = is_nothrow_assignable<T, U>::value;                            // C++17\n      template <class T> inline constexpr bool is_nothrow_copy_assignable_v\n        = is_nothrow_copy_assignable<T>::value;                          // C++17\n      template <class T> inline constexpr bool is_nothrow_move_assignable_v\n        = is_nothrow_move_assignable<T>::value;                          // C++17\n      template <class T, class U> inline constexpr bool is_nothrow_swappable_with_v\n        = is_nothrow_swappable_with<T, U>::value;                       // C++17\n      template <class T> inline constexpr bool is_nothrow_swappable_v\n        = is_nothrow_swappable<T>::value;                               // C++17\n      template <class T> inline constexpr bool is_nothrow_destructible_v\n        = is_nothrow_destructible<T>::value;                             // C++17\n      template <class T> inline constexpr bool has_virtual_destructor_v\n        = has_virtual_destructor<T>::value;                              // C++17\n      template<class T> inline constexpr bool has_unique_object_representations_v // C++17\n        = has_unique_object_representations<T>::value;\n\n      // See C++14 20.10.5, type property queries\n      template <class T> inline constexpr size_t alignment_of_v\n        = alignment_of<T>::value;                                        // C++17\n      template <class T> inline constexpr size_t rank_v\n        = rank<T>::value;                                                // C++17\n      template <class T, unsigned I = 0> inline constexpr size_t extent_v\n        = extent<T, I>::value;                                           // C++17\n\n      // See C++14 20.10.6, type relations\n      template <class T, class U> inline constexpr bool is_same_v\n        = is_same<T, U>::value;                                          // C++17\n      template <class Base, class Derived> inline constexpr bool is_base_of_v\n        = is_base_of<Base, Derived>::value;                              // C++17\n      template <class From, class To> inline constexpr bool is_convertible_v\n        = is_convertible<From, To>::value;                               // C++17\n      template <class Fn, class... ArgTypes> inline constexpr bool is_invocable_v\n        = is_invocable<Fn, ArgTypes...>::value;                          // C++17\n      template <class R, class Fn, class... ArgTypes> inline constexpr bool is_invocable_r_v\n        = is_invocable_r<R, Fn, ArgTypes...>::value;                     // C++17\n      template <class Fn, class... ArgTypes> inline constexpr bool is_nothrow_invocable_v\n        = is_nothrow_invocable<Fn, ArgTypes...>::value;                  // C++17\n      template <class R, class Fn, class... ArgTypes> inline constexpr bool is_nothrow_invocable_r_v\n        = is_nothrow_invocable_r<R, Fn, ArgTypes...>::value;             // C++17\n\n      // [meta.logical], logical operator traits:\n      template<class... B> struct conjunction;                           // C++17\n      template<class... B>\n        inline constexpr bool conjunction_v = conjunction<B...>::value;  // C++17\n      template<class... B> struct disjunction;                           // C++17\n      template<class... B>\n        inline constexpr bool disjunction_v = disjunction<B...>::value;  // C++17\n      template<class B> struct negation;                                 // C++17\n      template<class B>\n        inline constexpr bool negation_v = negation<B>::value;           // C++17\n\n}\n\n*/\n#ifndef __cuda_std__\n#include <__config>\n#else\n#ifndef _LIBCUDACXX_COMPILER_NVRTC\n#include <type_traits>\n#endif // _LIBCUDACXX_COMPILER_NVRTC\n#endif // __cuda_std__\n\n#include \"__assert\" // all public C++ headers provide the assertion handler\n#include \"__functional/identity.h\"\n#include \"__functional/invoke.h\"\n#include \"__memory/addressof.h\"\n#include \"__type_traits/add_const.h\"\n#include \"__type_traits/add_cv.h\"\n#include \"__type_traits/add_lvalue_reference.h\"\n#include \"__type_traits/add_pointer.h\"\n#include \"__type_traits/add_rvalue_reference.h\"\n#include \"__type_traits/add_volatile.h\"\n#include \"__type_traits/aligned_storage.h\"\n#include \"__type_traits/aligned_union.h\"\n#include \"__type_traits/alignment_of.h\"\n#include \"__type_traits/apply_cv.h\"\n#include \"__type_traits/can_extract_key.h\"\n#include \"__type_traits/common_reference.h\"\n#include \"__type_traits/common_type.h\"\n#include \"__type_traits/conditional.h\"\n#include \"__type_traits/conjunction.h\"\n#include \"__type_traits/copy_cv.h\"\n#include \"__type_traits/copy_cvref.h\"\n#include \"__type_traits/decay.h\"\n#include \"__type_traits/dependent_type.h\"\n#include \"__type_traits/disjunction.h\"\n#include \"__type_traits/enable_if.h\"\n#include \"__type_traits/extent.h\"\n#include \"__type_traits/has_unique_object_representation.h\"\n#include \"__type_traits/has_virtual_destructor.h\"\n#include \"__type_traits/integral_constant.h\"\n#include \"__type_traits/is_abstract.h\"\n#include \"__type_traits/is_aggregate.h\"\n#include \"__type_traits/is_allocator.h\"\n#include \"__type_traits/is_arithmetic.h\"\n#include \"__type_traits/is_array.h\"\n#include \"__type_traits/is_assignable.h\"\n#include \"__type_traits/is_base_of.h\"\n#include \"__type_traits/is_bounded_array.h\"\n#include \"__type_traits/is_callable.h\"\n#include \"__type_traits/is_char_like_type.h\"\n#include \"__type_traits/is_class.h\"\n#include \"__type_traits/is_compound.h\"\n#include \"__type_traits/is_const.h\"\n#include \"__type_traits/is_constant_evaluated.h\"\n#include \"__type_traits/is_constructible.h\"\n#include \"__type_traits/is_convertible.h\"\n#include \"__type_traits/is_copy_assignable.h\"\n#include \"__type_traits/is_copy_constructible.h\"\n#include \"__type_traits/is_core_convertible.h\"\n#include \"__type_traits/is_default_constructible.h\"\n#include \"__type_traits/is_destructible.h\"\n#include \"__type_traits/is_empty.h\"\n#include \"__type_traits/is_enum.h\"\n#include \"__type_traits/is_final.h\"\n#include \"__type_traits/is_floating_point.h\"\n#include \"__type_traits/is_function.h\"\n#include \"__type_traits/is_fundamental.h\"\n#include \"__type_traits/is_implicitly_default_constructible.h\"\n#include \"__type_traits/is_integral.h\"\n#include \"__type_traits/is_literal_type.h\"\n#include \"__type_traits/is_member_function_pointer.h\"\n#include \"__type_traits/is_member_object_pointer.h\"\n#include \"__type_traits/is_member_pointer.h\"\n#include \"__type_traits/is_move_assignable.h\"\n#include \"__type_traits/is_move_constructible.h\"\n#include \"__type_traits/is_nothrow_assignable.h\"\n#include \"__type_traits/is_nothrow_constructible.h\"\n#include \"__type_traits/is_nothrow_convertible.h\"\n#include \"__type_traits/is_nothrow_copy_assignable.h\"\n#include \"__type_traits/is_nothrow_copy_constructible.h\"\n#include \"__type_traits/is_nothrow_default_constructible.h\"\n#include \"__type_traits/is_nothrow_destructible.h\"\n#include \"__type_traits/is_nothrow_move_assignable.h\"\n#include \"__type_traits/is_nothrow_move_constructible.h\"\n#include \"__type_traits/is_null_pointer.h\"\n#include \"__type_traits/is_object.h\"\n#include \"__type_traits/is_pod.h\"\n#include \"__type_traits/is_pointer.h\"\n#include \"__type_traits/is_polymorphic.h\"\n#include \"__type_traits/is_primary_template.h\"\n#include \"__type_traits/is_reference_wrapper.h\"\n#include \"__type_traits/is_reference.h\"\n#include \"__type_traits/is_referenceable.h\"\n#include \"__type_traits/is_same.h\"\n#include \"__type_traits/is_scalar.h\"\n#include \"__type_traits/is_scoped_enum.h\"\n#include \"__type_traits/is_signed_integer.h\"\n#include \"__type_traits/is_signed.h\"\n#include \"__type_traits/is_standard_layout.h\"\n#include \"__type_traits/is_swappable.h\"\n#include \"__type_traits/is_trivial.h\"\n#include \"__type_traits/is_trivially_assignable.h\"\n#include \"__type_traits/is_trivially_constructible.h\"\n#include \"__type_traits/is_trivially_copy_assignable.h\"\n#include \"__type_traits/is_trivially_copy_constructible.h\"\n#include \"__type_traits/is_trivially_copyable.h\"\n#include \"__type_traits/is_trivially_default_constructible.h\"\n#include \"__type_traits/is_trivially_destructible.h\"\n#include \"__type_traits/is_trivially_move_assignable.h\"\n#include \"__type_traits/is_trivially_move_constructible.h\"\n#include \"__type_traits/is_unbounded_array.h\"\n#include \"__type_traits/is_union.h\"\n#include \"__type_traits/is_unsigned_integer.h\"\n#include \"__type_traits/is_unsigned.h\"\n#include \"__type_traits/is_valid_expansion.h\"\n#include \"__type_traits/is_void.h\"\n#include \"__type_traits/is_volatile.h\"\n#include \"__type_traits/lazy.h\"\n#include \"__type_traits/make_const_lvalue_ref.h\"\n#include \"__type_traits/make_32_64_or_128_bit.h\"\n#include \"__type_traits/make_signed.h\"\n#include \"__type_traits/make_unsigned.h\"\n#include \"__type_traits/maybe_const.h\"\n#include \"__type_traits/nat.h\"\n#include \"__type_traits/negation.h\"\n#include \"__type_traits/promote.h\"\n#include \"__type_traits/rank.h\"\n#include \"__type_traits/remove_all_extents.h\"\n#include \"__type_traits/remove_const_ref.h\"\n#include \"__type_traits/remove_const.h\"\n#include \"__type_traits/remove_cvref.h\"\n#include \"__type_traits/remove_cv.h\"\n#include \"__type_traits/remove_extent.h\"\n#include \"__type_traits/remove_pointer.h\"\n#include \"__type_traits/remove_reference.h\"\n#include \"__type_traits/remove_volatile.h\"\n#include \"__type_traits/result_of.h\"\n#include \"__type_traits/type_identity.h\"\n#include \"__type_traits/type_list.h\"\n#include \"__type_traits/underlying_type.h\"\n#include \"__type_traits/void_t.h\"\n#include \"__utility/convert_to_integral.h\"\n#include \"__utility/declval.h\"\n#include \"__utility/forward.h\"\n#include \"__utility/move.h\"\n#include \"__utility/swap.h\"\n#include \"cstdint\"\n#include \"cstddef\"\n#include \"version\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\ntemplate <class _T1, class _T2> struct _LIBCUDACXX_TEMPLATE_VIS pair;\ntemplate <class _Tp> class _LIBCUDACXX_TEMPLATE_VIS reference_wrapper;\ntemplate <class _Tp> struct _LIBCUDACXX_TEMPLATE_VIS hash;\n\ntemplate <bool> struct _MetaBase;\ntemplate <>\nstruct _MetaBase<true> {\n  template <class _Tp, class _Up>\n  using _SelectImpl _LIBCUDACXX_NODEBUG_TYPE = _Tp;\n  template <template <class...> class _FirstFn, template <class...> class, class ..._Args>\n  using _SelectApplyImpl _LIBCUDACXX_NODEBUG_TYPE = _FirstFn<_Args...>;\n  template <class _First, class...>\n  using _FirstImpl _LIBCUDACXX_NODEBUG_TYPE = _First;\n  template <class, class _Second, class...>\n  using _SecondImpl _LIBCUDACXX_NODEBUG_TYPE = _Second;\n  template <class _Tp = void>\n  using _EnableIfImpl _LIBCUDACXX_NODEBUG_TYPE = _Tp;\n  template <class _Result, class _First, class ..._Rest>\n  using _OrImpl _LIBCUDACXX_NODEBUG_TYPE = typename _MetaBase<_First::value != true && sizeof...(_Rest) != 0>::template _OrImpl<_First, _Rest...>;\n  template <class _Result, class _First, class ..._Rest>\n  using _AndImpl _LIBCUDACXX_NODEBUG_TYPE = typename _MetaBase<_First::value == true && sizeof...(_Rest) != 0>::template _AndImpl<_First, _Rest...>;\n};\n\ntemplate <>\nstruct _MetaBase<false> {\n  template <class _Tp, class _Up>\n  using _SelectImpl _LIBCUDACXX_NODEBUG_TYPE = _Up;\n  template <template <class...> class, template <class...> class _SecondFn, class ..._Args>\n  using _SelectApplyImpl _LIBCUDACXX_NODEBUG_TYPE = _SecondFn<_Args...>;\n  template <class _Result, class ...>\n  using _OrImpl _LIBCUDACXX_NODEBUG_TYPE = _Result;\n  template <class _Result, class ...>\n  using _AndImpl _LIBCUDACXX_NODEBUG_TYPE = _Result;\n};\ntemplate <bool _Cond, class _Ret = void>\nusing _EnableIf _LIBCUDACXX_NODEBUG_TYPE = typename _MetaBase<_Cond>::template _EnableIfImpl<_Ret>;\ntemplate <class ..._Args>\nusing _FirstType _LIBCUDACXX_NODEBUG_TYPE = typename _MetaBase<(sizeof...(_Args) >= 1)>::template _FirstImpl<_Args...>;\ntemplate <class ..._Args>\nusing _SecondType _LIBCUDACXX_NODEBUG_TYPE = typename _MetaBase<(sizeof...(_Args) >= 2)>::template _SecondImpl<_Args...>;\n\n// helper class:\n\ntemplate <class _Tp>\ninline _LIBCUDACXX_INLINE_VISIBILITY\n__decay_t<_Tp> __decay_copy(_Tp&& __t)\n{\n    return _CUDA_VSTD::forward<_Tp>(__t);\n}\n\n// iter_swap\n\ntemplate <class _ForwardIterator1, class _ForwardIterator2>\ninline _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\nvoid\niter_swap(_ForwardIterator1 __a, _ForwardIterator2 __b)\n    //                                  noexcept(noexcept(swap(*__a, *__b)))\n               noexcept(noexcept(swap(*_CUDA_VSTD::declval<_ForwardIterator1>(),\n                                          *_CUDA_VSTD::declval<_ForwardIterator2>())))\n{\n    swap(*__a, *__b);\n}\n\ntemplate <class _Tp>\nstruct __has_operator_addressof_member_imp\n{\n    template <class _Up>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        static auto __test(int)\n            -> typename __select_2nd<decltype(_CUDA_VSTD::declval<_Up>().operator&()), true_type>::type;\n    template <class>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        static auto __test(long) -> false_type;\n\n    static const bool value = decltype(__test<_Tp>(0))::value;\n};\n\ntemplate <class _Tp>\nstruct __has_operator_addressof_free_imp\n{\n    template <class _Up>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        static auto __test(int)\n            -> typename __select_2nd<decltype(operator&(_CUDA_VSTD::declval<_Up>())), true_type>::type;\n    template <class>\n        _LIBCUDACXX_INLINE_VISIBILITY\n        static auto __test(long) -> false_type;\n\n    static const bool value = decltype(__test<_Tp>(0))::value;\n};\n\ntemplate <class _Tp>\nstruct __has_operator_addressof\n    : public integral_constant<bool, __has_operator_addressof_member_imp<_Tp>::value\n                                  || __has_operator_addressof_free_imp<_Tp>::value>\n{};\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif  // _LIBCUDACXX_TYPE_TRAITS\n", "detail/libcxx/include/utility": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===-------------------------- utility -----------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX_UTILITY\n#define _LIBCUDACXX_UTILITY\n\n/*\n    utility synopsis\n\n#include <initializer_list>\n\nnamespace std\n{\n\ntemplate <class T>\n    void\n    swap(T& a, T& b);\n\nnamespace rel_ops\n{\n    template<class T> bool operator!=(const T&, const T&);\n    template<class T> bool operator> (const T&, const T&);\n    template<class T> bool operator<=(const T&, const T&);\n    template<class T> bool operator>=(const T&, const T&);\n}\n\ntemplate<class T>\nvoid\nswap(T& a, T& b) noexcept(is_nothrow_move_constructible<T>::value &&\n                          is_nothrow_move_assignable<T>::value);\n\ntemplate <class T, size_t N>\nvoid\nswap(T (&a)[N], T (&b)[N]) noexcept(noexcept(swap(*a, *b)));\n\ntemplate <class T> T&& forward(typename remove_reference<T>::type& t) noexcept;  // constexpr in C++14\ntemplate <class T> T&& forward(typename remove_reference<T>::type&& t) noexcept; // constexpr in C++14\n\ntemplate <typename T>\n[[nodiscard]] constexpr\nauto forward_like(auto&& x) noexcept -> see below;                               // since C++23\n\ntemplate <class T> typename remove_reference<T>::type&& move(T&&) noexcept;      // constexpr in C++14\n\ntemplate <class T>\n    typename conditional\n    <\n        !is_nothrow_move_constructible<T>::value && is_copy_constructible<T>::value,\n        const T&,\n        T&&\n    >::type\n    move_if_noexcept(T& x) noexcept; // constexpr in C++14\n\ntemplate <class T> constexpr add_const_t<T>& as_const(T& t) noexcept;      // C++17\ntemplate <class T>                      void as_const(const T&&) = delete; // C++17\n\ntemplate <class T> typename add_rvalue_reference<T>::type declval() noexcept;\n\ntemplate<class T, class U> constexpr bool cmp_equal(T t, U u) noexcept;         // C++20\ntemplate<class T, class U> constexpr bool cmp_not_equal(T t, U u) noexcept;     // C++20\ntemplate<class T, class U> constexpr bool cmp_less(T t, U u) noexcept;          // C++20\ntemplate<class T, class U> constexpr bool cmp_greater(T t, U u) noexcept;       // C++20\ntemplate<class T, class U> constexpr bool cmp_less_equal(T t, U u) noexcept;    // C++20\ntemplate<class T, class U> constexpr bool cmp_greater_equal(T t, U u) noexcept; // C++20\ntemplate<class R, class T> constexpr bool in_range(T t) noexcept;               // C++20\n\ntemplate <class T1, class T2>\nstruct pair\n{\n    typedef T1 first_type;\n    typedef T2 second_type;\n\n    T1 first;\n    T2 second;\n\n    pair(const pair&) = default;\n    pair(pair&&) = default;\n    explicit(see-below) constexpr pair();\n    explicit(see-below) pair(const T1& x, const T2& y);                          // constexpr in C++14\n    template <class U = T1, class V = T2> explicit(see-below) pair(U&&, V&&);    // constexpr in C++14\n    template <class U, class V> constexpr explicit(see below) pair(pair<U, V>&); // since C++23\n    template <class U, class V> explicit(see-below) pair(const pair<U, V>& p);   // constexpr in C++14\n    template <class U, class V> explicit(see-below) pair(pair<U, V>&& p);        // constexpr in C++14\n    template <class U, class V>\n    constexpr explicit(see below) pair(const pair<U, V>&&);                      // since C++23\n    template <class... Args1, class... Args2>\n        pair(piecewise_construct_t, tuple<Args1...> first_args,\n             tuple<Args2...> second_args);                                       // constexpr in C++20\n\n    constexpr const pair& operator=(const pair& p) const;                        // since C++23\n    template <class U, class V> pair& operator=(const pair<U, V>& p);            // constexpr in C++20\n    template <class U, class V>\n    constexpr const pair& operator=(const pair<U, V>& p) const;                  // since C++23\n    pair& operator=(pair&& p) noexcept(is_nothrow_move_assignable<T1>::value &&\n                                       is_nothrow_move_assignable<T2>::value);   // constexpr in C++20\n    constexpr const pair& operator=(pair&& p) const;                             // since C++23\n    template <class U, class V> pair& operator=(pair<U, V>&& p);                 // constexpr in C++20\n    template <class U, class V>\n    constexpr const pair& operator=(pair<U, V>&& p) const;                       // since C++23\n\n    void swap(pair& p) noexcept(is_nothrow_swappable_v<T1> &&\n                                is_nothrow_swappable_v<T2>);                     // constexpr in C++20\n    constexpr void swap(const pair& p) const noexcept(see below);                // since C++23\n};\n\ntemplate<class T1, class T2, class U1, class U2, template<class> class TQual, template<class> class UQual>\nstruct basic_common_reference<pair<T1, T2>, pair<U1, U2>, TQual, UQual>;         // since C++23\n\ntemplate<class T1, class T2, class U1, class U2>\nstruct common_type<pair<T1, T2>, pair<U1, U2>>;                                  // since C++23\n\ntemplate<class T1, class T2> pair(T1, T2) -> pair<T1, T2>;\n\ntemplate <class T1, class T2> bool operator==(const pair<T1,T2>&, const pair<T1,T2>&); // constexpr in C++14\ntemplate <class T1, class T2> bool operator!=(const pair<T1,T2>&, const pair<T1,T2>&); // constexpr in C++14, removed in C++20\ntemplate <class T1, class T2> bool operator< (const pair<T1,T2>&, const pair<T1,T2>&); // constexpr in C++14, removed in C++20\ntemplate <class T1, class T2> bool operator> (const pair<T1,T2>&, const pair<T1,T2>&); // constexpr in C++14, removed in C++20\ntemplate <class T1, class T2> bool operator>=(const pair<T1,T2>&, const pair<T1,T2>&); // constexpr in C++14, removed in C++20\ntemplate <class T1, class T2> bool operator<=(const pair<T1,T2>&, const pair<T1,T2>&); // constexpr in C++14, removed in C++20\ntemplate <class T1, class T2>\n  constexpr common_comparison_type_t<synth-three-way-result<T1>,\n                                     synth-three-way-result<T2>>\n    operator<=>(const pair<T1,T2>&, const pair<T1,T2>&);                               // C++20\n\ntemplate <class T1, class T2> pair<V1, V2> make_pair(T1&&, T2&&);                // constexpr in C++14\ntemplate <class T1, class T2>\nvoid\nswap(pair<T1, T2>& x, pair<T1, T2>& y) noexcept(noexcept(x.swap(y)));            // constexpr in C++20\n\ntemplate<class T1, class T2>\nconstexpr void swap(const pair<T1, T2>& x, const pair<T1, T2>& y) noexcept(noexcept(x.swap(y)));    // since C++23\n\nstruct piecewise_construct_t { explicit piecewise_construct_t() = default; };\ninline constexpr piecewise_construct_t piecewise_construct = piecewise_construct_t();\n\ntemplate <class T> struct tuple_size;\ntemplate <size_t I, class T> struct tuple_element;\n\ntemplate <class T1, class T2> struct tuple_size<pair<T1, T2> >;\ntemplate <class T1, class T2> struct tuple_element<0, pair<T1, T2> >;\ntemplate <class T1, class T2> struct tuple_element<1, pair<T1, T2> >;\n\ntemplate<size_t I, class T1, class T2>\n    typename tuple_element<I, pair<T1, T2> >::type&\n    get(pair<T1, T2>&) noexcept; // constexpr in C++14\n\ntemplate<size_t I, class T1, class T2>\n    const typename tuple_element<I, pair<T1, T2> >::type&\n    get(const pair<T1, T2>&) noexcept; // constexpr in C++14\n\ntemplate<size_t I, class T1, class T2>\n    typename tuple_element<I, pair<T1, T2> >::type&&\n    get(pair<T1, T2>&&) noexcept; // constexpr in C++14\n\ntemplate<size_t I, class T1, class T2>\n    const typename tuple_element<I, pair<T1, T2> >::type&&\n    get(const pair<T1, T2>&&) noexcept; // constexpr in C++14\n\ntemplate<class T1, class T2>\n    constexpr T1& get(pair<T1, T2>&) noexcept; // C++14\n\ntemplate<class T1, class T2>\n    constexpr const T1& get(const pair<T1, T2>&) noexcept; // C++14\n\ntemplate<class T1, class T2>\n    constexpr T1&& get(pair<T1, T2>&&) noexcept; // C++14\n\ntemplate<class T1, class T2>\n    constexpr const T1&& get(const pair<T1, T2>&&) noexcept; // C++14\n\ntemplate<class T1, class T2>\n    constexpr T1& get(pair<T2, T1>&) noexcept; // C++14\n\ntemplate<class T1, class T2>\n    constexpr const T1& get(const pair<T2, T1>&) noexcept; // C++14\n\ntemplate<class T1, class T2>\n    constexpr T1&& get(pair<T2, T1>&&) noexcept; // C++14\n\ntemplate<class T1, class T2>\n    constexpr const T1&& get(const pair<T2, T1>&&) noexcept; // C++14\n\n// C++14\n\ntemplate<class T, T... I>\nstruct integer_sequence\n{\n    typedef T value_type;\n\n    static constexpr size_t size() noexcept;\n};\n\ntemplate<size_t... I>\n  using index_sequence = integer_sequence<size_t, I...>;\n\ntemplate<class T, T N>\n  using make_integer_sequence = integer_sequence<T, 0, 1, ..., N-1>;\ntemplate<size_t N>\n  using make_index_sequence = make_integer_sequence<size_t, N>;\n\ntemplate<class... T>\n  using index_sequence_for = make_index_sequence<sizeof...(T)>;\n\ntemplate<class T, class U=T>\n    constexpr T exchange(T& obj, U&& new_value)\n      noexcept(is_nothrow_move_constructible<T>::value && is_nothrow_assignable<T&, U>::value); // constexpr in C++17, noexcept in C++23\n\n// 20.2.7, in-place construction // C++17\nstruct in_place_t {\n  explicit in_place_t() = default;\n};\ninline constexpr in_place_t in_place{};\ntemplate <class T>\n  struct in_place_type_t {\n    explicit in_place_type_t() = default;\n  };\ntemplate <class T>\n  inline constexpr in_place_type_t<T> in_place_type{};\ntemplate <size_t I>\n  struct in_place_index_t {\n    explicit in_place_index_t() = default;\n  };\ntemplate <size_t I>\n  inline constexpr in_place_index_t<I> in_place_index{};\n\n// [utility.underlying], to_underlying\ntemplate <class T>\n    constexpr underlying_type_t<T> to_underlying( T value ) noexcept; // C++2b\n\n}  // std\n\n*/\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"__assert\" // all public C++ headers provide the assertion handler\n#include \"__debug\"\n#include \"__functional/binary_function.h\"\n#include \"__functional/hash.h\"\n#include \"__functional/reference_wrapper.h\"\n#include \"__functional/unary_function.h\"\n#include \"__functional/unwrap_ref.h\"\n#include \"__functional/weak_result_type.h\"\n#include \"__fwd/get.h\"\n#include \"__tuple_dir/sfinae_helpers.h\"\n#include \"__tuple_dir/structured_bindings.h\"\n#include \"__utility/as_const.h\"\n#include \"__utility/auto_cast.h\"\n#include \"__utility/cmp.h\"\n#include \"__utility/convert_to_integral.h\"\n#include \"__utility/declval.h\"\n#include \"__utility/exchange.h\"\n#include \"__utility/forward_like.h\"\n#include \"__utility/forward.h\"\n#include \"__utility/in_place.h\"\n#include \"__utility/integer_sequence.h\"\n#include \"__utility/move.h\"\n#include \"__utility/pair.h\"\n#include \"__utility/piecewise_construct.h\"\n#include \"__utility/priority_tag.h\"\n#include \"__utility/rel_ops.h\"\n#include \"__utility/swap.h\"\n#include \"__utility/to_underlying.h\"\n#include \"__utility/unreachable.h\"\n\n#include \"__memory/construct_at.h\"\n#include \"__memory/voidify.h\"\n\n#include \"limits\"\n#include \"type_traits\"\n#include \"version\"\n\n// standard-mandated includes\n#include \"concepts\"\n#include \"version\"\n\n// [utility.syn]\n#ifndef _LIBCUDACXX_HAS_NO_SPACESHIP_OPERATOR\n#include \"compare\"\n#endif\n#include \"initializer_list\"\n\n// [tuple.helper]\n#include \"__tuple_dir/tuple_element.h\"\n#include \"__tuple_dir/tuple_size.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n#endif  // _LIBCUDACXX_UTILITY\n", "initializer_list": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===----------------------- initializer_list -----------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX_INITIALIZER_LIST\n#define _LIBCUDACXX_INITIALIZER_LIST\n\n/*\n    initializer_list synopsis\n\nnamespace std\n{\n\ntemplate<class E>\nclass initializer_list\n{\npublic:\n    typedef E        value_type;\n    typedef const E& reference;\n    typedef const E& const_reference;\n    typedef size_t   size_type;\n\n    typedef const E* iterator;\n    typedef const E* const_iterator;\n\n    initializer_list() noexcept; // constexpr in C++14\n\n    size_t   size()  const noexcept; // constexpr in C++14\n    const E* begin() const noexcept; // constexpr in C++14\n    const E* end()   const noexcept; // constexpr in C++14\n};\n\ntemplate<class E> const E* begin(initializer_list<E> il) noexcept; // constexpr in C++14\ntemplate<class E> const E* end(initializer_list<E> il) noexcept; // constexpr in C++14\n\n}  // std\n\n*/\n\n#ifndef __cuda_std__\n#include <__config>\n\n#include \"cstddef\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\nnamespace std  // purposefully not versioned\n{\n\ntemplate<class _Ep>\nclass _LIBCUDACXX_TEMPLATE_VIS initializer_list\n{\n    const _Ep* __begin_;\n    size_t    __size_;\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    initializer_list(const _Ep* __b, size_t __s) noexcept\n        : __begin_(__b),\n          __size_(__s)\n        {}\npublic:\n    typedef _Ep        value_type;\n    typedef const _Ep& reference;\n    typedef const _Ep& const_reference;\n    typedef size_t    size_type;\n\n    typedef const _Ep* iterator;\n    typedef const _Ep* const_iterator;\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    initializer_list() noexcept : __begin_(nullptr), __size_(0) {}\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    size_t    size()  const noexcept {return __size_;}\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    const _Ep* begin() const noexcept {return __begin_;}\n\n    _LIBCUDACXX_INLINE_VISIBILITY\n    _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n    const _Ep* end()   const noexcept {return __begin_ + __size_;}\n};\n\ntemplate<class _Ep>\ninline _LIBCUDACXX_INLINE_VISIBILITY\n_LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nconst _Ep*\nbegin(initializer_list<_Ep> __il) noexcept\n{\n    return __il.begin();\n}\n\ntemplate<class _Ep>\ninline _LIBCUDACXX_INLINE_VISIBILITY\n_LIBCUDACXX_CONSTEXPR_AFTER_CXX11\nconst _Ep*\nend(initializer_list<_Ep> __il) noexcept\n{\n    return __il.end();\n}\n\n}  // std\n\n#else\n\n#if !defined(_LIBCUDACXX_COMPILER_NVRTC)\n#include <initializer_list>\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n    using ::std::initializer_list;\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif // __cuda_std__\n\n#endif  // _LIBCUDACXX_INITIALIZER_LIST\n", "iosfwd": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===--------------------------- iosfwd -----------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX_IOSFWD\n#define _LIBCUDACXX_IOSFWD\n\n/*\n    iosfwd synopsis\n\nnamespace std\n{\n\ntemplate<class charT> struct char_traits;\ntemplate<>            struct char_traits<char>;\ntemplate<>            struct char_traits<char8_t>;  // C++20\ntemplate<>            struct char_traits<char16_t>;\ntemplate<>            struct char_traits<char32_t>;\ntemplate<>            struct char_traits<wchar_t>;\n\ntemplate<class T>     class allocator;\n\nclass ios_base;\ntemplate <class charT, class traits = char_traits<charT> > class basic_ios;\n\ntemplate <class charT, class traits = char_traits<charT> > class basic_streambuf;\ntemplate <class charT, class traits = char_traits<charT> > class basic_istream;\ntemplate <class charT, class traits = char_traits<charT> > class basic_ostream;\ntemplate <class charT, class traits = char_traits<charT> > class basic_iostream;\n\ntemplate <class charT, class traits = char_traits<charT>, class Allocator = allocator<charT> >\n    class basic_stringbuf;\ntemplate <class charT, class traits = char_traits<charT>, class Allocator = allocator<charT> >\n    class basic_istringstream;\ntemplate <class charT, class traits = char_traits<charT>, class Allocator = allocator<charT> >\n    class basic_ostringstream;\ntemplate <class charT, class traits = char_traits<charT>, class Allocator = allocator<charT> >\n    class basic_stringstream;\n\ntemplate <class charT, class traits = char_traits<charT> > class basic_filebuf;\ntemplate <class charT, class traits = char_traits<charT> > class basic_ifstream;\ntemplate <class charT, class traits = char_traits<charT> > class basic_ofstream;\ntemplate <class charT, class traits = char_traits<charT> > class basic_fstream;\n\ntemplate <class charT, class traits = char_traits<charT> > class istreambuf_iterator;\ntemplate <class charT, class traits = char_traits<charT> > class ostreambuf_iterator;\n\ntypedef basic_ios<char>              ios;\ntypedef basic_ios<wchar_t>           wios;\n\ntypedef basic_streambuf<char>        streambuf;\ntypedef basic_istream<char>          istream;\ntypedef basic_ostream<char>          ostream;\ntypedef basic_iostream<char>         iostream;\n\ntypedef basic_stringbuf<char>        stringbuf;\ntypedef basic_istringstream<char>    istringstream;\ntypedef basic_ostringstream<char>    ostringstream;\ntypedef basic_stringstream<char>     stringstream;\n\ntypedef basic_filebuf<char>          filebuf;\ntypedef basic_ifstream<char>         ifstream;\ntypedef basic_ofstream<char>         ofstream;\ntypedef basic_fstream<char>          fstream;\n\ntypedef basic_streambuf<wchar_t>     wstreambuf;\ntypedef basic_istream<wchar_t>       wistream;\ntypedef basic_ostream<wchar_t>       wostream;\ntypedef basic_iostream<wchar_t>      wiostream;\n\ntypedef basic_stringbuf<wchar_t>     wstringbuf;\ntypedef basic_istringstream<wchar_t> wistringstream;\ntypedef basic_ostringstream<wchar_t> wostringstream;\ntypedef basic_stringstream<wchar_t>  wstringstream;\n\ntypedef basic_filebuf<wchar_t>       wfilebuf;\ntypedef basic_ifstream<wchar_t>      wifstream;\ntypedef basic_ofstream<wchar_t>      wofstream;\ntypedef basic_fstream<wchar_t>       wfstream;\n\ntemplate <class state> class fpos;\ntypedef fpos<char_traits<char>::state_type>    streampos;\ntypedef fpos<char_traits<wchar_t>::state_type> wstreampos;\n\n}  // std\n\n*/\n\n#ifndef __cuda_std__\n#include <__config>\n#include <wchar.h>  // for mbstate_t\n#endif //__cuda_std__\n\n#include \"__assert\" // all public C++ headers provide the assertion handler\n#include \"__fwd/string.h\"\n#include \"version\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\nclass _LIBCUDACXX_TYPE_VIS ios_base;\n\ntemplate <class _CharT, class _Traits = char_traits<_CharT> >\n    class _LIBCUDACXX_TEMPLATE_VIS basic_ios;\n\ntemplate <class _CharT, class _Traits = char_traits<_CharT> >\n    class _LIBCUDACXX_TEMPLATE_VIS basic_streambuf;\ntemplate <class _CharT, class _Traits = char_traits<_CharT> >\n    class _LIBCUDACXX_TEMPLATE_VIS basic_istream;\ntemplate <class _CharT, class _Traits = char_traits<_CharT> >\n    class _LIBCUDACXX_TEMPLATE_VIS basic_ostream;\ntemplate <class _CharT, class _Traits = char_traits<_CharT> >\n    class _LIBCUDACXX_TEMPLATE_VIS basic_iostream;\n\ntemplate <class _CharT, class _Traits = char_traits<_CharT>,\n          class _Allocator = allocator<_CharT> >\n    class _LIBCUDACXX_TEMPLATE_VIS basic_stringbuf;\ntemplate <class _CharT, class _Traits = char_traits<_CharT>,\n          class _Allocator = allocator<_CharT> >\n    class _LIBCUDACXX_TEMPLATE_VIS basic_istringstream;\ntemplate <class _CharT, class _Traits = char_traits<_CharT>,\n          class _Allocator = allocator<_CharT> >\n    class _LIBCUDACXX_TEMPLATE_VIS basic_ostringstream;\ntemplate <class _CharT, class _Traits = char_traits<_CharT>,\n          class _Allocator = allocator<_CharT> >\n    class _LIBCUDACXX_TEMPLATE_VIS basic_stringstream;\n\ntemplate <class _CharT, class _Traits = char_traits<_CharT> >\n    class _LIBCUDACXX_TEMPLATE_VIS basic_filebuf;\ntemplate <class _CharT, class _Traits = char_traits<_CharT> >\n    class _LIBCUDACXX_TEMPLATE_VIS basic_ifstream;\ntemplate <class _CharT, class _Traits = char_traits<_CharT> >\n    class _LIBCUDACXX_TEMPLATE_VIS basic_ofstream;\ntemplate <class _CharT, class _Traits = char_traits<_CharT> >\n    class _LIBCUDACXX_TEMPLATE_VIS basic_fstream;\n\ntemplate <class _CharT, class _Traits = char_traits<_CharT> >\n    class _LIBCUDACXX_TEMPLATE_VIS istreambuf_iterator;\ntemplate <class _CharT, class _Traits = char_traits<_CharT> >\n    class _LIBCUDACXX_TEMPLATE_VIS ostreambuf_iterator;\n\ntypedef basic_ios<char>              ios;\ntypedef basic_ios<wchar_t>           wios;\n\ntypedef basic_streambuf<char>        streambuf;\ntypedef basic_istream<char>          istream;\ntypedef basic_ostream<char>          ostream;\ntypedef basic_iostream<char>         iostream;\n\ntypedef basic_stringbuf<char>        stringbuf;\ntypedef basic_istringstream<char>    istringstream;\ntypedef basic_ostringstream<char>    ostringstream;\ntypedef basic_stringstream<char>     stringstream;\n\ntypedef basic_filebuf<char>          filebuf;\ntypedef basic_ifstream<char>         ifstream;\ntypedef basic_ofstream<char>         ofstream;\ntypedef basic_fstream<char>          fstream;\n\ntypedef basic_streambuf<wchar_t>     wstreambuf;\ntypedef basic_istream<wchar_t>       wistream;\ntypedef basic_ostream<wchar_t>       wostream;\ntypedef basic_iostream<wchar_t>      wiostream;\n\ntypedef basic_stringbuf<wchar_t>     wstringbuf;\ntypedef basic_istringstream<wchar_t> wistringstream;\ntypedef basic_ostringstream<wchar_t> wostringstream;\ntypedef basic_stringstream<wchar_t>  wstringstream;\n\ntypedef basic_filebuf<wchar_t>       wfilebuf;\ntypedef basic_ifstream<wchar_t>      wifstream;\ntypedef basic_ofstream<wchar_t>      wofstream;\ntypedef basic_fstream<wchar_t>       wfstream;\n\n#if !defined(_LIBCUDACXX_HAS_NO_WCHAR_H)\ntemplate <class _State>             class _LIBCUDACXX_TEMPLATE_VIS fpos;\ntypedef fpos<mbstate_t>    streampos;\ntypedef fpos<mbstate_t>    wstreampos;\n#ifndef _LIBCUDACXX_NO_HAS_CHAR8_T\ntypedef fpos<mbstate_t>    u8streampos;\n#endif\n#ifndef _LIBCUDACXX_HAS_NO_UNICODE_CHARS\ntypedef fpos<mbstate_t>    u16streampos;\ntypedef fpos<mbstate_t>    u32streampos;\n#endif  // _LIBCUDACXX_HAS_NO_UNICODE_CHARS\n#endif\n\n#if defined(_NEWLIB_VERSION)\n// On newlib, off_t is 'long int'\ntypedef long int streamoff;         // for char_traits in <string>\n#else\ntypedef long long streamoff;        // for char_traits in <string>\n#endif\n\n// Include other forward declarations here\ntemplate <class _Tp, class _Alloc = allocator<_Tp> >\nclass _LIBCUDACXX_TEMPLATE_VIS vector;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#endif  // _LIBCUDACXX_IOSFWD\n", "iostream": "#ifndef _JITIFY_INCLUDE_GUARD_8C9C9A367719E286\n#define _JITIFY_INCLUDE_GUARD_8C9C9A367719E286\n#define cudaDeviceSynchronize() cudaSuccess\n#include <ostream>\n#include <istream>\n\n#endif // _JITIFY_INCLUDE_GUARD_8C9C9A367719E286\n", "istream": "#ifndef _JITIFY_INCLUDE_GUARD_1C3631070A038D7D\n#define _JITIFY_INCLUDE_GUARD_1C3631070A038D7D\n#define cudaDeviceSynchronize() cudaSuccess\nnamespace std {\ntemplate<class CharT,class Traits=void>\nstruct basic_istream {\n};\ntypedef basic_istream<char> istream;\n}  // namespace std\n\n#endif // _JITIFY_INCLUDE_GUARD_1C3631070A038D7D\n", "iterator": "#ifndef _JITIFY_INCLUDE_GUARD_2E84E33061B307E3\n#define _JITIFY_INCLUDE_GUARD_2E84E33061B307E3\n#define cudaDeviceSynchronize() cudaSuccess\n\nnamespace std {\nstruct output_iterator_tag {};\nstruct input_iterator_tag {};\nstruct forward_iterator_tag {};\nstruct bidirectional_iterator_tag {};\nstruct random_access_iterator_tag {};\ntemplate<class Iterator>\nstruct iterator_traits {\n  typedef typename Iterator::iterator_category iterator_category;\n  typedef typename Iterator::value_type        value_type;\n  typedef typename Iterator::difference_type   difference_type;\n  typedef typename Iterator::pointer           pointer;\n  typedef typename Iterator::reference         reference;\n};\ntemplate<class T>\nstruct iterator_traits<T*> {\n  typedef random_access_iterator_tag iterator_category;\n  typedef T                          value_type;\n  typedef ptrdiff_t                  difference_type;\n  typedef T*                         pointer;\n  typedef T&                         reference;\n};\ntemplate<class T>\nstruct iterator_traits<T const*> {\n  typedef random_access_iterator_tag iterator_category;\n  typedef T                          value_type;\n  typedef ptrdiff_t                  difference_type;\n  typedef T const*                   pointer;\n  typedef T const&                   reference;\n};\n}  // namespace std\n\n#endif // _JITIFY_INCLUDE_GUARD_2E84E33061B307E3\n", "jitify_preinclude.h": "\n//// WAR for Thrust (which appears to have forgotten to include this in result_of_adaptable_function.h\n//#include <type_traits>\n\n//// WAR for Thrust (which appear to have forgotten to include this in error_code.h)\n//#include <string>\n\n// WAR for generics/shfl.h\n#define THRUST_STATIC_ASSERT(x)\n\n// WAR for CUB\n#ifdef __host__\n#undef __host__\n#endif\n#define __host__\n\n// WAR to allow exceptions to be parsed\n#define try\n#define catch(...)\n", "libcxx/include/__config": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===--------------------------- __config ---------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX_CONFIG\n#define _LIBCUDACXX_CONFIG\n\n#if defined(__NVCOMPILER)\n#  define _LIBCUDACXX_COMPILER_NVHPC\n#elif defined(__clang__)\n#  define _LIBCUDACXX_COMPILER_CLANG\n#  ifndef __apple_build_version__\n#    define _LIBCUDACXX_CLANG_VER (__clang_major__ * 100 + __clang_minor__)\n#  endif\n#elif defined(__GNUC__)\n#  define _LIBCUDACXX_COMPILER_GCC\n#elif defined(_MSC_VER)\n#  define _LIBCUDACXX_COMPILER_MSVC\n#elif defined(__IBMCPP__)\n#  define _LIBCUDACXX_COMPILER_IBM\n#elif defined(__CUDACC_RTC__)\n#  define _LIBCUDACXX_COMPILER_NVRTC\n#endif\n\n#if defined(_LIBCUDACXX_COMPILER_MSVC)\n#if _MSC_VER < 1917\n#define _LIBCUDACXX_COMPILER_MSVC_2017\n#elif _MSC_VER < 1930\n#define _LIBCUDACXX_COMPILER_MSVC_2019\n#else\n#define _LIBCUDACXX_COMPILER_MSVC_2022\n#endif\n#endif // defined(_LIBCUDACXX_COMPILER_MSVC)\n\n#if defined(__NVCC__)\n// This is not mutually exclusive with other compilers, as NVCC uses a host\n// compiler.\n#  define _LIBCUDACXX_COMPILER_NVCC\n#elif defined(_NVHPC_CUDA)\n#  define _LIBCUDACXX_COMPILER_NVHPC_CUDA\n#elif defined(__CUDA__) && defined(_LIBCUDACXX_COMPILER_CLANG)\n#  define _LIBCUDACXX_COMPILER_CLANG_CUDA\n#endif\n\n#ifdef __CUDACC__\n#  define _LIBCUDACXX_CUDACC\n#  define _LIBCUDACXX_CUDACC_VER_MAJOR __CUDACC_VER_MAJOR__\n#  define _LIBCUDACXX_CUDACC_VER_MINOR __CUDACC_VER_MINOR__\n#  define _LIBCUDACXX_CUDACC_VER_BUILD __CUDACC_VER_BUILD__\n#  define _LIBCUDACXX_CUDACC_VER                                                  \\\n      _LIBCUDACXX_CUDACC_VER_MAJOR * 100000 + _LIBCUDACXX_CUDACC_VER_MINOR * 1000 + \\\n      _LIBCUDACXX_CUDACC_VER_BUILD\n\n// TODO: Determine if this is necessary, I don't know why we automatically include this in <config>\n#  if defined(__clang__) || defined(__FLT16_MANT_DIG__)\n#    include <cuda_fp16.h>\n#  endif\n#endif\n\n// Some convenience macros to filter CUDACC versions\n#if defined(_LIBCUDACXX_CUDACC) && _LIBCUDACXX_CUDACC_VER < 1102000\n#define _LIBCUDACXX_CUDACC_BELOW_11_2\n#endif // defined(_LIBCUDACXX_CUDACC) && _LIBCUDACXX_CUDACC_VER < 1102000\n#if defined(_LIBCUDACXX_CUDACC) && _LIBCUDACXX_CUDACC_VER < 1103000\n#define _LIBCUDACXX_CUDACC_BELOW_11_3\n#endif // defined(_LIBCUDACXX_CUDACC) && _LIBCUDACXX_CUDACC_VER < 1103000\n#if defined(_LIBCUDACXX_CUDACC) && _LIBCUDACXX_CUDACC_VER < 1108000\n#define _LIBCUDACXX_CUDACC_BELOW_11_8\n#endif // defined(_LIBCUDACXX_CUDACC) && _LIBCUDACXX_CUDACC_VER < 1108000\n\n#if defined(_MSC_VER) && !defined(__clang__)\n#  define _LIBCUDACXX_HAS_PRAGMA_MSVC_WARNING\n#  if !defined(_LIBCUDACXX_DISABLE_PRAGMA_MSVC_WARNING)\n#    define _LIBCUDACXX_USE_PRAGMA_MSVC_WARNING\n#  endif\n#else\n#  if !defined(_LIBCUDACXX_DISABLE_PRAGMA_GCC_SYSTEM_HEADER) && !defined(_LIBCUDACXX_COMPILER_NVRTC)\n#    define _LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER\n#  endif\n#endif\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n#ifdef __cplusplus\n\n// __config may be included in `extern \"C\"` contexts, switch back to include <nv/target>\nextern \"C++\" {\n#include <nv/target>\n}\n\n#ifdef __GNUC__\n#  define _GNUC_VER (__GNUC__ * 100 + __GNUC_MINOR__)\n#else\n#  define _GNUC_VER 0\n#endif\n\n#define _LIBCUDACXX_VERSION 10000\n\n#ifndef _LIBCUDACXX_ABI_VERSION\n#  define _LIBCUDACXX_ABI_VERSION 1\n#endif\n\n#ifndef _LIBCUDACXX_STD_VER\n#  if defined(_MSC_VER)\n#    if   _MSVC_LANG <= 201103L\n#      define _LIBCUDACXX_STD_VER 11\n#    elif _MSVC_LANG <= 201402L\n#      define _LIBCUDACXX_STD_VER 14\n#    elif _MSVC_LANG <= 201703L\n#      define _LIBCUDACXX_STD_VER 17\n#    elif _MSVC_LANG <= 202002L\n#      define _LIBCUDACXX_STD_VER 20\n#    else\n#      define _LIBCUDACXX_STD_VER 23 // current year, or date of c++2b ratification\n#    endif\n#  else\n#    if __cplusplus <= 201103L\n#      define _LIBCUDACXX_STD_VER 11\n#    elif __cplusplus <= 201402L\n#      define _LIBCUDACXX_STD_VER 14\n#    elif __cplusplus <= 201703L\n#      define _LIBCUDACXX_STD_VER 17\n#    elif __cplusplus <= 202002L\n#      define _LIBCUDACXX_STD_VER 20\n#    else\n#      define _LIBCUDACXX_STD_VER 23 // current year, or date of c++2b ratification\n#    endif\n#  endif\n#endif  // _LIBCUDACXX_STD_VER\n\n#if _LIBCUDACXX_STD_VER < 11\n#  error libcu++ requires C++11 or later\n#endif\n\n#if (defined(_LIBCUDACXX_COMPILER_NVHPC) && defined(__linux__)) \\\n || defined(_LIBCUDACXX_COMPILER_NVRTC)\n    #define __ELF__\n#endif\n\n#if defined(__ELF__)\n#  define _LIBCUDACXX_OBJECT_FORMAT_ELF   1\n#elif defined(__MACH__)\n#  define _LIBCUDACXX_OBJECT_FORMAT_MACHO 1\n#elif defined(_WIN32)\n#  define _LIBCUDACXX_OBJECT_FORMAT_COFF  1\n#elif defined(__wasm__)\n#  define _LIBCUDACXX_OBJECT_FORMAT_WASM  1\n#else\n#  error Unknown object file format\n#endif\n\n#if defined(_LIBCUDACXX_ABI_UNSTABLE) || _LIBCUDACXX_ABI_VERSION >= 2 || defined(__cuda_std__)\n// Change short string representation so that string data starts at offset 0,\n// improving its alignment in some cases.\n#  define _LIBCUDACXX_ABI_ALTERNATE_STRING_LAYOUT\n// Fix deque iterator type in order to support incomplete types.\n#  define _LIBCUDACXX_ABI_INCOMPLETE_TYPES_IN_DEQUE\n// Fix undefined behavior in how std::list stores its linked nodes.\n#  define _LIBCUDACXX_ABI_LIST_REMOVE_NODE_POINTER_UB\n// Fix undefined behavior in  how __tree stores its end and parent nodes.\n#  define _LIBCUDACXX_ABI_TREE_REMOVE_NODE_POINTER_UB\n// Fix undefined behavior in how __hash_table stores its pointer types.\n#  define _LIBCUDACXX_ABI_FIX_UNORDERED_NODE_POINTER_UB\n#  define _LIBCUDACXX_ABI_FORWARD_LIST_REMOVE_NODE_POINTER_UB\n#  define _LIBCUDACXX_ABI_FIX_UNORDERED_CONTAINER_SIZE_TYPE\n// Don't use a nullptr_t simulation type in C++03 instead using C++11 nullptr\n// provided under the alternate keyword __nullptr, which changes the mangling\n// of nullptr_t. This option is ABI incompatible with GCC in C++03 mode.\n#  define _LIBCUDACXX_ABI_ALWAYS_USE_CXX11_NULLPTR\n// Define the `pointer_safety` enum as a C++11 strongly typed enumeration\n// instead of as a class simulating an enum. If this option is enabled\n// `pointer_safety` and `get_pointer_safety()` will no longer be available\n// in C++03.\n#  define _LIBCUDACXX_ABI_POINTER_SAFETY_ENUM_TYPE\n// Define a key function for `bad_function_call` in the library, to centralize\n// its vtable and typeinfo to libc++ rather than having all other libraries\n// using that class define their own copies.\n#  define _LIBCUDACXX_ABI_BAD_FUNCTION_CALL_KEY_FUNCTION\n// Enable optimized version of __do_get_(un)signed which avoids redundant copies.\n#  define _LIBCUDACXX_ABI_OPTIMIZED_LOCALE_NUM_GET\n// Use the smallest possible integer type to represent the index of the variant.\n// Previously libc++ used \"unsigned int\" exclusively.\n#  define _LIBCUDACXX_ABI_VARIANT_INDEX_TYPE_OPTIMIZATION\n// Unstable attempt to provide a more optimized std::function\n#  define _LIBCUDACXX_ABI_OPTIMIZED_FUNCTION\n// All the regex constants must be distinct and nonzero.\n#  define _LIBCUDACXX_ABI_REGEX_CONSTANTS_NONZERO\n#elif _LIBCUDACXX_ABI_VERSION == 1\n#  if !defined(_LIBCUDACXX_OBJECT_FORMAT_COFF)\n// Enable compiling copies of now inline methods into the dylib to support\n// applications compiled against older libraries. This is unnecessary with\n// COFF dllexport semantics, since dllexport forces a non-inline definition\n// of inline functions to be emitted anyway. Our own non-inline copy would\n// conflict with the dllexport-emitted copy, so we disable it.\n#    define _LIBCUDACXX_DEPRECATED_ABI_LEGACY_LIBRARY_DEFINITIONS_FOR_INLINE_FUNCTIONS\n#  endif\n// Feature macros for disabling pre ABI v1 features. All of these options\n// are deprecated.\n#  if defined(__FreeBSD__)\n#    define _LIBCUDACXX_DEPRECATED_ABI_DISABLE_PAIR_TRIVIAL_COPY_CTOR\n#  endif\n#endif\n\n#ifdef _LIBCUDACXX_TRIVIAL_PAIR_COPY_CTOR\n#error \"_LIBCUDACXX_TRIVIAL_PAIR_COPY_CTOR\" is no longer supported. \\\n       use _LIBCUDACXX_DEPRECATED_ABI_DISABLE_PAIR_TRIVIAL_COPY_CTOR instead\n#endif\n\n#ifndef __has_attribute\n#define __has_attribute(__x) 0\n#endif\n\n#ifndef __has_builtin\n#define __has_builtin(__x) 0\n#endif\n\n#ifndef __has_extension\n#define __has_extension(__x) 0\n#endif\n\n#ifndef __has_feature\n#define __has_feature(__x) 0\n#endif\n\n#ifndef __has_cpp_attribute\n#define __has_cpp_attribute(__x) 0\n#endif\n\n// '__is_identifier' returns '0' if '__x' is a reserved identifier provided by\n// the compiler and '1' otherwise.\n#ifndef __is_identifier\n#define __is_identifier(__x) 1\n#endif\n\n#ifndef __has_declspec_attribute\n#define __has_declspec_attribute(__x) 0\n#endif\n\n#define __has_keyword(__x) !(__is_identifier(__x))\n\n#ifndef __has_include\n#define __has_include(...) 0\n#endif\n\n#if !defined(_LIBCUDACXX_COMPILER_NVCC) && !defined(_LIBCUDACXX_COMPILER_NVRTC)\n// If NVCC is not being used <complex> can safely use `long double` without warnings\n#  define _LIBCUDACXX_HAS_COMPLEX_LONG_DOUBLE\n// NVCC does not have a way of silencing non '_' prefixed UDLs\n#  define _LIBCUDACXX_HAS_STL_LITERALS\n#endif\n\n#if defined(_LIBCUDACXX_COMPILER_GCC) && __cplusplus < 201103L\n#error \"libc++ does not support using GCC with C++03. Please enable C++11\"\n#endif\n\n// FIXME: ABI detection should be done via compiler builtin macros. This\n// is just a placeholder until Clang implements such macros. For now assume\n// that Windows compilers pretending to be MSVC++ target the Microsoft ABI,\n// and allow the user to explicitly specify the ABI to handle cases where this\n// heuristic falls short.\n#if defined(_LIBCUDACXX_ABI_FORCE_ITANIUM) && defined(_LIBCUDACXX_ABI_FORCE_MICROSOFT)\n#  error \"Only one of _LIBCUDACXX_ABI_FORCE_ITANIUM and _LIBCUDACXX_ABI_FORCE_MICROSOFT can be defined\"\n#elif defined(_LIBCUDACXX_ABI_FORCE_ITANIUM)\n#  define _LIBCUDACXX_ABI_ITANIUM\n#elif defined(_LIBCUDACXX_ABI_FORCE_MICROSOFT)\n#  define _LIBCUDACXX_ABI_MICROSOFT\n#else\n#  if defined(_WIN32) && defined(_LIBCUDACXX_COMPILER_MSVC)\n#    define _LIBCUDACXX_ABI_MICROSOFT\n#  else\n#    define _LIBCUDACXX_ABI_ITANIUM\n#  endif\n#endif\n\n#if defined(_LIBCUDACXX_ABI_MICROSOFT) && !defined(_LIBCUDACXX_NO_VCRUNTIME)\n# define _LIBCUDACXX_ABI_VCRUNTIME\n#endif\n\n// Need to detect which libc we're using if we're on Linux.\n#if defined(__linux__)\n#  include <features.h>\n#  if defined(__GLIBC_PREREQ)\n#    define _LIBCUDACXX_GLIBC_PREREQ(a, b) __GLIBC_PREREQ(a, b)\n#  else\n#    define _LIBCUDACXX_GLIBC_PREREQ(a, b) 0\n#  endif // defined(__GLIBC_PREREQ)\n#endif // defined(__linux__)\n\n#ifdef __LITTLE_ENDIAN__\n#  if __LITTLE_ENDIAN__\n#    define _LIBCUDACXX_LITTLE_ENDIAN\n#  endif  // __LITTLE_ENDIAN__\n#endif  // __LITTLE_ENDIAN__\n\n#ifdef __BIG_ENDIAN__\n#  if __BIG_ENDIAN__\n#    define _LIBCUDACXX_BIG_ENDIAN\n#  endif  // __BIG_ENDIAN__\n#endif  // __BIG_ENDIAN__\n\n#ifdef __BYTE_ORDER__\n#  if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__\n#    define _LIBCUDACXX_LITTLE_ENDIAN\n#  elif __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__\n#    define _LIBCUDACXX_BIG_ENDIAN\n#  endif // __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__\n#endif // __BYTE_ORDER__\n\n#ifdef __FreeBSD__\n#  include <sys/endian.h>\n#  if _BYTE_ORDER == _LITTLE_ENDIAN\n#    define _LIBCUDACXX_LITTLE_ENDIAN\n#  else  // _BYTE_ORDER == _LITTLE_ENDIAN\n#    define _LIBCUDACXX_BIG_ENDIAN\n#  endif  // _BYTE_ORDER == _LITTLE_ENDIAN\n#  ifndef __LONG_LONG_SUPPORTED\n#    define _LIBCUDACXX_HAS_NO_LONG_LONG\n#  endif  // __LONG_LONG_SUPPORTED\n#endif  // __FreeBSD__\n\n#ifdef __NetBSD__\n#  include <sys/endian.h>\n#  if _BYTE_ORDER == _LITTLE_ENDIAN\n#    define _LIBCUDACXX_LITTLE_ENDIAN\n#  else  // _BYTE_ORDER == _LITTLE_ENDIAN\n#    define _LIBCUDACXX_BIG_ENDIAN\n#  endif  // _BYTE_ORDER == _LITTLE_ENDIAN\n#  define _LIBCUDACXX_HAS_QUICK_EXIT\n#endif  // __NetBSD__\n\n#if defined(_WIN32)\n#  define _LIBCUDACXX_WIN32API\n#  define _LIBCUDACXX_LITTLE_ENDIAN\n#  define _LIBCUDACXX_SHORT_WCHAR   1\n// Both MinGW and native MSVC provide a \"MSVC\"-like environment\n#  define _LIBCUDACXX_MSVCRT_LIKE\n// If mingw not explicitly detected, assume using MS C runtime only if\n// a MS compatibility version is specified.\n#  if defined(_LIBCUDACXX_COMPILER_MSVC) && !defined(__MINGW32__)\n#    define _LIBCUDACXX_MSVCRT // Using Microsoft's C Runtime library\n#  endif\n#  if (defined(_M_AMD64) || defined(__x86_64__)) || (defined(_M_ARM) || defined(__arm__))\n#    define _LIBCUDACXX_HAS_BITSCAN64\n#  endif\n#  define _LIBCUDACXX_HAS_OPEN_WITH_WCHAR\n#  if defined(_LIBCUDACXX_MSVCRT)\n#    define _LIBCUDACXX_HAS_QUICK_EXIT\n#  endif\n\n// Some CRT APIs are unavailable to store apps\n#  if defined(WINAPI_FAMILY)\n#    include <winapifamily.h>\n#    if !WINAPI_FAMILY_PARTITION(WINAPI_PARTITION_DESKTOP) &&                  \\\n        (!defined(WINAPI_PARTITION_SYSTEM) ||                                  \\\n         !WINAPI_FAMILY_PARTITION(WINAPI_PARTITION_SYSTEM))\n#      define _LIBCUDACXX_WINDOWS_STORE_APP\n#    endif\n#  endif\n#endif // defined(_WIN32)\n\n#ifdef __sun__\n#  include <sys/isa_defs.h>\n#  ifdef _LITTLE_ENDIAN\n#    define _LIBCUDACXX_LITTLE_ENDIAN\n#  else\n#    define _LIBCUDACXX_BIG_ENDIAN\n#  endif\n#endif // __sun__\n\n#if defined(__CloudABI__)\n   // Certain architectures provide arc4random(). Prefer using\n   // arc4random() over /dev/{u,}random to make it possible to obtain\n   // random data even when using sandboxing mechanisms such as chroots,\n   // Capsicum, etc.\n#  define _LIBCUDACXX_USING_ARC4_RANDOM\n#elif defined(__Fuchsia__) || defined(__wasi__)\n#  define _LIBCUDACXX_USING_GETENTROPY\n#elif defined(__native_client__)\n   // NaCl's sandbox (which PNaCl also runs in) doesn't allow filesystem access,\n   // including accesses to the special files under /dev. C++11's\n   // std::random_device is instead exposed through a NaCl syscall.\n#  define _LIBCUDACXX_USING_NACL_RANDOM\n#elif defined(_LIBCUDACXX_WIN32API)\n#  define _LIBCUDACXX_USING_WIN32_RANDOM\n#else\n#  define _LIBCUDACXX_USING_DEV_RANDOM\n#endif\n\n#ifndef _LIBCUDACXX_LITTLE_ENDIAN\n#if defined(_LIBCUDACXX_COMPILER_NVRTC)\n#  define _LIBCUDACXX_LITTLE_ENDIAN\n#endif\n#endif // _LIBCUDACXX_LITTLE_ENDIAN\n\n#if !defined(_LIBCUDACXX_LITTLE_ENDIAN) && !defined(_LIBCUDACXX_BIG_ENDIAN)\n#  include <endian.h>\n#  if __BYTE_ORDER == __LITTLE_ENDIAN\n#    define _LIBCUDACXX_LITTLE_ENDIAN\n#  elif __BYTE_ORDER == __BIG_ENDIAN\n#    define _LIBCUDACXX_BIG_ENDIAN\n#  else  // __BYTE_ORDER == __BIG_ENDIAN\n#    error unable to determine endian\n#  endif\n#endif  // !defined(_LIBCUDACXX_LITTLE_ENDIAN) && !defined(_LIBCUDACXX_BIG_ENDIAN)\n\n#if __has_attribute(__no_sanitize__) && !defined(_LIBCUDACXX_COMPILER_GCC)\n#  define _LIBCUDACXX_NO_CFI __attribute__((__no_sanitize__(\"cfi\")))\n#else\n#  define _LIBCUDACXX_NO_CFI\n#endif\n\n#if __ISO_C_VISIBLE >= 2011 || __cplusplus >= 201103L\n#  if defined(__FreeBSD__)\n#    define _LIBCUDACXX_HAS_QUICK_EXIT\n#    define _LIBCUDACXX_HAS_C11_FEATURES\n#  elif defined(__Fuchsia__) || defined(__wasi__)\n#    define _LIBCUDACXX_HAS_QUICK_EXIT\n#    define _LIBCUDACXX_HAS_TIMESPEC_GET\n#    define _LIBCUDACXX_HAS_C11_FEATURES\n#  elif defined(__linux__)\n#    if !defined(_LIBCUDACXX_HAS_MUSL_LIBC)\n#      if _LIBCUDACXX_GLIBC_PREREQ(2, 15) || defined(__BIONIC__)\n#        define _LIBCUDACXX_HAS_QUICK_EXIT\n#      endif\n#      if _LIBCUDACXX_GLIBC_PREREQ(2, 17)\n#        define _LIBCUDACXX_HAS_C11_FEATURES\n#        define _LIBCUDACXX_HAS_TIMESPEC_GET\n#      endif\n#    else // defined(_LIBCUDACXX_HAS_MUSL_LIBC)\n#      define _LIBCUDACXX_HAS_QUICK_EXIT\n#      define _LIBCUDACXX_HAS_TIMESPEC_GET\n#      define _LIBCUDACXX_HAS_C11_FEATURES\n#    endif\n#  endif // __linux__\n#endif\n\n#if defined(_LIBCUDACXX_COMPILER_NVRTC)\n#  define __alignof(x) alignof(x)\n#endif // _LIBCUDACXX_COMPILER_NVRTC\n\n#if defined(_LIBCUDACXX_COMPILER_MSVC)\n#  define __alignof__ __alignof\n#endif\n\n#define _LIBCUDACXX_ALIGNOF(_Tp) alignof(_Tp)\n#define _LIBCUDACXX_PREFERRED_ALIGNOF(_Tp) __alignof(_Tp)\n\n#if defined(_LIBCUDACXX_COMPILER_MSVC)\n#  define _ALIGNAS_TYPE(x) alignas(x)\n#  define _ALIGNAS(x) __declspec(align(x))\n#elif __has_feature(cxx_alignas)\n#  define _ALIGNAS_TYPE(x) alignas(x)\n#  define _ALIGNAS(x) alignas(x)\n#else\n#  define _ALIGNAS_TYPE(x) __attribute__((__aligned__(_LIBCUDACXX_ALIGNOF(x))))\n#  define _ALIGNAS(x) __attribute__((__aligned__(x)))\n#endif // !_LIBCUDACXX_COMPILER_MSVC && !__has_feature(cxx_alignas)\n\n#if defined(_LIBCUDACXX_COMPILER_MSVC)\n#define _LIBCUDACXX_ALWAYS_INLINE __forceinline\n#else\n#define _LIBCUDACXX_ALWAYS_INLINE __attribute__ ((__always_inline__))\n#endif // !_LIBCUDACXX_COMPILER_MSVC\n\n#if defined(__cuda_std__)\n#define _LIBCUDACXX_ATOMIC_ALWAYS_LOCK_FREE(size, ptr) (size <= 8)\n#elif defined(_LIBCUDACXX_COMPILER_CLANG) || defined(_LIBCUDACXX_COMPILER_GCC)\n#define _LIBCUDACXX_ATOMIC_ALWAYS_LOCK_FREE(...) __atomic_always_lock_free(__VA_ARGS__)\n#endif // __cuda_std__\n\n// https://bugs.llvm.org/show_bug.cgi?id=44517\n#define __check_builtin(__x) (__has_builtin(__##__x) || \\\n                              __has_keyword(__##__x) || \\\n                              __has_feature(__x))\n\n// We work around old clang versions (before clang-10) not supporting __has_builtin via __check_builtin\n// We work around old intel versions (before 2021.3)   not supporting __has_builtin via __check_builtin\n// We work around old nvhpc versions (before 2022.11)  not supporting __has_builtin via __check_builtin\n// MSVC needs manual handling, has no real way of checking builtins so all is manual\n// GCC  needs manual handling, before gcc-10 as that finally supports __has_builtin\n\n#if __check_builtin(array_rank)\n#define _LIBCUDACXX_ARRAY_RANK(...) __array_rank(__VA_ARGS__)\n#endif // __check_builtin(array_rank)\n\n// nvhpc has a bug where it supports __builtin_addressof but does not mark it via __check_builtin\n#if __check_builtin(builtin_addressof)                       \\\n || (defined(_LIBCUDACXX_COMPILER_GCC)  && _GNUC_VER >= 700) \\\n || defined(_LIBCUDACXX_COMPILER_MSVC)                       \\\n || defined(_LIBCUDACXX_COMPILER_NVHPC)\n#define _LIBCUDACXX_ADDRESSOF(...) __builtin_addressof(__VA_ARGS__)\n#endif // __check_builtin(builtin_addressof)\n\n#if __check_builtin(builtin_bit_cast) \\\n || (defined(_LIBCUDACXX_COMPILER_MSVC) && _MSC_VER  > 1925)\n#define _LIBCUDACXX_BIT_CAST(...) __builtin_bit_cast(__VA_ARGS__)\n#endif // __check_builtin(builtin_bit_cast)\n\n#if __check_builtin(builtin_is_constant_evaluated)           \\\n || (defined(_LIBCUDACXX_COMPILER_GCC)  && _GNUC_VER >= 900) \\\n || (defined(_LIBCUDACXX_COMPILER_MSVC) && _MSC_VER  > 1924)\n#define _LIBCUDACXX_IS_CONSTANT_EVALUATED(...) __builtin_is_constant_evaluated(__VA_ARGS__)\n#endif // __check_builtin(builtin_is_constant_evaluated)\n\n// NVCC and NVRTC in C++11 mode freaks out about `__builtin_is_constant_evaluated`.\n#if _LIBCUDACXX_STD_VER < 14              \\\n && (defined(_LIBCUDACXX_COMPILER_NVCC)   \\\n ||  defined(_LIBCUDACXX_COMPILER_NVRTC)  \\\n ||  defined(_LIBCUDACXX_COMPILER_NVHPC))\n#undef _LIBCUDACXX_IS_CONSTANT_EVALUATED\n#endif // _LIBCUDACXX_STD_VER < 14 && defined(_LIBCUDACXX_COMPILER_NVCC)\n\n#if __check_builtin(builtin_launder)                         \\\n || (defined(_LIBCUDACXX_COMPILER_GCC)  && _GNUC_VER >= 700)\n#define _LIBCUDACXX_LAUNDER(...) __builtin_launder(__VA_ARGS__)\n#endif // __check_builtin(builtin_launder)\n\n// Disabled due to libstdc++ conflict\n#if 0 // __check_builtin(decay)\n#define _LIBCUDACXX_DECAY(...) __decay(__VA_ARGS__)\n#endif // __check_builtin(decay)\n\n#if __check_builtin(has_nothrow_assign)                      \\\n || (defined(_LIBCUDACXX_COMPILER_GCC)  && _GNUC_VER >= 403) \\\n || defined(_LIBCUDACXX_COMPILER_MSVC)                       \\\n || defined(_LIBCUDACXX_COMPILER_NVRTC)\n#define _LIBCUDACXX_HAS_NOTHROW_ASSIGN(...) __has_nothrow_assign(__VA_ARGS__)\n#endif // __check_builtin(has_nothrow_assign)\n\n#if __check_builtin(has_nothrow_constructor)                 \\\n || (defined(_LIBCUDACXX_COMPILER_GCC)  && _GNUC_VER >= 403) \\\n || defined(_LIBCUDACXX_COMPILER_MSVC)                       \\\n || defined(_LIBCUDACXX_COMPILER_NVRTC)\n#define _LIBCUDACXX_HAS_NOTHROW_CONSTRUCTOR(...) __has_nothrow_constructor(__VA_ARGS__)\n#endif // __check_builtin(has_nothrow_constructor)\n\n#if __check_builtin(has_nothrow_copy)                        \\\n || (defined(_LIBCUDACXX_COMPILER_GCC)  && _GNUC_VER >= 403) \\\n || defined(_LIBCUDACXX_COMPILER_MSVC)                       \\\n || defined(_LIBCUDACXX_COMPILER_NVRTC)\n#define _LIBCUDACXX_HAS_NOTHROW_COPY(...) __has_nothrow_copy(__VA_ARGS__)\n#endif // __check_builtin(has_nothrow_copy)\n\n#if __check_builtin(has_trivial_constructor)                 \\\n || (defined(_LIBCUDACXX_COMPILER_GCC)  && _GNUC_VER >= 403) \\\n || defined(_LIBCUDACXX_COMPILER_MSVC)                       \\\n || defined(_LIBCUDACXX_COMPILER_NVRTC)\n#define _LIBCUDACXX_HAS_TRIVIAL_CONSTRUCTOR(...) __has_trivial_constructor(__VA_ARGS__)\n#endif // __check_builtin(has_trivial_constructor)\n\n#if __check_builtin(has_trivial_destructor)                  \\\n || (defined(_LIBCUDACXX_COMPILER_GCC)  && _GNUC_VER >= 403) \\\n || defined(_LIBCUDACXX_COMPILER_MSVC)                       \\\n || defined(_LIBCUDACXX_COMPILER_NVRTC)\n#define _LIBCUDACXX_HAS_TRIVIAL_DESTRUCTOR(...) __has_trivial_destructor(__VA_ARGS__)\n#endif // __check_builtin(has_trivial_destructor)\n\n#if __check_builtin(has_unique_object_representations)       \\\n || (defined(_LIBCUDACXX_COMPILER_GCC)  && _GNUC_VER >= 700)\n#define _LIBCUDACXX_HAS_UNIQUE_OBJECT_REPRESENTATIONS(...) __has_unique_object_representations(__VA_ARGS__)\n#endif // __check_builtin(has_unique_object_representations)\n\n#if __check_builtin(has_virtual_destructor)                  \\\n || (defined(_LIBCUDACXX_COMPILER_GCC)  && _GNUC_VER >= 403) \\\n || defined(_LIBCUDACXX_COMPILER_MSVC)                       \\\n || defined(_LIBCUDACXX_COMPILER_NVRTC)\n#define _LIBCUDACXX_HAS_VIRTUAL_DESTRUCTOR(...) __has_virtual_destructor(__VA_ARGS__)\n#endif // __check_builtin(has_virtual_destructor)\n\n#if __check_builtin(is_aggregate)                            \\\n || (defined(_LIBCUDACXX_COMPILER_GCC)  && _GNUC_VER >= 700) \\\n || (defined(_LIBCUDACXX_COMPILER_MSVC) && _MSC_VER  > 1914) \\\n || defined(_LIBCUDACXX_COMPILER_NVRTC)\n#define _LIBCUDACXX_IS_AGGREGATE(...) __is_aggregate(__VA_ARGS__)\n#endif // __check_builtin(is_aggregate)\n\n#if __check_builtin(is_array)\n#define _LIBCUDACXX_IS_ARRAY(...) __is_array(__VA_ARGS__)\n#endif // __check_builtin(is_array)\n\n// TODO: Clang incorrectly reports that __is_array is true for T[0].\n//       Re-enable the branch once https://llvm.org/PR54705 is fixed.\n#if defined(_LIBCUDACXX_COMPILER_CLANG)\n#define _LIBCUDACXX_USE_IS_ARRAY_FALLBACK\n#endif // _LIBCUDACXX_COMPILER_CLANG\n\n#if __check_builtin(is_assignable)     \\\n || defined(_LIBCUDACXX_COMPILER_MSVC)\n#define _LIBCUDACXX_IS_ASSIGNABLE(...) __is_assignable(__VA_ARGS__)\n#endif // __check_builtin(is_assignable)\n\n#if __check_builtin(is_base_of)                              \\\n || (defined(_LIBCUDACXX_COMPILER_GCC)  && _GNUC_VER >= 403) \\\n || defined(_LIBCUDACXX_COMPILER_MSVC)                       \\\n || defined(_LIBCUDACXX_COMPILER_NVRTC)\n#define _LIBCUDACXX_IS_BASE_OF(...) __is_base_of(__VA_ARGS__)\n#endif // __check_builtin(is_base_of)\n\n#if __check_builtin(is_class)                                \\\n || (defined(_LIBCUDACXX_COMPILER_GCC)  && _GNUC_VER >= 403) \\\n || defined(_LIBCUDACXX_COMPILER_MSVC)                       \\\n || defined(_LIBCUDACXX_COMPILER_NVRTC)\n#define _LIBCUDACXX_IS_CLASS(...) __is_class(__VA_ARGS__)\n#endif // __check_builtin(is_class)\n\n#if __check_builtin(is_constructible)                       \\\n || (defined(_LIBCUDACXX_COMPILER_GCC) && _GNUC_VER >= 800) \\\n || defined(_LIBCUDACXX_COMPILER_MSVC)                      \\\n || defined(_LIBCUDACXX_COMPILER_NVRTC)\n#define _LIBCUDACXX_IS_CONSTRUCTIBLE(...) __is_constructible(__VA_ARGS__)\n#endif // __check_builtin(is_constructible)\n\n#if __check_builtin(is_convertible_to) \\\n || defined(_LIBCUDACXX_COMPILER_MSVC) \\\n || defined(_LIBCUDACXX_COMPILER_NVRTC)\n#define _LIBCUDACXX_IS_CONVERTIBLE_TO(...) __is_convertible_to(__VA_ARGS__)\n#endif // __check_builtin(is_convertible_to)\n\n#if __check_builtin(is_destructible)   \\\n || defined(_LIBCUDACXX_COMPILER_MSVC)\n#define _LIBCUDACXX_IS_DESTRUCTIBLE(...) __is_destructible(__VA_ARGS__)\n#endif // __check_builtin(is_destructible)\n\n#if __check_builtin(is_empty)                                \\\n || (defined(_LIBCUDACXX_COMPILER_GCC)  && _GNUC_VER >= 403) \\\n || defined(_LIBCUDACXX_COMPILER_MSVC)                       \\\n || defined(_LIBCUDACXX_COMPILER_NVRTC)\n#define _LIBCUDACXX_IS_EMPTY(...) __is_empty(__VA_ARGS__)\n#endif // __check_builtin(is_empty)\n\n#if __check_builtin(is_enum)                                 \\\n || (defined(_LIBCUDACXX_COMPILER_GCC)  && _GNUC_VER >= 403) \\\n || defined(_LIBCUDACXX_COMPILER_MSVC)                       \\\n || defined(_LIBCUDACXX_COMPILER_NVRTC)\n#define _LIBCUDACXX_IS_ENUM(...) __is_enum(__VA_ARGS__)\n#endif // __check_builtin(is_enum)\n\n#if __check_builtin(is_final)                                \\\n || (defined(_LIBCUDACXX_COMPILER_GCC)  && _GNUC_VER >= 407) \\\n || defined(_LIBCUDACXX_COMPILER_MSVC)                       \\\n || defined(_LIBCUDACXX_COMPILER_NVRTC)\n#define _LIBCUDACXX_IS_FINAL(...) __is_final(__VA_ARGS__)\n#endif // __check_builtin(is_final)\n\n#if __check_builtin(is_function)       \\\n && !defined(_LIBCUDACXX_COMPILER_NVCC)\n#define _LIBCUDACXX_IS_FUNCTION(...) __is_function(__VA_ARGS__)\n#endif // __check_builtin(is_function)\n\n#if __check_builtin(is_literal_type)                         \\\n || (defined(_LIBCUDACXX_COMPILER_GCC)  && _GNUC_VER >= 406) \\\n || defined(_LIBCUDACXX_COMPILER_MSVC)                       \\\n || defined(_LIBCUDACXX_COMPILER_NVRTC)\n#define _LIBCUDACXX_IS_LITERAL(...) __is_literal_type(__VA_ARGS__)\n#endif // __check_builtin(is_literal_type)\n\n#if __check_builtin(is_lvalue_reference)\n#define _LIBCUDACXX_IS_LVALUE_REFERENCE(...) __is_lvalue_reference(__VA_ARGS__)\n#endif // __check_builtin(is_lvalue_reference)\n\n#if defined(_LIBCUDACXX_CUDACC_BELOW_11_3)\n#define _LIBCUDACXX_USE_IS_LVALUE_REFERENCE_FALLBACK\n#endif // nvcc < 11.3\n\n#if __check_builtin(is_nothrow_assignable) \\\n || defined(_LIBCUDACXX_COMPILER_MSVC)     \\\n || defined(_LIBCUDACXX_COMPILER_NVRTC)\n#define _LIBCUDACXX_IS_NOTHROW_ASSIGNABLE(...) __is_nothrow_assignable(__VA_ARGS__)\n#endif // __check_builtin(is_nothrow_assignable)\n\n#if __check_builtin(is_nothrow_constructible) \\\n || defined(_LIBCUDACXX_COMPILER_MSVC)        \\\n || defined(_LIBCUDACXX_COMPILER_NVRTC)\n#define _LIBCUDACXX_IS_NOTHROW_CONSTRUCTIBLE(...) __is_nothrow_constructible(__VA_ARGS__)\n#endif // __check_builtin(is_nothrow_constructible)\n\n#if __check_builtin(is_nothrow_destructible) \\\n || defined(_LIBCUDACXX_COMPILER_MSVC)       \\\n || defined(_LIBCUDACXX_COMPILER_NVRTC)\n#define _LIBCUDACXX_IS_NOTHROW_DESTRUCTIBLE(...) __is_nothrow_destructible(__VA_ARGS__)\n#endif // __check_builtin(is_nothrow_destructible)\n\n#if __check_builtin(is_object)\n#define _LIBCUDACXX_IS_OBJECT(...) __is_object(__VA_ARGS__)\n#endif // __check_builtin(is_object)\n\n#if defined(_LIBCUDACXX_CUDACC_BELOW_11_3)\n#define _LIBCUDACXX_USE_IS_OBJECT_FALLBACK\n#endif // nvcc < 11.3\n\n#if __check_builtin(is_pod)                                  \\\n || (defined(_LIBCUDACXX_COMPILER_GCC)  && _GNUC_VER >= 403) \\\n || defined(_LIBCUDACXX_COMPILER_MSVC)                       \\\n || defined(_LIBCUDACXX_COMPILER_NVRTC)\n#define _LIBCUDACXX_IS_POD(...) __is_pod(__VA_ARGS__)\n#endif // __check_builtin(is_pod)\n\n// libstdc++ defines this as a function, breaking functionality\n#if 0 // __check_builtin(is_pointer)\n#define _LIBCUDACXX_IS_POINTER(...) __is_pointer(__VA_ARGS__)\n#endif // __check_builtin(is_pointer)\n\n#if __check_builtin(is_polymorphic)                          \\\n || (defined(_LIBCUDACXX_COMPILER_GCC)  && _GNUC_VER >= 403) \\\n || defined(_LIBCUDACXX_COMPILER_MSVC)                       \\\n || defined(_LIBCUDACXX_COMPILER_NVRTC)\n#define _LIBCUDACXX_IS_POLYMORPHIC(...) __is_polymorphic(__VA_ARGS__)\n#endif // __check_builtin(is_polymorphic)\n\n#if __check_builtin(is_reference)\n#define _LIBCUDACXX_IS_REFERENCE(...) __is_reference(__VA_ARGS__)\n#endif // __check_builtin(is_reference)\n\n// Disabled due to libstdc++ conflict\n#if 0 // __check_builtin(is_referenceable)\n#define _LIBCUDACXX_IS_REFERENCEABLE(...) __is_referenceable(__VA_ARGS__)\n#endif // __check_builtin(is_referenceable)\n\n#if __check_builtin(is_rvalue_reference)\n#define _LIBCUDACXX_IS_RVALUE_REFERENCE(...) __is_rvalue_reference(__VA_ARGS__)\n#endif // __check_builtin(is_rvalue_reference)\n\n#if __check_builtin(is_same)            \\\n && !defined(_LIBCUDACXX_COMPILER_NVCC)\n#define _LIBCUDACXX_IS_SAME(...) __is_same(__VA_ARGS__)\n#endif // __check_builtin(is_same)\n\n// libstdc++ defines this as a function, breaking functionality\n#if 0 // __check_builtin(is_scalar)\n#define _LIBCUDACXX_IS_SCALAR(...) __is_scalar(__VA_ARGS__)\n#endif // __check_builtin(is_scalar)\n\n// libstdc++ defines this as a function, breaking functionality\n#if 0 // __check_builtin(is_signed)\n#define _LIBCUDACXX_IS_SIGNED(...) __is_signed(__VA_ARGS__)\n#endif // __check_builtin(is_signed)\n\n#if __check_builtin(is_standard_layout)                      \\\n || (defined(_LIBCUDACXX_COMPILER_GCC)  && _GNUC_VER >= 407) \\\n || defined(_LIBCUDACXX_COMPILER_MSVC)                       \\\n || defined(_LIBCUDACXX_COMPILER_NVRTC)\n#define _LIBCUDACXX_IS_STANDARD_LAYOUT(...) __is_standard_layout(__VA_ARGS__)\n#endif // __check_builtin(is_standard_layout)\n\n#if __check_builtin(is_trivial)                              \\\n || (defined(_LIBCUDACXX_COMPILER_GCC)  && _GNUC_VER >= 405) \\\n || defined(_LIBCUDACXX_COMPILER_MSVC)                       \\\n || defined(_LIBCUDACXX_COMPILER_NVRTC)\n#define _LIBCUDACXX_IS_TRIVIAL(...) __is_trivial(__VA_ARGS__)\n#endif // __check_builtin(is_trivial)\n\n#if __check_builtin(is_trivially_assignable)                 \\\n || (defined(_LIBCUDACXX_COMPILER_GCC)  && _GNUC_VER >= 501) \\\n || defined(_LIBCUDACXX_COMPILER_MSVC)                       \\\n || defined(_LIBCUDACXX_COMPILER_NVRTC)\n#define _LIBCUDACXX_IS_TRIVIALLY_ASSIGNABLE(...) __is_trivially_assignable(__VA_ARGS__)\n#endif // __check_builtin(is_trivially_assignable)\n\n#if __check_builtin(is_trivially_constructible)              \\\n || (defined(_LIBCUDACXX_COMPILER_GCC)  && _GNUC_VER >= 501) \\\n || defined(_LIBCUDACXX_COMPILER_MSVC)                       \\\n || defined(_LIBCUDACXX_COMPILER_NVRTC)\n#define _LIBCUDACXX_IS_TRIVIALLY_CONSTRUCTIBLE(...) __is_trivially_constructible(__VA_ARGS__)\n#endif // __check_builtin(is_trivially_constructible)\n\n#if __check_builtin(is_trivially_copyable)                   \\\n || (defined(_LIBCUDACXX_COMPILER_GCC)  && _GNUC_VER >= 501) \\\n || defined(_LIBCUDACXX_COMPILER_MSVC)                       \\\n || defined(_LIBCUDACXX_COMPILER_NVRTC)\n#define _LIBCUDACXX_IS_TRIVIALLY_COPYABLE(...) __is_trivially_copyable(__VA_ARGS__)\n#endif // __check_builtin(is_trivially_copyable)\n\n#if __check_builtin(is_trivially_destructible) \\\n || defined(_LIBCUDACXX_COMPILER_MSVC)\n#define _LIBCUDACXX_IS_TRIVIALLY_DESTRUCTIBLE(...) __is_trivially_destructible(__VA_ARGS__)\n#endif // __check_builtin(is_trivially_destructible)\n\n#if __check_builtin(is_union)                                \\\n || (defined(_LIBCUDACXX_COMPILER_GCC)  && _GNUC_VER >= 403) \\\n || defined(_LIBCUDACXX_COMPILER_MSVC)                       \\\n || defined(_LIBCUDACXX_COMPILER_NVRTC)\n#define _LIBCUDACXX_IS_UNION(...) __is_union(__VA_ARGS__)\n#endif // __check_builtin(is_union)\n\n#if __check_builtin(is_unsigned)\n#define _LIBCUDACXX_IS_UNSIGNED(...) __is_unsigned(__VA_ARGS__)\n#endif // __check_builtin(is_unsigned)\n\n#if defined(_LIBCUDACXX_CUDACC_BELOW_11_3)\n#define _LIBCUDACXX_USE_IS_UNSIGNED_FALLBACK\n#endif // nvcc < 11.3\n\n// libstdc++ defines this as a function, breaking functionality\n#if 0 // __check_builtin(is_void)\n#define _LIBCUDACXX_IS_VOID(...) __is_void(__VA_ARGS__)\n#endif // __check_builtin(is_void)\n\n// Disabled due to libstdc++ conflict\n#if 0 // __check_builtin(make_signed)\n#define _LIBCUDACXX_MAKE_SIGNED(...) __make_signed(__VA_ARGS__)\n#endif // __check_builtin(make_signed)\n\n// Disabled due to libstdc++ conflict\n#if 0 // __check_builtin(make_unsigned)\n#define _LIBCUDACXX_MAKE_UNSIGNED(...) __make_unsigned(__VA_ARGS__)\n#endif // __check_builtin(make_unsigned)\n\n// Disabled due to libstdc++ conflict\n#if 0 // __check_builtin(remove_all_extents)\n#define _LIBCUDACXX_REMOVE_ALL_EXTENTS(...) __remove_all_extents(__VA_ARGS__)\n#endif // __check_builtin(remove_all_extents)\n\n// Disabled due to libstdc++ conflict\n#if 0 // __check_builtin(remove_const)\n#define _LIBCUDACXX_REMOVE_CONST(...) __remove_const(__VA_ARGS__)\n#endif // __check_builtin(remove_const)\n\n// Disabled due to libstdc++ conflict\n#if 0 // __check_builtin(remove_cv)\n#define _LIBCUDACXX_REMOVE_CV(...) __remove_cv(__VA_ARGS__)\n#endif // __check_builtin(remove_cv)\n\n// Disabled due to libstdc++ conflict\n#if 0 // __check_builtin(remove_cvref)\n#define _LIBCUDACXX_REMOVE_CVREF(...) __remove_cvref(__VA_ARGS__)\n#endif // __check_builtin(remove_cvref)\n\n// Disabled due to libstdc++ conflict\n#if 0 // __check_builtin(remove_extent)\n#define _LIBCUDACXX_REMOVE_EXTENT(...) __remove_extent(__VA_ARGS__)\n#endif // __check_builtin(remove_extent)\n\n// Disabled due to libstdc++ conflict\n#if 0 // __check_builtin(remove_pointer)\n#define _LIBCUDACXX_REMOVE_POINTER(...) __remove_pointer(__VA_ARGS__)\n#endif // __check_builtin(remove_pointer)\n\n// Disabled due to libstdc++ conflict\n#if 0 // __check_builtin(remove_reference_t)\n#define _LIBCUDACXX_REMOVE_REFERENCE_T(...) __remove_reference_t(__VA_ARGS__)\n#endif // __check_builtin(remove_reference_t)\n\n// Disabled due to libstdc++ conflict\n#if 0 // __check_builtin(remove_volatile)\n#define _LIBCUDACXX_REMOVE_VOLATILE(...) __remove_volatile(__VA_ARGS__)\n#endif // __check_builtin(remove_volatile)\n\n#if __check_builtin(underlying_type)                         \\\n || (defined(_LIBCUDACXX_COMPILER_GCC)  && _GNUC_VER >= 407) \\\n || defined(_LIBCUDACXX_COMPILER_MSVC)                       \\\n || defined(_LIBCUDACXX_COMPILER_NVRTC)\n#define _LIBCUDACXX_UNDERLYING_TYPE(...) __underlying_type(__VA_ARGS__)\n#endif // __check_builtin(underlying_type)\n\n#define _LIBCUDACXX_TOSTRING2(_STR) #_STR\n#define _LIBCUDACXX_TOSTRING(_STR) _LIBCUDACXX_TOSTRING2(_STR)\n#if defined(_LIBCUDACXX_CUDACC)\n#  if defined(__NVCC_DIAG_PRAGMA_SUPPORT__)\n#    if defined(_LIBCUDACXX_COMPILER_MSVC)\n#      define _LIBCUDACXX_NV_DIAG_SUPPRESS(_WARNING) __pragma(_LIBCUDACXX_TOSTRING(nv_diag_suppress _WARNING))\n#      define _LIBCUDACXX_NV_DIAG_DEFAULT(_WARNING)  __pragma(_LIBCUDACXX_TOSTRING(nv_diag_default _WARNING))\n#    else // ^^^ MSVC ^^^ / vvv not MSVC\n#      define _LIBCUDACXX_NV_DIAG_SUPPRESS(_WARNING)     \\\n         _Pragma(_LIBCUDACXX_TOSTRING(nv_diagnostic push)) \\\n         _Pragma(_LIBCUDACXX_TOSTRING(nv_diag_suppress _WARNING))\n#      define _LIBCUDACXX_NV_DIAG_DEFAULT(_WARNING) _Pragma(_LIBCUDACXX_TOSTRING(nv_diagnostic pop))\n#    endif // not MSVC\n#  elif defined(_LIBCUDACXX_COMPILER_NVHPC)\n#    define _LIBCUDACXX_NV_DIAG_SUPPRESS(_WARNING)  \\\n       _Pragma(_LIBCUDACXX_TOSTRING(diagnostic push)) \\\n       _Pragma(_LIBCUDACXX_TOSTRING(diag_suppress _WARNING))\n#    define _LIBCUDACXX_NV_DIAG_DEFAULT(_WARNING) _Pragma(_LIBCUDACXX_TOSTRING(diagnostic pop))\n#  else // _LIBCUDACXX_CUDACC_BELOW_11_3\n#    if defined(_LIBCUDACXX_COMPILER_MSVC)\n#      define _LIBCUDACXX_NV_DIAG_SUPPRESS(_WARNING) __pragma(_LIBCUDACXX_TOSTRING(diag_suppress _WARNING))\n#      define _LIBCUDACXX_NV_DIAG_DEFAULT(_WARNING)  __pragma(_LIBCUDACXX_TOSTRING(diag_default _WARNING))\n#    else // ^^^ MSVC ^^^ / vvv not MSVC\n#      define _LIBCUDACXX_NV_DIAG_SUPPRESS(_WARNING) _Pragma(_LIBCUDACXX_TOSTRING(diag_suppress _WARNING))\n#      define _LIBCUDACXX_NV_DIAG_DEFAULT(_WARNING)  _Pragma(_LIBCUDACXX_TOSTRING(diag_default _WARNING))\n#    endif // not MSVC\n#  endif // !__NVCC_DIAG_PRAGMA_SUPPORT__\n#else // ^^^ _LIBCUDACXX_CUDACC ^^^ / vvv other compiler vvv\n#  define _LIBCUDACXX_NV_DIAG_SUPPRESS(_WARNING)\n#  define _LIBCUDACXX_NV_DIAG_DEFAULT(_WARNING)\n#endif // other compilers\n\n#if defined(_LIBCUDACXX_COMPILER_CLANG)\n\n// _LIBCUDACXX_ALTERNATE_STRING_LAYOUT is an old name for\n// _LIBCUDACXX_ABI_ALTERNATE_STRING_LAYOUT left here for backward compatibility.\n#if defined(_LIBCUDACXX_ALTERNATE_STRING_LAYOUT)\n#define _LIBCUDACXX_ABI_ALTERNATE_STRING_LAYOUT\n#endif\n\n#if __cplusplus < 201103L\ntypedef __char16_t char16_t;\ntypedef __char32_t char32_t;\n#endif\n\n#if !(__has_feature(cxx_strong_enums))\n#define _LIBCUDACXX_HAS_NO_STRONG_ENUMS\n#endif\n\n#if !(__has_feature(cxx_lambdas))\n#define _LIBCUDACXX_HAS_NO_LAMBDAS\n#endif\n\n#if !(__has_feature(cxx_nullptr))\n#  if (__has_extension(cxx_nullptr) || __has_keyword(__nullptr)) && defined(_LIBCUDACXX_ABI_ALWAYS_USE_CXX11_NULLPTR)\n#    define nullptr __nullptr\n#  else\n#    define _LIBCUDACXX_HAS_NO_NULLPTR\n#  endif\n#endif\n\n#if !(__has_feature(cxx_rvalue_references))\n#define _LIBCUDACXX_HAS_NO_RVALUE_REFERENCES\n#endif\n\n#if !(__has_feature(cxx_auto_type))\n#define _LIBCUDACXX_HAS_NO_AUTO_TYPE\n#endif\n\n#if !(__has_feature(cxx_variadic_templates))\n#define _LIBCUDACXX_HAS_NO_VARIADICS\n#endif\n\n#if !(__has_feature(cxx_generalized_initializers))\n#define _LIBCUDACXX_HAS_NO_GENERALIZED_INITIALIZERS\n#endif\n\n// Objective-C++ features (opt-in)\n#if __has_feature(objc_arc)\n#define _LIBCUDACXX_HAS_OBJC_ARC\n#endif\n\n#if __has_feature(objc_arc_weak)\n#define _LIBCUDACXX_HAS_OBJC_ARC_WEAK\n#endif\n\n#if !(__has_feature(cxx_relaxed_constexpr))\n#define _LIBCUDACXX_HAS_NO_CXX14_CONSTEXPR\n#endif\n\n#if !(__has_feature(cxx_variable_templates))\n#define _LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES\n#endif\n\n#if !(__has_feature(cxx_noexcept))\n#define _LIBCUDACXX_HAS_NO_NOEXCEPT\n#endif\n\n// Allow for build-time disabling of unsigned integer sanitization\n#if !defined(_LIBCUDACXX_DISABLE_UBSAN_UNSIGNED_INTEGER_CHECK) && __has_attribute(no_sanitize)\n#define _LIBCUDACXX_DISABLE_UBSAN_UNSIGNED_INTEGER_CHECK __attribute__((__no_sanitize__(\"unsigned-integer-overflow\")))\n#endif\n\n#define _LIBCUDACXX_DISABLE_EXTENSION_WARNING __extension__\n\n#elif defined(_LIBCUDACXX_COMPILER_GCC)\n\n// FIXME: GCC 8.0 supports this trait, but it has a bug.\n// https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91592\n// https://godbolt.org/z/IljfIw\n#define _LIBCUDACXX_USE_IS_ASSIGNABLE_FALLBACK\n\n\n// Determine if GCC supports relaxed constexpr\n#if !defined(__cpp_constexpr) || __cpp_constexpr < 201304L\n#define _LIBCUDACXX_HAS_NO_CXX14_CONSTEXPR\n#endif\n\n// GCC 5 supports variable templates\n#if !defined(__cpp_variable_templates) || __cpp_variable_templates < 201304L\n#define _LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES\n#endif\n\n#if _GNUC_VER < 600\n#define _LIBCUDACXX_GCC_MATH_IN_STD\n#endif\n\n// NVCC cannot properly handle some deductions occuring within NOEXCEPT\n// C++17 mode causes reference instatiation errors in tuple\n#if (_GNUC_VER >= 702 && _GNUC_VER <= 805)\n#if defined(_LIBCUDACXX_COMPILER_NVCC) && _LIBCUDACXX_STD_VER == 17\n#define _LIBCUDACXX_NO_TUPLE_NOEXCEPT\n#endif\n#endif\n\n#define _LIBCUDACXX_DISABLE_EXTENSION_WARNING __extension__\n\n#elif defined(_LIBCUDACXX_COMPILER_MSVC)\n\n#define _LIBCUDACXX_WARNING(x) __pragma(message(__FILE__ \"(\" _LIBCUDACXX_TOSTRING(__LINE__) \") : warning note: \" x))\n\n// https://github.com/microsoft/STL/blob/master/stl/inc/yvals_core.h#L353\n// warning C4100: 'quack': unreferenced formal parameter\n// warning C4127: conditional expression is constant\n// warning C4180: qualifier applied to function type has no meaning; ignored\n// warning C4197: 'purr': top-level volatile in cast is ignored\n// warning C4324: 'roar': structure was padded due to alignment specifier\n// warning C4455: literal suffix identifiers that do not start with an underscore are reserved\n// warning C4503: 'hum': decorated name length exceeded, name was truncated\n// warning C4522: 'woof' : multiple assignment operators specified\n// warning C4668: 'meow' is not defined as a preprocessor macro, replacing with '0' for '#if/#elif'\n// warning C4800: 'boo': forcing value to bool 'true' or 'false' (performance warning)\n// warning C4996: 'meow': was declared deprecated\n#define _LIBCUDACXX_MSVC_DISABLED_WARNINGS \\\n  4100 \\\n  4127 \\\n  4180 \\\n  4197 \\\n  4296 \\\n  4324 \\\n  4455 \\\n  4503 \\\n  4522 \\\n  4668 \\\n  4800 \\\n  4996 \\\n  /**/\n\n#if _MSC_VER < 1900\n#error \"MSVC versions prior to Visual Studio 2015 are not supported\"\n#endif\n\n// MSVC implemented P0030R1 in 15.7, only available under C++17\n#if _MSC_VER < 1914\n#define _LIBCUDACXX_NO_HOST_CPP17_HYPOT\n#endif\n\n#if _MSC_VER < 1920\n#define _LIBCUDACXX_HAS_NO_NOEXCEPT_SFINAE\n#define _LIBCUDACXX_HAS_NO_LOGICAL_METAFUNCTION_ALIASES\n#endif\n\n// MSVC exposed __iso_volatile intrinsics beginning on 1924 for x86\n#if _MSC_VER < 1924\n    #define _LIBCUDACXX_MSVC_HAS_NO_ISO_INTRIN\n#endif\n\n#if _LIBCUDACXX_STD_VER < 14\n#define _LIBCUDACXX_HAS_NO_CXX14_CONSTEXPR\n#define _LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES\n#endif\n\n#define _LIBCUDACXX_WEAK\n\n#define _LIBCUDACXX_HAS_NO_VECTOR_EXTENSION\n\n#define _LIBCUDACXX_DISABLE_EXTENSION_WARNING\n\n#elif defined(_LIBCUDACXX_COMPILER_IBM)\n\n#define _ATTRIBUTE(x) __attribute__((x))\n\n#define _LIBCUDACXX_HAS_NO_UNICODE_CHARS\n#define _LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES\n\n#if defined(_AIX)\n#define __MULTILOCALE_API\n#endif\n\n#define _LIBCUDACXX_HAS_NO_VECTOR_EXTENSION\n\n#elif defined(_LIBCUDACXX_COMPILER_NVRTC) || defined(_LIBCUDACXX_COMPILER_NVHPC)\n\n#if !defined(__cpp_constexpr) || __cpp_constexpr < 201304L\n#define _LIBCUDACXX_HAS_NO_CXX14_CONSTEXPR\n#endif\n\n#if !defined(__cpp_variable_templates) || __cpp_variable_templates < 201304L\n#define _LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES\n#endif\n\n#define _LIBCUDACXX_DISABLE_EXTENSION_WARNING\n\n#endif // _LIBCUDACXX_COMPILER_[CLANG|GCC|MSVC|IBM|NVRTC]\n\n#if defined(_LIBCUDACXX_COMPILER_NVHPC) && !defined(__cuda_std__)\n// Forcefully disable visibility controls when used as the standard library with NVC++.\n// TODO: reevaluate.\n#define _LIBCUDACXX_HIDE_FROM_ABI\n#ifndef _LIBCUDACXX_DISABLE_EXTERN_TEMPLATE\n#define _LIBCUDACXX_DISABLE_EXTERN_TEMPLATE\n#endif\n#endif\n\n#ifndef _LIBCUDACXX_FREESTANDING\n#if defined(__cuda_std__)     \\\n || !defined(__STDC_HOSTED__)\n#  define _LIBCUDACXX_FREESTANDING\n#endif\n#endif // !_LIBCUDACXX_FREESTANDING\n\n#ifndef _LIBCUDACXX_DISABLE_VISIBILITY_ANNOTATIONS\n#if defined(_LIBCUDACXX_COMPILER_NVRTC) \\\n || (defined(_LIBCUDACXX_COMPILER_NVHPC) && !defined(__cuda_std__))\n#  define _LIBCUDACXX_DISABLE_VISIBILITY_ANNOTATIONS\n#endif\n#endif // _LIBCUDACXX_DISABLE_VISIBILITY_ANNOTATIONS\n\n#ifndef _LIBCUDACXX_HAS_CUDA_ATOMIC_EXT\n#if defined(__cuda_std__)\n#  define _LIBCUDACXX_HAS_CUDA_ATOMIC_EXT\n#endif\n#endif // _LIBCUDACXX_HAS_CUDA_ATOMIC_EXT\n\n#ifndef _LIBCUDACXX_HAS_EXTERNAL_ATOMIC_IMP\n#if defined(__cuda_std__)\n#  define _LIBCUDACXX_HAS_EXTERNAL_ATOMIC_IMP\n#endif\n#endif // _LIBCUDACXX_HAS_EXTERNAL_ATOMIC_IMP\n\n#ifndef _LIBCUDACXX_HAS_NO_ASAN\n#if defined(_LIBCUDACXX_COMPILER_GCC)\n#  if !defined(__SANITIZE_ADDRESS__)\n#    define _LIBCUDACXX_HAS_NO_ASAN\n#  endif // !__SANITIZE_ADDRESS__\n#elif defined(_LIBCUDACXX_COMPILER_CLANG)\n#  if !__has_feature(address_sanitizer)\n#    define _LIBCUDACXX_HAS_NO_ASAN\n#  endif // !__has_feature(address_sanitizer)\n#else\n#  define _LIBCUDACXX_HAS_NO_ASAN\n#endif // _LIBCUDACXX_COMPILER[MSVC|IBM|NVHPC|NVRTC]\n#endif // _LIBCUDACXX_HAS_NO_ASAN\n\n#ifndef _LIBCUDACXX_HAS_NO_CXX20_CHRONO_LITERALS\n#if defined(__cuda_std__) \\\n || (defined(_LIBCUDACXX_COMPILER_CLANG) && _LIBCUDACXX_CLANG_VER < 800)\n#  define _LIBCUDACXX_HAS_NO_CXX20_CHRONO_LITERALS\n#endif // __cuda_std__\n#endif // _LIBCUDACXX_HAS_NO_CXX20_CHRONO_LITERALS\n\n#ifndef _LIBCUDACXX_HAS_NO_INT128\n#if defined(_LIBCUDACXX_COMPILER_MSVC)                                          \\\n || (defined(_LIBCUDACXX_COMPILER_NVRTC) && !defined(__CUDACC_RTC_INT128__))    \\\n || (defined(_LIBCUDACXX_COMPILER_NVCC)  && (_LIBCUDACXX_CUDACC_VER < 1105000)) \\\n || !defined(__SIZEOF_INT128__)\n#  define _LIBCUDACXX_HAS_NO_INT128\n#endif\n#endif // !_LIBCUDACXX_HAS_NO_INT128\n\n#ifndef _LIBCUDACXX_HAS_NO_LONG_DOUBLE\n#if defined(_LIBCUDACXX_CUDACC)\n#  define _LIBCUDACXX_HAS_NO_LONG_DOUBLE\n#endif\n#endif // _LIBCUDACXX_HAS_NO_LONG_DOUBLE\n\n#ifndef _LIBCUDACXX_HAS_NO_ATTRIBUTE_NO_UNIQUE_ADDRESS\n#if __has_cpp_attribute(msvc::no_unique_address)\n// MSVC implements [[no_unique_address]] as a silent no-op currently.\n// (If/when MSVC breaks its C++ ABI, it will be changed to work as intended.)\n// However, MSVC implements [[msvc::no_unique_address]] which does what\n// [[no_unique_address]] is supposed to do, in general.\n\n// Clang-cl does not yet (14.0) implement either [[no_unique_address]] or\n// [[msvc::no_unique_address]] though. If/when it does implement\n// [[msvc::no_unique_address]], this should be preferred though.\n#  define _LIBCUDACXX_NO_UNIQUE_ADDRESS [[msvc::no_unique_address]]\n#elif defined(_LIBCUDACXX_CUDACC_BELOW_11_3) \\\n || (__has_cpp_attribute(no_unique_address) < 201803L)\n#  define _LIBCUDACXX_HAS_NO_ATTRIBUTE_NO_UNIQUE_ADDRESS\n#  define _LIBCUDACXX_NO_UNIQUE_ADDRESS\n#elif __has_cpp_attribute(no_unique_address)\n#  define _LIBCUDACXX_NO_UNIQUE_ADDRESS [[no_unique_address]]\n#else\n#  define _LIBCUDACXX_HAS_NO_ATTRIBUTE_NO_UNIQUE_ADDRESS\n#  define _LIBCUDACXX_NO_UNIQUE_ADDRESS\n// Note that this can be replaced by #error as soon as clang-cl\n// implements msvc::no_unique_address, since there should be no C++20\n// compiler that doesn't support one of the two attributes at that point.\n// We generally don't want to use this macro outside of C++20-only code,\n// because using it conditionally in one language version only would make\n// the ABI inconsistent.\n#endif\n#endif // _LIBCUDACXX_HAS_NO_ATTRIBUTE_NO_UNIQUE_ADDRESS\n\n#ifndef _LIBCUDACXX_HAS_NO_MONOTONIC_CLOCK\n#if defined(__cuda_std__)\n#  define _LIBCUDACXX_HAS_NO_MONOTONIC_CLOCK\n#endif\n#endif // _LIBCUDACXX_HAS_NO_MONOTONIC_CLOCK\n\n#ifndef _LIBCUDACXX_HAS_NO_PLATFORM_WAIT\n#if defined(__cuda_std__)\n#  define _LIBCUDACXX_HAS_NO_PLATFORM_WAIT\n#endif\n#endif // _LIBCUDACXX_HAS_NO_PLATFORM_WAIT\n\n#ifndef _LIBCUDACXX_HAS_NO_PRAGMA_PUSH_POP_MACRO\n#if (defined(_LIBCUDACXX_COMPILER_MSVC) && _MSC_VER < 1920) \\\n || defined(_LIBCUDACXX_COMPILER_NVRTC)                     \\\n || defined(_LIBCUDACXX_COMPILER_IBM)\n#define _LIBCUDACXX_HAS_NO_PRAGMA_PUSH_POP_MACRO\n#endif\n#endif // _LIBCUDACXX_HAS_NO_PRAGMA_PUSH_POP_MACRO\n\n#ifndef _LIBCUDACXX_HAS_NO_THREAD_CONTENTION_TABLE\n#if defined(__cuda_std__)\n#  define _LIBCUDACXX_HAS_NO_THREAD_CONTENTION_TABLE\n#endif\n#endif // _LIBCUDACXX_HAS_NO_THREAD_CONTENTION_TABLE\n\n#ifndef _LIBCUDACXX_HAS_NO_TREE_BARRIER\n#if defined(__cuda_std__)\n#  define _LIBCUDACXX_HAS_NO_TREE_BARRIER\n#endif\n#endif // _LIBCUDACXX_HAS_NO_TREE_BARRIER\n\n#ifndef _LIBCUDACXX_HAS_NO_WCHAR_H\n#if defined(__cuda_std__)\n#  define _LIBCUDACXX_HAS_NO_WCHAR_H\n#endif\n#endif // _LIBCUDACXX_HAS_NO_WCHAR_H\n\n#ifndef _LIBCUDACXX_NO_EXCEPTIONS\n#if defined(__cuda_std__)                                                     \\\n || (defined(_LIBCUDACXX_COMPILER_CLANG) && !(__has_feature(cxx_exceptions))) \\\n || (defined(_LIBCUDACXX_COMPILER_GCC) && !__EXCEPTIONS)\n#  define _LIBCUDACXX_NO_EXCEPTIONS\n#endif\n#endif // !_LIBCUDACXX_NO_EXCEPTIONS\n\n// Try to find out if RTTI is disabled.\n// g++ and cl.exe have RTTI on by default and define a macro when it is.\n#ifndef _LIBCUDACXX_NO_RTTI\n#if defined(__cuda_std__)                                               \\\n || (defined(_LIBCUDACXX_COMPILER_CLANG) && !(__has_feature(cxx_rtti))) \\\n || (defined(_LIBCUDACXX_COMPILER_GCC)   && !defined(__GXX_RTTI))       \\\n || (defined(_LIBCUDACXX_COMPILER_MSVC)  && !defined(_CPPRTTI))\n#  define _LIBCUDACXX_NO_RTTI\n#endif\n#endif // !_LIBCUDACXX_NO_RTTI\n\n#ifndef _LIBCUDACXX_NODEBUG_TYPE\n#if defined(__cuda_std__)\n#  define _LIBCUDACXX_NODEBUG_TYPE\n#elif __has_attribute(__nodebug__)                                         \\\n && (defined(_LIBCUDACXX_COMPILER_CLANG) && _LIBCUDACXX_CLANG_VER >= 1210)\n#  define _LIBCUDACXX_NODEBUG_TYPE __attribute__((nodebug))\n#else\n#  define _LIBCUDACXX_NODEBUG_TYPE\n#endif\n#endif // !_LIBCUDACXX_NODEBUG_TYPE\n\n#if defined(_LIBCUDACXX_COMPILER_MSVC)\n#  define _LIBCUDACXX_NORETURN __declspec(noreturn)\n#elif __has_feature(cxx_attributes)\n#  define _LIBCUDACXX_NORETURN [[noreturn]]\n#else\n#  define _LIBCUDACXX_NORETURN __attribute__ ((noreturn))\n#endif\n\n#if defined(_LIBCUDACXX_OBJECT_FORMAT_COFF)\n\n#ifdef _DLL\n#  define _LIBCUDACXX_CRT_FUNC __declspec(dllimport)\n#else\n#  define _LIBCUDACXX_CRT_FUNC\n#endif\n\n#if defined(_LIBCUDACXX_DISABLE_VISIBILITY_ANNOTATIONS)\n#  define _LIBCUDACXX_DLL_VIS\n#  define _LIBCUDACXX_EXTERN_TEMPLATE_TYPE_VIS\n#  define _LIBCUDACXX_CLASS_TEMPLATE_INSTANTIATION_VIS\n#  define _LIBCUDACXX_OVERRIDABLE_FUNC_VIS\n#  define _LIBCUDACXX_EXPORTED_FROM_ABI\n#elif defined(_LIBCUDACXX_BUILDING_LIBRARY)\n#  define _LIBCUDACXX_DLL_VIS __declspec(dllexport)\n#  if defined(__MINGW32__)\n#    define _LIBCUDACXX_EXTERN_TEMPLATE_TYPE_VIS _LIBCUDACXX_DLL_VIS\n#    define _LIBCUDACXX_CLASS_TEMPLATE_INSTANTIATION_VIS\n#  else\n#    define _LIBCUDACXX_EXTERN_TEMPLATE_TYPE_VIS\n#    define _LIBCUDACXX_CLASS_TEMPLATE_INSTANTIATION_VIS _LIBCUDACXX_DLL_VIS\n#  endif\n#  define _LIBCUDACXX_OVERRIDABLE_FUNC_VIS _LIBCUDACXX_DLL_VIS\n#  define _LIBCUDACXX_EXPORTED_FROM_ABI __declspec(dllexport)\n#else\n#  define _LIBCUDACXX_DLL_VIS __declspec(dllimport)\n#  define _LIBCUDACXX_EXTERN_TEMPLATE_TYPE_VIS _LIBCUDACXX_DLL_VIS\n#  define _LIBCUDACXX_CLASS_TEMPLATE_INSTANTIATION_VIS\n#  define _LIBCUDACXX_OVERRIDABLE_FUNC_VIS\n#  define _LIBCUDACXX_EXPORTED_FROM_ABI __declspec(dllimport)\n#endif\n\n#define _LIBCUDACXX_TYPE_VIS            _LIBCUDACXX_DLL_VIS\n#define _LIBCUDACXX_FUNC_VIS            _LIBCUDACXX_DLL_VIS\n#define _LIBCUDACXX_EXCEPTION_ABI       _LIBCUDACXX_DLL_VIS\n#define _LIBCUDACXX_HIDDEN\n#define _LIBCUDACXX_METHOD_TEMPLATE_IMPLICIT_INSTANTIATION_VIS\n#define _LIBCUDACXX_TEMPLATE_VIS\n#define _LIBCUDACXX_ENUM_VIS\n\n#endif // defined(_LIBCUDACXX_OBJECT_FORMAT_COFF)\n\n#ifndef _LIBCUDACXX_HIDDEN\n#  if !defined(_LIBCUDACXX_DISABLE_VISIBILITY_ANNOTATIONS)\n#    define _LIBCUDACXX_HIDDEN __attribute__ ((__visibility__(\"hidden\")))\n#  else\n#    define _LIBCUDACXX_HIDDEN\n#  endif\n#endif\n\n#ifndef _LIBCUDACXX_METHOD_TEMPLATE_IMPLICIT_INSTANTIATION_VIS\n#  if !defined(_LIBCUDACXX_DISABLE_VISIBILITY_ANNOTATIONS)\n// The inline should be removed once PR32114 is resolved\n#    define _LIBCUDACXX_METHOD_TEMPLATE_IMPLICIT_INSTANTIATION_VIS inline _LIBCUDACXX_HIDDEN\n#  else\n#    define _LIBCUDACXX_METHOD_TEMPLATE_IMPLICIT_INSTANTIATION_VIS\n#  endif\n#endif\n\n#ifndef _LIBCUDACXX_FUNC_VIS\n#  if !defined(_LIBCUDACXX_DISABLE_VISIBILITY_ANNOTATIONS)\n#    define _LIBCUDACXX_FUNC_VIS __attribute__ ((__visibility__(\"default\")))\n#  else\n#    define _LIBCUDACXX_FUNC_VIS\n#  endif\n#endif\n\n#ifndef _LIBCUDACXX_TYPE_VIS\n#  if !defined(_LIBCUDACXX_DISABLE_VISIBILITY_ANNOTATIONS)\n#    define _LIBCUDACXX_TYPE_VIS __attribute__ ((__visibility__(\"default\")))\n#  else\n#    define _LIBCUDACXX_TYPE_VIS\n#  endif\n#endif\n\n#ifndef _LIBCUDACXX_TEMPLATE_VIS\n#  if !defined(_LIBCUDACXX_DISABLE_VISIBILITY_ANNOTATIONS)\n#    if __has_attribute(__type_visibility__)\n#      define _LIBCUDACXX_TEMPLATE_VIS __attribute__ ((__type_visibility__(\"default\")))\n#    else\n#      define _LIBCUDACXX_TEMPLATE_VIS __attribute__ ((__visibility__(\"default\")))\n#    endif\n#  else\n#    define _LIBCUDACXX_TEMPLATE_VIS\n#  endif\n#endif\n\n#ifndef _LIBCUDACXX_EXPORTED_FROM_ABI\n#  if !defined(_LIBCUDACXX_DISABLE_VISIBILITY_ANNOTATIONS)\n#    define _LIBCUDACXX_EXPORTED_FROM_ABI __attribute__((__visibility__(\"default\")))\n#  else\n#    define _LIBCUDACXX_EXPORTED_FROM_ABI\n#  endif\n#endif\n\n#ifndef _LIBCUDACXX_OVERRIDABLE_FUNC_VIS\n#define _LIBCUDACXX_OVERRIDABLE_FUNC_VIS _LIBCUDACXX_FUNC_VIS\n#endif\n\n#ifndef _LIBCUDACXX_EXCEPTION_ABI\n#  if !defined(_LIBCUDACXX_DISABLE_VISIBILITY_ANNOTATIONS)\n#    define _LIBCUDACXX_EXCEPTION_ABI __attribute__ ((__visibility__(\"default\")))\n#  else\n#    define _LIBCUDACXX_EXCEPTION_ABI\n#  endif\n#endif\n\n#ifndef _LIBCUDACXX_ENUM_VIS\n#  if !defined(_LIBCUDACXX_DISABLE_VISIBILITY_ANNOTATIONS) && __has_attribute(__type_visibility__)\n#    define _LIBCUDACXX_ENUM_VIS __attribute__ ((__type_visibility__(\"default\")))\n#  else\n#    define _LIBCUDACXX_ENUM_VIS\n#  endif\n#endif\n\n#ifndef _LIBCUDACXX_EXTERN_TEMPLATE_TYPE_VIS\n#  if !defined(_LIBCUDACXX_DISABLE_VISIBILITY_ANNOTATIONS) && __has_attribute(__type_visibility__)\n#    define _LIBCUDACXX_EXTERN_TEMPLATE_TYPE_VIS __attribute__ ((__visibility__(\"default\")))\n#  else\n#    define _LIBCUDACXX_EXTERN_TEMPLATE_TYPE_VIS\n#  endif\n#endif\n\n#ifndef _LIBCUDACXX_CLASS_TEMPLATE_INSTANTIATION_VIS\n#define _LIBCUDACXX_CLASS_TEMPLATE_INSTANTIATION_VIS\n#endif\n\n#if __has_attribute(internal_linkage)\n#  define _LIBCUDACXX_INTERNAL_LINKAGE __attribute__ ((internal_linkage))\n#else\n#  define _LIBCUDACXX_INTERNAL_LINKAGE _LIBCUDACXX_ALWAYS_INLINE\n#endif\n\n#if __has_attribute(exclude_from_explicit_instantiation)\n#  define _LIBCUDACXX_EXCLUDE_FROM_EXPLICIT_INSTANTIATION __attribute__ ((__exclude_from_explicit_instantiation__))\n#else\n   // Try to approximate the effect of exclude_from_explicit_instantiation\n   // (which is that entities are not assumed to be provided by explicit\n   // template instantiations in the dylib) by always inlining those entities.\n#  define _LIBCUDACXX_EXCLUDE_FROM_EXPLICIT_INSTANTIATION _LIBCUDACXX_ALWAYS_INLINE\n#endif\n\n#ifndef _LIBCUDACXX_HIDE_FROM_ABI_PER_TU\n#  ifndef _LIBCUDACXX_HIDE_FROM_ABI_PER_TU_BY_DEFAULT\n#    define _LIBCUDACXX_HIDE_FROM_ABI_PER_TU 0\n#  else\n#    define _LIBCUDACXX_HIDE_FROM_ABI_PER_TU 1\n#  endif\n#endif\n\n#ifndef _LIBCUDACXX_HAS_MERGED_TYPEINFO_NAMES_DEFAULT\n# ifdef _LIBCUDACXX_OBJECT_FORMAT_COFF // Windows binaries can't merge typeinfos.\n# define _LIBCUDACXX_HAS_MERGED_TYPEINFO_NAMES_DEFAULT 0\n#else\n// TODO: This isn't strictly correct on ELF platforms due to llvm.org/PR37398\n// And we should consider defaulting to OFF.\n# define _LIBCUDACXX_HAS_MERGED_TYPEINFO_NAMES_DEFAULT 1\n#endif\n#endif\n\n#ifndef _LIBCUDACXX_HIDE_FROM_ABI\n#  if _LIBCUDACXX_HIDE_FROM_ABI_PER_TU\n#    define _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_HIDDEN _LIBCUDACXX_INTERNAL_LINKAGE\n#  else\n#    define _LIBCUDACXX_HIDE_FROM_ABI _LIBCUDACXX_HIDDEN _LIBCUDACXX_EXCLUDE_FROM_EXPLICIT_INSTANTIATION\n#  endif\n#endif\n\n#ifdef _LIBCUDACXX_BUILDING_LIBRARY\n#  if _LIBCUDACXX_ABI_VERSION > 1\n#    define _LIBCUDACXX_HIDE_FROM_ABI_AFTER_V1 _LIBCUDACXX_HIDE_FROM_ABI\n#  else\n#    define _LIBCUDACXX_HIDE_FROM_ABI_AFTER_V1\n#  endif\n#else\n#  define _LIBCUDACXX_HIDE_FROM_ABI_AFTER_V1 _LIBCUDACXX_HIDE_FROM_ABI\n#endif\n\n#ifdef _LIBCUDACXX_CUDACC\n#  define _LIBCUDACXX_HOST __host__\n#  define _LIBCUDACXX_DEVICE __device__\n#  define _LIBCUDACXX_HOST_DEVICE __host__ __device__\n#  define _LIBCUDACXX_FORCE_INLINE __forceinline__\n#else // ^^^ _LIBCUDACXX_CUDACC ^^^ / vvv !_LIBCUDACXX_CUDACC\n#  define _LIBCUDACXX_HOST\n#  define _LIBCUDACXX_DEVICE\n#  define _LIBCUDACXX_HOST_DEVICE\n#  define _LIBCUDACXX_FORCE_INLINE\n#endif // !_LIBCUDACXX_CUDACC\n\n// Just so we can migrate to the new macros gradually.\n\n#ifdef __cuda_std__\n#  define _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_HOST_DEVICE\n#else\n#  define _LIBCUDACXX_INLINE_VISIBILITY _LIBCUDACXX_HIDE_FROM_ABI\n#endif // __cuda_std__\n\n#define _LIBCUDACXX_CONCAT1(_LIBCUDACXX_X,_LIBCUDACXX_Y) _LIBCUDACXX_X##_LIBCUDACXX_Y\n#define _LIBCUDACXX_CONCAT(_LIBCUDACXX_X,_LIBCUDACXX_Y) _LIBCUDACXX_CONCAT1(_LIBCUDACXX_X,_LIBCUDACXX_Y)\n\n#ifndef _LIBCUDACXX_ABI_NAMESPACE\n#ifdef __cuda_std__\n# define _LIBCUDACXX_ABI_NAMESPACE _LIBCUDACXX_CONCAT(__,_LIBCUDACXX_CUDA_ABI_VERSION)\n#else\n# define _LIBCUDACXX_ABI_NAMESPACE _LIBCUDACXX_CONCAT(__,_LIBCUDACXX_ABI_VERSION)\n#endif // __cuda_std__\n#endif // _LIBCUDACXX_ABI_NAMESPACE\n\n#ifdef __cuda_std__\n#  define _LIBCUDACXX_BEGIN_NAMESPACE_STD_NOVERSION namespace cuda { namespace std {\n#  define _LIBCUDACXX_END_NAMESPACE_STD_NOVERSION } }\n#  define _CUDA_VSTD ::cuda::std::_LIBCUDACXX_ABI_NAMESPACE\n#  define _CUDA_VRANGES ::cuda::std::ranges::_LIBCUDACXX_ABI_NAMESPACE\n#else\n#  define _LIBCUDACXX_BEGIN_NAMESPACE_STD_NOVERSION namespace std {\n#  define _LIBCUDACXX_END_NAMESPACE_STD_NOVERSION }\n#  define _CUDA_VSTD ::std::_LIBCUDACXX_ABI_NAMESPACE\n#  define _CUDA_VRANGES ::std::ranges::_LIBCUDACXX_ABI_NAMESPACE\n#endif\n\n#ifdef __cuda_std__\n#define _LIBCUDACXX_BEGIN_NAMESPACE_CUDA namespace cuda { inline namespace _LIBCUDACXX_ABI_NAMESPACE {\n#define _LIBCUDACXX_END_NAMESPACE_CUDA  } }\n#define _LIBCUDACXX_BEGIN_NAMESPACE_CUDA_DEVICE namespace cuda { namespace device { inline namespace _LIBCUDACXX_ABI_NAMESPACE {\n#define _LIBCUDACXX_END_NAMESPACE_CUDA_DEVICE  } } }\n#endif\n\n// Inline namespaces are available in Clang/GCC/MSVC regardless of C++ dialect.\n#define _LIBCUDACXX_BEGIN_NAMESPACE_STD _LIBCUDACXX_BEGIN_NAMESPACE_STD_NOVERSION inline namespace _LIBCUDACXX_ABI_NAMESPACE {\n#define _LIBCUDACXX_END_NAMESPACE_STD   } _LIBCUDACXX_END_NAMESPACE_STD_NOVERSION\n\n#ifndef __cuda_std__\n_LIBCUDACXX_BEGIN_NAMESPACE_STD _LIBCUDACXX_END_NAMESPACE_STD\n#endif\n\n#define _LIBCUDACXX_BEGIN_NAMESPACE_RANGES _LIBCUDACXX_BEGIN_NAMESPACE_STD_NOVERSION namespace ranges { inline namespace _LIBCUDACXX_ABI_NAMESPACE {\n#define _LIBCUDACXX_END_NAMESPACE_RANGES   } } _LIBCUDACXX_END_NAMESPACE_STD_NOVERSION\n\n#if !defined(__cuda_std__)\n_LIBCUDACXX_BEGIN_NAMESPACE_RANGES _LIBCUDACXX_END_NAMESPACE_RANGES\n#endif\n\n#if _LIBCUDACXX_STD_VER > 17\n#define _LIBCUDACXX_BEGIN_NAMESPACE_RANGES_ABI inline namespace __cxx20 {\n#else\n#define _LIBCUDACXX_BEGIN_NAMESPACE_RANGES_ABI inline namespace __cxx17 {\n#endif\n#define _LIBCUDACXX_END_NAMESPACE_RANGES_ABI }\n\n#define _LIBCUDACXX_BEGIN_NAMESPACE_CPO(_CPO) namespace _CPO { _LIBCUDACXX_BEGIN_NAMESPACE_RANGES_ABI\n#define _LIBCUDACXX_END_NAMESPACE_CPO } }\n\n#if _LIBCUDACXX_STD_VER >= 17\n#define _LIBCUDACXX_BEGIN_NAMESPACE_FILESYSTEM \\\n  _LIBCUDACXX_BEGIN_NAMESPACE_STD inline namespace __fs { namespace filesystem {\n#else\n#define _LIBCUDACXX_BEGIN_NAMESPACE_FILESYSTEM \\\n  _LIBCUDACXX_BEGIN_NAMESPACE_STD namespace __fs { namespace filesystem {\n#endif\n\n#define _LIBCUDACXX_END_NAMESPACE_FILESYSTEM \\\n  _LIBCUDACXX_END_NAMESPACE_STD } }\n\n#define _CUDA_VSTD_FS _CUDA_VSTD::__fs::filesystem\n\n#ifndef _LIBCUDACXX_PREFERRED_OVERLOAD\n#  if __has_attribute(__enable_if__)\n#    define _LIBCUDACXX_PREFERRED_OVERLOAD __attribute__ ((__enable_if__(true, \"\")))\n#  endif\n#endif\n\n#ifdef _LIBCUDACXX_HAS_NO_UNICODE_CHARS\ntypedef unsigned short char16_t;\ntypedef unsigned int   char32_t;\n#endif  // _LIBCUDACXX_HAS_NO_UNICODE_CHARS\n\n#if defined(_LIBCUDACXX_COMPILER_GCC)   \\\n || defined(_LIBCUDACXX_COMPILER_CLANG)\n#  define _LIBCUDACXX_NOALIAS __attribute__((__malloc__))\n#else\n#  define _LIBCUDACXX_NOALIAS\n#endif\n\n#if __has_feature(cxx_explicit_conversions) \\\n || defined(_LIBCUDACXX_COMPILER_IBM)       \\\n || defined(_LIBCUDACXX_COMPILER_GCC)       \\\n || defined(_LIBCUDACXX_COMPILER_CLANG)\n#  define _LIBCUDACXX_EXPLICIT explicit\n#else\n#  define _LIBCUDACXX_EXPLICIT\n#endif\n\n#if !__has_builtin(__builtin_operator_new) || !__has_builtin(__builtin_operator_delete)\n#define _LIBCUDACXX_HAS_NO_BUILTIN_OPERATOR_NEW_DELETE\n#endif\n\n#ifdef _LIBCUDACXX_HAS_NO_STRONG_ENUMS\n#  define _LIBCUDACXX_DECLARE_STRONG_ENUM(x) struct _LIBCUDACXX_TYPE_VIS x { enum __lx\n#  define _LIBCUDACXX_DECLARE_STRONG_ENUM_EPILOG(x) \\\n     __lx __v_; \\\n     _LIBCUDACXX_INLINE_VISIBILITY x(__lx __v) : __v_(__v) {} \\\n     _LIBCUDACXX_INLINE_VISIBILITY explicit x(int __v) : __v_(static_cast<__lx>(__v)) {} \\\n     _LIBCUDACXX_INLINE_VISIBILITY operator int() const {return __v_;} \\\n     };\n#else  // _LIBCUDACXX_HAS_NO_STRONG_ENUMS\n#  define _LIBCUDACXX_DECLARE_STRONG_ENUM(x) enum class _LIBCUDACXX_ENUM_VIS x\n#  define _LIBCUDACXX_DECLARE_STRONG_ENUM_EPILOG(x)\n#endif  // _LIBCUDACXX_HAS_NO_STRONG_ENUMS\n\n#ifdef _LIBCUDACXX_DEBUG\n#  if _LIBCUDACXX_DEBUG == 0\n#    define _LIBCUDACXX_DEBUG_LEVEL 1\n#  elif _LIBCUDACXX_DEBUG == 1\n#    define _LIBCUDACXX_DEBUG_LEVEL 2\n#  else\n#    error Supported values for _LIBCUDACXX_DEBUG are 0 and 1\n#  endif\n#  if !defined(_LIBCUDACXX_BUILDING_LIBRARY)\n#    define _LIBCUDACXX_EXTERN_TEMPLATE(...)\n#  endif\n#endif\n\n#ifdef _LIBCUDACXX_DISABLE_EXTERN_TEMPLATE\n#define _LIBCUDACXX_EXTERN_TEMPLATE(...)\n#define _LIBCUDACXX_EXTERN_TEMPLATE2(...)\n#endif\n\n#ifndef _LIBCUDACXX_EXTERN_TEMPLATE\n#define _LIBCUDACXX_EXTERN_TEMPLATE(...) extern template __VA_ARGS__;\n#endif\n\n#ifndef _LIBCUDACXX_EXTERN_TEMPLATE2\n#define _LIBCUDACXX_EXTERN_TEMPLATE2(...) extern template __VA_ARGS__;\n#endif\n\n#if defined(__APPLE__) || defined(__FreeBSD__) || defined(_LIBCUDACXX_MSVCRT_LIKE) || \\\n    defined(__sun__) || defined(__NetBSD__) || defined(__CloudABI__)\n#define _LIBCUDACXX_LOCALE__L_EXTENSIONS 1\n#endif\n\n#if defined(__unix__) || (defined(__APPLE__) && defined(__MACH__))\n// Most unix variants have catopen.  These are the specific ones that don't.\n#  if !defined(__BIONIC__) && !defined(_NEWLIB_VERSION)\n#    define _LIBCUDACXX_HAS_CATOPEN 1\n#  endif\n#endif\n\n#ifdef __FreeBSD__\n#define _DECLARE_C99_LDBL_MATH 1\n#endif\n\n#if defined(_LIBCUDACXX_ABI_MICROSOFT) && !defined(_LIBCUDACXX_NO_VCRUNTIME)\n#  define _LIBCUDACXX_DEFER_NEW_TO_VCRUNTIME\n#endif\n\n// If we are getting operator new from the MSVC CRT, then allocation overloads\n// for align_val_t were added in 19.12, aka VS 2017 version 15.3.\n#if defined(_LIBCUDACXX_MSVCRT) && defined(_LIBCUDACXX_COMPILER_MSVC) && _MSC_VER < 1912\n#  define _LIBCUDACXX_HAS_NO_LIBRARY_ALIGNED_ALLOCATION\n#elif defined(_LIBCUDACXX_ABI_VCRUNTIME) && !defined(__cpp_aligned_new)\n   // We're deferring to Microsoft's STL to provide aligned new et al. We don't\n   // have it unless the language feature test macro is defined.\n#  define _LIBCUDACXX_HAS_NO_LIBRARY_ALIGNED_ALLOCATION\n#endif\n\n#if defined(__APPLE__)\n#  if !defined(__MAC_OS_X_VERSION_MIN_REQUIRED) && \\\n      defined(__ENVIRONMENT_MAC_OS_X_VERSION_MIN_REQUIRED__)\n#    define __MAC_OS_X_VERSION_MIN_REQUIRED __ENVIRONMENT_MAC_OS_X_VERSION_MIN_REQUIRED__\n#  endif\n#endif // defined(__APPLE__)\n\n#if !defined(_LIBCUDACXX_HAS_NO_ALIGNED_ALLOCATION) && \\\n    (defined(_LIBCUDACXX_HAS_NO_LIBRARY_ALIGNED_ALLOCATION) || \\\n    (!defined(__cpp_aligned_new) || __cpp_aligned_new < 201606))\n#  define _LIBCUDACXX_HAS_NO_ALIGNED_ALLOCATION\n#endif\n\n#if defined(__APPLE__) || defined(__FreeBSD__)\n#define _LIBCUDACXX_HAS_DEFAULTRUNELOCALE\n#endif\n\n#if defined(__APPLE__) || defined(__FreeBSD__) || defined(__sun__)\n#define _LIBCUDACXX_WCTYPE_IS_MASK\n#endif\n\n#if _LIBCUDACXX_STD_VER <= 17 || !defined(__cpp_char8_t)\n#define _LIBCUDACXX_NO_HAS_CHAR8_T\n#endif\n\n// Deprecation macros.\n//\n// Deprecations warnings are always enabled, except when users explicitly opt-out\n// by defining _LIBCUDACXX_DISABLE_DEPRECATION_WARNINGS.\n// NVCC 11.1 and 11.2 are broken with the deprecated attribute, so disable it\n#if !defined(_LIBCUDACXX_DISABLE_DEPRECATION_WARNINGS) \\\n && !defined(_LIBCUDACXX_CUDACC_BELOW_11_3)\n#  if __has_attribute(deprecated)\n#    define _LIBCUDACXX_DEPRECATED __attribute__ ((deprecated))\n#  elif _LIBCUDACXX_STD_VER > 11\n#    define _LIBCUDACXX_DEPRECATED [[deprecated]]\n#  else\n#    define _LIBCUDACXX_DEPRECATED\n#  endif\n#else\n#  define _LIBCUDACXX_DEPRECATED\n#endif\n\n#define _LIBCUDACXX_DEPRECATED_IN_CXX11 _LIBCUDACXX_DEPRECATED\n\n#if _LIBCUDACXX_STD_VER >= 14\n#  define _LIBCUDACXX_DEPRECATED_IN_CXX14 _LIBCUDACXX_DEPRECATED\n#else\n#  define _LIBCUDACXX_DEPRECATED_IN_CXX14\n#endif\n\n#if _LIBCUDACXX_STD_VER >= 17\n#  define _LIBCUDACXX_DEPRECATED_IN_CXX17 _LIBCUDACXX_DEPRECATED\n#else\n#  define _LIBCUDACXX_DEPRECATED_IN_CXX17\n#endif\n\n#if _LIBCUDACXX_STD_VER <= 11\n#  define _LIBCUDACXX_EXPLICIT_AFTER_CXX11\n#else\n#  define _LIBCUDACXX_EXPLICIT_AFTER_CXX11 explicit\n#endif\n\n#if _LIBCUDACXX_STD_VER > 11 && !defined(_LIBCUDACXX_HAS_NO_CXX14_CONSTEXPR)\n#  define _LIBCUDACXX_CONSTEXPR_AFTER_CXX11 constexpr\n#else\n#  define _LIBCUDACXX_CONSTEXPR_AFTER_CXX11\n#endif\n\n#if _LIBCUDACXX_STD_VER > 14 && !defined(_LIBCUDACXX_HAS_NO_CXX14_CONSTEXPR)\n#  define _LIBCUDACXX_CONSTEXPR_AFTER_CXX14 constexpr\n#else\n#  define _LIBCUDACXX_CONSTEXPR_AFTER_CXX14\n#endif\n\n#if _LIBCUDACXX_STD_VER > 17 && !defined(_LIBCUDACXX_HAS_NO_CXX14_CONSTEXPR)\n#  define _LIBCUDACXX_CONSTEXPR_AFTER_CXX17 constexpr\n#else\n#  define _LIBCUDACXX_CONSTEXPR_AFTER_CXX17\n#endif\n\n// Macros to enter and leave a state where deprecation warnings are suppressed.\n#if defined(_LIBCUDACXX_COMPILER_CLANG) || defined(_LIBCUDACXX_COMPILER_GCC)\n#   define _LIBCUDACXX_SUPPRESS_DEPRECATED_PUSH \\\n        _Pragma(\"GCC diagnostic push\") \\\n        _Pragma(\"GCC diagnostic ignored \\\"-Wdeprecated\\\"\") \\\n        _Pragma(\"GCC diagnostic ignored \\\"-Wdeprecated-declarations\\\"\")\n#   define _LIBCUDACXX_SUPPRESS_DEPRECATED_POP \\\n        _Pragma(\"GCC diagnostic pop\")\n#else\n#   define _LIBCUDACXX_SUPPRESS_DEPRECATED_PUSH\n#   define _LIBCUDACXX_SUPPRESS_DEPRECATED_POP\n#endif\n\n// The _LIBCUDACXX_NODISCARD_ATTRIBUTE should only be used to define other\n// NODISCARD macros to the correct attribute.\n#if __has_cpp_attribute(nodiscard) || (defined(_LIBCUDACXX_COMPILER_MSVC) && _LIBCUDACXX_STD_VER > 14)\n#  define _LIBCUDACXX_NODISCARD_ATTRIBUTE [[nodiscard]]\n#elif defined(_LIBCUDACXX_COMPILER_CLANG)\n#  define _LIBCUDACXX_NODISCARD_ATTRIBUTE [[clang::warn_unused_result]]\n#else\n// We can't use GCC's [[gnu::warn_unused_result]] and\n// __attribute__((warn_unused_result)), because GCC does not silence them via\n// (void) cast.\n#  define _LIBCUDACXX_NODISCARD_ATTRIBUTE\n#endif\n\n#  ifdef _LIBCUDACXX_COMPILER_CLANG\n#    define _LIBCUDACXX_DIAGNOSTIC_PUSH _Pragma(\"clang diagnostic push\")\n#    define _LIBCUDACXX_DIAGNOSTIC_POP _Pragma(\"clang diagnostic pop\")\n#    define _LIBCUDACXX_CLANG_DIAGNOSTIC_IGNORED(str) _Pragma(_LIBCUDACXX_TOSTRING(clang diagnostic ignored str))\n#    define _LIBCUDACXX_GCC_DIAGNOSTIC_IGNORED(str)\n#  elif defined(_LIBCUDACXX_COMPILER_GCC)\n#    define _LIBCUDACXX_DIAGNOSTIC_PUSH _Pragma(\"GCC diagnostic push\")\n#    define _LIBCUDACXX_DIAGNOSTIC_POP _Pragma(\"GCC diagnostic pop\")\n#    define _LIBCUDACXX_CLANG_DIAGNOSTIC_IGNORED(str)\n#    define _LIBCUDACXX_GCC_DIAGNOSTIC_IGNORED(str) _Pragma(_LIBCUDACXX_TOSTRING(GCC diagnostic ignored str))\n#  else\n#    define _LIBCUDACXX_DIAGNOSTIC_PUSH\n#    define _LIBCUDACXX_DIAGNOSTIC_POP\n#    define _LIBCUDACXX_CLANG_DIAGNOSTIC_IGNORED(str)\n#    define _LIBCUDACXX_GCC_DIAGNOSTIC_IGNORED(str)\n#  endif\n\n// _LIBCUDACXX_NODISCARD_EXT may be used to apply [[nodiscard]] to entities not\n// specified as such as an extension.\n#if defined(_LIBCUDACXX_ENABLE_NODISCARD) && !defined(_LIBCUDACXX_DISABLE_NODISCARD_EXT)\n#  define _LIBCUDACXX_NODISCARD_EXT _LIBCUDACXX_NODISCARD_ATTRIBUTE\n#else\n#  define _LIBCUDACXX_NODISCARD_EXT\n#endif\n\n#if !defined(_LIBCUDACXX_DISABLE_NODISCARD_AFTER_CXX17) && \\\n    (_LIBCUDACXX_STD_VER > 17 || defined(_LIBCUDACXX_ENABLE_NODISCARD))\n#  define _LIBCUDACXX_NODISCARD_AFTER_CXX17 _LIBCUDACXX_NODISCARD_ATTRIBUTE\n#else\n#  define _LIBCUDACXX_NODISCARD_AFTER_CXX17\n#endif\n\n#if _LIBCUDACXX_STD_VER > 14 && defined(__cpp_inline_variables) && (__cpp_inline_variables >= 201606L)\n#  define _LIBCUDACXX_INLINE_VAR inline\n#else\n#  define _LIBCUDACXX_INLINE_VAR\n#endif\n\n#ifdef _LIBCUDACXX_HAS_NO_RVALUE_REFERENCES\n#  define _LIBCUDACXX_EXPLICIT_MOVE(x) _CUDA_VSTD::move(x)\n#else\n#  define _LIBCUDACXX_EXPLICIT_MOVE(x) (x)\n#endif\n\n#ifndef _LIBCUDACXX_CONSTEXPR_IF_NODEBUG\n#if defined(_LIBCUDACXX_DEBUG) || defined(_LIBCUDACXX_HAS_NO_CXX14_CONSTEXPR)\n#define _LIBCUDACXX_CONSTEXPR_IF_NODEBUG\n#else\n#define _LIBCUDACXX_CONSTEXPR_IF_NODEBUG constexpr\n#endif\n#endif\n\n#if __has_attribute(no_destroy)\n#  define _LIBCUDACXX_NO_DESTROY __attribute__((__no_destroy__))\n#else\n#  define _LIBCUDACXX_NO_DESTROY\n#endif\n\n#ifndef _LIBCUDACXX_HAS_NO_ASAN\nextern \"C\" _LIBCUDACXX_FUNC_VIS void __sanitizer_annotate_contiguous_container(\n  const void *, const void *, const void *, const void *);\n#endif\n\n#ifndef _LIBCUDACXX_WEAK\n#define _LIBCUDACXX_WEAK __attribute__((__weak__))\n#endif\n\n// Redefine some macros for internal use\n#if defined(__cuda_std__)\n#  undef _LIBCUDACXX_FUNC_VIS\n#  define _LIBCUDACXX_FUNC_VIS _LIBCUDACXX_INLINE_VISIBILITY\n#  undef _LIBCUDACXX_TYPE_VIS\n#  define _LIBCUDACXX_TYPE_VIS\n#endif // __cuda_std__\n\n// Thread API\n#ifndef _LIBCUDACXX_HAS_THREAD_API_EXTERNAL\n#if defined(_LIBCUDACXX_COMPILER_NVRTC) \\\n || defined(__EMSCRIPTEN__)\n#  define _LIBCUDACXX_HAS_THREAD_API_EXTERNAL\n#endif\n#endif // _LIBCUDACXX_HAS_THREAD_API_EXTERNAL\n\n#ifndef _LIBCUDACXX_HAS_THREAD_API_CUDA\n#if defined(__cuda_std__)                               \\\n && (defined(__CUDA_ARCH__) || defined(__EMSCRIPTEN__))\n#  define _LIBCUDACXX_HAS_THREAD_API_CUDA\n#endif // __cuda_std__\n#endif // _LIBCUDACXX_HAS_THREAD_API_CUDA\n\n#ifndef _LIBCUDACXX_HAS_THREAD_API_WIN32\n#if defined(_LIBCUDACXX_COMPILER_MSVC)        \\\n && !defined(_LIBCUDACXX_HAS_THREAD_API_CUDA)\n#  define _LIBCUDACXX_HAS_THREAD_API_WIN32\n#endif\n#endif // _LIBCUDACXX_HAS_THREAD_API_WIN32\n\n#if !defined(_LIBCUDACXX_HAS_NO_THREADS)          \\\n && !defined(_LIBCUDACXX_HAS_THREAD_API_PTHREAD)  \\\n && !defined(_LIBCUDACXX_HAS_THREAD_API_WIN32)    \\\n && !defined(_LIBCUDACXX_HAS_THREAD_API_EXTERNAL)\n#  if defined(__FreeBSD__) || \\\n      defined(__Fuchsia__) || \\\n      defined(__wasi__) || \\\n      defined(__NetBSD__) || \\\n      defined(__linux__) || \\\n      defined(__GNU__) || \\\n      defined(__APPLE__) || \\\n      defined(__CloudABI__) || \\\n      defined(__sun__) || \\\n      (defined(__MINGW32__) && __has_include(<pthread.h>))\n#    define _LIBCUDACXX_HAS_THREAD_API_PTHREAD\n#  elif defined(_LIBCUDACXX_WIN32API)\n#    define _LIBCUDACXX_HAS_THREAD_API_WIN32\n#  else\n#    define _LIBCUDACXX_UNSUPPORTED_THREAD_API\n#  endif // _LIBCUDACXX_HAS_THREAD_API\n#endif // _LIBCUDACXX_HAS_NO_THREADS\n\n#if defined(_LIBCUDACXX_HAS_THREAD_API_PTHREAD)\n#if defined(__ANDROID__) && __ANDROID_API__ >= 30\n#define _LIBCUDACXX_HAS_COND_CLOCKWAIT\n#elif defined(_LIBCUDACXX_GLIBC_PREREQ)\n#if _LIBCUDACXX_GLIBC_PREREQ(2, 30)\n#define _LIBCUDACXX_HAS_COND_CLOCKWAIT\n#endif\n#endif\n#endif\n\n#if defined(_LIBCUDACXX_HAS_NO_THREADS) && defined(_LIBCUDACXX_HAS_THREAD_API_PTHREAD)\n#error _LIBCUDACXX_HAS_THREAD_API_PTHREAD may only be defined when \\\n       _LIBCUDACXX_HAS_NO_THREADS is not defined.\n#endif\n\n#if defined(_LIBCUDACXX_HAS_NO_THREADS) && defined(_LIBCUDACXX_HAS_THREAD_API_EXTERNAL)\n#error _LIBCUDACXX_HAS_THREAD_API_EXTERNAL may not be defined when \\\n       _LIBCUDACXX_HAS_NO_THREADS is defined.\n#endif\n\n#if defined(__STDCPP_THREADS__) && defined(_LIBCUDACXX_HAS_NO_THREADS)\n#error _LIBCUDACXX_HAS_NO_THREADS cannot be set when __STDCPP_THREADS__ is set.\n#endif\n\n#if !defined(_LIBCUDACXX_HAS_NO_THREADS) && !defined(__STDCPP_THREADS__)\n#define __STDCPP_THREADS__ 1\n#endif\n\n// The glibc and Bionic implementation of pthreads implements\n// pthread_mutex_destroy as nop for regular mutexes. Additionally, Win32\n// mutexes have no destroy mechanism.\n//\n// This optimization can't be performed on Apple platforms, where\n// pthread_mutex_destroy can allow the kernel to release resources.\n// See https://llvm.org/D64298 for details.\n//\n// TODO(EricWF): Enable this optimization on Bionic after speaking to their\n//               respective stakeholders.\n#if (defined(_LIBCUDACXX_HAS_THREAD_API_PTHREAD) && defined(__GLIBC__)) \\\n  || defined(_LIBCUDACXX_HAS_THREAD_API_WIN32)\n# define _LIBCUDACXX_HAS_TRIVIAL_MUTEX_DESTRUCTION\n#endif\n\n// Destroying a condvar is a nop on Windows.\n//\n// This optimization can't be performed on Apple platforms, where\n// pthread_cond_destroy can allow the kernel to release resources.\n// See https://llvm.org/D64298 for details.\n//\n// TODO(EricWF): This is potentially true for some pthread implementations\n// as well.\n#if defined(_LIBCUDACXX_HAS_THREAD_API_WIN32)\n# define _LIBCUDACXX_HAS_TRIVIAL_CONDVAR_DESTRUCTION\n#endif\n\n// Systems that use capability-based security (FreeBSD with Capsicum,\n// Nuxi CloudABI) may only provide local filesystem access (using *at()).\n// Functions like open(), rename(), unlink() and stat() should not be\n// used, as they attempt to access the global filesystem namespace.\n#ifdef __CloudABI__\n#define _LIBCUDACXX_HAS_NO_GLOBAL_FILESYSTEM_NAMESPACE\n#endif\n\n// CloudABI is intended for running networked services. Processes do not\n// have standard input and output channels.\n#ifdef __CloudABI__\n#define _LIBCUDACXX_HAS_NO_STDIN\n#define _LIBCUDACXX_HAS_NO_STDOUT\n#endif\n\n// Some systems do not provide gets() in their C library, for security reasons.\n#ifndef _LIBCUDACXX_C_HAS_NO_GETS\n#  if defined(_LIBCUDACXX_MSVCRT) || (defined(__FreeBSD__) && __FreeBSD__ >= 13)\n#    define _LIBCUDACXX_C_HAS_NO_GETS\n#  endif\n#endif\n\n#if defined(__BIONIC__) || defined(__CloudABI__) ||                            \\\n    defined(__Fuchsia__) || defined(__wasi__) || defined(_LIBCUDACXX_HAS_MUSL_LIBC)\n#define _LIBCUDACXX_PROVIDES_DEFAULT_RUNE_TABLE\n#endif\n\n// Thread-unsafe functions such as strtok() and localtime()\n// are not available.\n#ifdef __CloudABI__\n#define _LIBCUDACXX_HAS_NO_THREAD_UNSAFE_C_FUNCTIONS\n#endif\n\n// TODO: Support C11 Atomics?\n// #if __has_feature(cxx_atomic) || __has_extension(c_atomic) || __has_keyword(_Atomic)\n// #  define _LIBCUDACXX_HAS_C_ATOMIC_IMP\n#if defined(_LIBCUDACXX_COMPILER_CLANG)\n#  define _LIBCUDACXX_HAS_GCC_ATOMIC_IMP\n#elif defined(_LIBCUDACXX_COMPILER_GCC) || defined(_LIBCUDACXX_COMPILER_NVHPC)\n#  define _LIBCUDACXX_HAS_GCC_ATOMIC_IMP\n#elif defined(_LIBCUDACXX_COMPILER_NVHPC)\n#  define _LIBCUDACXX_HAS_GCC_ATOMIC_IMP\n#elif defined(_LIBCUDACXX_COMPILER_MSVC)\n#  define _LIBCUDACXX_HAS_MSVC_ATOMIC_IMPL\n#endif\n\n// CUDA Atomics supersede host atomics in order to insert the host/device dispatch layer\n#if defined(_LIBCUDACXX_COMPILER_NVCC) || defined(_LIBCUDACXX_COMPILER_NVRTC) || defined(_LIBCUDACXX_COMPILER_NVHPC) || defined(_LIBCUDACXX_CUDACC)\n#  define _LIBCUDACXX_HAS_CUDA_ATOMIC_IMPL\n#endif\n\n#if (!defined(_LIBCUDACXX_HAS_C_ATOMIC_IMP) && \\\n     !defined(_LIBCUDACXX_HAS_GCC_ATOMIC_IMP) && \\\n     !defined(_LIBCUDACXX_HAS_EXTERNAL_ATOMIC_IMP)) \\\n     || defined(_LIBCUDACXX_HAS_NO_THREADS)\n#  define _LIBCUDACXX_HAS_NO_ATOMIC_HEADER\n#else\n#  ifdef __cuda_std__\n#    undef _LIBCUDACXX_ATOMIC_FLAG_TYPE\n#    define _LIBCUDACXX_ATOMIC_FLAG_TYPE int\n#  endif\n#  ifndef _LIBCUDACXX_ATOMIC_FLAG_TYPE\n#    define _LIBCUDACXX_ATOMIC_FLAG_TYPE bool\n#  endif\n#  ifdef _LIBCUDACXX_FREESTANDING\n#    define _LIBCUDACXX_ATOMIC_ONLY_USE_BUILTINS\n#  endif\n#endif\n\n#ifndef _LIBCUDACXX_DISABLE_UBSAN_UNSIGNED_INTEGER_CHECK\n#define _LIBCUDACXX_DISABLE_UBSAN_UNSIGNED_INTEGER_CHECK\n#endif\n\n#if defined(_LIBCUDACXX_ENABLE_THREAD_SAFETY_ANNOTATIONS)\n#  if defined(_LIBCUDACXX_COMPILER_CLANG) && __has_attribute(acquire_capability)\n// Work around the attribute handling in clang.  When both __declspec and\n// __attribute__ are present, the processing goes awry preventing the definition\n// of the types.\n#    if !defined(_LIBCUDACXX_OBJECT_FORMAT_COFF)\n#      define _LIBCUDACXX_HAS_THREAD_SAFETY_ANNOTATIONS\n#    endif\n#  endif\n#endif\n\n#if __has_attribute(require_constant_initialization)\n#  define _LIBCUDACXX_SAFE_STATIC __attribute__((__require_constant_initialization__))\n#else\n#  define _LIBCUDACXX_SAFE_STATIC\n#endif\n\n#if !defined(_LIBCUDACXX_HAS_NO_OFF_T_FUNCTIONS)\n#  if defined(_LIBCUDACXX_MSVCRT) || defined(_NEWLIB_VERSION)\n#    define _LIBCUDACXX_HAS_NO_OFF_T_FUNCTIONS\n#  endif\n#endif\n\n#if __has_attribute(diagnose_if) && !defined(_LIBCUDACXX_DISABLE_ADDITIONAL_DIAGNOSTICS)\n#  define _LIBCUDACXX_DIAGNOSE_WARNING(...) \\\n     __attribute__((diagnose_if(__VA_ARGS__, \"warning\")))\n#  define _LIBCUDACXX_DIAGNOSE_ERROR(...) \\\n     __attribute__((diagnose_if(__VA_ARGS__, \"error\")))\n#else\n#  define _LIBCUDACXX_DIAGNOSE_WARNING(...)\n#  define _LIBCUDACXX_DIAGNOSE_ERROR(...)\n#endif\n\n// Use a function like macro to imply that it must be followed by a semicolon\n#if __cplusplus > 201402L && __has_cpp_attribute(fallthrough)\n#  define _LIBCUDACXX_FALLTHROUGH() [[fallthrough]]\n#elif defined(_LIBCUDACXX_COMPILER_NVRTC)\n#  define _LIBCUDACXX_FALLTHROUGH() ((void)0)\n#elif __has_cpp_attribute(clang::fallthrough)\n#  define _LIBCUDACXX_FALLTHROUGH() [[clang::fallthrough]]\n#elif defined(_LIBCUDACXX_COMPILER_NVHPC)\n#  define _LIBCUDACXX_FALLTHROUGH()\n#elif __has_attribute(fallthough) || _GNUC_VER >= 700\n#  define _LIBCUDACXX_FALLTHROUGH() __attribute__((__fallthrough__))\n#else\n#  define _LIBCUDACXX_FALLTHROUGH() ((void)0)\n#endif\n\n#if __has_attribute(__nodebug__)\n#define _LIBCUDACXX_NODEBUG __attribute__((__nodebug__))\n#else\n#define _LIBCUDACXX_NODEBUG\n#endif\n\n#  if __has_attribute(__preferred_name__)\n#    define _LIBCUDACXX_PREFERRED_NAME(x) __attribute__((__preferred_name__(x)))\n#  else\n#    define _LIBCUDACXX_PREFERRED_NAME(x)\n#  endif\n\n#if defined(_LIBCUDACXX_ABI_MICROSOFT) && \\\n    (defined(_LIBCUDACXX_COMPILER_MSVC) || __has_declspec_attribute(empty_bases))\n#  define _LIBCUDACXX_DECLSPEC_EMPTY_BASES __declspec(empty_bases)\n#else\n#  define _LIBCUDACXX_DECLSPEC_EMPTY_BASES\n#endif\n\n#if defined(_LIBCUDACXX_ENABLE_CXX17_REMOVED_FEATURES)\n#define _LIBCUDACXX_ENABLE_CXX17_REMOVED_AUTO_PTR\n#define _LIBCUDACXX_ENABLE_CXX17_REMOVED_UNEXPECTED_FUNCTIONS\n#define _LIBCUDACXX_ENABLE_CXX17_REMOVED_RANDOM_SHUFFLE\n#define _LIBCUDACXX_ENABLE_CXX17_REMOVED_BINDERS\n#endif // _LIBCUDACXX_ENABLE_CXX17_REMOVED_FEATURES\n\n#if !defined(__cpp_deduction_guides) || __cpp_deduction_guides < 201611\n#define _LIBCUDACXX_HAS_NO_DEDUCTION_GUIDES\n#endif\n\n#if !defined(__cpp_coroutines) || __cpp_coroutines < 201703L\n#define _LIBCUDACXX_HAS_NO_COROUTINES\n#endif\n\n// We need `is_constant_evaluated` for clang and gcc. MSVC also needs extensive rework\n#if !defined(_LIBCUDACXX_IS_CONSTANT_EVALUATED)\n#define _LIBCUDACXX_HAS_NO_CONSTEXPR_COMPLEX_OPERATIONS\n#elif defined(__CUDACC_RTC__)\n#define _LIBCUDACXX_HAS_NO_CONSTEXPR_COMPLEX_OPERATIONS\n#elif defined(_MSC_VER)\n#define _LIBCUDACXX_HAS_NO_CONSTEXPR_COMPLEX_OPERATIONS\n#elif defined(_LIBCUDACXX_CUDACC_BELOW_11_8)\n#define _LIBCUDACXX_HAS_NO_CONSTEXPR_COMPLEX_OPERATIONS\n#endif\n\n// FIXME: Correct this macro when either (A) a feature test macro for the\n// spaceship operator is provided, or (B) a compiler provides a complete\n// implementation.\n#define _LIBCUDACXX_HAS_NO_SPACESHIP_OPERATOR\n\n#define _LIBCUDACXX_HAS_NO_VENDOR_AVAILABILITY_ANNOTATIONS\n\n// The stream API was dropped and re-added in the dylib shipped on macOS\n// and iOS. We can only assume the dylib to provide these definitions for\n// macosx >= 10.9 and ios >= 7.0. Otherwise, the definitions are available\n// from the headers, but not from the dylib. Explicit instantiation\n// declarations for streams exist conditionally to this; if we provide\n// an explicit instantiation declaration and we try to deploy to a dylib\n// that does not provide those symbols, we'll get a load-time error.\n#if !defined(_LIBCUDACXX_BUILDING_LIBRARY) &&                                      \\\n    ((defined(__ENVIRONMENT_MAC_OS_X_VERSION_MIN_REQUIRED__) &&                \\\n      __ENVIRONMENT_MAC_OS_X_VERSION_MIN_REQUIRED__ < 1090) ||                 \\\n     (defined(__ENVIRONMENT_IPHONE_OS_VERSION_MIN_REQUIRED__) &&               \\\n      __ENVIRONMENT_IPHONE_OS_VERSION_MIN_REQUIRED__ < 70000))\n#  define _LIBCUDACXX_DO_NOT_ASSUME_STREAMS_EXPLICIT_INSTANTIATION_IN_DYLIB\n#endif\n\n#if defined(_LIBCUDACXX_HAS_NO_PRAGMA_PUSH_POP_MACRO)\n#  define _LIBCUDACXX_PUSH_MACROS\n#  define _LIBCUDACXX_POP_MACROS\n#else\n  // Don't warn about macro conflicts when we can restore them at the\n  // end of the header.\n#  ifndef _LIBCUDACXX_DISABLE_MACRO_CONFLICT_WARNINGS\n#    define _LIBCUDACXX_DISABLE_MACRO_CONFLICT_WARNINGS\n#  endif\n#  if defined(_LIBCUDACXX_COMPILER_MSVC)\n#    define _LIBCUDACXX_PUSH_MACROS    \\\n       __pragma(push_macro(\"min\")) \\\n       __pragma(push_macro(\"max\"))\n#    define _LIBCUDACXX_POP_MACROS     \\\n       __pragma(pop_macro(\"min\"))  \\\n       __pragma(pop_macro(\"max\"))\n#  else\n#    define _LIBCUDACXX_PUSH_MACROS        \\\n       _Pragma(\"push_macro(\\\"min\\\")\")  \\\n       _Pragma(\"push_macro(\\\"max\\\")\")\n#    define _LIBCUDACXX_POP_MACROS         \\\n       _Pragma(\"pop_macro(\\\"min\\\")\")   \\\n       _Pragma(\"pop_macro(\\\"max\\\")\")\n#  endif\n#endif // defined(_LIBCUDACXX_HAS_NO_PRAGMA_PUSH_POP_MACRO)\n\n#if !defined(_LIBCUDACXX_NO_AUTO_LINK) && !defined(__cuda_std__)\n#  if defined(_LIBCUDACXX_ABI_MICROSOFT) && !defined(_LIBCUDACXX_BUILDING_LIBRARY)\n#    if defined(_DLL)\n#      pragma comment(lib, \"c++.lib\")\n#    else\n#      pragma comment(lib, \"libc++.lib\")\n#    endif\n#  endif // defined(_LIBCUDACXX_ABI_MICROSOFT) && !defined(_LIBCUDACXX_BUILDING_LIBRARY)\n#endif // !defined(_LIBCUDACXX_NO_AUTO_LINK)\n\n#define _LIBCUDACXX_UNUSED_VAR(x) ((void)(x))\n\n// Configures the fopen close-on-exec mode character, if any. This string will\n// be appended to any mode string used by fstream for fopen/fdopen.\n//\n// Not all platforms support this, but it helps avoid fd-leaks on platforms that\n// do.\n#if defined(__BIONIC__)\n#  define _LIBCUDACXX_FOPEN_CLOEXEC_MODE \"e\"\n#else\n#  define _LIBCUDACXX_FOPEN_CLOEXEC_MODE\n#endif\n\n#  if __has_attribute(__format__)\n// The attribute uses 1-based indices for ordinary and static member functions.\n// The attribute uses 2-based indices for non-static member functions.\n#    define _LIBCUDACXX_ATTRIBUTE_FORMAT(archetype, format_string_index, first_format_arg_index)                           \\\n      __attribute__((__format__(archetype, format_string_index, first_format_arg_index)))\n#  else\n#    define _LIBCUDACXX_ATTRIBUTE_FORMAT(archetype, format_string_index, first_format_arg_index) /* nothing */\n#  endif\n\n#ifndef _LIBCUDACXX_SYS_CLOCK_DURATION\n#if defined(__cuda_std__)\n#  define _LIBCUDACXX_SYS_CLOCK_DURATION nanoseconds\n#else\n#  define _LIBCUDACXX_SYS_CLOCK_DURATION microseconds\n#endif\n#endif // _LIBCUDACXX_SYS_CLOCK_DURATION\n\n// There are a handful of public standard library types that are intended to\n// support CTAD but don't need any explicit deduction guides to do so. This\n// macro is used to mark them as such, which suppresses the\n// '-Wctad-maybe-unsupported' compiler warning when CTAD is used in user code\n// with these classes.\n#if (!defined(_LIBCUDACXX_COMPILER_GCC) || __GNUC__ > 6) \\\n && _LIBCUDACXX_STD_VER >= 17\n#  define _LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(_ClassName)                                                                \\\n      template <class ..._Tag>                                                                                         \\\n      _ClassName(typename _Tag::__allow_ctad...) -> _ClassName<_Tag...>\n#else\n#  define _LIBCUDACXX_CTAD_SUPPORTED_FOR_TYPE(_ClassName) static_assert(true, \"\")\n#endif\n\n#if (defined(__CUDACC_VER_MAJOR__) && __CUDACC_VER_MAJOR__ <= 11) \\\n && (defined(__CUDACC_VER_MINOR__) && __CUDACC_VER_MINOR__ <= 2)\n#  define _LIBCUDACXX_CONSTEXPR_GLOBAL const\n#else\n#  define _LIBCUDACXX_CONSTEXPR_GLOBAL constexpr\n#endif\n\n#if defined(__CUDA_ARCH__)\n#  define _LIBCUDACXX_CPO_ACCESSIBILITY _LIBCUDACXX_DEVICE _LIBCUDACXX_CONSTEXPR_GLOBAL\n#else\n#  define _LIBCUDACXX_CPO_ACCESSIBILITY _LIBCUDACXX_INLINE_VAR constexpr\n#endif\n\n#if _LIBCUDACXX_STD_VER > 14\n#  define _LIBCUDACXX_TRAIT(__TRAIT, ...) __TRAIT##_v<__VA_ARGS__>\n#else\n#  define _LIBCUDACXX_TRAIT(__TRAIT, ...) __TRAIT<__VA_ARGS__>::value\n#endif\n\n// Older nvcc do not handle the constraint of `construct_at` in earlier std modes\n// So to preserve our performance optimization we default to the unconstrained\n// `__construct_at` and only in C++20 use `construct_at`\n#if _LIBCUDACXX_STD_VER > 17\n#  define _LIBCUDACXX_CONSTRUCT_AT(_LOCATION, ...) \\\n  _CUDA_VSTD::construct_at(_CUDA_VSTD::addressof(_LOCATION), __VA_ARGS__)\n#else\n#  define _LIBCUDACXX_CONSTRUCT_AT(_LOCATION, ...) \\\n  _CUDA_VSTD::__construct_at(_CUDA_VSTD::addressof(_LOCATION), __VA_ARGS__)\n#endif\n\n#if !defined(_LIBCUDACXX_DISABLE_EXEC_CHECK)\n#if defined(_LIBCUDACXX_CUDACC)                       \\\n && !defined(_LIBCUDACXX_COMPILER_NVRTC)      \\\n && !defined(_LIBCUDACXX_COMPILER_NVHPC_CUDA) \\\n && !defined(_LIBCUDACXX_COMPILER_CLANG_CUDA)\n#  if defined(_LIBCUDACXX_COMPILER_MSVC)\n#    define _LIBCUDACXX_DISABLE_EXEC_CHECK __pragma(\"nv_exec_check_disable\")\n#  else // ^^^ _LIBCUDACXX_COMPILER_MSVC ^^^ / vvv !_LIBCUDACXX_COMPILER_MSVC vvv\n#    define _LIBCUDACXX_DISABLE_EXEC_CHECK _Pragma(\"nv_exec_check_disable\")\n#  endif // !_LIBCUDACXX_COMPILER_MSVC\n#else // ^^^ !NVRTC && !NVHPC-cuda && !clang-cuda ^^^ / vvv NVRTC || NVHPC-cuda || clang-cuda vvv\n#  define _LIBCUDACXX_DISABLE_EXEC_CHECK\n#endif // NVRTC || NVHPC-cuda || clang-cuda\n#endif // !_LIBCUDACXX_DISABLE_EXEC_CHECK\n\n#define _LIBCUDACXX_HAS_NO_INCOMPLETE_RANGES\n\n#endif // __cplusplus\n\n#endif // _LIBCUDACXX_CONFIG\n", "libcxx/include/__pragma_pop": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===---------------------------- chrono ----------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_MSVC_WARNING)\n  _Pragma(\"warning(pop)\")\n#endif\n\n#if defined(_LIBCUDACXX_POP_MACROS)\n  _LIBCUDACXX_POP_MACROS\n#endif\n", "libcxx/include/__pragma_push": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===---------------------------- chrono ----------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n  _Pragma(\"GCC system_header\")\n#endif\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_MSVC_WARNING)\n  _Pragma(\"warning(push)\")\n  _Pragma(\"warning(disable : _LIBCUDACXX_MSVC_DISABLED_WARNINGS)\")\n#endif\n\n#if defined(_LIBCUDACXX_PUSH_MACROS)\n  _LIBCUDACXX_PUSH_MACROS\n#endif\n\n#ifndef __cuda_std__\n#include <__undef_macros>\n#endif\n", "libcxx/include/__undef_macros": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===------------------------ __undef_macros ------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n\n#ifdef min\n#if !defined(_LIBCUDACXX_DISABLE_MACRO_CONFLICT_WARNINGS)\n#if defined(_LIBCUDACXX_WARNING)\n_LIBCUDACXX_WARNING(\"macro min is incompatible with C++.  Try #define NOMINMAX \"\n                \"before any Windows header. #undefing min\")\n#else\n#warning: macro min is incompatible with C++.  #undefing min\n#endif\n#endif\n#undef min\n#endif\n\n#ifdef max\n#if !defined(_LIBCUDACXX_DISABLE_MACRO_CONFLICT_WARNINGS)\n#if defined(_LIBCUDACXX_WARNING)\n_LIBCUDACXX_WARNING(\"macro max is incompatible with C++.  Try #define NOMINMAX \"\n                \"before any Windows header. #undefing max\")\n#else\n#warning: macro max is incompatible with C++.  #undefing max\n#endif\n#endif\n#undef max\n#endif\n", "limits": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===---------------------------- limits ----------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX_LIMITS\n#define _LIBCUDACXX_LIMITS\n\n/*\n    limits synopsis\n\nnamespace std\n{\n\ntemplate<class T>\nclass numeric_limits\n{\npublic:\n    static constexpr bool is_specialized = false;\n    static constexpr T min() noexcept;\n    static constexpr T max() noexcept;\n    static constexpr T lowest() noexcept;\n\n    static constexpr int  digits = 0;\n    static constexpr int  digits10 = 0;\n    static constexpr int  max_digits10 = 0;\n    static constexpr bool is_signed = false;\n    static constexpr bool is_integer = false;\n    static constexpr bool is_exact = false;\n    static constexpr int  radix = 0;\n    static constexpr T epsilon() noexcept;\n    static constexpr T round_error() noexcept;\n\n    static constexpr int  min_exponent = 0;\n    static constexpr int  min_exponent10 = 0;\n    static constexpr int  max_exponent = 0;\n    static constexpr int  max_exponent10 = 0;\n\n    static constexpr bool has_infinity = false;\n    static constexpr bool has_quiet_NaN = false;\n    static constexpr bool has_signaling_NaN = false;\n    static constexpr float_denorm_style has_denorm = denorm_absent;\n    static constexpr bool has_denorm_loss = false;\n    static constexpr T infinity() noexcept;\n    static constexpr T quiet_NaN() noexcept;\n    static constexpr T signaling_NaN() noexcept;\n    static constexpr T denorm_min() noexcept;\n\n    static constexpr bool is_iec559 = false;\n    static constexpr bool is_bounded = false;\n    static constexpr bool is_modulo = false;\n\n    static constexpr bool traps = false;\n    static constexpr bool tinyness_before = false;\n    static constexpr float_round_style round_style = round_toward_zero;\n};\n\nenum float_round_style\n{\n    round_indeterminate       = -1,\n    round_toward_zero         =  0,\n    round_to_nearest          =  1,\n    round_toward_infinity     =  2,\n    round_toward_neg_infinity =  3\n};\n\nenum float_denorm_style\n{\n    denorm_indeterminate = -1,\n    denorm_absent = 0,\n    denorm_present = 1\n};\n\ntemplate<> class numeric_limits<cv bool>;\n\ntemplate<> class numeric_limits<cv char>;\ntemplate<> class numeric_limits<cv signed char>;\ntemplate<> class numeric_limits<cv unsigned char>;\ntemplate<> class numeric_limits<cv wchar_t>;\ntemplate<> class numeric_limits<cv char8_t>; // C++20\ntemplate<> class numeric_limits<cv char16_t>;\ntemplate<> class numeric_limits<cv char32_t>;\n\ntemplate<> class numeric_limits<cv short>;\ntemplate<> class numeric_limits<cv int>;\ntemplate<> class numeric_limits<cv long>;\ntemplate<> class numeric_limits<cv long long>;\ntemplate<> class numeric_limits<cv unsigned short>;\ntemplate<> class numeric_limits<cv unsigned int>;\ntemplate<> class numeric_limits<cv unsigned long>;\ntemplate<> class numeric_limits<cv unsigned long long>;\n\ntemplate<> class numeric_limits<cv float>;\ntemplate<> class numeric_limits<cv double>;\ntemplate<> class numeric_limits<cv long double>;\n\n}  // std\n\n*/\n\n#ifndef __cuda_std__\n#include <__config>\n#else\n#ifdef _LIBCUDACXX_COMPILER_NVRTC\n#include \"climits\"\n#endif // _LIBCUDACXX_COMPILER_NVRTC\n#endif // __cuda_std__\n\n#include \"__assert\" // all public C++ headers provide the assertion handler\n#include \"type_traits\"\n#include \"version\"\n\n#ifndef __cuda_std__\n#include <__pragma_push>\n#endif //__cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n#if defined(_LIBCUDACXX_COMPILER_MSVC)\n#include \"support/win32/limits_msvc_win32.h\"\n#endif // _LIBCUDACXX_MSVCRT\n\n#if defined(_LIBCUDACXX_COMPILER_IBM)\n#include \"support/ibm/limits.h\"\n#endif // _LIBCUDACXX_COMPILER_IBM\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\nenum float_round_style\n{\n    round_indeterminate       = -1,\n    round_toward_zero         =  0,\n    round_to_nearest          =  1,\n    round_toward_infinity     =  2,\n    round_toward_neg_infinity =  3\n};\n\nenum float_denorm_style\n{\n    denorm_indeterminate = -1,\n    denorm_absent = 0,\n    denorm_present = 1\n};\n\ntemplate <class _Tp, bool = is_arithmetic<_Tp>::value>\nclass __libcpp_numeric_limits\n{\nprotected:\n    typedef _Tp type;\n\n    static constexpr bool is_specialized = false;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type min() noexcept {return type();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type max() noexcept {return type();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type lowest() noexcept {return type();}\n\n    static constexpr int  digits = 0;\n    static constexpr int  digits10 = 0;\n    static constexpr int  max_digits10 = 0;\n    static constexpr bool is_signed = false;\n    static constexpr bool is_integer = false;\n    static constexpr bool is_exact = false;\n    static constexpr int  radix = 0;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type epsilon() noexcept {return type();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type round_error() noexcept {return type();}\n\n    static constexpr int  min_exponent = 0;\n    static constexpr int  min_exponent10 = 0;\n    static constexpr int  max_exponent = 0;\n    static constexpr int  max_exponent10 = 0;\n\n    static constexpr bool has_infinity = false;\n    static constexpr bool has_quiet_NaN = false;\n    static constexpr bool has_signaling_NaN = false;\n    static constexpr float_denorm_style has_denorm = denorm_absent;\n    static constexpr bool has_denorm_loss = false;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type infinity() noexcept {return type();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type quiet_NaN() noexcept {return type();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type signaling_NaN() noexcept {return type();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type denorm_min() noexcept {return type();}\n\n    static constexpr bool is_iec559 = false;\n    static constexpr bool is_bounded = false;\n    static constexpr bool is_modulo = false;\n\n    static constexpr bool traps = false;\n    static constexpr bool tinyness_before = false;\n    static constexpr float_round_style round_style = round_toward_zero;\n};\n\ntemplate <class _Tp, int __digits, bool _IsSigned>\nstruct __libcpp_compute_min\n{\n    static constexpr _Tp value = static_cast<_Tp>(_Tp(1) << __digits);\n};\n\ntemplate <class _Tp, int __digits>\nstruct __libcpp_compute_min<_Tp, __digits, false>\n{\n    static constexpr _Tp value = _Tp(0);\n};\n\ntemplate <class _Tp>\nclass __libcpp_numeric_limits<_Tp, true>\n{\nprotected:\n    typedef _Tp type;\n\n    static constexpr bool is_specialized = true;\n\n    static constexpr bool is_signed = type(-1) < type(0);\n    static constexpr int  digits = static_cast<int>(sizeof(type) * __CHAR_BIT__ - is_signed);\n    static constexpr int  digits10 = digits * 3 / 10;\n    static constexpr int  max_digits10 = 0;\n    static constexpr type __min = __libcpp_compute_min<type, digits, is_signed>::value;\n    static constexpr type __max = is_signed ? type(type(~0) ^ __min) : type(~0);\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type min() noexcept {return __min;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type max() noexcept {return __max;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type lowest() noexcept {return min();}\n\n    static constexpr bool is_integer = true;\n    static constexpr bool is_exact = true;\n    static constexpr int  radix = 2;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type epsilon() noexcept {return type(0);}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type round_error() noexcept {return type(0);}\n\n    static constexpr int  min_exponent = 0;\n    static constexpr int  min_exponent10 = 0;\n    static constexpr int  max_exponent = 0;\n    static constexpr int  max_exponent10 = 0;\n\n    static constexpr bool has_infinity = false;\n    static constexpr bool has_quiet_NaN = false;\n    static constexpr bool has_signaling_NaN = false;\n    static constexpr float_denorm_style has_denorm = denorm_absent;\n    static constexpr bool has_denorm_loss = false;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type infinity() noexcept {return type(0);}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type quiet_NaN() noexcept {return type(0);}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type signaling_NaN() noexcept {return type(0);}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type denorm_min() noexcept {return type(0);}\n\n    static constexpr bool is_iec559 = false;\n    static constexpr bool is_bounded = true;\n    static constexpr bool is_modulo = !_CUDA_VSTD::is_signed<_Tp>::value;\n\n#if defined(__i386__) || defined(__x86_64__) || defined(__pnacl__) || \\\n    defined(__wasm__)\n    static constexpr bool traps = true;\n#else\n    static constexpr bool traps = false;\n#endif\n    static constexpr bool tinyness_before = false;\n    static constexpr float_round_style round_style = round_toward_zero;\n};\n\ntemplate <>\nclass __libcpp_numeric_limits<bool, true>\n{\nprotected:\n    typedef bool type;\n\n    static constexpr bool is_specialized = true;\n\n    static constexpr bool is_signed = false;\n    static constexpr int  digits = 1;\n    static constexpr int  digits10 = 0;\n    static constexpr int  max_digits10 = 0;\n    static constexpr type __min = false;\n    static constexpr type __max = true;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type min() noexcept {return __min;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type max() noexcept {return __max;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type lowest() noexcept {return min();}\n\n    static constexpr bool is_integer = true;\n    static constexpr bool is_exact = true;\n    static constexpr int  radix = 2;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type epsilon() noexcept {return type(0);}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type round_error() noexcept {return type(0);}\n\n    static constexpr int  min_exponent = 0;\n    static constexpr int  min_exponent10 = 0;\n    static constexpr int  max_exponent = 0;\n    static constexpr int  max_exponent10 = 0;\n\n    static constexpr bool has_infinity = false;\n    static constexpr bool has_quiet_NaN = false;\n    static constexpr bool has_signaling_NaN = false;\n    static constexpr float_denorm_style has_denorm = denorm_absent;\n    static constexpr bool has_denorm_loss = false;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type infinity() noexcept {return type(0);}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type quiet_NaN() noexcept {return type(0);}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type signaling_NaN() noexcept {return type(0);}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type denorm_min() noexcept {return type(0);}\n\n    static constexpr bool is_iec559 = false;\n    static constexpr bool is_bounded = true;\n    static constexpr bool is_modulo = false;\n\n    static constexpr bool traps = false;\n    static constexpr bool tinyness_before = false;\n    static constexpr float_round_style round_style = round_toward_zero;\n};\n\ntemplate <>\nclass __libcpp_numeric_limits<float, true>\n{\nprotected:\n    typedef float type;\n\n    static constexpr bool is_specialized = true;\n\n    static constexpr bool is_signed = true;\n    static constexpr int  digits = __FLT_MANT_DIG__;\n    static constexpr int  digits10 = __FLT_DIG__;\n    static constexpr int  max_digits10 = 2+(digits * 30103l)/100000l;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type min() noexcept {return __FLT_MIN__;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type max() noexcept {return __FLT_MAX__;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type lowest() noexcept {return -max();}\n\n    static constexpr bool is_integer = false;\n    static constexpr bool is_exact = false;\n    static constexpr int  radix = __FLT_RADIX__;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type epsilon() noexcept {return __FLT_EPSILON__;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type round_error() noexcept {return 0.5F;}\n\n    static constexpr int  min_exponent = __FLT_MIN_EXP__;\n    static constexpr int  min_exponent10 = __FLT_MIN_10_EXP__;\n    static constexpr int  max_exponent = __FLT_MAX_EXP__;\n    static constexpr int  max_exponent10 = __FLT_MAX_10_EXP__;\n\n    static constexpr bool has_infinity = true;\n    static constexpr bool has_quiet_NaN = true;\n    static constexpr bool has_signaling_NaN = true;\n    static constexpr float_denorm_style has_denorm = denorm_present;\n    static constexpr bool has_denorm_loss = false;\n#ifdef _LIBCUDACXX_COMPILER_NVRTC\n    _LIBCUDACXX_INLINE_VISIBILITY static type infinity() noexcept {return __builtin_huge_valf();}\n    _LIBCUDACXX_INLINE_VISIBILITY static type quiet_NaN() noexcept {return __builtin_nanf(\"\");}\n    _LIBCUDACXX_INLINE_VISIBILITY static type signaling_NaN() noexcept {return __builtin_nansf(\"\");}\n#else\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type infinity() noexcept {return __builtin_huge_valf();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type quiet_NaN() noexcept {return __builtin_nanf(\"\");}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type signaling_NaN() noexcept {return __builtin_nansf(\"\");}\n#endif\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type denorm_min() noexcept {return __FLT_DENORM_MIN__;}\n\n    static constexpr bool is_iec559 = true;\n    static constexpr bool is_bounded = true;\n    static constexpr bool is_modulo = false;\n\n    static constexpr bool traps = false;\n    static constexpr bool tinyness_before = false;\n    static constexpr float_round_style round_style = round_to_nearest;\n};\n\ntemplate <>\nclass __libcpp_numeric_limits<double, true>\n{\nprotected:\n    typedef double type;\n\n    static constexpr bool is_specialized = true;\n\n    static constexpr bool is_signed = true;\n    static constexpr int  digits = __DBL_MANT_DIG__;\n    static constexpr int  digits10 = __DBL_DIG__;\n    static constexpr int  max_digits10 = 2+(digits * 30103l)/100000l;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type min() noexcept {return __DBL_MIN__;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type max() noexcept {return __DBL_MAX__;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type lowest() noexcept {return -max();}\n\n    static constexpr bool is_integer = false;\n    static constexpr bool is_exact = false;\n    static constexpr int  radix = __FLT_RADIX__;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type epsilon() noexcept {return __DBL_EPSILON__;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type round_error() noexcept {return 0.5;}\n\n    static constexpr int  min_exponent = __DBL_MIN_EXP__;\n    static constexpr int  min_exponent10 = __DBL_MIN_10_EXP__;\n    static constexpr int  max_exponent = __DBL_MAX_EXP__;\n    static constexpr int  max_exponent10 = __DBL_MAX_10_EXP__;\n\n    static constexpr bool has_infinity = true;\n    static constexpr bool has_quiet_NaN = true;\n    static constexpr bool has_signaling_NaN = true;\n    static constexpr float_denorm_style has_denorm = denorm_present;\n    static constexpr bool has_denorm_loss = false;\n#ifdef _LIBCUDACXX_COMPILER_NVRTC\n    _LIBCUDACXX_INLINE_VISIBILITY static type infinity() noexcept {return __builtin_huge_val();}\n    _LIBCUDACXX_INLINE_VISIBILITY static type quiet_NaN() noexcept {return __builtin_nan(\"\");}\n    _LIBCUDACXX_INLINE_VISIBILITY static type signaling_NaN() noexcept {return __builtin_nans(\"\");}\n#else\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type infinity() noexcept {return __builtin_huge_val();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type quiet_NaN() noexcept {return __builtin_nan(\"\");}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type signaling_NaN() noexcept {return __builtin_nans(\"\");}\n#endif\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type denorm_min() noexcept {return __DBL_DENORM_MIN__;}\n\n    static constexpr bool is_iec559 = true;\n    static constexpr bool is_bounded = true;\n    static constexpr bool is_modulo = false;\n\n    static constexpr bool traps = false;\n    static constexpr bool tinyness_before = false;\n    static constexpr float_round_style round_style = round_to_nearest;\n};\n\ntemplate <>\nclass __libcpp_numeric_limits<long double, true>\n{\n#ifndef _LIBCUDACXX_HAS_NO_LONG_DOUBLE\nprotected:\n    typedef long double type;\n\n    static constexpr bool is_specialized = true;\n\n    static constexpr bool is_signed = true;\n    static constexpr int  digits = __LDBL_MANT_DIG__;\n    static constexpr int  digits10 = __LDBL_DIG__;\n    static constexpr int  max_digits10 = 2+(digits * 30103l)/100000l;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type min() noexcept {return __LDBL_MIN__;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type max() noexcept {return __LDBL_MAX__;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type lowest() noexcept {return -max();}\n\n    static constexpr bool is_integer = false;\n    static constexpr bool is_exact = false;\n    static constexpr int  radix = __FLT_RADIX__;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type epsilon() noexcept {return __LDBL_EPSILON__;}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type round_error() noexcept {return 0.5L;}\n\n    static constexpr int  min_exponent = __LDBL_MIN_EXP__;\n    static constexpr int  min_exponent10 = __LDBL_MIN_10_EXP__;\n    static constexpr int  max_exponent = __LDBL_MAX_EXP__;\n    static constexpr int  max_exponent10 = __LDBL_MAX_10_EXP__;\n\n    static constexpr bool has_infinity = true;\n    static constexpr bool has_quiet_NaN = true;\n    static constexpr bool has_signaling_NaN = true;\n    static constexpr float_denorm_style has_denorm = denorm_present;\n    static constexpr bool has_denorm_loss = false;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type infinity() noexcept {return __builtin_huge_vall();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type quiet_NaN() noexcept {return __builtin_nanl(\"\");}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type signaling_NaN() noexcept {return __builtin_nansl(\"\");}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type denorm_min() noexcept {return __LDBL_DENORM_MIN__;}\n\n#if (defined(__ppc__) || defined(__ppc64__))\n    static constexpr bool is_iec559 = false;\n#else\n    static constexpr bool is_iec559 = true;\n#endif\n    static constexpr bool is_bounded = true;\n    static constexpr bool is_modulo = false;\n\n    static constexpr bool traps = false;\n    static constexpr bool tinyness_before = false;\n    static constexpr float_round_style round_style = round_to_nearest;\n#endif\n};\n\ntemplate <class _Tp>\nclass _LIBCUDACXX_TEMPLATE_VIS numeric_limits\n    : private __libcpp_numeric_limits<__remove_cv_t<_Tp>>\n{\n    typedef __libcpp_numeric_limits<__remove_cv_t<_Tp>> __base;\n    typedef typename __base::type type;\npublic:\n    static constexpr bool is_specialized = __base::is_specialized;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type min() noexcept {return __base::min();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type max() noexcept {return __base::max();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type lowest() noexcept {return __base::lowest();}\n\n    static constexpr int  digits = __base::digits;\n    static constexpr int  digits10 = __base::digits10;\n    static constexpr int  max_digits10 = __base::max_digits10;\n    static constexpr bool is_signed = __base::is_signed;\n    static constexpr bool is_integer = __base::is_integer;\n    static constexpr bool is_exact = __base::is_exact;\n    static constexpr int  radix = __base::radix;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type epsilon() noexcept {return __base::epsilon();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type round_error() noexcept {return __base::round_error();}\n\n    static constexpr int  min_exponent = __base::min_exponent;\n    static constexpr int  min_exponent10 = __base::min_exponent10;\n    static constexpr int  max_exponent = __base::max_exponent;\n    static constexpr int  max_exponent10 = __base::max_exponent10;\n\n    static constexpr bool has_infinity = __base::has_infinity;\n    static constexpr bool has_quiet_NaN = __base::has_quiet_NaN;\n    static constexpr bool has_signaling_NaN = __base::has_signaling_NaN;\n    static constexpr float_denorm_style has_denorm = __base::has_denorm;\n    static constexpr bool has_denorm_loss = __base::has_denorm_loss;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type infinity() noexcept {return __base::infinity();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type quiet_NaN() noexcept {return __base::quiet_NaN();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type signaling_NaN() noexcept {return __base::signaling_NaN();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type denorm_min() noexcept {return __base::denorm_min();}\n\n    static constexpr bool is_iec559 = __base::is_iec559;\n    static constexpr bool is_bounded = __base::is_bounded;\n    static constexpr bool is_modulo = __base::is_modulo;\n\n    static constexpr bool traps = __base::traps;\n    static constexpr bool tinyness_before = __base::tinyness_before;\n    static constexpr float_round_style round_style = __base::round_style;\n};\n\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::is_specialized;\ntemplate <class _Tp>\n    constexpr int numeric_limits<_Tp>::digits;\ntemplate <class _Tp>\n    constexpr int numeric_limits<_Tp>::digits10;\ntemplate <class _Tp>\n    constexpr int numeric_limits<_Tp>::max_digits10;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::is_signed;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::is_integer;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::is_exact;\ntemplate <class _Tp>\n    constexpr int numeric_limits<_Tp>::radix;\ntemplate <class _Tp>\n    constexpr int numeric_limits<_Tp>::min_exponent;\ntemplate <class _Tp>\n    constexpr int numeric_limits<_Tp>::min_exponent10;\ntemplate <class _Tp>\n    constexpr int numeric_limits<_Tp>::max_exponent;\ntemplate <class _Tp>\n    constexpr int numeric_limits<_Tp>::max_exponent10;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::has_infinity;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::has_quiet_NaN;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::has_signaling_NaN;\ntemplate <class _Tp>\n    constexpr float_denorm_style numeric_limits<_Tp>::has_denorm;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::has_denorm_loss;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::is_iec559;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::is_bounded;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::is_modulo;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::traps;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<_Tp>::tinyness_before;\ntemplate <class _Tp>\n    constexpr float_round_style numeric_limits<_Tp>::round_style;\n\ntemplate <class _Tp>\nclass _LIBCUDACXX_TEMPLATE_VIS numeric_limits<const _Tp>\n    : private numeric_limits<_Tp>\n{\n    typedef numeric_limits<_Tp> __base;\n    typedef _Tp type;\npublic:\n    static constexpr bool is_specialized = __base::is_specialized;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type min() noexcept {return __base::min();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type max() noexcept {return __base::max();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type lowest() noexcept {return __base::lowest();}\n\n    static constexpr int  digits = __base::digits;\n    static constexpr int  digits10 = __base::digits10;\n    static constexpr int  max_digits10 = __base::max_digits10;\n    static constexpr bool is_signed = __base::is_signed;\n    static constexpr bool is_integer = __base::is_integer;\n    static constexpr bool is_exact = __base::is_exact;\n    static constexpr int  radix = __base::radix;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type epsilon() noexcept {return __base::epsilon();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type round_error() noexcept {return __base::round_error();}\n\n    static constexpr int  min_exponent = __base::min_exponent;\n    static constexpr int  min_exponent10 = __base::min_exponent10;\n    static constexpr int  max_exponent = __base::max_exponent;\n    static constexpr int  max_exponent10 = __base::max_exponent10;\n\n    static constexpr bool has_infinity = __base::has_infinity;\n    static constexpr bool has_quiet_NaN = __base::has_quiet_NaN;\n    static constexpr bool has_signaling_NaN = __base::has_signaling_NaN;\n    static constexpr float_denorm_style has_denorm = __base::has_denorm;\n    static constexpr bool has_denorm_loss = __base::has_denorm_loss;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type infinity() noexcept {return __base::infinity();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type quiet_NaN() noexcept {return __base::quiet_NaN();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type signaling_NaN() noexcept {return __base::signaling_NaN();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type denorm_min() noexcept {return __base::denorm_min();}\n\n    static constexpr bool is_iec559 = __base::is_iec559;\n    static constexpr bool is_bounded = __base::is_bounded;\n    static constexpr bool is_modulo = __base::is_modulo;\n\n    static constexpr bool traps = __base::traps;\n    static constexpr bool tinyness_before = __base::tinyness_before;\n    static constexpr float_round_style round_style = __base::round_style;\n};\n\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::is_specialized;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const _Tp>::digits;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const _Tp>::digits10;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const _Tp>::max_digits10;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::is_signed;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::is_integer;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::is_exact;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const _Tp>::radix;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const _Tp>::min_exponent;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const _Tp>::min_exponent10;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const _Tp>::max_exponent;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const _Tp>::max_exponent10;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::has_infinity;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::has_quiet_NaN;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::has_signaling_NaN;\ntemplate <class _Tp>\n    constexpr float_denorm_style numeric_limits<const _Tp>::has_denorm;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::has_denorm_loss;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::is_iec559;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::is_bounded;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::is_modulo;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::traps;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const _Tp>::tinyness_before;\ntemplate <class _Tp>\n    constexpr float_round_style numeric_limits<const _Tp>::round_style;\n\ntemplate <class _Tp>\nclass _LIBCUDACXX_TEMPLATE_VIS numeric_limits<volatile _Tp>\n    : private numeric_limits<_Tp>\n{\n    typedef numeric_limits<_Tp> __base;\n    typedef _Tp type;\npublic:\n    static constexpr bool is_specialized = __base::is_specialized;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type min() noexcept {return __base::min();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type max() noexcept {return __base::max();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type lowest() noexcept {return __base::lowest();}\n\n    static constexpr int  digits = __base::digits;\n    static constexpr int  digits10 = __base::digits10;\n    static constexpr int  max_digits10 = __base::max_digits10;\n    static constexpr bool is_signed = __base::is_signed;\n    static constexpr bool is_integer = __base::is_integer;\n    static constexpr bool is_exact = __base::is_exact;\n    static constexpr int  radix = __base::radix;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type epsilon() noexcept {return __base::epsilon();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type round_error() noexcept {return __base::round_error();}\n\n    static constexpr int  min_exponent = __base::min_exponent;\n    static constexpr int  min_exponent10 = __base::min_exponent10;\n    static constexpr int  max_exponent = __base::max_exponent;\n    static constexpr int  max_exponent10 = __base::max_exponent10;\n\n    static constexpr bool has_infinity = __base::has_infinity;\n    static constexpr bool has_quiet_NaN = __base::has_quiet_NaN;\n    static constexpr bool has_signaling_NaN = __base::has_signaling_NaN;\n    static constexpr float_denorm_style has_denorm = __base::has_denorm;\n    static constexpr bool has_denorm_loss = __base::has_denorm_loss;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type infinity() noexcept {return __base::infinity();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type quiet_NaN() noexcept {return __base::quiet_NaN();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type signaling_NaN() noexcept {return __base::signaling_NaN();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type denorm_min() noexcept {return __base::denorm_min();}\n\n    static constexpr bool is_iec559 = __base::is_iec559;\n    static constexpr bool is_bounded = __base::is_bounded;\n    static constexpr bool is_modulo = __base::is_modulo;\n\n    static constexpr bool traps = __base::traps;\n    static constexpr bool tinyness_before = __base::tinyness_before;\n    static constexpr float_round_style round_style = __base::round_style;\n};\n\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::is_specialized;\ntemplate <class _Tp>\n    constexpr int numeric_limits<volatile _Tp>::digits;\ntemplate <class _Tp>\n    constexpr int numeric_limits<volatile _Tp>::digits10;\ntemplate <class _Tp>\n    constexpr int numeric_limits<volatile _Tp>::max_digits10;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::is_signed;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::is_integer;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::is_exact;\ntemplate <class _Tp>\n    constexpr int numeric_limits<volatile _Tp>::radix;\ntemplate <class _Tp>\n    constexpr int numeric_limits<volatile _Tp>::min_exponent;\ntemplate <class _Tp>\n    constexpr int numeric_limits<volatile _Tp>::min_exponent10;\ntemplate <class _Tp>\n    constexpr int numeric_limits<volatile _Tp>::max_exponent;\ntemplate <class _Tp>\n    constexpr int numeric_limits<volatile _Tp>::max_exponent10;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::has_infinity;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::has_quiet_NaN;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::has_signaling_NaN;\ntemplate <class _Tp>\n    constexpr float_denorm_style numeric_limits<volatile _Tp>::has_denorm;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::has_denorm_loss;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::is_iec559;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::is_bounded;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::is_modulo;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::traps;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<volatile _Tp>::tinyness_before;\ntemplate <class _Tp>\n    constexpr float_round_style numeric_limits<volatile _Tp>::round_style;\n\ntemplate <class _Tp>\nclass _LIBCUDACXX_TEMPLATE_VIS numeric_limits<const volatile _Tp>\n    : private numeric_limits<_Tp>\n{\n    typedef numeric_limits<_Tp> __base;\n    typedef _Tp type;\npublic:\n    static constexpr bool is_specialized = __base::is_specialized;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type min() noexcept {return __base::min();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type max() noexcept {return __base::max();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type lowest() noexcept {return __base::lowest();}\n\n    static constexpr int  digits = __base::digits;\n    static constexpr int  digits10 = __base::digits10;\n    static constexpr int  max_digits10 = __base::max_digits10;\n    static constexpr bool is_signed = __base::is_signed;\n    static constexpr bool is_integer = __base::is_integer;\n    static constexpr bool is_exact = __base::is_exact;\n    static constexpr int  radix = __base::radix;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type epsilon() noexcept {return __base::epsilon();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type round_error() noexcept {return __base::round_error();}\n\n    static constexpr int  min_exponent = __base::min_exponent;\n    static constexpr int  min_exponent10 = __base::min_exponent10;\n    static constexpr int  max_exponent = __base::max_exponent;\n    static constexpr int  max_exponent10 = __base::max_exponent10;\n\n    static constexpr bool has_infinity = __base::has_infinity;\n    static constexpr bool has_quiet_NaN = __base::has_quiet_NaN;\n    static constexpr bool has_signaling_NaN = __base::has_signaling_NaN;\n    static constexpr float_denorm_style has_denorm = __base::has_denorm;\n    static constexpr bool has_denorm_loss = __base::has_denorm_loss;\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type infinity() noexcept {return __base::infinity();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type quiet_NaN() noexcept {return __base::quiet_NaN();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type signaling_NaN() noexcept {return __base::signaling_NaN();}\n    _LIBCUDACXX_INLINE_VISIBILITY static constexpr type denorm_min() noexcept {return __base::denorm_min();}\n\n    static constexpr bool is_iec559 = __base::is_iec559;\n    static constexpr bool is_bounded = __base::is_bounded;\n    static constexpr bool is_modulo = __base::is_modulo;\n\n    static constexpr bool traps = __base::traps;\n    static constexpr bool tinyness_before = __base::tinyness_before;\n    static constexpr float_round_style round_style = __base::round_style;\n};\n\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::is_specialized;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const volatile _Tp>::digits;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const volatile _Tp>::digits10;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const volatile _Tp>::max_digits10;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::is_signed;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::is_integer;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::is_exact;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const volatile _Tp>::radix;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const volatile _Tp>::min_exponent;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const volatile _Tp>::min_exponent10;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const volatile _Tp>::max_exponent;\ntemplate <class _Tp>\n    constexpr int numeric_limits<const volatile _Tp>::max_exponent10;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::has_infinity;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::has_quiet_NaN;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::has_signaling_NaN;\ntemplate <class _Tp>\n    constexpr float_denorm_style numeric_limits<const volatile _Tp>::has_denorm;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::has_denorm_loss;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::is_iec559;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::is_bounded;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::is_modulo;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::traps;\ntemplate <class _Tp>\n    constexpr bool numeric_limits<const volatile _Tp>::tinyness_before;\ntemplate <class _Tp>\n    constexpr float_round_style numeric_limits<const volatile _Tp>::round_style;\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#ifndef __cuda_std__\n#include <__pragma_pop>\n#endif //__cuda_std__\n\n#endif  // _LIBCUDACXX_LIMITS\n", "limits.h": "\n#pragma once\n#if __has_include(<cuda/std/climits>)\n #include <cuda/std/climits>\n #include <cuda/std/limits>\n #include <cuda/std/cstdint>\n#else\n #if defined _WIN32 || defined _WIN64\n  #define __WORDSIZE 32\n #else\n  #if defined(__LP64__) || (defined __x86_64__ && !defined __ILP32__)\n   #define __WORDSIZE 64\n  #else\n   #define __WORDSIZE 32\n  #endif\n #endif\n #define MB_LEN_MAX  16\n #define CHAR_BIT    8\n #define SCHAR_MIN   (-128)\n #define SCHAR_MAX   127\n #define UCHAR_MAX   255\n enum {\n   _JITIFY_CHAR_IS_UNSIGNED = (char)-1 >= 0,\n   CHAR_MIN = _JITIFY_CHAR_IS_UNSIGNED ? 0 : SCHAR_MIN,\n   CHAR_MAX = _JITIFY_CHAR_IS_UNSIGNED ? UCHAR_MAX : SCHAR_MAX,\n };\n #define SHRT_MIN    (-SHRT_MAX - 1)\n #define SHRT_MAX    0x7fff\n #define USHRT_MAX   0xffff\n #define INT_MIN     (-INT_MAX - 1)\n #define INT_MAX     0x7fffffff\n #define UINT_MAX    0xffffffff\n #if __WORDSIZE == 64\n  # define LONG_MAX  LLONG_MAX\n #else\n  # define LONG_MAX  UINT_MAX\n #endif\n #define LONG_MIN    (-LONG_MAX - 1)\n #if __WORDSIZE == 64\n  #define ULONG_MAX  ULLONG_MAX\n #else\n  #define ULONG_MAX  UINT_MAX\n #endif\n #define LLONG_MAX  0x7fffffffffffffff\n #define LLONG_MIN  (-LLONG_MAX - 1)\n #define ULLONG_MAX 0xffffffffffffffff\n#endif\n", "math.h": "#pragma once\nnamespace __jitify_math_ns {\n#if __cplusplus >= 201103L\n#define DEFINE_MATH_UNARY_FUNC_WRAPPER(f) \\\n\tinline double      f(double x)         { return ::f(x); } \\\n\tinline float       f##f(float x)       { return ::f(x); } \\\n\t/*inline long double f##l(long double x) { return ::f(x); }*/ \\\n\tinline float       f(float x)          { return ::f(x); } \\\n\t/*inline long double f(long double x)    { return ::f(x); }*/\n#else\n#define DEFINE_MATH_UNARY_FUNC_WRAPPER(f) \\\n\tinline double      f(double x)         { return ::f(x); } \\\n\tinline float       f##f(float x)       { return ::f(x); } \\\n\t/*inline long double f##l(long double x) { return ::f(x); }*/\n#endif\nDEFINE_MATH_UNARY_FUNC_WRAPPER(cos)\nDEFINE_MATH_UNARY_FUNC_WRAPPER(sin)\nDEFINE_MATH_UNARY_FUNC_WRAPPER(tan)\nDEFINE_MATH_UNARY_FUNC_WRAPPER(acos)\nDEFINE_MATH_UNARY_FUNC_WRAPPER(asin)\nDEFINE_MATH_UNARY_FUNC_WRAPPER(atan)\ntemplate<typename T> inline T atan2(T y, T x) { return ::atan2(y, x); }\nDEFINE_MATH_UNARY_FUNC_WRAPPER(cosh)\nDEFINE_MATH_UNARY_FUNC_WRAPPER(sinh)\nDEFINE_MATH_UNARY_FUNC_WRAPPER(tanh)\nDEFINE_MATH_UNARY_FUNC_WRAPPER(exp)\ntemplate<typename T> inline T frexp(T x, int* exp) { return ::frexp(x, exp); }\ntemplate<typename T> inline T ldexp(T x, int  exp) { return ::ldexp(x, exp); }\nDEFINE_MATH_UNARY_FUNC_WRAPPER(log)\nDEFINE_MATH_UNARY_FUNC_WRAPPER(log10)\ntemplate<typename T> inline T modf(T x, T* intpart) { return ::modf(x, intpart); }\ntemplate<typename T> inline T pow(T x, T y) { return ::pow(x, y); }\nDEFINE_MATH_UNARY_FUNC_WRAPPER(sqrt)\nDEFINE_MATH_UNARY_FUNC_WRAPPER(ceil)\nDEFINE_MATH_UNARY_FUNC_WRAPPER(floor)\ntemplate<typename T> inline T fmod(T n, T d) { return ::fmod(n, d); }\nDEFINE_MATH_UNARY_FUNC_WRAPPER(fabs)\ntemplate<typename T> inline T abs(T x) { return ::abs(x); }\n#if __cplusplus >= 201103L\nDEFINE_MATH_UNARY_FUNC_WRAPPER(acosh)\nDEFINE_MATH_UNARY_FUNC_WRAPPER(asinh)\nDEFINE_MATH_UNARY_FUNC_WRAPPER(atanh)\nDEFINE_MATH_UNARY_FUNC_WRAPPER(exp2)\nDEFINE_MATH_UNARY_FUNC_WRAPPER(expm1)\ntemplate<typename T> inline int ilogb(T x) { return ::ilogb(x); }\nDEFINE_MATH_UNARY_FUNC_WRAPPER(log1p)\nDEFINE_MATH_UNARY_FUNC_WRAPPER(log2)\nDEFINE_MATH_UNARY_FUNC_WRAPPER(logb)\ntemplate<typename T> inline T scalbn (T x, int n)  { return ::scalbn(x, n); }\ntemplate<typename T> inline T scalbln(T x, long n) { return ::scalbn(x, n); }\nDEFINE_MATH_UNARY_FUNC_WRAPPER(cbrt)\ntemplate<typename T> inline T hypot(T x, T y) { return ::hypot(x, y); }\nDEFINE_MATH_UNARY_FUNC_WRAPPER(erf)\nDEFINE_MATH_UNARY_FUNC_WRAPPER(erfc)\nDEFINE_MATH_UNARY_FUNC_WRAPPER(tgamma)\nDEFINE_MATH_UNARY_FUNC_WRAPPER(lgamma)\nDEFINE_MATH_UNARY_FUNC_WRAPPER(trunc)\nDEFINE_MATH_UNARY_FUNC_WRAPPER(round)\ntemplate<typename T> inline long lround(T x) { return ::lround(x); }\ntemplate<typename T> inline long long llround(T x) { return ::llround(x); }\nDEFINE_MATH_UNARY_FUNC_WRAPPER(rint)\ntemplate<typename T> inline long lrint(T x) { return ::lrint(x); }\ntemplate<typename T> inline long long llrint(T x) { return ::llrint(x); }\nDEFINE_MATH_UNARY_FUNC_WRAPPER(nearbyint)\n#endif\n#undef DEFINE_MATH_UNARY_FUNC_WRAPPER\n} // namespace __jitify_math_ns\nnamespace std { using namespace __jitify_math_ns; }\n#define M_PI 3.14159265358979323846\n//using namespace __jitify_math_ns;\n", "memory.h": "\n    #pragma once\n    #include <string.h>\n ", "nv/target": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of libcu++, the C++ Standard Library for your entire system,\n// under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n// This header contains a preview of a portability system that enables\n// CUDA C++ development with NVC++, NVCC, and supported host compilers.\n// These interfaces are not guaranteed to be stable.\n\n#ifndef __NV_TARGET_H\n#define __NV_TARGET_H\n\n#if defined(__NVCC__) || defined(__CUDACC_RTC__)\n#  define _NV_COMPILER_NVCC\n#elif defined(__NVCOMPILER) && __cplusplus >= 201103L\n#  define _NV_COMPILER_NVCXX\n#elif defined(__clang__) && defined(__CUDA__) && defined(__CUDA_ARCH__)\n// clang compiling CUDA code, device mode.\n#  define _NV_COMPILER_CLANG_CUDA\n#endif\n\n#if (!defined(__ibmxl__)) && \\\n    ((defined(__cplusplus) && __cplusplus >= 201103L) || \\\n     (defined(_MSC_VER) && _MSVC_LANG >= 201103L))\n#  define _NV_TARGET_CPP11\n#endif\n\n\n// Hide `if target` support from NVRTC\n#if defined(_NV_TARGET_CPP11) && !defined(__CUDACC_RTC__)\n\n#if defined(_NV_COMPILER_NVCXX)\n#  define _NV_BITSET_ATTRIBUTE [[nv::__target_bitset]]\n#else\n#  define _NV_BITSET_ATTRIBUTE\n#endif\n\nnamespace nv {\n  namespace target {\n    namespace detail {\n\n      typedef unsigned long long base_int_t;\n\n      // No host specialization\n      constexpr base_int_t all_hosts = 1;\n\n      // NVIDIA GPUs\n      constexpr base_int_t sm_35_bit = 1 << 1;\n      constexpr base_int_t sm_37_bit = 1 << 2;\n      constexpr base_int_t sm_50_bit = 1 << 3;\n      constexpr base_int_t sm_52_bit = 1 << 4;\n      constexpr base_int_t sm_53_bit = 1 << 5;\n      constexpr base_int_t sm_60_bit = 1 << 6;\n      constexpr base_int_t sm_61_bit = 1 << 7;\n      constexpr base_int_t sm_62_bit = 1 << 8;\n      constexpr base_int_t sm_70_bit = 1 << 9;\n      constexpr base_int_t sm_72_bit = 1 << 10;\n      constexpr base_int_t sm_75_bit = 1 << 11;\n      constexpr base_int_t sm_80_bit = 1 << 12;\n      constexpr base_int_t sm_86_bit = 1 << 13;\n      constexpr base_int_t sm_87_bit = 1 << 14;\n      constexpr base_int_t sm_89_bit = 1 << 15;\n      constexpr base_int_t sm_90_bit = 1 << 16;\n      constexpr base_int_t all_devices =\n          sm_35_bit | sm_37_bit |\n          sm_50_bit | sm_52_bit | sm_53_bit |\n          sm_60_bit | sm_61_bit | sm_62_bit |\n          sm_70_bit | sm_72_bit | sm_75_bit |\n          sm_80_bit | sm_86_bit | sm_87_bit |\n          sm_89_bit | sm_90_bit;\n\n      // Store a set of targets as a set of bits\n      struct _NV_BITSET_ATTRIBUTE target_description {\n        base_int_t targets;\n\n        constexpr target_description(base_int_t a) : targets(a) { }\n      };\n\n      // The type of the user-visible names of the NVIDIA GPU targets\n      enum class sm_selector : base_int_t {\n        sm_35 = 35, sm_37 = 37,\n        sm_50 = 50, sm_52 = 52, sm_53 = 53,\n        sm_60 = 60, sm_61 = 61, sm_62 = 62,\n        sm_70 = 70, sm_72 = 72, sm_75 = 75,\n        sm_80 = 80, sm_86 = 86, sm_87 = 87,\n        sm_89 = 89, sm_90 = 90,\n      };\n\n      constexpr base_int_t toint(sm_selector a) {\n        return static_cast<base_int_t>(a);\n      }\n\n      constexpr base_int_t bitexact(sm_selector a) {\n        return toint(a) == 35 ? sm_35_bit :\n               toint(a) == 37 ? sm_37_bit :\n               toint(a) == 50 ? sm_50_bit :\n               toint(a) == 52 ? sm_52_bit :\n               toint(a) == 53 ? sm_53_bit :\n               toint(a) == 60 ? sm_60_bit :\n               toint(a) == 61 ? sm_61_bit :\n               toint(a) == 62 ? sm_62_bit :\n               toint(a) == 70 ? sm_70_bit :\n               toint(a) == 72 ? sm_72_bit :\n               toint(a) == 75 ? sm_75_bit :\n               toint(a) == 80 ? sm_80_bit :\n               toint(a) == 86 ? sm_86_bit :\n               toint(a) == 87 ? sm_87_bit :\n               toint(a) == 89 ? sm_89_bit :\n               toint(a) == 90 ? sm_90_bit : 0;\n      }\n\n      constexpr base_int_t bitrounddown(sm_selector a) {\n        return toint(a) >= 90 ? sm_90_bit :\n               toint(a) >= 89 ? sm_89_bit :\n               toint(a) >= 87 ? sm_87_bit :\n               toint(a) >= 86 ? sm_86_bit :\n               toint(a) >= 80 ? sm_80_bit :\n               toint(a) >= 75 ? sm_75_bit :\n               toint(a) >= 72 ? sm_72_bit :\n               toint(a) >= 70 ? sm_70_bit :\n               toint(a) >= 62 ? sm_62_bit :\n               toint(a) >= 61 ? sm_61_bit :\n               toint(a) >= 60 ? sm_60_bit :\n               toint(a) >= 53 ? sm_53_bit :\n               toint(a) >= 52 ? sm_52_bit :\n               toint(a) >= 50 ? sm_50_bit :\n               toint(a) >= 37 ? sm_37_bit :\n               toint(a) >= 35 ? sm_35_bit : 0;\n      }\n\n      // Public API for NVIDIA GPUs\n\n      constexpr target_description is_exactly(sm_selector a) {\n        return target_description(bitexact(a));\n      }\n\n      constexpr target_description provides(sm_selector a) {\n        return target_description(~(bitrounddown(a) - 1) & all_devices);\n      }\n\n      // Boolean operations on target sets\n\n      constexpr target_description operator&&(target_description a,\n                                              target_description b) {\n        return target_description(a.targets & b.targets);\n      }\n\n      constexpr target_description operator||(target_description a,\n                                              target_description b) {\n        return target_description(a.targets | b.targets);\n      }\n\n      constexpr target_description operator!(target_description a) {\n        return target_description(~a.targets & (all_devices | all_hosts));\n      }\n    }\n\n    using detail::target_description;\n    using detail::sm_selector;\n\n    // The predicates for basic host/device selection\n    constexpr target_description is_host =\n      target_description(detail::all_hosts);\n    constexpr target_description is_device =\n      target_description(detail::all_devices);\n    constexpr target_description any_target =\n      target_description(detail::all_hosts | detail::all_devices);\n    constexpr target_description no_target =\n      target_description(0);\n\n    // The public names for NVIDIA GPU architectures\n    constexpr sm_selector sm_35 = sm_selector::sm_35;\n    constexpr sm_selector sm_37 = sm_selector::sm_37;\n    constexpr sm_selector sm_50 = sm_selector::sm_50;\n    constexpr sm_selector sm_52 = sm_selector::sm_52;\n    constexpr sm_selector sm_53 = sm_selector::sm_53;\n    constexpr sm_selector sm_60 = sm_selector::sm_60;\n    constexpr sm_selector sm_61 = sm_selector::sm_61;\n    constexpr sm_selector sm_62 = sm_selector::sm_62;\n    constexpr sm_selector sm_70 = sm_selector::sm_70;\n    constexpr sm_selector sm_72 = sm_selector::sm_72;\n    constexpr sm_selector sm_75 = sm_selector::sm_75;\n    constexpr sm_selector sm_80 = sm_selector::sm_80;\n    constexpr sm_selector sm_86 = sm_selector::sm_86;\n    constexpr sm_selector sm_87 = sm_selector::sm_87;\n    constexpr sm_selector sm_89 = sm_selector::sm_89;\n    constexpr sm_selector sm_90 = sm_selector::sm_90;\n\n    using detail::is_exactly;\n    using detail::provides;\n  }\n}\n\n#endif // C++11  && !defined(__CUDACC_RTC__)\n\n#include \"detail/__target_macros\"\n\n#endif // __NV_TARGET_H\n", "ostream": "#ifndef _JITIFY_INCLUDE_GUARD_45DDD0249C92B8D3\n#define _JITIFY_INCLUDE_GUARD_45DDD0249C92B8D3\n#define cudaDeviceSynchronize() cudaSuccess\nnamespace std {\ntemplate<class CharT,class Traits=void>\nstruct basic_ostream {\n};\ntypedef basic_ostream<char> ostream;\nostream& endl(ostream& os);\nostream& operator<<( ostream&, ostream& (*f)( ostream& ) );\ntemplate< class CharT, class Traits > basic_ostream<CharT, Traits>& endl( basic_ostream<CharT, Traits>& os );\ntemplate< class CharT, class Traits > basic_ostream<CharT, Traits>& operator<<( basic_ostream<CharT,Traits>& os, const char* c );\n#if __cplusplus >= 201103L\ntemplate< class CharT, class Traits, class T > basic_ostream<CharT, Traits>& operator<<( basic_ostream<CharT,Traits>&& os, const T& value );\n#endif  // __cplusplus >= 201103L\n}  // namespace std\n\n#endif // _JITIFY_INCLUDE_GUARD_45DDD0249C92B8D3\n", "ratio": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===---------------------------- ratio -----------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX_RATIO\n#define _LIBCUDACXX_RATIO\n\n/*\n    ratio synopsis\n\nnamespace std\n{\n\ntemplate <intmax_t N, intmax_t D = 1>\nclass ratio\n{\npublic:\n    static constexpr intmax_t num;\n    static constexpr intmax_t den;\n    typedef ratio<num, den> type;\n};\n\n// ratio arithmetic\ntemplate <class R1, class R2> using ratio_add = ...;\ntemplate <class R1, class R2> using ratio_subtract = ...;\ntemplate <class R1, class R2> using ratio_multiply = ...;\ntemplate <class R1, class R2> using ratio_divide = ...;\n\n// ratio comparison\ntemplate <class R1, class R2> struct ratio_equal;\ntemplate <class R1, class R2> struct ratio_not_equal;\ntemplate <class R1, class R2> struct ratio_less;\ntemplate <class R1, class R2> struct ratio_less_equal;\ntemplate <class R1, class R2> struct ratio_greater;\ntemplate <class R1, class R2> struct ratio_greater_equal;\n\n// convenience SI typedefs\ntypedef ratio<1, 1000000000000000000000000> yocto;  // not supported\ntypedef ratio<1,    1000000000000000000000> zepto;  // not supported\ntypedef ratio<1,       1000000000000000000> atto;\ntypedef ratio<1,          1000000000000000> femto;\ntypedef ratio<1,             1000000000000> pico;\ntypedef ratio<1,                1000000000> nano;\ntypedef ratio<1,                   1000000> micro;\ntypedef ratio<1,                      1000> milli;\ntypedef ratio<1,                       100> centi;\ntypedef ratio<1,                        10> deci;\ntypedef ratio<                       10, 1> deca;\ntypedef ratio<                      100, 1> hecto;\ntypedef ratio<                     1000, 1> kilo;\ntypedef ratio<                  1000000, 1> mega;\ntypedef ratio<               1000000000, 1> giga;\ntypedef ratio<            1000000000000, 1> tera;\ntypedef ratio<         1000000000000000, 1> peta;\ntypedef ratio<      1000000000000000000, 1> exa;\ntypedef ratio<   1000000000000000000000, 1> zetta;  // not supported\ntypedef ratio<1000000000000000000000000, 1> yotta;  // not supported\n\n  // 20.11.5, ratio comparison\n  template <class R1, class R2> inline constexpr bool ratio_equal_v\n    = ratio_equal<R1, R2>::value;                                       // C++17\n  template <class R1, class R2> inline constexpr bool ratio_not_equal_v\n    = ratio_not_equal<R1, R2>::value;                                   // C++17\n  template <class R1, class R2> inline constexpr bool ratio_less_v\n    = ratio_less<R1, R2>::value;                                        // C++17\n  template <class R1, class R2> inline constexpr bool ratio_less_equal_v\n    = ratio_less_equal<R1, R2>::value;                                  // C++17\n  template <class R1, class R2> inline constexpr bool ratio_greater_v\n    = ratio_greater<R1, R2>::value;                                     // C++17\n  template <class R1, class R2> inline constexpr bool ratio_greater_equal_v\n    = ratio_greater_equal<R1, R2>::value;                               // C++17\n}\n*/\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n#include \"__assert\" // all public C++ headers provide the assertion handler\n#include \"__type_traits/integral_constant.h\"\n#include \"climits\"\n#include \"cstdint\"\n#include \"type_traits\"\n\n#ifndef __cuda_std__\n#include <__pragma_push>\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n_LIBCUDACXX_BEGIN_NAMESPACE_STD\n\n// __static_gcd\n\ntemplate <intmax_t _Xp, intmax_t _Yp>\nstruct __static_gcd\n{\n    static const intmax_t value = __static_gcd<_Yp, _Xp % _Yp>::value;\n};\n\ntemplate <intmax_t _Xp>\nstruct __static_gcd<_Xp, 0>\n{\n    static const intmax_t value = _Xp;\n};\n\ntemplate <>\nstruct __static_gcd<0, 0>\n{\n    static const intmax_t value = 1;\n};\n\n// __static_lcm\n\ntemplate <intmax_t _Xp, intmax_t _Yp>\nstruct __static_lcm\n{\n    static const intmax_t value = _Xp / __static_gcd<_Xp, _Yp>::value * _Yp;\n};\n\ntemplate <intmax_t _Xp>\nstruct __static_abs\n{\n    static const intmax_t value = _Xp < 0 ? -_Xp : _Xp;\n};\n\ntemplate <intmax_t _Xp>\nstruct __static_sign\n{\n    static const intmax_t value = _Xp == 0 ? 0 : (_Xp < 0 ? -1 : 1);\n};\n\ntemplate <intmax_t _Xp, intmax_t _Yp, intmax_t = __static_sign<_Yp>::value>\nclass __ll_add;\n\ntemplate <intmax_t _Xp, intmax_t _Yp>\nclass __ll_add<_Xp, _Yp, 1>\n{\n    static const intmax_t min = (1LL << (sizeof(intmax_t) * CHAR_BIT - 1)) + 1;\n    static const intmax_t max = -min;\n\n    static_assert(_Xp <= max - _Yp, \"overflow in __ll_add\");\npublic:\n    static const intmax_t value = _Xp + _Yp;\n};\n\ntemplate <intmax_t _Xp, intmax_t _Yp>\nclass __ll_add<_Xp, _Yp, 0>\n{\npublic:\n    static const intmax_t value = _Xp;\n};\n\ntemplate <intmax_t _Xp, intmax_t _Yp>\nclass __ll_add<_Xp, _Yp, -1>\n{\n    static const intmax_t min = (1LL << (sizeof(intmax_t) * CHAR_BIT - 1)) + 1;\n    static const intmax_t max = -min;\n\n    static_assert(min - _Yp <= _Xp, \"overflow in __ll_add\");\npublic:\n    static const intmax_t value = _Xp + _Yp;\n};\n\ntemplate <intmax_t _Xp, intmax_t _Yp, intmax_t = __static_sign<_Yp>::value>\nclass __ll_sub;\n\ntemplate <intmax_t _Xp, intmax_t _Yp>\nclass __ll_sub<_Xp, _Yp, 1>\n{\n    static const intmax_t min = (1LL << (sizeof(intmax_t) * CHAR_BIT - 1)) + 1;\n    static const intmax_t max = -min;\n\n    static_assert(min + _Yp <= _Xp, \"overflow in __ll_sub\");\npublic:\n    static const intmax_t value = _Xp - _Yp;\n};\n\ntemplate <intmax_t _Xp, intmax_t _Yp>\nclass __ll_sub<_Xp, _Yp, 0>\n{\npublic:\n    static const intmax_t value = _Xp;\n};\n\ntemplate <intmax_t _Xp, intmax_t _Yp>\nclass __ll_sub<_Xp, _Yp, -1>\n{\n    static const intmax_t min = (1LL << (sizeof(intmax_t) * CHAR_BIT - 1)) + 1;\n    static const intmax_t max = -min;\n\n    static_assert(_Xp <= max + _Yp, \"overflow in __ll_sub\");\npublic:\n    static const intmax_t value = _Xp - _Yp;\n};\n\ntemplate <intmax_t _Xp, intmax_t _Yp>\nclass __ll_mul\n{\n    static const intmax_t nan = (1LL << (sizeof(intmax_t) * CHAR_BIT - 1));\n    static const intmax_t min = nan + 1;\n    static const intmax_t max = -min;\n    static const intmax_t __a_x = __static_abs<_Xp>::value;\n    static const intmax_t __a_y = __static_abs<_Yp>::value;\n\n    static_assert(_Xp != nan && _Yp != nan && __a_x <= max / __a_y, \"overflow in __ll_mul\");\npublic:\n    static const intmax_t value = _Xp * _Yp;\n};\n\ntemplate <intmax_t _Yp>\nclass __ll_mul<0, _Yp>\n{\npublic:\n    static const intmax_t value = 0;\n};\n\ntemplate <intmax_t _Xp>\nclass __ll_mul<_Xp, 0>\n{\npublic:\n    static const intmax_t value = 0;\n};\n\ntemplate <>\nclass __ll_mul<0, 0>\n{\npublic:\n    static const intmax_t value = 0;\n};\n\n// Not actually used but left here in case needed in future maintenance\ntemplate <intmax_t _Xp, intmax_t _Yp>\nclass __ll_div\n{\n    static const intmax_t nan = (1LL << (sizeof(intmax_t) * CHAR_BIT - 1));\n    static const intmax_t min = nan + 1;\n    static const intmax_t max = -min;\n\n    static_assert(_Xp != nan && _Yp != nan && _Yp != 0, \"overflow in __ll_div\");\npublic:\n    static const intmax_t value = _Xp / _Yp;\n};\n\ntemplate <intmax_t _Num, intmax_t _Den = 1>\nclass _LIBCUDACXX_TEMPLATE_VIS ratio\n{\n    static_assert(__static_abs<_Num>::value >= 0, \"ratio numerator is out of range\");\n    static_assert(_Den != 0, \"ratio divide by 0\");\n    static_assert(__static_abs<_Den>::value >  0, \"ratio denominator is out of range\");\n    static constexpr intmax_t __na = __static_abs<_Num>::value;\n    static constexpr intmax_t __da = __static_abs<_Den>::value;\n    static constexpr intmax_t __s = __static_sign<_Num>::value * __static_sign<_Den>::value;\n    static constexpr intmax_t __gcd = __static_gcd<__na, __da>::value;\npublic:\n    static constexpr intmax_t num = __s * __na / __gcd;\n    static constexpr intmax_t den = __da / __gcd;\n\n    typedef ratio<num, den> type;\n};\n\ntemplate <intmax_t _Num, intmax_t _Den>\nconstexpr intmax_t ratio<_Num, _Den>::num;\n\ntemplate <intmax_t _Num, intmax_t _Den>\nconstexpr intmax_t ratio<_Num, _Den>::den;\n\ntemplate <class _Tp>                    struct __is_ratio                     : false_type {};\ntemplate <intmax_t _Num, intmax_t _Den> struct __is_ratio<ratio<_Num, _Den> > : true_type  {};\n\ntypedef ratio<1LL, 1000000000000000000LL> atto;\ntypedef ratio<1LL,    1000000000000000LL> femto;\ntypedef ratio<1LL,       1000000000000LL> pico;\ntypedef ratio<1LL,          1000000000LL> nano;\ntypedef ratio<1LL,             1000000LL> micro;\ntypedef ratio<1LL,                1000LL> milli;\ntypedef ratio<1LL,                 100LL> centi;\ntypedef ratio<1LL,                  10LL> deci;\ntypedef ratio<                 10LL, 1LL> deca;\ntypedef ratio<                100LL, 1LL> hecto;\ntypedef ratio<               1000LL, 1LL> kilo;\ntypedef ratio<            1000000LL, 1LL> mega;\ntypedef ratio<         1000000000LL, 1LL> giga;\ntypedef ratio<      1000000000000LL, 1LL> tera;\ntypedef ratio<   1000000000000000LL, 1LL> peta;\ntypedef ratio<1000000000000000000LL, 1LL> exa;\n\ntemplate <class _R1, class _R2>\nstruct __ratio_multiply\n{\n//private:\n    static const intmax_t __gcd_n1_d2 = __static_gcd<_R1::num, _R2::den>::value;\n    static const intmax_t __gcd_d1_n2 = __static_gcd<_R1::den, _R2::num>::value;\npublic:\n    typedef typename ratio\n        <\n            __ll_mul<_R1::num / __gcd_n1_d2, _R2::num / __gcd_d1_n2>::value,\n            __ll_mul<_R2::den / __gcd_n1_d2, _R1::den / __gcd_d1_n2>::value\n        >::type type;\n};\n\ntemplate <class _R1, class _R2> using ratio_multiply\n                                    = typename __ratio_multiply<_R1, _R2>::type;\n\ntemplate <class _R1, class _R2>\nstruct __ratio_divide\n{\n//private:\n    static const intmax_t __gcd_n1_n2 = __static_gcd<_R1::num, _R2::num>::value;\n    static const intmax_t __gcd_d1_d2 = __static_gcd<_R1::den, _R2::den>::value;\npublic:\n    typedef typename ratio\n        <\n            __ll_mul<_R1::num / __gcd_n1_n2, _R2::den / __gcd_d1_d2>::value,\n            __ll_mul<_R2::num / __gcd_n1_n2, _R1::den / __gcd_d1_d2>::value\n        >::type type;\n};\n\ntemplate <class _R1, class _R2> using ratio_divide\n                                      = typename __ratio_divide<_R1, _R2>::type;\n\ntemplate <class _R1, class _R2>\nstruct __ratio_add\n{\n//private:\n    static const intmax_t __gcd_n1_n2 = __static_gcd<_R1::num, _R2::num>::value;\n    static const intmax_t __gcd_d1_d2 = __static_gcd<_R1::den, _R2::den>::value;\npublic:\n    typedef typename ratio_multiply\n        <\n            ratio<__gcd_n1_n2, _R1::den / __gcd_d1_d2>,\n            ratio\n            <\n                __ll_add\n                <\n                    __ll_mul<_R1::num / __gcd_n1_n2, _R2::den / __gcd_d1_d2>::value,\n                    __ll_mul<_R2::num / __gcd_n1_n2, _R1::den / __gcd_d1_d2>::value\n                >::value,\n                _R2::den\n            >\n        >::type type;\n};\n\ntemplate <class _R1, class _R2> using ratio_add\n                                         = typename __ratio_add<_R1, _R2>::type;\n\ntemplate <class _R1, class _R2>\nstruct __ratio_subtract\n{\n//private:\n    static const intmax_t __gcd_n1_n2 = __static_gcd<_R1::num, _R2::num>::value;\n    static const intmax_t __gcd_d1_d2 = __static_gcd<_R1::den, _R2::den>::value;\npublic:\n    typedef typename ratio_multiply\n        <\n            ratio<__gcd_n1_n2, _R1::den / __gcd_d1_d2>,\n            ratio\n            <\n                __ll_sub\n                <\n                    __ll_mul<_R1::num / __gcd_n1_n2, _R2::den / __gcd_d1_d2>::value,\n                    __ll_mul<_R2::num / __gcd_n1_n2, _R1::den / __gcd_d1_d2>::value\n                >::value,\n                _R2::den\n            >\n        >::type type;\n};\n\ntemplate <class _R1, class _R2> using ratio_subtract\n                                    = typename __ratio_subtract<_R1, _R2>::type;\n\n// ratio_equal\n\ntemplate <class _R1, class _R2>\nstruct _LIBCUDACXX_TEMPLATE_VIS ratio_equal\n    : public _LIBCUDACXX_BOOL_CONSTANT((_R1::num == _R2::num && _R1::den == _R2::den)) {};\n\ntemplate <class _R1, class _R2>\nstruct _LIBCUDACXX_TEMPLATE_VIS ratio_not_equal\n    : public _LIBCUDACXX_BOOL_CONSTANT((!ratio_equal<_R1, _R2>::value)) {};\n\n// ratio_less\n\ntemplate <class _R1, class _R2, bool _Odd = false,\n          intmax_t _Q1 = _R1::num / _R1::den, intmax_t _M1 = _R1::num % _R1::den,\n          intmax_t _Q2 = _R2::num / _R2::den, intmax_t _M2 = _R2::num % _R2::den>\nstruct __ratio_less1\n{\n    static const bool value = _Odd ? _Q2 < _Q1 : _Q1 < _Q2;\n};\n\ntemplate <class _R1, class _R2, bool _Odd, intmax_t _Qp>\nstruct __ratio_less1<_R1, _R2, _Odd, _Qp, 0, _Qp, 0>\n{\n    static const bool value = false;\n};\n\ntemplate <class _R1, class _R2, bool _Odd, intmax_t _Qp, intmax_t _M2>\nstruct __ratio_less1<_R1, _R2, _Odd, _Qp, 0, _Qp, _M2>\n{\n    static const bool value = !_Odd;\n};\n\ntemplate <class _R1, class _R2, bool _Odd, intmax_t _Qp, intmax_t _M1>\nstruct __ratio_less1<_R1, _R2, _Odd, _Qp, _M1, _Qp, 0>\n{\n    static const bool value = _Odd;\n};\n\ntemplate <class _R1, class _R2, bool _Odd, intmax_t _Qp, intmax_t _M1,\n                                                        intmax_t _M2>\nstruct __ratio_less1<_R1, _R2, _Odd, _Qp, _M1, _Qp, _M2>\n{\n    static const bool value = __ratio_less1<ratio<_R1::den, _M1>,\n                                            ratio<_R2::den, _M2>, !_Odd>::value;\n};\n\ntemplate <class _R1, class _R2, intmax_t _S1 = __static_sign<_R1::num>::value,\n                                intmax_t _S2 = __static_sign<_R2::num>::value>\nstruct __ratio_less\n{\n    static const bool value = _S1 < _S2;\n};\n\ntemplate <class _R1, class _R2>\nstruct __ratio_less<_R1, _R2, 1LL, 1LL>\n{\n    static const bool value = __ratio_less1<_R1, _R2>::value;\n};\n\ntemplate <class _R1, class _R2>\nstruct __ratio_less<_R1, _R2, -1LL, -1LL>\n{\n    static const bool value = __ratio_less1<ratio<-_R2::num, _R2::den>, ratio<-_R1::num, _R1::den> >::value;\n};\n\ntemplate <class _R1, class _R2>\nstruct _LIBCUDACXX_TEMPLATE_VIS ratio_less\n    : public _LIBCUDACXX_BOOL_CONSTANT((__ratio_less<_R1, _R2>::value)) {};\n\ntemplate <class _R1, class _R2>\nstruct _LIBCUDACXX_TEMPLATE_VIS ratio_less_equal\n    : public _LIBCUDACXX_BOOL_CONSTANT((!ratio_less<_R2, _R1>::value)) {};\n\ntemplate <class _R1, class _R2>\nstruct _LIBCUDACXX_TEMPLATE_VIS ratio_greater\n    : public _LIBCUDACXX_BOOL_CONSTANT((ratio_less<_R2, _R1>::value)) {};\n\ntemplate <class _R1, class _R2>\nstruct _LIBCUDACXX_TEMPLATE_VIS ratio_greater_equal\n    : public _LIBCUDACXX_BOOL_CONSTANT((!ratio_less<_R1, _R2>::value)) {};\n\ntemplate <class _R1, class _R2>\nstruct __ratio_gcd\n{\n    typedef ratio<__static_gcd<_R1::num, _R2::num>::value,\n                  __static_lcm<_R1::den, _R2::den>::value> type;\n};\n\n#if _LIBCUDACXX_STD_VER > 14 && !defined(_LIBCUDACXX_HAS_NO_VARIABLE_TEMPLATES)\ntemplate <class _R1, class _R2>\n_LIBCUDACXX_INLINE_VAR constexpr bool ratio_equal_v\n    = ratio_equal<_R1, _R2>::value;\n\ntemplate <class _R1, class _R2>\n_LIBCUDACXX_INLINE_VAR constexpr bool ratio_not_equal_v\n    = ratio_not_equal<_R1, _R2>::value;\n\ntemplate <class _R1, class _R2>\n_LIBCUDACXX_INLINE_VAR constexpr bool ratio_less_v\n    = ratio_less<_R1, _R2>::value;\n\ntemplate <class _R1, class _R2>\n_LIBCUDACXX_INLINE_VAR constexpr bool ratio_less_equal_v\n    = ratio_less_equal<_R1, _R2>::value;\n\ntemplate <class _R1, class _R2>\n_LIBCUDACXX_INLINE_VAR constexpr bool ratio_greater_v\n    = ratio_greater<_R1, _R2>::value;\n\ntemplate <class _R1, class _R2>\n_LIBCUDACXX_INLINE_VAR constexpr bool ratio_greater_equal_v\n    = ratio_greater_equal<_R1, _R2>::value;\n#endif\n\n_LIBCUDACXX_END_NAMESPACE_STD\n\n#ifndef __cuda_std__\n#include <__pragma_pop>\n#endif //__cuda_std__\n\n#endif  // _LIBCUDACXX_RATIO\n", "specializations/block_reduce_raking.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_D0D85A453EB36817\n#define _JITIFY_INCLUDE_GUARD_D0D85A453EB36817\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.\n * \n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n * \n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * cub::BlockReduceRaking provides raking-based methods of parallel reduction across a CUDA thread block.  Supports non-commutative reduction operators.\n */\n\n#include \"../../block/block_raking_layout.cuh\"\n#include \"../../warp/warp_reduce.cuh\"\n#include \"../../thread/thread_reduce.cuh\"\n#include \"../../config.cuh\"\n#include \"../../util_ptx.cuh\"\n\nCUB_NAMESPACE_BEGIN\n\n\n/**\n * \\brief BlockReduceRaking provides raking-based methods of parallel reduction across a CUDA thread block.  Supports non-commutative reduction operators.\n *\n * Supports non-commutative binary reduction operators.  Unlike commutative\n * reduction operators (e.g., addition), the application of a non-commutative\n * reduction operator (e.g, string concatenation) across a sequence of inputs must\n * honor the relative ordering of items and partial reductions when applying the\n * reduction operator.\n *\n * Compared to the implementation of BlockReduceRakingCommutativeOnly (which\n * does not support non-commutative operators), this implementation requires a\n * few extra rounds of inter-thread communication.\n */\ntemplate <\n    typename    T,              ///< Data type being reduced\n    int         BLOCK_DIM_X,    ///< The thread block length in threads along the X dimension\n    int         BLOCK_DIM_Y,    ///< The thread block length in threads along the Y dimension\n    int         BLOCK_DIM_Z,    ///< The thread block length in threads along the Z dimension\n    int         LEGACY_PTX_ARCH = 0> ///< The PTX compute capability for which to to specialize this collective\nstruct BlockReduceRaking\n{\n    /// Constants\n    enum\n    {\n        /// The thread block size in threads\n        BLOCK_THREADS = BLOCK_DIM_X * BLOCK_DIM_Y * BLOCK_DIM_Z,\n    };\n\n    /// Layout type for padded thread block raking grid\n    typedef BlockRakingLayout<T, BLOCK_THREADS> BlockRakingLayout;\n\n    ///  WarpReduce utility type\n    typedef typename WarpReduce<T, BlockRakingLayout::RAKING_THREADS>::InternalWarpReduce WarpReduce;\n\n    /// Constants\n    enum\n    {\n        /// Number of raking threads\n        RAKING_THREADS = BlockRakingLayout::RAKING_THREADS,\n\n        /// Number of raking elements per warp synchronous raking thread\n        SEGMENT_LENGTH = BlockRakingLayout::SEGMENT_LENGTH,\n\n        /// Cooperative work can be entirely warp synchronous\n        WARP_SYNCHRONOUS = (int(RAKING_THREADS) == int(BLOCK_THREADS)),\n\n        /// Whether or not warp-synchronous reduction should be unguarded (i.e., the warp-reduction elements is a power of two\n        WARP_SYNCHRONOUS_UNGUARDED = PowerOfTwo<RAKING_THREADS>::VALUE,\n\n        /// Whether or not accesses into smem are unguarded\n        RAKING_UNGUARDED = BlockRakingLayout::UNGUARDED,\n\n    };\n\n\n    /// Shared memory storage layout type\n    union _TempStorage\n    {\n        typename WarpReduce::TempStorage            warp_storage;        ///< Storage for warp-synchronous reduction\n        typename BlockRakingLayout::TempStorage     raking_grid;         ///< Padded thread block raking grid\n    };\n\n\n    /// Alias wrapper allowing storage to be unioned\n    struct TempStorage : Uninitialized<_TempStorage> {};\n\n\n    // Thread fields\n    _TempStorage &temp_storage;\n    unsigned int linear_tid;\n\n\n    /// Constructor\n    __device__ __forceinline__ BlockReduceRaking(\n        TempStorage &temp_storage)\n    :\n        temp_storage(temp_storage.Alias()),\n        linear_tid(RowMajorTid(BLOCK_DIM_X, BLOCK_DIM_Y, BLOCK_DIM_Z))\n    {}\n\n\n    template <bool IS_FULL_TILE, typename ReductionOp, int ITERATION>\n    __device__ __forceinline__ T RakingReduction(\n        ReductionOp                 reduction_op,       ///< [in] Binary reduction operator\n        T                           *raking_segment,\n        T                           partial,            ///< [in] <b>[<em>lane</em><sub>0</sub> only]</b> Warp-wide aggregate reduction of input items\n        int                         num_valid,          ///< [in] Number of valid elements (may be less than BLOCK_THREADS)\n        Int2Type<ITERATION>         /*iteration*/)\n    {\n        // Update partial if addend is in range\n        if ((IS_FULL_TILE && RAKING_UNGUARDED) || ((linear_tid * SEGMENT_LENGTH) + ITERATION < num_valid))\n        {\n            T addend = raking_segment[ITERATION];\n            partial = reduction_op(partial, addend);\n        }\n        return RakingReduction<IS_FULL_TILE>(reduction_op, raking_segment, partial, num_valid, Int2Type<ITERATION + 1>());\n    }\n\n    template <bool IS_FULL_TILE, typename ReductionOp>\n    __device__ __forceinline__ T RakingReduction(\n        ReductionOp                 /*reduction_op*/,   ///< [in] Binary reduction operator\n        T                           * /*raking_segment*/,\n        T                           partial,            ///< [in] <b>[<em>lane</em><sub>0</sub> only]</b> Warp-wide aggregate reduction of input items\n        int                         /*num_valid*/,      ///< [in] Number of valid elements (may be less than BLOCK_THREADS)\n        Int2Type<SEGMENT_LENGTH>    /*iteration*/)\n    {\n        return partial;\n    }\n\n\n\n    /// Computes a thread block-wide reduction using the specified reduction operator. The first num_valid threads each contribute one reduction partial.  The return value is only valid for thread<sub>0</sub>.\n    template <\n        bool                IS_FULL_TILE,\n        typename            ReductionOp>\n    __device__ __forceinline__ T Reduce(\n        T                   partial,            ///< [in] Calling thread's input partial reductions\n        int                 num_valid,          ///< [in] Number of valid elements (may be less than BLOCK_THREADS)\n        ReductionOp         reduction_op)       ///< [in] Binary reduction operator\n    {\n        if (WARP_SYNCHRONOUS)\n        {\n            // Short-circuit directly to warp synchronous reduction (unguarded if active threads is a power-of-two)\n            partial = WarpReduce(temp_storage.warp_storage).template Reduce<IS_FULL_TILE>(\n                partial,\n                num_valid,\n                reduction_op);\n        }\n        else\n        {\n            // Place partial into shared memory grid.\n            *BlockRakingLayout::PlacementPtr(temp_storage.raking_grid, linear_tid) = partial;\n\n            CTA_SYNC();\n\n            // Reduce parallelism to one warp\n            if (linear_tid < RAKING_THREADS)\n            {\n                // Raking reduction in grid\n                T *raking_segment = BlockRakingLayout::RakingPtr(temp_storage.raking_grid, linear_tid);\n                partial = raking_segment[0];\n\n                partial = RakingReduction<IS_FULL_TILE>(reduction_op, raking_segment, partial, num_valid, Int2Type<1>());\n\n                int valid_raking_threads = (IS_FULL_TILE) ?\n                    RAKING_THREADS :\n                    (num_valid + SEGMENT_LENGTH - 1) / SEGMENT_LENGTH;\n\n                partial = WarpReduce(temp_storage.warp_storage).template Reduce<IS_FULL_TILE && RAKING_UNGUARDED>(\n                    partial,\n                    valid_raking_threads,\n                    reduction_op);\n\n            }\n        }\n\n        return partial;\n    }\n\n\n    /// Computes a thread block-wide reduction using addition (+) as the reduction operator. The first num_valid threads each contribute one reduction partial.  The return value is only valid for thread<sub>0</sub>.\n    template <bool IS_FULL_TILE>\n    __device__ __forceinline__ T Sum(\n        T                   partial,            ///< [in] Calling thread's input partial reductions\n        int                 num_valid)          ///< [in] Number of valid elements (may be less than BLOCK_THREADS)\n    {\n        cub::Sum reduction_op;\n\n        return Reduce<IS_FULL_TILE>(partial, num_valid, reduction_op);\n    }\n\n\n\n};\n\nCUB_NAMESPACE_END\n\n\n#endif // _JITIFY_INCLUDE_GUARD_D0D85A453EB36817\n", "specializations/block_reduce_raking_commutative_only.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_6D8CA3BB4FFF4ACA\n#define _JITIFY_INCLUDE_GUARD_6D8CA3BB4FFF4ACA\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.\n * \n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n * \n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * cub::BlockReduceRakingCommutativeOnly provides raking-based methods of parallel reduction across a CUDA thread block.  Does not support non-commutative reduction operators.\n */\n\n#include \"block_reduce_raking.cuh\"\n#include \"../../warp/warp_reduce.cuh\"\n#include \"../../thread/thread_reduce.cuh\"\n#include \"../../config.cuh\"\n#include \"../../util_ptx.cuh\"\n\nCUB_NAMESPACE_BEGIN\n\n\n/**\n * \\brief BlockReduceRakingCommutativeOnly provides raking-based methods of parallel reduction across a CUDA thread block.  Does not support non-commutative reduction operators.  Does not support block sizes that are not a multiple of the warp size.\n */\ntemplate <\n    typename    T,              ///< Data type being reduced\n    int         BLOCK_DIM_X,    ///< The thread block length in threads along the X dimension\n    int         BLOCK_DIM_Y,    ///< The thread block length in threads along the Y dimension\n    int         BLOCK_DIM_Z,    ///< The thread block length in threads along the Z dimension\n    int         LEGACY_PTX_ARCH = 0> ///< The PTX compute capability for which to to specialize this collective\nstruct BlockReduceRakingCommutativeOnly\n{\n    /// Constants\n    enum\n    {\n        /// The thread block size in threads\n        BLOCK_THREADS = BLOCK_DIM_X * BLOCK_DIM_Y * BLOCK_DIM_Z,\n    };\n\n    // The fall-back implementation to use when BLOCK_THREADS is not a multiple of the warp size or not all threads have valid values\n    typedef BlockReduceRaking<T, BLOCK_DIM_X, BLOCK_DIM_Y, BLOCK_DIM_Z> FallBack;\n\n    /// Constants\n    enum\n    {\n        /// Number of warp threads\n        WARP_THREADS = CUB_WARP_THREADS(0),\n\n        /// Whether or not to use fall-back\n        USE_FALLBACK = ((BLOCK_THREADS % WARP_THREADS != 0) || (BLOCK_THREADS <= WARP_THREADS)),\n\n        /// Number of raking threads\n        RAKING_THREADS = WARP_THREADS,\n\n        /// Number of threads actually sharing items with the raking threads\n        SHARING_THREADS = CUB_MAX(1, BLOCK_THREADS - RAKING_THREADS),\n\n        /// Number of raking elements per warp synchronous raking thread\n        SEGMENT_LENGTH = SHARING_THREADS / WARP_THREADS,\n    };\n\n    ///  WarpReduce utility type\n    typedef WarpReduce<T, RAKING_THREADS> WarpReduce;\n\n    /// Layout type for padded thread block raking grid\n    typedef BlockRakingLayout<T, SHARING_THREADS> BlockRakingLayout;\n\n    /// Shared memory storage layout type\n    union _TempStorage\n    {\n        struct DefaultStorage\n        {\n            typename WarpReduce::TempStorage        warp_storage;        ///< Storage for warp-synchronous reduction\n            typename BlockRakingLayout::TempStorage raking_grid;         ///< Padded thread block raking grid\n        } default_storage;\n\n        typename FallBack::TempStorage              fallback_storage;    ///< Fall-back storage for non-commutative block reduction\n    };\n\n\n    /// Alias wrapper allowing storage to be unioned\n    struct TempStorage : Uninitialized<_TempStorage> {};\n\n\n    // Thread fields\n    _TempStorage &temp_storage;\n    unsigned int linear_tid;\n\n\n    /// Constructor\n    __device__ __forceinline__ BlockReduceRakingCommutativeOnly(\n        TempStorage &temp_storage)\n    :\n        temp_storage(temp_storage.Alias()),\n        linear_tid(RowMajorTid(BLOCK_DIM_X, BLOCK_DIM_Y, BLOCK_DIM_Z))\n    {}\n\n\n    /// Computes a thread block-wide reduction using addition (+) as the reduction operator. The first num_valid threads each contribute one reduction partial.  The return value is only valid for thread<sub>0</sub>.\n    template <bool FULL_TILE>\n    __device__ __forceinline__ T Sum(\n        T                   partial,            ///< [in] Calling thread's input partial reductions\n        int                 num_valid)          ///< [in] Number of valid elements (may be less than BLOCK_THREADS)\n    {\n        if (USE_FALLBACK || !FULL_TILE)\n        {\n            return FallBack(temp_storage.fallback_storage).template Sum<FULL_TILE>(partial, num_valid);\n        }\n        else\n        {\n            // Place partial into shared memory grid\n            if (linear_tid >= RAKING_THREADS)\n                *BlockRakingLayout::PlacementPtr(temp_storage.default_storage.raking_grid, linear_tid - RAKING_THREADS) = partial;\n\n            CTA_SYNC();\n\n            // Reduce parallelism to one warp\n            if (linear_tid < RAKING_THREADS)\n            {\n                // Raking reduction in grid\n                T *raking_segment = BlockRakingLayout::RakingPtr(temp_storage.default_storage.raking_grid, linear_tid);\n                partial = internal::ThreadReduce<SEGMENT_LENGTH>(raking_segment, cub::Sum(), partial);\n\n                // Warp reduction\n                partial = WarpReduce(temp_storage.default_storage.warp_storage).Sum(partial);\n            }\n        }\n\n        return partial;\n    }\n\n\n    /// Computes a thread block-wide reduction using the specified reduction operator. The first num_valid threads each contribute one reduction partial.  The return value is only valid for thread<sub>0</sub>.\n    template <\n        bool                FULL_TILE,\n        typename            ReductionOp>\n    __device__ __forceinline__ T Reduce(\n        T                   partial,            ///< [in] Calling thread's input partial reductions\n        int                 num_valid,          ///< [in] Number of valid elements (may be less than BLOCK_THREADS)\n        ReductionOp         reduction_op)       ///< [in] Binary reduction operator\n    {\n        if (USE_FALLBACK || !FULL_TILE)\n        {\n            return FallBack(temp_storage.fallback_storage).template Reduce<FULL_TILE>(partial, num_valid, reduction_op);\n        }\n        else\n        {\n            // Place partial into shared memory grid\n            if (linear_tid >= RAKING_THREADS)\n                *BlockRakingLayout::PlacementPtr(temp_storage.default_storage.raking_grid, linear_tid - RAKING_THREADS) = partial;\n\n            CTA_SYNC();\n\n            // Reduce parallelism to one warp\n            if (linear_tid < RAKING_THREADS)\n            {\n                // Raking reduction in grid\n                T *raking_segment = BlockRakingLayout::RakingPtr(temp_storage.default_storage.raking_grid, linear_tid);\n                partial = internal::ThreadReduce<SEGMENT_LENGTH>(raking_segment, reduction_op, partial);\n\n                // Warp reduction\n                partial = WarpReduce(temp_storage.default_storage.warp_storage).Reduce(partial, reduction_op);\n            }\n        }\n\n        return partial;\n    }\n\n};\n\nCUB_NAMESPACE_END\n\n\n#endif // _JITIFY_INCLUDE_GUARD_6D8CA3BB4FFF4ACA\n", "specializations/block_reduce_warp_reductions.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_DE0753A67AE85DA8\n#define _JITIFY_INCLUDE_GUARD_DE0753A67AE85DA8\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.\n * \n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n * \n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * cub::BlockReduceWarpReductions provides variants of warp-reduction-based parallel reduction across a CUDA thread block.  Supports non-commutative reduction operators.\n */\n\n#include <cub/config.cuh>\n#include <cub/detail/uninitialized_copy.cuh>\n#include <cub/util_ptx.cuh>\n#include <cub/warp/warp_reduce.cuh>\n\nCUB_NAMESPACE_BEGIN\n\n\n/**\n * \\brief BlockReduceWarpReductions provides variants of warp-reduction-based parallel reduction across a CUDA thread block.  Supports non-commutative reduction operators.\n */\ntemplate <\n    typename    T,              ///< Data type being reduced\n    int         BLOCK_DIM_X,    ///< The thread block length in threads along the X dimension\n    int         BLOCK_DIM_Y,    ///< The thread block length in threads along the Y dimension\n    int         BLOCK_DIM_Z,    ///< The thread block length in threads along the Z dimension\n    int         LEGACY_PTX_ARCH = 0> ///< The PTX compute capability for which to to specialize this collective\nstruct BlockReduceWarpReductions\n{\n    /// Constants\n    enum\n    {\n        /// The thread block size in threads\n        BLOCK_THREADS = BLOCK_DIM_X * BLOCK_DIM_Y * BLOCK_DIM_Z,\n\n        /// Number of warp threads\n        WARP_THREADS = CUB_WARP_THREADS(0),\n\n        /// Number of active warps\n        WARPS = (BLOCK_THREADS + WARP_THREADS - 1) / WARP_THREADS,\n\n        /// The logical warp size for warp reductions\n        LOGICAL_WARP_SIZE = CUB_MIN(BLOCK_THREADS, WARP_THREADS),\n\n        /// Whether or not the logical warp size evenly divides the thread block size\n        EVEN_WARP_MULTIPLE = (BLOCK_THREADS % LOGICAL_WARP_SIZE == 0)\n    };\n\n\n    ///  WarpReduce utility type\n    typedef typename WarpReduce<T, LOGICAL_WARP_SIZE>::InternalWarpReduce WarpReduce;\n\n\n    /// Shared memory storage layout type\n    struct _TempStorage\n    {\n        typename WarpReduce::TempStorage    warp_reduce[WARPS];         ///< Buffer for warp-synchronous reduction\n        T                                   warp_aggregates[WARPS];     ///< Shared totals from each warp-synchronous reduction\n        T                                   block_prefix;               ///< Shared prefix for the entire thread block\n    };\n\n    /// Alias wrapper allowing storage to be unioned\n    struct TempStorage : Uninitialized<_TempStorage> {};\n\n\n    // Thread fields\n    _TempStorage &temp_storage;\n    int linear_tid;\n    int warp_id;\n    int lane_id;\n\n\n    /// Constructor\n    __device__ __forceinline__ BlockReduceWarpReductions(\n        TempStorage &temp_storage)\n    :\n        temp_storage(temp_storage.Alias()),\n        linear_tid(RowMajorTid(BLOCK_DIM_X, BLOCK_DIM_Y, BLOCK_DIM_Z)),\n        warp_id((WARPS == 1) ? 0 : linear_tid / WARP_THREADS),\n        lane_id(LaneId())\n    {}\n\n\n    template <bool FULL_TILE, typename ReductionOp, int SUCCESSOR_WARP>\n    __device__ __forceinline__ T ApplyWarpAggregates(\n        ReductionOp                 reduction_op,       ///< [in] Binary reduction operator\n        T                           warp_aggregate,     ///< [in] <b>[<em>lane</em><sub>0</sub> only]</b> Warp-wide aggregate reduction of input items\n        int                         num_valid,          ///< [in] Number of valid elements (may be less than BLOCK_THREADS)\n        Int2Type<SUCCESSOR_WARP>    /*successor_warp*/)\n    {\n        if (FULL_TILE || (SUCCESSOR_WARP * LOGICAL_WARP_SIZE < num_valid))\n        {\n            T addend = temp_storage.warp_aggregates[SUCCESSOR_WARP];\n            warp_aggregate = reduction_op(warp_aggregate, addend);\n        }\n        return ApplyWarpAggregates<FULL_TILE>(reduction_op, warp_aggregate, num_valid, Int2Type<SUCCESSOR_WARP + 1>());\n    }\n\n    template <bool FULL_TILE, typename ReductionOp>\n    __device__ __forceinline__ T ApplyWarpAggregates(\n        ReductionOp         /*reduction_op*/,   ///< [in] Binary reduction operator\n        T                   warp_aggregate,     ///< [in] <b>[<em>lane</em><sub>0</sub> only]</b> Warp-wide aggregate reduction of input items\n        int                 /*num_valid*/,      ///< [in] Number of valid elements (may be less than BLOCK_THREADS)\n        Int2Type<WARPS>     /*successor_warp*/)\n    {\n        return warp_aggregate;\n    }\n\n\n    /// Returns block-wide aggregate in <em>thread</em><sub>0</sub>.\n    template <\n        bool                FULL_TILE,\n        typename            ReductionOp>\n    __device__ __forceinline__ T ApplyWarpAggregates(\n        ReductionOp         reduction_op,       ///< [in] Binary reduction operator\n        T                   warp_aggregate,     ///< [in] <b>[<em>lane</em><sub>0</sub> only]</b> Warp-wide aggregate reduction of input items\n        int                 num_valid)          ///< [in] Number of valid elements (may be less than BLOCK_THREADS)\n    {\n        // Share lane aggregates\n        if (lane_id == 0)\n        {\n          detail::uninitialized_copy(temp_storage.warp_aggregates + warp_id,\n                                     warp_aggregate);\n        }\n\n        CTA_SYNC();\n\n        // Update total aggregate in warp 0, lane 0\n        if (linear_tid == 0)\n        {\n            warp_aggregate = ApplyWarpAggregates<FULL_TILE>(reduction_op, warp_aggregate, num_valid, Int2Type<1>());\n        }\n\n        return warp_aggregate;\n    }\n\n\n    /// Computes a thread block-wide reduction using addition (+) as the reduction operator. The first num_valid threads each contribute one reduction partial.  The return value is only valid for thread<sub>0</sub>.\n    template <bool FULL_TILE>\n    __device__ __forceinline__ T Sum(\n        T                   input,          ///< [in] Calling thread's input partial reductions\n        int                 num_valid)      ///< [in] Number of valid elements (may be less than BLOCK_THREADS)\n    {\n        cub::Sum    reduction_op;\n        int         warp_offset = (warp_id * LOGICAL_WARP_SIZE);\n        int         warp_num_valid = ((FULL_TILE && EVEN_WARP_MULTIPLE) || (warp_offset + LOGICAL_WARP_SIZE <= num_valid)) ?\n                            LOGICAL_WARP_SIZE :\n                            num_valid - warp_offset;\n\n        // Warp reduction in every warp\n        T warp_aggregate = WarpReduce(temp_storage.warp_reduce[warp_id]).template Reduce<(FULL_TILE && EVEN_WARP_MULTIPLE)>(\n            input,\n            warp_num_valid,\n            cub::Sum());\n\n        // Update outputs and block_aggregate with warp-wide aggregates from lane-0s\n        return ApplyWarpAggregates<FULL_TILE>(reduction_op, warp_aggregate, num_valid);\n    }\n\n\n    /// Computes a thread block-wide reduction using the specified reduction operator. The first num_valid threads each contribute one reduction partial.  The return value is only valid for thread<sub>0</sub>.\n    template <\n        bool                FULL_TILE,\n        typename            ReductionOp>\n    __device__ __forceinline__ T Reduce(\n        T                   input,              ///< [in] Calling thread's input partial reductions\n        int                 num_valid,          ///< [in] Number of valid elements (may be less than BLOCK_THREADS)\n        ReductionOp         reduction_op)       ///< [in] Binary reduction operator\n    {\n        int         warp_offset = warp_id * LOGICAL_WARP_SIZE;\n        int         warp_num_valid = ((FULL_TILE && EVEN_WARP_MULTIPLE) || (warp_offset + LOGICAL_WARP_SIZE <= num_valid)) ?\n                            LOGICAL_WARP_SIZE :\n                            num_valid - warp_offset;\n\n        // Warp reduction in every warp\n        T warp_aggregate = WarpReduce(temp_storage.warp_reduce[warp_id]).template Reduce<(FULL_TILE && EVEN_WARP_MULTIPLE)>(\n            input,\n            warp_num_valid,\n            reduction_op);\n\n        // Update outputs and block_aggregate with warp-wide aggregates from lane-0s\n        return ApplyWarpAggregates<FULL_TILE>(reduction_op, warp_aggregate, num_valid);\n    }\n\n};\n\n\nCUB_NAMESPACE_END\n\n\n#endif // _JITIFY_INCLUDE_GUARD_DE0753A67AE85DA8\n", "std/barrier": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of libcu++, the C++ Standard Library for your entire system,\n// under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _CUDA_STD_BARRIER\n#define _CUDA_STD_BARRIER\n\n#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 700\n#  error \"CUDA synchronization primitives are only supported for sm_70 and up.\"\n#endif\n\n#include \"detail/__config\"\n\n#include \"detail/__pragma_push\"\n\n#include \"detail/libcxx/include/barrier\"\n\n#include \"detail/__pragma_pop\"\n\n#endif // _CUDA_STD_BARRIER\n", "stddef.h": "#ifndef _JITIFY_INCLUDE_GUARD_AA2482F12E932C5E\n#define _JITIFY_INCLUDE_GUARD_AA2482F12E932C5E\n#define cudaDeviceSynchronize() cudaSuccess\n#include <climits>\nnamespace __jitify_stddef_ns {\n#if __cplusplus >= 201103L\ntypedef decltype(nullptr) nullptr_t;\n#if defined(_MSC_VER)\n  typedef double max_align_t;\n#elif defined(__APPLE__)\n  typedef long double max_align_t;\n#else\n  // Define max_align_t to match the GCC definition.\n  typedef struct {\n    long long __jitify_max_align_nonce1\n        __attribute__((__aligned__(__alignof__(long long))));\n    long double __jitify_max_align_nonce2\n        __attribute__((__aligned__(__alignof__(long double))));\n  } max_align_t;\n#endif\n#endif  // __cplusplus >= 201103L\n#if __cplusplus >= 201703L\nenum class byte : unsigned char {};\n#endif  // __cplusplus >= 201703L\n} // namespace __jitify_stddef_ns\nnamespace std {\n  // NVRTC provides built-in definitions of ::size_t and ::ptrdiff_t.\n  using ::size_t;\n  using ::ptrdiff_t;\n  using namespace __jitify_stddef_ns;\n} // namespace std\nusing namespace __jitify_stddef_ns;\n\n#endif // _JITIFY_INCLUDE_GUARD_AA2482F12E932C5E\n", "stdint.h": "#pragma once\n#if __has_include(<cuda/std/cstdint>)\n #include <cuda/std/climits>\n #include <cuda/std/cstdint>\n #define __jitify_using_libcudacxx\n#endif\n#include <climits>\nnamespace __jitify_stdint_ns {\ntypedef signed char      int8_t;\ntypedef signed short     int16_t;\ntypedef signed int       int32_t;\ntypedef signed long long int64_t;\ntypedef signed char      int_fast8_t;\ntypedef signed short     int_fast16_t;\ntypedef signed int       int_fast32_t;\ntypedef signed long long int_fast64_t;\ntypedef signed char      int_least8_t;\ntypedef signed short     int_least16_t;\ntypedef signed int       int_least32_t;\ntypedef signed long long int_least64_t;\ntypedef signed long long intmax_t;\ntypedef unsigned char      uint8_t;\ntypedef unsigned short     uint16_t;\ntypedef unsigned int       uint32_t;\ntypedef unsigned long long uint64_t;\ntypedef unsigned char      uint_fast8_t;\ntypedef unsigned short     uint_fast16_t;\ntypedef unsigned int       uint_fast32_t;\ntypedef unsigned long long uint_fast64_t;\ntypedef unsigned char      uint_least8_t;\ntypedef unsigned short     uint_least16_t;\ntypedef unsigned int       uint_least32_t;\ntypedef unsigned long long uint_least64_t;\ntypedef unsigned long long uintmax_t;\n#ifndef __jitify_using_libcudacxx\n typedef signed long      intptr_t; //optional\n #define INT8_MIN    SCHAR_MIN\n #define INT16_MIN   SHRT_MIN\n #define INT32_MIN   INT_MIN\n #define INT64_MIN   LLONG_MIN\n #define INT8_MAX    SCHAR_MAX\n #define INT16_MAX   SHRT_MAX\n #define INT32_MAX   INT_MAX\n #define INT64_MAX   LLONG_MAX\n #define UINT8_MAX   UCHAR_MAX\n #define UINT16_MAX  USHRT_MAX\n #define UINT32_MAX  UINT_MAX\n #define UINT64_MAX  ULLONG_MAX\n #define INTPTR_MIN  LONG_MIN\n #define INTMAX_MIN  LLONG_MIN\n #define INTPTR_MAX  LONG_MAX\n #define INTMAX_MAX  LLONG_MAX\n #define UINTPTR_MAX ULONG_MAX\n #define UINTMAX_MAX ULLONG_MAX\n #define PTRDIFF_MIN INTPTR_MIN\n #define PTRDIFF_MAX INTPTR_MAX\n #define SIZE_MAX    UINT64_MAX\n#endif\n#if defined _WIN32 || defined _WIN64\n #define WCHAR_MIN   0\n #define WCHAR_MAX   USHRT_MAX\n #ifndef __jitify_using_libcudacxx\n  typedef unsigned long long uintptr_t; //optional\n #endif\n#else\n #define WCHAR_MIN   INT_MIN\n #define WCHAR_MAX   INT_MAX\n #ifndef __jitify_using_libcudacxx\n  typedef unsigned long      uintptr_t; //optional\n #endif\n#endif\n} // namespace __jitify_stdint_ns\nnamespace std { using namespace __jitify_stdint_ns; }\nusing namespace __jitify_stdint_ns;\n", "stdio.h": "#pragma once\n#include <stddef.h>\n#define FILE int\nint fflush ( FILE * stream );\nint fprintf ( FILE * stream, const char * format, ... );\n", "stdlib.h": "#pragma once\n#include <stddef.h>\n", "string.h": "#pragma once\nchar* strcpy ( char * destination, const char * source );\nint strcmp ( const char * str1, const char * str2 );\nchar* strerror( int errnum );\n", "support/atomic/atomic_cuda.h": "#define cudaDeviceSynchronize() cudaSuccess\n//===----------------------------------------------------------------------===//\n//\n// Part of libcu++, the C++ Standard Library for your entire system,\n// under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n// SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.\n//\n//===----------------------------------------------------------------------===//\n\n#if defined(__CUDA_MINIMUM_ARCH__) && ((!defined(_LIBCUDACXX_COMPILER_MSVC) && __CUDA_MINIMUM_ARCH__ < 600) || (defined(_LIBCUDACXX_COMPILER_MSVC) && __CUDA_MINIMUM_ARCH__ < 700))\n#  error \"CUDA atomics are only supported for sm_60 and up on *nix and sm_70 and up on Windows.\"\n#endif\n\ninline _LIBCUDACXX_HOST_DEVICE int __stronger_order_cuda(int __a, int __b) {\n    int const __max = __a > __b ? __a : __b;\n    if(__max != __ATOMIC_RELEASE)\n        return __max;\n    static int const __xform[] = {\n        __ATOMIC_RELEASE,\n        __ATOMIC_ACQ_REL,\n        __ATOMIC_ACQ_REL,\n        __ATOMIC_RELEASE };\n    return __xform[__a < __b ? __a : __b];\n}\n\n// pre-define lock free query for heterogeneous compatibility\n#ifndef _LIBCUDACXX_ATOMIC_IS_LOCK_FREE\n#define _LIBCUDACXX_ATOMIC_IS_LOCK_FREE(__x) (__x <= 8)\n#endif\n\n// Wrap host atomic implementations into a sub-namespace\nnamespace __host {\n#if defined(_LIBCUDACXX_COMPILER_MSVC)\n#  include \"atomic_msvc.h\"\n#elif defined (_LIBCUDACXX_HAS_GCC_ATOMIC_IMP)\n#  include \"atomic_gcc.h\"\n#elif defined (_LIBCUDACXX_HAS_C11_ATOMIC_IMP)\n//TODO\n// #  include \"atomic_c11.h\"\n#elif defined(_LIBCUDACXX_COMPILER_NVRTC)\n#  include \"atomic_nvrtc.h\"\n#endif\n}\n\nusing __host::__cxx_atomic_underlying_t;\n\n#include \"atomic_cuda_generated.h\"\n#include \"atomic_cuda_derived.h\"\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline\n void __cxx_atomic_thread_fence(memory_order __order) {\n    NV_DISPATCH_TARGET(\n        NV_IS_DEVICE, (\n            __atomic_thread_fence_cuda(static_cast<__memory_order_underlying_t>(__order), __thread_scope_system_tag());\n        ),\n        NV_IS_HOST, (\n            __host::__cxx_atomic_thread_fence(__order);\n        )\n    )\n}\n\n_LIBCUDACXX_INLINE_VISIBILITY\ninline\n void __cxx_atomic_signal_fence(memory_order __order) {\n    NV_DISPATCH_TARGET(\n        NV_IS_DEVICE, (\n            __atomic_signal_fence_cuda(static_cast<__memory_order_underlying_t>(__order));\n        ),\n        NV_IS_HOST, (\n            __host::__cxx_atomic_signal_fence(__order);\n        )\n    )\n}\n\ntemplate <typename _Tp, int _Sco, bool _Ref = false>\nstruct __cxx_atomic_base_heterogeneous_impl {\n    __cxx_atomic_base_heterogeneous_impl() noexcept = default;\n\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr explicit\n      __cxx_atomic_base_heterogeneous_impl(_Tp __value) : __a_value(__value) {\n    }\n\n    using __underlying_t = _Tp;\n    static constexpr int __sco = _Sco;\n\n    __host::__cxx_atomic_base_impl<_Tp, _Sco> __a_value;\n};\n\ntemplate <typename _Tp, int _Sco>\nstruct __cxx_atomic_base_heterogeneous_impl<_Tp, _Sco, true> {\n    __cxx_atomic_base_heterogeneous_impl() noexcept = default;\n\n    static_assert(sizeof(_Tp) >= 4, \"atomic_ref does not support 1 or 2 byte types\");\n    static_assert(sizeof(_Tp) <= 8, \"atomic_ref does not support types larger than 8 bytes\");\n\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr explicit\n      __cxx_atomic_base_heterogeneous_impl(_Tp& __value) : __a_value(__value) {\n    }\n\n    using __underlying_t = _Tp;\n    static constexpr int __sco = _Sco;\n\n    __host::__cxx_atomic_ref_base_impl<_Tp, _Sco> __a_value;\n};\n\ntemplate <typename _Tp, int _Sco, bool _Ref>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr\n_Tp* __cxx_get_underlying_device_atomic(__cxx_atomic_base_heterogeneous_impl<_Tp, _Sco, _Ref> * __a) noexcept {\n  return __cxx_get_underlying_atomic(&__a->__a_value);\n}\n\ntemplate <typename _Tp, int _Sco, bool _Ref>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr\nvolatile _Tp* __cxx_get_underlying_device_atomic(__cxx_atomic_base_heterogeneous_impl<_Tp, _Sco, _Ref> volatile* __a) noexcept {\n  return __cxx_get_underlying_atomic(&__a->__a_value);\n}\n\ntemplate <typename _Tp, int _Sco, bool _Ref>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr\nconst _Tp* __cxx_get_underlying_device_atomic(__cxx_atomic_base_heterogeneous_impl<_Tp, _Sco, _Ref> const* __a) noexcept {\n  return __cxx_get_underlying_atomic(&__a->__a_value);\n}\n\ntemplate <typename _Tp, int _Sco, bool _Ref>\n_LIBCUDACXX_INLINE_VISIBILITY constexpr\nconst volatile _Tp* __cxx_get_underlying_device_atomic(__cxx_atomic_base_heterogeneous_impl<_Tp, _Sco, _Ref> const volatile* __a) noexcept {\n  return __cxx_get_underlying_atomic(&__a->__a_value);\n}\n\ntemplate <typename _Tp>\nusing __cxx_atomic_small_to_32 = __conditional_t<is_signed<_Tp>::value, int32_t, uint32_t>;\n\n// Arithmetic conversions to/from proxy types\ntemplate<class _Tp, __enable_if_t<is_arithmetic<_Tp>::value, int> = 0>\nconstexpr _LIBCUDACXX_INLINE_VISIBILITY inline __cxx_atomic_small_to_32<_Tp> __cxx_small_to_32(_Tp __val) {\n    return static_cast<__cxx_atomic_small_to_32<_Tp>>(__val);\n}\n\ntemplate<class _Tp, __enable_if_t<is_arithmetic<_Tp>::value, int> = 0>\nconstexpr _LIBCUDACXX_INLINE_VISIBILITY inline _Tp __cxx_small_from_32(__cxx_atomic_small_to_32<_Tp> __val) {\n    return static_cast<_Tp>(__val);\n}\n\n// Non-arithmetic conversion to/from proxy types\ntemplate<class _Tp, __enable_if_t<!is_arithmetic<_Tp>::value, int> = 0>\n_LIBCUDACXX_INLINE_VISIBILITY inline __cxx_atomic_small_to_32<_Tp> __cxx_small_to_32(_Tp __val) {\n    __cxx_atomic_small_to_32<_Tp> __temp{};\n    memcpy(&__temp, &__val, sizeof(_Tp));\n    return __temp;\n}\n\ntemplate<class _Tp, __enable_if_t<!is_arithmetic<_Tp>::value, int> = 0>\n_LIBCUDACXX_INLINE_VISIBILITY inline _Tp __cxx_small_from_32(__cxx_atomic_small_to_32<_Tp> __val) {\n    _Tp __temp{};\n    memcpy(&__temp, &__val, sizeof(_Tp));\n    return __temp;\n}\n\ntemplate <typename _Tp, int _Sco>\nstruct __cxx_atomic_base_small_impl {\n    __cxx_atomic_base_small_impl() noexcept = default;\n    _LIBCUDACXX_INLINE_VISIBILITY constexpr explicit\n      __cxx_atomic_base_small_impl(_Tp __value) : __a_value(__cxx_small_to_32(__value)) {\n    }\n\n    using __underlying_t = _Tp;\n    static constexpr int __sco = _Sco;\n\n    __cxx_atomic_base_heterogeneous_impl<__cxx_atomic_small_to_32<_Tp>, _Sco, false> __a_value;\n};\n\ntemplate <typename _Tp, int _Sco>\nusing __cxx_atomic_base_impl = __conditional_t<sizeof(_Tp) < 4,\n                                    __cxx_atomic_base_small_impl<_Tp, _Sco>,\n                                    __cxx_atomic_base_heterogeneous_impl<_Tp, _Sco> >;\n\n\ntemplate <typename _Tp, int _Sco>\nusing __cxx_atomic_ref_base_impl = __cxx_atomic_base_heterogeneous_impl<_Tp, _Sco, true>;\n\ntemplate <typename _Tp, int _Sco, bool _Ref>\n_LIBCUDACXX_HOST_DEVICE\n void __cxx_atomic_init(__cxx_atomic_base_heterogeneous_impl<_Tp, _Sco, _Ref> volatile* __a, _Tp __val) {\n    alignas(_Tp) auto __tmp = __val;\n    __cxx_atomic_assign_volatile(*__cxx_get_underlying_device_atomic(__a), __tmp);\n}\n\ntemplate <typename _Tp, int _Sco, bool _Ref>\n_LIBCUDACXX_HOST_DEVICE\n void __cxx_atomic_store(__cxx_atomic_base_heterogeneous_impl<_Tp, _Sco, _Ref> volatile* __a, _Tp __val, memory_order __order) {\n    alignas(_Tp) auto __tmp = __val;\n    NV_DISPATCH_TARGET(\n        NV_IS_DEVICE, (\n            __atomic_store_n_cuda(__cxx_get_underlying_device_atomic(__a), __tmp, static_cast<__memory_order_underlying_t>(__order), __scope_tag<_Sco>());\n        ),\n        NV_IS_HOST, (\n            __host::__cxx_atomic_store(&__a->__a_value, __tmp, __order);\n        )\n    )\n}\n\ntemplate <typename _Tp, int _Sco, bool _Ref>\n_LIBCUDACXX_HOST_DEVICE\n _Tp __cxx_atomic_load(__cxx_atomic_base_heterogeneous_impl<_Tp, _Sco, _Ref> const volatile* __a, memory_order __order) {\n    NV_DISPATCH_TARGET(\n        NV_IS_DEVICE, (\n            return __atomic_load_n_cuda(__cxx_get_underlying_device_atomic(__a), static_cast<__memory_order_underlying_t>(__order), __scope_tag<_Sco>());\n        ),\n        NV_IS_HOST, (\n            return __host::__cxx_atomic_load(&__a->__a_value, __order);\n        )\n    )\n}\n\ntemplate <typename _Tp, int _Sco, bool _Ref>\n_LIBCUDACXX_HOST_DEVICE\n _Tp __cxx_atomic_exchange(__cxx_atomic_base_heterogeneous_impl<_Tp, _Sco, _Ref> volatile* __a, _Tp __val, memory_order __order) {\n    alignas(_Tp) auto __tmp = __val;\n    NV_DISPATCH_TARGET(\n        NV_IS_DEVICE, (\n            return __atomic_exchange_n_cuda(__cxx_get_underlying_device_atomic(__a), __tmp, static_cast<__memory_order_underlying_t>(__order), __scope_tag<_Sco>());\n        ),\n        NV_IS_HOST, (\n            return __host::__cxx_atomic_exchange(&__a->__a_value, __tmp, __order);\n        )\n    )\n}\n\ntemplate <typename _Tp, int _Sco, bool _Ref>\n_LIBCUDACXX_HOST_DEVICE\n bool __cxx_atomic_compare_exchange_strong(__cxx_atomic_base_heterogeneous_impl<_Tp, _Sco, _Ref> volatile* __a, _Tp* __expected, _Tp __val, memory_order __success, memory_order __failure) {\n    alignas(_Tp) auto __tmp = *__expected;\n    bool __result = false;\n    NV_DISPATCH_TARGET(\n        NV_IS_DEVICE, (\n            alignas(_Tp) auto __tmp_v = __val;\n            __result = __atomic_compare_exchange_cuda(__cxx_get_underlying_device_atomic(__a), &__tmp, &__tmp_v, false, static_cast<__memory_order_underlying_t>(__success), static_cast<__memory_order_underlying_t>(__failure), __scope_tag<_Sco>());\n        ),\n        NV_IS_HOST, (\n            __result = __host::__cxx_atomic_compare_exchange_strong(&__a->__a_value, &__tmp, __val, __success, __failure);\n        )\n    )\n    *__expected = __tmp;\n    return __result;\n}\n\ntemplate <typename _Tp, int _Sco, bool _Ref>\n_LIBCUDACXX_HOST_DEVICE\n bool __cxx_atomic_compare_exchange_weak(__cxx_atomic_base_heterogeneous_impl<_Tp, _Sco, _Ref> volatile* __a, _Tp* __expected, _Tp __val, memory_order __success, memory_order __failure) {\n    alignas(_Tp) auto __tmp = *__expected;\n    bool __result = false;\n    NV_DISPATCH_TARGET(\n        NV_IS_DEVICE, (\n            alignas(_Tp) auto __tmp_v = __val;\n            __result = __atomic_compare_exchange_cuda(__cxx_get_underlying_device_atomic(__a), &__tmp, &__tmp_v, true, static_cast<__memory_order_underlying_t>(__success), static_cast<__memory_order_underlying_t>(__failure), __scope_tag<_Sco>());\n        ),\n        NV_IS_HOST, (\n            __result = __host::__cxx_atomic_compare_exchange_weak(&__a->__a_value, &__tmp, __val, __success, __failure);\n        )\n    )\n    *__expected = __tmp;\n    return __result;\n}\n\ntemplate <typename _Tp, int _Sco, bool _Ref>\n_LIBCUDACXX_HOST_DEVICE\n _Tp __cxx_atomic_fetch_add(__cxx_atomic_base_heterogeneous_impl<_Tp, _Sco, _Ref> volatile* __a, _Tp __delta, memory_order __order) {\n    NV_DISPATCH_TARGET(\n        NV_IS_DEVICE, (\n            return __atomic_fetch_add_cuda(__cxx_get_underlying_device_atomic(__a), __delta, static_cast<__memory_order_underlying_t>(__order), __scope_tag<_Sco>());\n        ),\n        NV_IS_HOST, (\n            return __host::__cxx_atomic_fetch_add(&__a->__a_value, __delta, __order);\n        )\n    )\n}\n\ntemplate <typename _Tp, int _Sco, bool _Ref>\n_LIBCUDACXX_HOST_DEVICE\n _Tp* __cxx_atomic_fetch_add(__cxx_atomic_base_heterogeneous_impl<_Tp*, _Sco, _Ref> volatile* __a, ptrdiff_t __delta, memory_order __order) {\n    NV_DISPATCH_TARGET(\n        NV_IS_DEVICE, (\n            return __atomic_fetch_add_cuda(__cxx_get_underlying_device_atomic(__a), __delta, static_cast<__memory_order_underlying_t>(__order), __scope_tag<_Sco>());\n        ),\n        NV_IS_HOST, (\n            return __host::__cxx_atomic_fetch_add(&__a->__a_value, __delta, __order);\n        )\n    )\n}\n\ntemplate <typename _Tp, int _Sco, bool _Ref>\n_LIBCUDACXX_HOST_DEVICE\n _Tp __cxx_atomic_fetch_sub(__cxx_atomic_base_heterogeneous_impl<_Tp, _Sco, _Ref> volatile* __a, _Tp __delta, memory_order __order) {\n    NV_DISPATCH_TARGET(\n        NV_IS_DEVICE, (\n            return __atomic_fetch_sub_cuda(__cxx_get_underlying_device_atomic(__a), __delta, static_cast<__memory_order_underlying_t>(__order), __scope_tag<_Sco>());\n        ),\n        NV_IS_HOST, (\n            return __host::__cxx_atomic_fetch_sub(&__a->__a_value, __delta, __order);\n        )\n    )\n}\n\ntemplate <typename _Tp, int _Sco, bool _Ref>\n_LIBCUDACXX_HOST_DEVICE\n _Tp* __cxx_atomic_fetch_sub(__cxx_atomic_base_heterogeneous_impl<_Tp*, _Sco, _Ref> volatile* __a, ptrdiff_t __delta, memory_order __order) {\n    NV_DISPATCH_TARGET(\n        NV_IS_DEVICE, (\n            return __atomic_fetch_sub_cuda(__cxx_get_underlying_device_atomic(__a), __delta, static_cast<__memory_order_underlying_t>(__order), __scope_tag<_Sco>());\n        ),\n        NV_IS_HOST, (\n            return __host::__cxx_atomic_fetch_sub(&__a->__a_value, __delta, __order);\n        )\n    )\n}\n\ntemplate <typename _Tp, int _Sco, bool _Ref>\n_LIBCUDACXX_HOST_DEVICE\n _Tp __cxx_atomic_fetch_and(__cxx_atomic_base_heterogeneous_impl<_Tp, _Sco, _Ref> volatile* __a, _Tp __pattern, memory_order __order) {\n    NV_DISPATCH_TARGET(\n        NV_IS_DEVICE, (\n            return __atomic_fetch_and_cuda(__cxx_get_underlying_device_atomic(__a), __pattern, static_cast<__memory_order_underlying_t>(__order), __scope_tag<_Sco>());\n        ),\n        NV_IS_HOST, (\n            return __host::__cxx_atomic_fetch_and(&__a->__a_value, __pattern, __order);\n        )\n    )\n}\n\ntemplate <typename _Tp, int _Sco, bool _Ref>\n_LIBCUDACXX_HOST_DEVICE\n _Tp __cxx_atomic_fetch_or(__cxx_atomic_base_heterogeneous_impl<_Tp, _Sco, _Ref> volatile* __a, _Tp __pattern, memory_order __order) {\n    NV_DISPATCH_TARGET(\n        NV_IS_DEVICE, (\n            return __atomic_fetch_or_cuda(__cxx_get_underlying_device_atomic(__a), __pattern, static_cast<__memory_order_underlying_t>(__order), __scope_tag<_Sco>());\n        ),\n        NV_IS_HOST, (\n            return __host::__cxx_atomic_fetch_or(&__a->__a_value, __pattern, __order);\n        )\n    )\n}\n\ntemplate <typename _Tp, int _Sco, bool _Ref>\n_LIBCUDACXX_HOST_DEVICE\n _Tp __cxx_atomic_fetch_xor(__cxx_atomic_base_heterogeneous_impl<_Tp, _Sco, _Ref> volatile* __a, _Tp __pattern, memory_order __order) {\n    NV_DISPATCH_TARGET(\n        NV_IS_DEVICE, (\n            return __atomic_fetch_xor_cuda(__cxx_get_underlying_device_atomic(__a), __pattern, static_cast<__memory_order_underlying_t>(__order), __scope_tag<_Sco>());\n        ),\n        NV_IS_HOST, (\n            return __host::__cxx_atomic_fetch_xor(&__a->__a_value, __pattern, __order);\n        )\n    )\n}\n\ntemplate <typename _Tp, typename _Delta, int _Sco, bool _Ref>\n_LIBCUDACXX_HOST_DEVICE\n _Tp __cxx_atomic_fetch_max(__cxx_atomic_base_heterogeneous_impl<_Tp, _Sco, _Ref> volatile* __a, _Delta __val, memory_order __order) {\n    NV_IF_TARGET(\n        NV_IS_DEVICE, (\n            return __atomic_fetch_max_cuda(__cxx_get_underlying_device_atomic(__a), __val, static_cast<__memory_order_underlying_t>(__order), __scope_tag<_Sco>());\n        ), (\n            return __host::__cxx_atomic_fetch_max(&__a->__a_value, __val, __order);\n        )\n    )\n}\n\ntemplate <typename _Tp, typename _Delta, int _Sco, bool _Ref>\n_LIBCUDACXX_HOST_DEVICE\n _Tp __cxx_atomic_fetch_min(__cxx_atomic_base_heterogeneous_impl<_Tp, _Sco, _Ref> volatile* __a, _Delta __val, memory_order __order) {\n    NV_IF_TARGET(\n        NV_IS_DEVICE, (\n            return __atomic_fetch_min_cuda(__cxx_get_underlying_device_atomic(__a), __val, static_cast<__memory_order_underlying_t>(__order), __scope_tag<_Sco>());\n        ), (\n            return __host::__cxx_atomic_fetch_min(&__a->__a_value, __val, __order);\n        )\n    )\n}\n\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_HOST_DEVICE inline void __cxx_atomic_init(__cxx_atomic_base_small_impl<_Tp, _Sco> volatile* __a, _Tp __val) {\n    __cxx_atomic_init(&__a->__a_value, __cxx_small_to_32(__val));\n}\n\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_HOST_DEVICE inline void __cxx_atomic_store(__cxx_atomic_base_small_impl<_Tp, _Sco> volatile* __a, _Tp __val, memory_order __order) {\n    __cxx_atomic_store(&__a->__a_value, __cxx_small_to_32(__val), __order);\n}\n\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_HOST_DEVICE inline _Tp __cxx_atomic_load(__cxx_atomic_base_small_impl<_Tp, _Sco> const volatile* __a, memory_order __order) {\n    return __cxx_small_from_32<_Tp>(__cxx_atomic_load(&__a->__a_value, __order));\n}\n\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_HOST_DEVICE inline _Tp __cxx_atomic_exchange(__cxx_atomic_base_small_impl<_Tp, _Sco> volatile* __a, _Tp __value, memory_order __order) {\n    return __cxx_small_from_32<_Tp>(__cxx_atomic_exchange(&__a->__a_value, __cxx_small_to_32(__value), __order));\n}\n_LIBCUDACXX_HOST_DEVICE\ninline int __cuda_memcmp(void const * __lhs, void const * __rhs, size_t __count) {\n    NV_DISPATCH_TARGET(\n        NV_IS_DEVICE, (\n            auto __lhs_c = reinterpret_cast<unsigned char const *>(__lhs);\n            auto __rhs_c = reinterpret_cast<unsigned char const *>(__rhs);\n            while (__count--) {\n                auto const __lhs_v = *__lhs_c++;\n                auto const __rhs_v = *__rhs_c++;\n                if (__lhs_v < __rhs_v) { return -1; }\n                if (__lhs_v > __rhs_v) { return 1; }\n            }\n            return 0;\n        ),\n        NV_IS_HOST, (\n            return memcmp(__lhs, __rhs, __count);\n        )\n    )\n}\n\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_HOST_DEVICE inline bool __cxx_atomic_compare_exchange_weak(__cxx_atomic_base_small_impl<_Tp, _Sco> volatile* __a, _Tp* __expected, _Tp __value, memory_order __success, memory_order __failure) {\n    auto __temp = __cxx_small_to_32(*__expected);\n    auto const __ret = __cxx_atomic_compare_exchange_weak(&__a->__a_value, &__temp, __cxx_small_to_32(__value), __success, __failure);\n    auto const __actual = __cxx_small_from_32<_Tp>(__temp);\n    constexpr auto __mask = static_cast<decltype(__temp)>((1u << (8*sizeof(_Tp))) - 1);\n    if(!__ret) {\n        if(0 == __cuda_memcmp(&__actual, __expected, sizeof(_Tp)))\n            __cxx_atomic_fetch_and(&__a->__a_value, __mask, memory_order_relaxed);\n        else\n            *__expected = __actual;\n    }\n    return __ret;\n}\n\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_HOST_DEVICE inline bool __cxx_atomic_compare_exchange_strong(__cxx_atomic_base_small_impl<_Tp, _Sco> volatile* __a, _Tp* __expected, _Tp __value, memory_order __success, memory_order __failure) {\n    auto const __old = *__expected;\n    while(1) {\n        if(__cxx_atomic_compare_exchange_weak(__a, __expected, __value, __success, __failure))\n            return true;\n        if(0 != __cuda_memcmp(&__old, __expected, sizeof(_Tp)))\n            return false;\n    }\n}\n\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_HOST_DEVICE inline _Tp __cxx_atomic_fetch_add(__cxx_atomic_base_small_impl<_Tp, _Sco> volatile* __a, _Tp __delta, memory_order __order) {\n    return __cxx_small_from_32<_Tp>(__cxx_atomic_fetch_add(&__a->__a_value, __cxx_small_to_32(__delta), __order));\n}\n\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_HOST_DEVICE inline _Tp __cxx_atomic_fetch_sub(__cxx_atomic_base_small_impl<_Tp, _Sco> volatile* __a, _Tp __delta, memory_order __order) {\n    return __cxx_small_from_32<_Tp>(__cxx_atomic_fetch_sub(&__a->__a_value, __cxx_small_to_32(__delta), __order));\n}\n\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_HOST_DEVICE inline _Tp __cxx_atomic_fetch_and(__cxx_atomic_base_small_impl<_Tp, _Sco> volatile* __a, _Tp __pattern, memory_order __order) {\n    return __cxx_small_from_32<_Tp>(__cxx_atomic_fetch_and(&__a->__a_value, __cxx_small_to_32(__pattern), __order));\n}\n\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_HOST_DEVICE inline _Tp __cxx_atomic_fetch_or(__cxx_atomic_base_small_impl<_Tp, _Sco> volatile* __a, _Tp __pattern, memory_order __order) {\n    return __cxx_small_from_32<_Tp>(__cxx_atomic_fetch_or(&__a->__a_value, __cxx_small_to_32(__pattern), __order));\n}\n\ntemplate <typename _Tp, int _Sco>\n_LIBCUDACXX_HOST_DEVICE inline _Tp __cxx_atomic_fetch_xor(__cxx_atomic_base_small_impl<_Tp, _Sco> volatile* __a, _Tp __pattern, memory_order __order) {\n    return __cxx_small_from_32<_Tp>(__cxx_atomic_fetch_xor(&__a->__a_value, __cxx_small_to_32(__pattern), __order));\n}\n\ntemplate <typename _Tp, typename _Delta, int _Sco>\n_LIBCUDACXX_HOST_DEVICE inline _Tp __cxx_atomic_fetch_max(__cxx_atomic_base_small_impl<_Tp, _Sco> volatile* __a, _Delta __val, memory_order __order) {\n    return __cxx_small_from_32<_Tp>(__cxx_atomic_fetch_max(&__a->__a_value, __cxx_small_to_32(__val), __order));\n}\n\ntemplate <typename _Tp, typename _Delta, int _Sco>\n_LIBCUDACXX_HOST_DEVICE inline _Tp __cxx_atomic_fetch_min(__cxx_atomic_base_small_impl<_Tp, _Sco> volatile* __a, _Delta __val, memory_order __order) {\n    return __cxx_small_from_32<_Tp>(__cxx_atomic_fetch_min(&__a->__a_value, __cxx_small_to_32(__val), __order));\n}\n", "support/atomic/atomic_scopes.h": "#define cudaDeviceSynchronize() cudaSuccess\n#ifndef __LIBCUDACXX_ATOMIC_SCOPES_H\n#define __LIBCUDACXX_ATOMIC_SCOPES_H\n\n// REMEMBER CHANGES TO THESE ARE ABI BREAKING\n// TODO: Space values out for potential new scopes\n#ifndef __ATOMIC_BLOCK\n#define __ATOMIC_SYSTEM 0 // 0 indicates default\n#define __ATOMIC_DEVICE 1\n#define __ATOMIC_BLOCK 2\n#define __ATOMIC_THREAD 10\n#endif //__ATOMIC_BLOCK\n\nenum thread_scope {\n    thread_scope_system = __ATOMIC_SYSTEM,\n    thread_scope_device = __ATOMIC_DEVICE,\n    thread_scope_block = __ATOMIC_BLOCK,\n    thread_scope_thread = __ATOMIC_THREAD\n};\n\n#define _LIBCUDACXX_ATOMIC_SCOPE_TYPE ::cuda::thread_scope\n#define _LIBCUDACXX_ATOMIC_SCOPE_DEFAULT ::cuda::thread_scope::system\n\nstruct __thread_scope_thread_tag { };\nstruct __thread_scope_block_tag { };\nstruct __thread_scope_device_tag { };\nstruct __thread_scope_system_tag { };\n\ntemplate<int _Scope>  struct __scope_enum_to_tag { };\n/* This would be the implementation once an actual thread-scope backend exists.\ntemplate<> struct __scope_enum_to_tag<(int)thread_scope_thread> {\n    using type = __thread_scope_thread_tag; };\nUntil then: */\ntemplate<> struct __scope_enum_to_tag<(int)thread_scope_thread> {\n    using type = __thread_scope_block_tag; };\ntemplate<> struct __scope_enum_to_tag<(int)thread_scope_block> {\n    using type = __thread_scope_block_tag; };\ntemplate<> struct __scope_enum_to_tag<(int)thread_scope_device> {\n    using type = __thread_scope_device_tag; };\ntemplate<> struct __scope_enum_to_tag<(int)thread_scope_system> {\n    using type = __thread_scope_system_tag; };\n\ntemplate <int _Scope>\n_LIBCUDACXX_INLINE_VISIBILITY auto constexpr __scope_tag() ->\n        typename __scope_enum_to_tag<_Scope>::type {\n    return typename __scope_enum_to_tag<_Scope>::type();\n}\n\n#endif // __LIBCUDACXX_ATOMIC_SCOPES_H\n", "time.h": "\n    #pragma once\n    #define NULL 0\n    #define CLOCKS_PER_SEC 1000000\n    namespace __jitify_time_ns {\n    typedef long time_t;\n    struct tm {\n      int tm_sec;\n      int tm_min;\n      int tm_hour;\n      int tm_mday;\n      int tm_mon;\n      int tm_year;\n      int tm_wday;\n      int tm_yday;\n      int tm_isdst;\n    };\n    #if __cplusplus >= 201703L\n    struct timespec {\n      time_t tv_sec;\n      long tv_nsec;\n    };\n    #endif\n    }  // namespace __jitify_time_ns\n    namespace std {\n      // NVRTC provides built-in definitions of ::size_t and ::clock_t.\n      using ::size_t;\n      using ::clock_t;\n      using namespace __jitify_time_ns;\n    }\n    using namespace __jitify_time_ns;\n ", "tuple": "#include <cupy/cuda_workaround.h>\n", "type_traits": "#include <cupy/cuda_workaround.h>\n", "util_arch.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_99336375E0757819\n#define _JITIFY_INCLUDE_GUARD_99336375E0757819\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * Static architectural properties by SM version.\n */\n\n#include <cub/util_cpp_dialect.cuh>\n#include <cub/util_namespace.cuh>\n#include <cub/util_macro.cuh>\n\n// Legacy include; this functionality used to be defined in here.\n#include <cub/detail/detect_cuda_runtime.cuh>\n\nCUB_NAMESPACE_BEGIN\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS    // Do not document\n\n// \\deprecated [Since 2.1.0] \n#define CUB_USE_COOPERATIVE_GROUPS\n\n/// In device code, CUB_PTX_ARCH expands to the PTX version for which we are\n/// compiling. In host code, CUB_PTX_ARCH's value is implementation defined.\n#ifndef CUB_PTX_ARCH\n    #if defined(_NVHPC_CUDA)\n        // __NVCOMPILER_CUDA_ARCH__ is the target PTX version, and is defined\n        // when compiling both host code and device code. Currently, only one\n        // PTX version can be targeted.\n        #define CUB_PTX_ARCH __NVCOMPILER_CUDA_ARCH__\n    #elif !defined(__CUDA_ARCH__)\n        #define CUB_PTX_ARCH 0\n    #else\n        #define CUB_PTX_ARCH __CUDA_ARCH__\n    #endif\n#endif\n\n// These definitions were intended for internal use only and are now obsolete.\n// If you relied on them, consider porting your code to use the functionality\n// in libcu++'s <nv/target> header.\n// For a temporary workaround, define CUB_PROVIDE_LEGACY_ARCH_MACROS to make\n// them available again. These should be considered deprecated and will be\n// fully removed in a future version.\n#ifdef CUB_PROVIDE_LEGACY_ARCH_MACROS\n    #ifndef CUB_IS_DEVICE_CODE\n        #if defined(_NVHPC_CUDA)\n            #define CUB_IS_DEVICE_CODE __builtin_is_device_code()\n            #define CUB_IS_HOST_CODE (!__builtin_is_device_code())\n            #define CUB_INCLUDE_DEVICE_CODE 1\n            #define CUB_INCLUDE_HOST_CODE 1\n        #elif CUB_PTX_ARCH > 0\n            #define CUB_IS_DEVICE_CODE 1\n            #define CUB_IS_HOST_CODE 0\n            #define CUB_INCLUDE_DEVICE_CODE 1\n            #define CUB_INCLUDE_HOST_CODE 0\n        #else\n            #define CUB_IS_DEVICE_CODE 0\n            #define CUB_IS_HOST_CODE 1\n            #define CUB_INCLUDE_DEVICE_CODE 0\n            #define CUB_INCLUDE_HOST_CODE 1\n        #endif\n    #endif\n#endif // CUB_PROVIDE_LEGACY_ARCH_MACROS\n\n/// Maximum number of devices supported.\n#ifndef CUB_MAX_DEVICES\n    #define CUB_MAX_DEVICES (128)\n#endif\n\nstatic_assert(CUB_MAX_DEVICES > 0, \"CUB_MAX_DEVICES must be greater than 0.\");\n\n\n/// Number of threads per warp\n#ifndef CUB_LOG_WARP_THREADS\n    #define CUB_LOG_WARP_THREADS(unused) (5)\n    #define CUB_WARP_THREADS(unused) (1 << CUB_LOG_WARP_THREADS(0))\n\n    #define CUB_PTX_WARP_THREADS        CUB_WARP_THREADS(0)\n    #define CUB_PTX_LOG_WARP_THREADS    CUB_LOG_WARP_THREADS(0)\n#endif\n\n\n/// Number of smem banks\n#ifndef CUB_LOG_SMEM_BANKS\n    #define CUB_LOG_SMEM_BANKS(unused) (5)\n    #define CUB_SMEM_BANKS(unused) (1 << CUB_LOG_SMEM_BANKS(0))\n\n    #define CUB_PTX_LOG_SMEM_BANKS      CUB_LOG_SMEM_BANKS(0)\n    #define CUB_PTX_SMEM_BANKS          CUB_SMEM_BANKS\n#endif\n\n\n/// Oversubscription factor\n#ifndef CUB_SUBSCRIPTION_FACTOR\n    #define CUB_SUBSCRIPTION_FACTOR(unused) (5)\n    #define CUB_PTX_SUBSCRIPTION_FACTOR CUB_SUBSCRIPTION_FACTOR(0)\n#endif\n\n\n/// Prefer padding overhead vs X-way conflicts greater than this threshold\n#ifndef CUB_PREFER_CONFLICT_OVER_PADDING\n    #define CUB_PREFER_CONFLICT_OVER_PADDING(unused) (1)\n    #define CUB_PTX_PREFER_CONFLICT_OVER_PADDING CUB_PREFER_CONFLICT_OVER_PADDING(0)\n#endif\n\n\ntemplate <\n    int NOMINAL_4B_BLOCK_THREADS,\n    int NOMINAL_4B_ITEMS_PER_THREAD,\n    typename T>\nstruct RegBoundScaling\n{\n    enum {\n        ITEMS_PER_THREAD    = CUB_MAX(1, NOMINAL_4B_ITEMS_PER_THREAD * 4 / CUB_MAX(4, sizeof(T))),\n        BLOCK_THREADS       = CUB_MIN(NOMINAL_4B_BLOCK_THREADS, (((1024 * 48) / (sizeof(T) * ITEMS_PER_THREAD)) + 31) / 32 * 32),\n    };\n};\n\n\ntemplate <\n    int NOMINAL_4B_BLOCK_THREADS,\n    int NOMINAL_4B_ITEMS_PER_THREAD,\n    typename T>\nstruct MemBoundScaling\n{\n    enum {\n        ITEMS_PER_THREAD    = CUB_MAX(1, CUB_MIN(NOMINAL_4B_ITEMS_PER_THREAD * 4 / sizeof(T), NOMINAL_4B_ITEMS_PER_THREAD * 2)),\n        BLOCK_THREADS       = CUB_MIN(NOMINAL_4B_BLOCK_THREADS, (((1024 * 48) / (sizeof(T) * ITEMS_PER_THREAD)) + 31) / 32 * 32),\n    };\n};\n\n\n\n\n#endif  // Do not document\n\nCUB_NAMESPACE_END\n\n#endif // _JITIFY_INCLUDE_GUARD_99336375E0757819\n", "util_compiler.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_1969ACC0820923D8\n#define _JITIFY_INCLUDE_GUARD_1969ACC0820923D8\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n *AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n *IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * Detect compiler information.\n */\n\n// enumerate host compilers we know about\n#define CUB_HOST_COMPILER_UNKNOWN 0\n#define CUB_HOST_COMPILER_MSVC 1\n#define CUB_HOST_COMPILER_GCC 2\n#define CUB_HOST_COMPILER_CLANG 3\n\n// enumerate device compilers we know about\n#define CUB_DEVICE_COMPILER_UNKNOWN 0\n#define CUB_DEVICE_COMPILER_MSVC 1\n#define CUB_DEVICE_COMPILER_GCC 2\n#define CUB_DEVICE_COMPILER_NVCC 3\n#define CUB_DEVICE_COMPILER_CLANG 4\n\n// figure out which host compiler we're using\n#if defined(_MSC_VER)\n#  define CUB_HOST_COMPILER CUB_HOST_COMPILER_MSVC\n#  define CUB_MSVC_VERSION _MSC_VER\n#  define CUB_MSVC_VERSION_FULL _MSC_FULL_VER\n#elif defined(__clang__)\n#  define CUB_HOST_COMPILER CUB_HOST_COMPILER_CLANG\n#  define CUB_CLANG_VERSION                                                    \\\n    (__clang_major__ * 10000 + __clang_minor__ * 100 + __clang_patchlevel__)\n#elif defined(__GNUC__)\n#  define CUB_HOST_COMPILER CUB_HOST_COMPILER_GCC\n#  define CUB_GCC_VERSION                                                      \\\n    (__GNUC__ * 10000 + __GNUC_MINOR__ * 100 + __GNUC_PATCHLEVEL__)\n#else\n#  define CUB_HOST_COMPILER CUB_HOST_COMPILER_UNKNOWN\n#endif // CUB_HOST_COMPILER\n\n// figure out which device compiler we're using\n#if defined(__CUDACC__) || defined(_NVHPC_CUDA)\n#  define CUB_DEVICE_COMPILER CUB_DEVICE_COMPILER_NVCC\n#elif CUB_HOST_COMPILER == CUB_HOST_COMPILER_MSVC\n#  define CUB_DEVICE_COMPILER CUB_DEVICE_COMPILER_MSVC\n#elif CUB_HOST_COMPILER == CUB_HOST_COMPILER_GCC\n#  define CUB_DEVICE_COMPILER CUB_DEVICE_COMPILER_GCC\n#elif CUB_HOST_COMPILER == CUB_HOST_COMPILER_CLANG\n// CUDA-capable clang should behave similar to NVCC.\n#  if defined(__CUDA__)\n#    define CUB_DEVICE_COMPILER CUB_DEVICE_COMPILER_NVCC\n#  else\n#    define CUB_DEVICE_COMPILER CUB_DEVICE_COMPILER_CLANG\n#  endif\n#else\n#  define CUB_DEVICE_COMPILER CUB_DEVICE_COMPILER_UNKNOWN\n#endif\n\n#endif // _JITIFY_INCLUDE_GUARD_1969ACC0820923D8\n", "util_cpp_dialect.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_44D938EB20B46E5E\n#define _JITIFY_INCLUDE_GUARD_44D938EB20B46E5E\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n *AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n *IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/*! \\file\n *  \\brief Detect the version of the C++ standard used by the compiler.\n */\n\n#include \"util_compiler.cuh\"\n\n// Deprecation warnings may be silenced by defining the following macros. These\n// may be combined.\n// - CUB_IGNORE_DEPRECATED_CPP_DIALECT:\n//   Ignore all deprecated C++ dialects and outdated compilers.\n// - CUB_IGNORE_DEPRECATED_CPP_11:\n//   Ignore deprecation warnings when compiling with C++11. C++03 and outdated\n//   compilers will still issue warnings.\n// - CUB_IGNORE_DEPRECATED_COMPILER\n//   Ignore deprecation warnings when using deprecated compilers. Compiling\n//   with C++03 and C++11 will still issue warnings.\n\n// Check for the thrust opt-outs as well:\n#if !defined(CUB_IGNORE_DEPRECATED_CPP_DIALECT) && \\\n     defined(THRUST_IGNORE_DEPRECATED_CPP_DIALECT)\n#  define    CUB_IGNORE_DEPRECATED_CPP_DIALECT\n#endif\n#if !defined(CUB_IGNORE_DEPRECATED_CPP_11) && \\\n     defined(THRUST_IGNORE_DEPRECATED_CPP_11)\n#  define    CUB_IGNORE_DEPRECATED_CPP_11\n#endif\n#if !defined(CUB_IGNORE_DEPRECATED_COMPILER) && \\\n     defined(THRUST_IGNORE_DEPRECATED_COMPILER)\n#  define    CUB_IGNORE_DEPRECATED_COMPILER\n#endif\n\n#ifdef CUB_IGNORE_DEPRECATED_CPP_DIALECT\n#  define CUB_IGNORE_DEPRECATED_CPP_11\n#  define CUB_IGNORE_DEPRECATED_COMPILER\n#endif\n\n// Define this to override the built-in detection.\n#ifndef CUB_CPP_DIALECT\n\n// MSVC does not define __cplusplus correctly. _MSVC_LANG is used instead.\n// This macro is only defined in MSVC 2015U3+.\n#  ifdef _MSVC_LANG // Do not replace with CUB_HOST_COMPILER test (see above)\n// MSVC2015 reports C++14 but lacks extended constexpr support. Treat as C++11.\n#    if CUB_MSVC_VERSION < 1910 && _MSVC_LANG > 201103L /* MSVC < 2017 && CPP > 2011 */\n#      define CUB_CPLUSPLUS 201103L /* Fix to 2011 */\n#    else\n#      define CUB_CPLUSPLUS _MSVC_LANG /* We'll trust this for now. */\n#    endif // MSVC 2015 C++14 fix\n#  else\n#    define CUB_CPLUSPLUS __cplusplus\n#  endif\n\n// Detect current dialect:\n#  if CUB_CPLUSPLUS < 201103L\n#    define CUB_CPP_DIALECT 2003\n#  elif CUB_CPLUSPLUS < 201402L\n#    define CUB_CPP_DIALECT 2011\n#  elif CUB_CPLUSPLUS < 201703L\n#    define CUB_CPP_DIALECT 2014\n#  elif CUB_CPLUSPLUS == 201703L\n#    define CUB_CPP_DIALECT 2017\n#  elif CUB_CPLUSPLUS > 201703L // unknown, but is higher than 2017.\n#    define CUB_CPP_DIALECT 2020\n#  endif\n\n#  undef CUB_CPLUSPLUS // cleanup\n\n#endif // !CUB_CPP_DIALECT\n\n// Define CUB_COMPILER_DEPRECATION macro:\n#if CUB_HOST_COMPILER == CUB_HOST_COMPILER_MSVC\n#  define CUB_COMP_DEPR_IMPL(msg) \\\n    __pragma(message(__FILE__ \":\" CUB_COMP_DEPR_IMPL0(__LINE__) \": warning: \" #msg))\n#  define CUB_COMP_DEPR_IMPL0(x) CUB_COMP_DEPR_IMPL1(x)\n#  define CUB_COMP_DEPR_IMPL1(x) #x\n#else // clang / gcc:\n#  define CUB_COMP_DEPR_IMPL(msg) CUB_COMP_DEPR_IMPL0(GCC warning #msg)\n#  define CUB_COMP_DEPR_IMPL0(expr) _Pragma(#expr)\n#  define CUB_COMP_DEPR_IMPL1 /* intentionally blank */\n#endif\n\n#define CUB_COMPILER_DEPRECATION(REQ) \\\n  CUB_COMP_DEPR_IMPL(CUB requires at least REQ. Define CUB_IGNORE_DEPRECATED_CPP_DIALECT to suppress this message.)\n\n#define CUB_COMPILER_DEPRECATION_SOFT(REQ, CUR) \\\n  CUB_COMP_DEPR_IMPL(CUB requires at least REQ. CUR is deprecated but still supported. CUR support will be removed in a future release. Define CUB_IGNORE_DEPRECATED_CPP_DIALECT to suppress this message.)\n\n#ifndef CUB_IGNORE_DEPRECATED_COMPILER\n\n// Compiler checks:\n#  if CUB_HOST_COMPILER == CUB_HOST_COMPILER_GCC && CUB_GCC_VERSION < 50000\n     CUB_COMPILER_DEPRECATION(GCC 5.0);\n#  elif CUB_HOST_COMPILER == CUB_HOST_COMPILER_CLANG && CUB_CLANG_VERSION < 70000\n     CUB_COMPILER_DEPRECATION(Clang 7.0);\n#  elif CUB_HOST_COMPILER == CUB_HOST_COMPILER_MSVC && CUB_MSVC_VERSION < 1910\n     // <2017. Hard upgrade message:\n     CUB_COMPILER_DEPRECATION(MSVC 2019 (19.20/16.0/14.20));\n#  elif CUB_HOST_COMPILER == CUB_HOST_COMPILER_MSVC && CUB_MSVC_VERSION < 1920\n     // >=2017, <2019. Soft deprecation message:\n     CUB_COMPILER_DEPRECATION_SOFT(MSVC 2019 (19.20/16.0/14.20), MSVC 2017);\n#  endif\n\n#endif // CUB_IGNORE_DEPRECATED_COMPILER\n\n#ifndef CUB_IGNORE_DEPRECATED_DIALECT\n\n// Dialect checks:\n#  if CUB_CPP_DIALECT < 2011\n     // <C++11. Hard upgrade message:\n     CUB_COMPILER_DEPRECATION(C++14);\n#  elif CUB_CPP_DIALECT == 2011 && !defined(CUB_IGNORE_DEPRECATED_CPP_11)\n     // =C++11. Soft upgrade message:\n     CUB_COMPILER_DEPRECATION_SOFT(C++14, C++11);\n#  endif\n\n#endif // CUB_IGNORE_DEPRECATED_DIALECT\n\n#undef CUB_COMPILER_DEPRECATION_SOFT\n#undef CUB_COMPILER_DEPRECATION\n#undef CUB_COMP_DEPR_IMPL\n#undef CUB_COMP_DEPR_IMPL0\n#undef CUB_COMP_DEPR_IMPL1\n\n#endif // _JITIFY_INCLUDE_GUARD_44D938EB20B46E5E\n", "util_debug.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_E187B871CF4D598D\n#define _JITIFY_INCLUDE_GUARD_E187B871CF4D598D\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2022, NVIDIA CORPORATION.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * Error and event logging routines.\n *\n * The following macros definitions are supported:\n * - \\p CUB_LOG.  Simple event messages are printed to \\p stdout.\n */\n\n#include <cub/util_namespace.cuh>\n#include <cub/util_arch.cuh>\n\n#include <nv/target>\n\n#include <cstdio>\n\nCUB_NAMESPACE_BEGIN\n\n\n#ifdef DOXYGEN_SHOULD_SKIP_THIS // Only parse this during doxygen passes:\n\n/**\n * @def CUB_DEBUG_LOG\n *\n * Causes kernel launch configurations to be printed to the console\n */\n#define CUB_DEBUG_LOG\n\n/**\n * @def CUB_DEBUG_SYNC\n *\n * Causes synchronization of the stream after every kernel launch to check \n * for errors. Also causes kernel launch configurations to be printed to the \n * console.\n */\n#define CUB_DEBUG_SYNC\n\n/**\n * @def CUB_DEBUG_HOST_ASSERTIONS\n *\n * Extends `CUB_DEBUG_SYNC` effects by checking host-side precondition \n * assertions.\n */\n#define CUB_DEBUG_HOST_ASSERTIONS\n\n/**\n * @def CUB_DEBUG_DEVICE_ASSERTIONS\n *\n * Extends `CUB_DEBUG_HOST_ASSERTIONS` effects by checking device-side \n * precondition assertions.\n */\n#define CUB_DEBUG_DEVICE_ASSERTIONS\n\n/**\n * @def CUB_DEBUG_ALL\n *\n * Causes host and device-side precondition assertions to be checked. Apart \n * from that, causes synchronization of the stream after every kernel launch to \n * check for errors. Also causes kernel launch configurations to be printed to \n * the console.\n */\n#define CUB_DEBUG_ALL\n\n#endif // DOXYGEN_SHOULD_SKIP_THIS \n\n/**\n * \\addtogroup UtilMgmt\n * @{\n */\n\n\n// `CUB_DETAIL_DEBUG_LEVEL_*`: Implementation details, internal use only:\n\n#define CUB_DETAIL_DEBUG_LEVEL_NONE 0\n#define CUB_DETAIL_DEBUG_LEVEL_HOST_ASSERTIONS_ONLY 1\n#define CUB_DETAIL_DEBUG_LEVEL_LOG 2\n#define CUB_DETAIL_DEBUG_LEVEL_SYNC 3\n#define CUB_DETAIL_DEBUG_LEVEL_HOST_ASSERTIONS 4\n#define CUB_DETAIL_DEBUG_LEVEL_DEVICE_ASSERTIONS 5\n#define CUB_DETAIL_DEBUG_LEVEL_ALL 1000\n\n// `CUB_DEBUG_*`: User interfaces:\n\n// Extra logging, no syncs\n#ifdef CUB_DEBUG_LOG\n#define CUB_DETAIL_DEBUG_LEVEL CUB_DETAIL_DEBUG_LEVEL_LOG\n#endif\n\n// Logging + syncs\n#ifdef CUB_DEBUG_SYNC\n#define CUB_DETAIL_DEBUG_LEVEL CUB_DETAIL_DEBUG_LEVEL_SYNC\n#endif\n\n// Logging + syncs + host assertions\n#ifdef CUB_DEBUG_HOST_ASSERTIONS\n#define CUB_DETAIL_DEBUG_LEVEL CUB_DETAIL_DEBUG_LEVEL_HOST_ASSERTIONS\n#endif\n\n// Logging + syncs + host assertions + device assertions\n#ifdef CUB_DEBUG_DEVICE_ASSERTIONS\n#define CUB_DETAIL_DEBUG_LEVEL CUB_DETAIL_DEBUG_LEVEL_DEVICE_ASSERTIONS\n#endif\n\n// All\n#ifdef CUB_DEBUG_ALL\n#define CUB_DETAIL_DEBUG_LEVEL CUB_DETAIL_DEBUG_LEVEL_ALL \n#endif\n\n// Default case, no extra debugging:\n#ifndef CUB_DETAIL_DEBUG_LEVEL\n#ifdef NDEBUG\n#define CUB_DETAIL_DEBUG_LEVEL CUB_DETAIL_DEBUG_LEVEL_NONE\n#else\n#define CUB_DETAIL_DEBUG_LEVEL CUB_DETAIL_DEBUG_LEVEL_HOST_ASSERTIONS_ONLY\n#endif\n#endif\n\n/*\n * `CUB_DETAIL_DEBUG_ENABLE_*`:\n * Internal implementation details, used for testing enabled debug features:\n */\n\n#if CUB_DETAIL_DEBUG_LEVEL >= CUB_DETAIL_DEBUG_LEVEL_LOG\n#define CUB_DETAIL_DEBUG_ENABLE_LOG\n#endif\n\n#if CUB_DETAIL_DEBUG_LEVEL >= CUB_DETAIL_DEBUG_LEVEL_SYNC\n#define CUB_DETAIL_DEBUG_ENABLE_SYNC\n#endif\n\n#if (CUB_DETAIL_DEBUG_LEVEL >= CUB_DETAIL_DEBUG_LEVEL_HOST_ASSERTIONS) || \\\n    (CUB_DETAIL_DEBUG_LEVEL == CUB_DETAIL_DEBUG_LEVEL_HOST_ASSERTIONS_ONLY)\n#define CUB_DETAIL_DEBUG_ENABLE_HOST_ASSERTIONS\n#endif\n\n#if CUB_DETAIL_DEBUG_LEVEL >= CUB_DETAIL_DEBUG_LEVEL_DEVICE_ASSERTIONS\n#define CUB_DETAIL_DEBUG_ENABLE_DEVICE_ASSERTIONS\n#endif\n\n\n/// CUB error reporting macro (prints error messages to stderr)\n#if (defined(DEBUG) || defined(_DEBUG)) && !defined(CUB_STDERR)\n    #define CUB_STDERR\n#endif\n\n/**\n * \\brief %If \\p CUB_STDERR is defined and \\p error is not \\p cudaSuccess, the\n * corresponding error message is printed to \\p stderr (or \\p stdout in device\n * code) along with the supplied source context.\n *\n * \\return The CUDA error.\n */\n__host__ __device__\n__forceinline__\ncudaError_t Debug(cudaError_t error, const char *filename, int line)\n{\n  // Clear the global CUDA error state which may have been set by the last\n  // call. Otherwise, errors may \"leak\" to unrelated kernel launches.\n\n  // clang-format off\n  #ifndef CUB_RDC_ENABLED\n  #define CUB_TEMP_DEVICE_CODE\n  #else\n  #define CUB_TEMP_DEVICE_CODE cudaGetLastError()\n  #endif\n\n  NV_IF_TARGET(\n    NV_IS_HOST, \n    (cudaGetLastError();),\n    (CUB_TEMP_DEVICE_CODE;)\n  );\n  \n  #undef CUB_TEMP_DEVICE_CODE\n  // clang-format on\n\n#ifdef CUB_STDERR\n  if (error)\n  {\n    NV_IF_TARGET(\n      NV_IS_HOST, (\n        fprintf(stderr,\n                \"CUDA error %d [%s, %d]: %s\\n\",\n                error,\n                filename,\n                line,\n                cudaGetErrorString(error));\n        fflush(stderr);\n      ),\n      (\n        printf(\"CUDA error %d [block (%d,%d,%d) thread (%d,%d,%d), %s, %d]\\n\",\n               error,\n               blockIdx.z,\n               blockIdx.y,\n               blockIdx.x,\n               threadIdx.z,\n               threadIdx.y,\n               threadIdx.x,\n               filename,\n               line);\n      )\n    );\n  }\n#else\n  (void)filename;\n  (void)line;\n#endif\n\n  return error;\n}\n\n/**\n * \\brief Debug macro\n */\n#ifndef CubDebug\n    #define CubDebug(e) CUB_NS_QUALIFIER::Debug((cudaError_t) (e), __FILE__, __LINE__)\n#endif\n\n\n/**\n * \\brief Debug macro with exit\n */\n#ifndef CubDebugExit\n    #define CubDebugExit(e) if (CUB_NS_QUALIFIER::Debug((cudaError_t) (e), __FILE__, __LINE__)) { exit(1); }\n#endif\n\n\n/**\n * \\brief Log macro for printf statements.\n */\n#if !defined(_CubLog)\n#if defined(_NVHPC_CUDA) || !(defined(__clang__) && defined(__CUDA__))\n\n// NVCC / NVC++\n#define _CubLog(format, ...)                                                   \\\n  do                                                                           \\\n  {                                                                            \\\n    NV_IF_TARGET(NV_IS_HOST,                                                   \\\n                 (printf(format, __VA_ARGS__);),                               \\\n                 (printf(\"[block (%d,%d,%d), thread (%d,%d,%d)]: \" format,     \\\n                         blockIdx.z,                                           \\\n                         blockIdx.y,                                           \\\n                         blockIdx.x,                                           \\\n                         threadIdx.z,                                          \\\n                         threadIdx.y,                                          \\\n                         threadIdx.x,                                          \\\n                         __VA_ARGS__);));                                      \\\n  } while (false)\n\n#else // Clang:\n\n// XXX shameless hack for clang around variadic printf...\n//     Compilies w/o supplying -std=c++11 but shows warning,\n//     so we silence them :)\n_Pragma(\"clang diagnostic ignored \\\"-Wc++11-extensions\\\"\")\n_Pragma(\"clang diagnostic ignored \\\"-Wunnamed-type-template-args\\\"\")\ntemplate <class... Args>\ninline __host__ __device__ void va_printf(char const *format,\n                                          Args const &...args)\n{\n#ifdef __CUDA_ARCH__\n  printf(format,\n         blockIdx.z,\n         blockIdx.y,\n         blockIdx.x,\n         threadIdx.z,\n         threadIdx.y,\n         threadIdx.x,\n         args...);\n#else\n  printf(format, args...);\n#endif\n}\n#ifndef __CUDA_ARCH__\n#define _CubLog(format, ...) CUB_NS_QUALIFIER::va_printf(format, __VA_ARGS__);\n#else\n#define _CubLog(format, ...)                                                   \\\n  CUB_NS_QUALIFIER::va_printf(\"[block (%d,%d,%d), thread \"                     \\\n                              \"(%d,%d,%d)]: \" format,                          \\\n                              __VA_ARGS__);\n#endif\n#endif\n#endif\n\n/** @} */       // end group UtilMgmt\n\nCUB_NAMESPACE_END\n\n#endif // _JITIFY_INCLUDE_GUARD_E187B871CF4D598D\n", "util_deprecated.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_D74F77C0B5838493\n#define _JITIFY_INCLUDE_GUARD_D74F77C0B5838493\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n *AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n *IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * Define CUB_DEPRECATED macro.\n */\n\n\n#include <cub/detail/type_traits.cuh>\n#include <cub/util_compiler.cuh>\n#include <cub/util_cpp_dialect.cuh>\n#include <cub/util_debug.cuh>\n\n\n#if defined(THRUST_IGNORE_DEPRECATED_API) && !defined(CUB_IGNORE_DEPRECATED_API)\n#  define CUB_IGNORE_DEPRECATED_API\n#endif\n\n#ifdef CUB_IGNORE_DEPRECATED_API\n#  define CUB_DEPRECATED\n#  define CUB_DEPRECATED_BECAUSE(MSG)\n#elif CUB_CPP_DIALECT >= 2014\n#  define CUB_DEPRECATED [[deprecated]]\n#  define CUB_DEPRECATED_BECAUSE(MSG) [[deprecated(MSG)]]\n#elif CUB_HOST_COMPILER == CUB_HOST_COMPILER_MSVC\n#  define CUB_DEPRECATED __declspec(deprecated)\n#  define CUB_DEPRECATED_BECAUSE(MSG) __declspec(deprecated(MSG))\n#elif CUB_HOST_COMPILER == CUB_HOST_COMPILER_CLANG\n#  define CUB_DEPRECATED __attribute__((deprecated))\n#  define CUB_DEPRECATED_BECAUSE(MSG) __attribute__((deprecated(MSG)))\n#elif CUB_HOST_COMPILER == CUB_HOST_COMPILER_GCC\n#  define CUB_DEPRECATED __attribute__((deprecated))\n#  define CUB_DEPRECATED_BECAUSE(MSG) __attribute__((deprecated(MSG)))\n#else\n#  define CUB_DEPRECATED\n#  define CUB_DEPRECATED_BECAUSE(MSG)\n#endif\n\n#define CUB_DETAIL_RUNTIME_DEBUG_SYNC_IS_NOT_SUPPORTED                         \\\n  CUB_DEPRECATED_BECAUSE(                                                      \\\n    \"CUB no longer accepts `debug_synchronous` parameter. \"                    \\\n    \"Define CUB_DEBUG_SYNC instead, or silence this message with \"             \\\n    \"CUB_IGNORE_DEPRECATED_API.\")\n\n#define CUB_DETAIL_RUNTIME_DEBUG_SYNC_USAGE_LOG                                \\\n  if (debug_synchronous)                                                       \\\n  {                                                                            \\\n    _CubLog(\"%s\\n\",                                                            \\\n            \"CUB no longer accepts `debug_synchronous` parameter. \"            \\\n            \"Define CUB_DEBUG_SYNC instead.\");                                 \\\n  }\n\n\n#endif // _JITIFY_INCLUDE_GUARD_D74F77C0B5838493\n", "util_macro.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_45CEB8529D0A8BEE\n#define _JITIFY_INCLUDE_GUARD_45CEB8529D0A8BEE\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.\n * \n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n * \n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/******************************************************************************\n * Common C/C++ macro utilities\n ******************************************************************************/\n\n#include <cuda/std/utility>\n\n#include \"util_namespace.cuh\"\n\nCUB_NAMESPACE_BEGIN\n\n\n/**\n * \\addtogroup UtilModule\n * @{\n */\n\n#ifndef CUB_ALIGN\n    #if defined(_WIN32) || defined(_WIN64)\n        /// Align struct\n        #define CUB_ALIGN(bytes) __declspec(align(32))\n    #else\n        /// Align struct\n        #define CUB_ALIGN(bytes) __attribute__((aligned(bytes)))\n    #endif\n#endif\n\n#define CUB_PREVENT_MACRO_SUBSTITUTION\n\ntemplate <typename T, typename U>\nconstexpr __host__ __device__ auto min CUB_PREVENT_MACRO_SUBSTITUTION(T &&t,\n                                                                      U &&u)\n  -> decltype(t < u ? ::cuda::std::forward<T>(t) : ::cuda::std::forward<U>(u))\n{\n  return t < u ? ::cuda::std::forward<T>(t) : ::cuda::std::forward<U>(u);\n}\n\ntemplate <typename T, typename U>\nconstexpr __host__ __device__ auto max CUB_PREVENT_MACRO_SUBSTITUTION(T &&t,\n                                                                      U &&u)\n  -> decltype(t < u ? ::cuda::std::forward<U>(u) : ::cuda::std::forward<T>(t))\n{\n  return t < u ? ::cuda::std::forward<U>(u) : ::cuda::std::forward<T>(t);\n}\n\n#ifndef CUB_MAX\n    /// Select maximum(a, b)\n    #define CUB_MAX(a, b) (((b) > (a)) ? (b) : (a))\n#endif\n\n#ifndef CUB_MIN\n    /// Select minimum(a, b)\n    #define CUB_MIN(a, b) (((b) < (a)) ? (b) : (a))\n#endif\n\n#ifndef CUB_QUOTIENT_FLOOR\n    /// Quotient of x/y rounded down to nearest integer\n    #define CUB_QUOTIENT_FLOOR(x, y) ((x) / (y))\n#endif\n\n#ifndef CUB_QUOTIENT_CEILING\n    /// Quotient of x/y rounded up to nearest integer\n    #define CUB_QUOTIENT_CEILING(x, y) (((x) + (y) - 1) / (y))\n#endif\n\n#ifndef CUB_ROUND_UP_NEAREST\n    /// x rounded up to the nearest multiple of y\n    #define CUB_ROUND_UP_NEAREST(x, y) ((((x) + (y) - 1) / (y)) * y)\n#endif\n\n#ifndef CUB_ROUND_DOWN_NEAREST\n    /// x rounded down to the nearest multiple of y\n    #define CUB_ROUND_DOWN_NEAREST(x, y) (((x) / (y)) * y)\n#endif\n\n\n#ifndef CUB_STATIC_ASSERT\n    #ifndef DOXYGEN_SHOULD_SKIP_THIS    // Do not document\n        #define CUB_CAT_(a, b) a ## b\n        #define CUB_CAT(a, b) CUB_CAT_(a, b)\n    #endif // DOXYGEN_SHOULD_SKIP_THIS\n\n    /// Static assert\n    #define CUB_STATIC_ASSERT(cond, msg) typedef int CUB_CAT(cub_static_assert, __LINE__)[(cond) ? 1 : -1]\n#endif\n\n/** @} */       // end group UtilModule\n\nCUB_NAMESPACE_END\n\n#endif // _JITIFY_INCLUDE_GUARD_45CEB8529D0A8BEE\n", "util_namespace.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_AE4EE898DA832DA0\n#define _JITIFY_INCLUDE_GUARD_AE4EE898DA832DA0\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file util_namespace.cuh\n * \\brief Utilities that allow `cub::` to be placed inside an\n * application-specific namespace.\n */\n\n\n// This is not used by this file; this is a hack so that we can detect the\n// CUB version from Thrust on older versions of CUB that did not have\n// version.cuh.\n#include \"version.cuh\"\n\n// Prior to 1.13.1, only the PREFIX/POSTFIX macros were used. Notify users\n// that they must now define the qualifier macro, too.\n#if (defined(CUB_NS_PREFIX) || defined(CUB_NS_POSTFIX)) && !defined(CUB_NS_QUALIFIER)\n#error CUB requires a definition of CUB_NS_QUALIFIER when CUB_NS_PREFIX/POSTFIX are defined.\n#endif\n\n/**\n * \\def THRUST_CUB_WRAPPED_NAMESPACE\n * If defined, this value will be used as the name of a namespace that wraps the\n * `thrust::` and `cub::` namespaces.\n * This macro should not be used with any other CUB namespace macros.\n */\n#ifdef THRUST_CUB_WRAPPED_NAMESPACE\n#define CUB_WRAPPED_NAMESPACE THRUST_CUB_WRAPPED_NAMESPACE\n#endif\n\n/**\n * \\def CUB_WRAPPED_NAMESPACE\n * If defined, this value will be used as the name of a namespace that wraps the\n * `cub::` namespace.\n * If THRUST_CUB_WRAPPED_NAMESPACE is set, this will inherit that macro's value.\n * This macro should not be used with any other CUB namespace macros.\n */\n#ifdef CUB_WRAPPED_NAMESPACE\n#define CUB_NS_PREFIX                                                       \\\n  namespace CUB_WRAPPED_NAMESPACE                                           \\\n  {\n\n#define CUB_NS_POSTFIX }\n\n#define CUB_NS_QUALIFIER ::CUB_WRAPPED_NAMESPACE::cub\n#endif\n\n/**\n * \\def CUB_NS_PREFIX\n * This macro is inserted prior to all `namespace cub { ... }` blocks. It is\n * derived from CUB_WRAPPED_NAMESPACE, if set, and will be empty otherwise.\n * It may be defined by users, in which case CUB_NS_PREFIX,\n * CUB_NS_POSTFIX, and CUB_NS_QUALIFIER must all be set consistently.\n */\n#ifndef CUB_NS_PREFIX\n#define CUB_NS_PREFIX\n#endif\n\n/**\n * \\def CUB_NS_POSTFIX\n * This macro is inserted following the closing braces of all\n * `namespace cub { ... }` block. It is defined appropriately when\n * CUB_WRAPPED_NAMESPACE is set, and will be empty otherwise. It may be\n * defined by users, in which case CUB_NS_PREFIX, CUB_NS_POSTFIX, and\n * CUB_NS_QUALIFIER must all be set consistently.\n */\n#ifndef CUB_NS_POSTFIX\n#define CUB_NS_POSTFIX\n#endif\n\n/**\n * \\def CUB_NS_QUALIFIER\n * This macro is used to qualify members of cub:: when accessing them from\n * outside of their namespace. By default, this is just `::cub`, and will be\n * set appropriately when CUB_WRAPPED_NAMESPACE is defined. This macro may be\n * defined by users, in which case CUB_NS_PREFIX, CUB_NS_POSTFIX, and\n * CUB_NS_QUALIFIER must all be set consistently.\n */\n#ifndef CUB_NS_QUALIFIER\n#define CUB_NS_QUALIFIER ::cub\n#endif\n\n#if !defined(CUB_DETAIL_MAGIC_NS_NAME)\n#define CUB_DETAIL_COUNT_N(_1, _2, _3, _4, _5, _6, _7, _8, _9, _10, _11, _12, _13, \\\n                           _14, _15, _16, _17, _18, _19, _20, N, ...)              \\\n                           N\n#define CUB_DETAIL_COUNT(...)                                                      \\\n  CUB_DETAIL_IDENTITY(CUB_DETAIL_COUNT_N(__VA_ARGS__, 20, 19, 18, 17, 16, 15, 14, 13, 12, \\\n                                         11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1))\n#define CUB_DETAIL_IDENTITY(N) N\n#define CUB_DETAIL_APPLY(MACRO, ...) CUB_DETAIL_IDENTITY(MACRO(__VA_ARGS__))\n#define CUB_DETAIL_MAGIC_NS_NAME1(P1) \\\n    CUB_##P1##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME2(P1, P2) \\\n    CUB_##P1##_##P2##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME3(P1, P2, P3) \\\n    CUB_##P1##_##P2##_##P3##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME4(P1, P2, P3, P4) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME5(P1, P2, P3, P4, P5) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_##P5##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME6(P1, P2, P3, P4, P5, P6) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_##P5##_##P6##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME7(P1, P2, P3, P4, P5, P6, P7) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_##P5##_##P6##_##P7##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME8(P1, P2, P3, P4, P5, P6, P7, P8) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_##P5##_##P6##_##P7##_##P8##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME9(P1, P2, P3, P4, P5, P6, P7, P8, P9) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_##P5##_##P6##_##P7##_##P8##_##P9##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME10(P1, P2, P3, P4, P5, P6, P7, P8, P9, P10) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_##P5##_##P6##_##P7##_##P8##_##P9##_##P10##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME11(P1, P2, P3, P4, P5, P6, P7, P8, P9, P10, P11) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_##P5##_##P6##_##P7##_##P8##_##P9##_##P10##_##P11##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME12(P1, P2, P3, P4, P5, P6, P7, P8, P9, P10, P11, P12) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_##P5##_##P6##_##P7##_##P8##_##P9##_##P10##_##P11##_##P12##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME13(P1, P2, P3, P4, P5, P6, P7, P8, P9, P10, P11, P12, P13) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_##P5##_##P6##_##P7##_##P8##_##P9##_##P10##_##P11##_##P12##_##P13##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME14(P1, P2, P3, P4, P5, P6, P7, P8, P9, P10, P11, P12, P13, P14) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_##P5##_##P6##_##P7##_##P8##_##P9##_##P10##_##P11##_##P12##_##P13##_##P14##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME15(P1, P2, P3, P4, P5, P6, P7, P8, P9, P10, P11, P12, P13, P14, P15) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_##P5##_##P6##_##P7##_##P8##_##P9##_##P10##_##P11##_##P12##_##P13##_##P14##_##P15##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME16(P1, P2, P3, P4, P5, P6, P7, P8, P9, P10, P11, P12, P13, P14, P15, P16) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_##P5##_##P6##_##P7##_##P8##_##P9##_##P10##_##P11##_##P12##_##P13##_##P14##_##P15##_##P16##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME17(P1, P2, P3, P4, P5, P6, P7, P8, P9, P10, P11, P12, P13, P14, P15, P16, P17) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_##P5##_##P6##_##P7##_##P8##_##P9##_##P10##_##P11##_##P12##_##P13##_##P14##_##P15##_##P16##_##P17##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME18(P1, P2, P3, P4, P5, P6, P7, P8, P9, P10, P11, P12, P13, P14, P15, P16, P17, P18) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_##P5##_##P6##_##P7##_##P8##_##P9##_##P10##_##P11##_##P12##_##P13##_##P14##_##P15##_##P16##_##P17##_##P18##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME19(P1, P2, P3, P4, P5, P6, P7, P8, P9, P10, P11, P12, P13, P14, P15, P16, P17, P18, P19) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_##P5##_##P6##_##P7##_##P8##_##P9##_##P10##_##P11##_##P12##_##P13##_##P14##_##P15##_##P16##_##P17##_##P18##_##P19##_NS\n#define CUB_DETAIL_MAGIC_NS_NAME20(P1, P2, P3, P4, P5, P6, P7, P8, P9, P10, P11, P12, P13, P14, P15, P16, P17, P18, P19, P20) \\\n    CUB_##P1##_##P2##_##P3##_##P4##_##P5##_##P6##_##P7##_##P8##_##P9##_##P10##_##P11##_##P12##_##P13##_##P14##_##P15##_##P16##_##P17##_##P18##_##P19##_##P20##_NS\n#define CUB_DETAIL_DISPATCH(N) CUB_DETAIL_MAGIC_NS_NAME ## N\n#define CUB_DETAIL_MAGIC_NS_NAME(...) CUB_DETAIL_IDENTITY(CUB_DETAIL_APPLY(CUB_DETAIL_DISPATCH, CUB_DETAIL_COUNT(__VA_ARGS__))(__VA_ARGS__))\n#endif // !defined(CUB_DETAIL_MAGIC_NS_NAME)\n\n#if defined(CUB_DISABLE_NAMESPACE_MAGIC)\n#if !defined(CUB_WRAPPED_NAMESPACE)\n#if !defined(CUB_IGNORE_NAMESPACE_MAGIC_ERROR)\n#error \"Disabling namespace magic is unsafe without wrapping namespace\"\n#endif // !defined(CUB_IGNORE_NAMESPACE_MAGIC_ERROR)\n#endif // !defined(CUB_WRAPPED_NAMESPACE)\n#define CUB_DETAIL_MAGIC_NS_BEGIN\n#define CUB_DETAIL_MAGIC_NS_END\n#else // not defined(CUB_DISABLE_NAMESPACE_MAGIC)\n#if defined(_NVHPC_CUDA)\n#define CUB_DETAIL_MAGIC_NS_BEGIN inline namespace CUB_DETAIL_MAGIC_NS_NAME(CUB_VERSION, NV_TARGET_SM_INTEGER_LIST) {\n#define CUB_DETAIL_MAGIC_NS_END }\n#else // not defined(_NVHPC_CUDA)\n#define CUB_DETAIL_MAGIC_NS_BEGIN inline namespace CUB_DETAIL_MAGIC_NS_NAME(CUB_VERSION, __CUDA_ARCH_LIST__) {\n#define CUB_DETAIL_MAGIC_NS_END }\n#endif // not defined(_NVHPC_CUDA)\n#endif // not defined(CUB_DISABLE_NAMESPACE_MAGIC)\n\n/**\n * \\def CUB_NAMESPACE_BEGIN\n * This macro is used to open a `cub::` namespace block, along with any\n * enclosing namespaces requested by CUB_WRAPPED_NAMESPACE, etc.\n * This macro is defined by CUB and may not be overridden.\n */\n#define CUB_NAMESPACE_BEGIN                                                 \\\n  CUB_NS_PREFIX                                                             \\\n  namespace cub                                                             \\\n  {                                                                         \\\n  CUB_DETAIL_MAGIC_NS_BEGIN                                                        \n\n/**\n * \\def CUB_NAMESPACE_END\n * This macro is used to close a `cub::` namespace block, along with any\n * enclosing namespaces requested by CUB_WRAPPED_NAMESPACE, etc.\n * This macro is defined by CUB and may not be overridden.\n */\n#define CUB_NAMESPACE_END                                                   \\\n  CUB_DETAIL_MAGIC_NS_END                                                   \\\n  } /* end namespace cub */                                                 \\\n  CUB_NS_POSTFIX\n\n// Declare these namespaces here for the purpose of Doxygenating them\nCUB_NS_PREFIX\n\n/*! \\namespace cub\n *  \\brief \\p cub is the top-level namespace which contains all CUB\n *         functions and types.\n */\nnamespace cub\n{\n}\n\nCUB_NS_POSTFIX\n\n#endif // _JITIFY_INCLUDE_GUARD_AE4EE898DA832DA0\n", "util_type.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_1375AABB974C3530\n#define _JITIFY_INCLUDE_GUARD_1375AABB974C3530\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011, Duane Merrill.  All rights reserved.\n * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.\n * \n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n * \n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/**\n * \\file\n * Common type manipulation (metaprogramming) utilities\n */\n\n#include <cfloat>\n#include <iostream>\n#include <iterator>\n#include <limits>\n\n#include <cuda.h>\n\n#if !_NVHPC_CUDA\n    #include <cuda_fp16.h>\n#endif\n#if !_NVHPC_CUDA && !defined(CUB_DISABLE_BF16_SUPPORT)\n    #include <cuda_bf16.h>\n#endif\n\n#include <cub/detail/uninitialized_copy.cuh>\n#include <cub/util_arch.cuh>\n#include <cub/util_compiler.cuh>\n#include <cub/util_deprecated.cuh>\n#include <cub/util_macro.cuh>\n#include <cub/util_namespace.cuh>\n\n#include <cuda/std/type_traits>\n\nCUB_NAMESPACE_BEGIN\n\n#ifndef CUB_IS_INT128_ENABLED\n#if defined(__CUDACC_RTC__)\n#if defined(__CUDACC_RTC_INT128__)\n#define CUB_IS_INT128_ENABLED 1\n#endif // !defined(__CUDACC_RTC_INT128__)\n#else  // !defined(__CUDACC_RTC__)\n#if CUDA_VERSION >= 11050\n#if (CUB_HOST_COMPILER == CUB_HOST_COMPILER_GCC) || \\\n    (CUB_HOST_COMPILER == CUB_HOST_COMPILER_CLANG) || \\\n    defined(__ICC) || defined(_NVHPC_CUDA)\n#define CUB_IS_INT128_ENABLED 1\n#endif // GCC || CLANG || ICC || NVHPC\n#endif // CTK >= 11.5\n#endif // !defined(__CUDACC_RTC__)\n#endif // !defined(CUB_IS_INT128_ENABLED)\n\n/**\n * \\addtogroup UtilModule\n * @{\n */\n\n\n\n/******************************************************************************\n * Conditional types\n ******************************************************************************/\n\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS // Do not document\nnamespace detail\n{\n\n\ntemplate <bool Test, class T1, class T2>\nusing conditional_t = typename std::conditional<Test, T1, T2>::type;\n\n\ntemplate <typename Iterator>\nusing value_t = typename std::iterator_traits<Iterator>::value_type;\n\ntemplate <typename It,\n          typename FallbackT,\n          bool = ::cuda::std::is_same<\n            typename ::cuda::std::remove_cv<typename ::cuda::std::remove_pointer<It>::type>::type,\n            void>::value>\nstruct non_void_value_impl\n{\n  using type = FallbackT;\n};\n\ntemplate <typename It, typename FallbackT>\nstruct non_void_value_impl<It, FallbackT, false>\n{\n  using type = typename ::cuda::std::conditional<\n    ::cuda::std::is_same<typename std::iterator_traits<It>::value_type, void>::value,\n    FallbackT,\n    typename std::iterator_traits<It>::value_type>::type;\n};\n\n/**\n * The output value type\n * type = (if IteratorT's value type is void) ?\n * ... then the FallbackT,\n * ... else the IteratorT's value type\n */\ntemplate <typename It, typename FallbackT>\nusing non_void_value_t = typename non_void_value_impl<It, FallbackT>::type;\n} // namespace detail\n\n\n/**\n * \\brief Type selection (<tt>IF ? ThenType : ElseType</tt>)\n *\n * \\deprecated [Since 1.16.0] The cub::If APIs are deprecated.\n *             Use cub::detail::conditional_t instead.\n */\ntemplate <bool IF, typename ThenType, typename ElseType>\nstruct CUB_DEPRECATED If\n{\n  using Type = cub::detail::conditional_t<IF, ThenType, ElseType>;\n};\n\n\n/******************************************************************************\n * Type equality\n ******************************************************************************/\n\n/**\n * \\brief Type equality test\n *\n * \\deprecated [Since 1.16.0] The cub::Equals APIs are deprecated.\n *             Use std::is_same instead.\n */\ntemplate <typename A, typename B>\nstruct CUB_DEPRECATED Equals\n{\n  static constexpr int VALUE = std::is_same<A, B>::value ? 1 : 0;\n  static constexpr int NEGATE = VALUE ? 0 : 1;\n};\n\n\n/******************************************************************************\n * Static math\n ******************************************************************************/\n\n/**\n * \\brief Statically determine log2(N), rounded up.\n *\n * For example:\n *     Log2<8>::VALUE   // 3\n *     Log2<3>::VALUE   // 2\n */\ntemplate <int N, int CURRENT_VAL = N, int COUNT = 0>\nstruct Log2\n{\n    /// Static logarithm value\n    enum { VALUE = Log2<N, (CURRENT_VAL >> 1), COUNT + 1>::VALUE };         // Inductive case\n};\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS    // Do not document\n\ntemplate <int N, int COUNT>\nstruct Log2<N, 0, COUNT>\n{\n    enum {VALUE = (1 << (COUNT - 1) < N) ?                                  // Base case\n        COUNT :\n        COUNT - 1 };\n};\n\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\n/**\n * \\brief Statically determine if N is a power-of-two\n */\ntemplate <int N>\nstruct PowerOfTwo\n{\n    enum { VALUE = ((N & (N - 1)) == 0) };\n};\n\n\n\n/******************************************************************************\n * Pointer vs. iterator detection\n ******************************************************************************/\n\n/**\n * \\brief Pointer vs. iterator\n *\n * \\deprecated [Since 1.16.0] The cub::IsPointer APIs are deprecated.\n *             Use std::is_pointer instead.\n */\ntemplate <typename Tp>\nstruct CUB_DEPRECATED IsPointer\n{\n  static constexpr int VALUE = std::is_pointer<Tp>::value;\n};\n\n\n/******************************************************************************\n * Qualifier detection\n ******************************************************************************/\n\n/**\n * \\brief Volatile modifier test\n *\n * \\deprecated [Since 1.16.0] The cub::IsVolatile APIs are deprecated.\n *             Use std::is_volatile instead.\n */\ntemplate <typename Tp>\nstruct CUB_DEPRECATED IsVolatile\n{\n  static constexpr int VALUE = std::is_volatile<Tp>::value;\n};\n\n/******************************************************************************\n * Qualifier removal\n ******************************************************************************/\n\n/**\n * \\brief Removes \\p const and \\p volatile qualifiers from type \\p Tp.\n *\n * \\deprecated [Since 1.16.0] The cub::RemoveQualifiers APIs are deprecated.\n *             Use std::remove_cv instead.\n *\n * For example:\n *     <tt>typename RemoveQualifiers<volatile int>::Type         // int;</tt>\n */\ntemplate <typename Tp, typename Up = Tp>\nstruct CUB_DEPRECATED RemoveQualifiers\n{\n  using Type = typename std::remove_cv<Tp>::type;\n};\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\n\n/******************************************************************************\n * Marker types\n ******************************************************************************/\n\n#ifndef DOXYGEN_SHOULD_SKIP_THIS    // Do not document\n\n/**\n * \\brief A simple \"NULL\" marker type\n */\nstruct NullType\n{\n    using value_type = NullType;\n\n    template <typename T>\n    __host__ __device__ __forceinline__ NullType& operator =(const T&) { return *this; }\n\n    __host__ __device__ __forceinline__ bool operator ==(const NullType&) { return true; }\n\n    __host__ __device__ __forceinline__ bool operator !=(const NullType&) { return false; }\n};\n\n\n/**\n * \\brief Allows for the treatment of an integral constant as a type at compile-time (e.g., to achieve static call dispatch based on constant integral values)\n */\ntemplate <int A>\nstruct Int2Type\n{\n    enum {VALUE = A};\n};\n\n/**\n * \\brief Allows algorithms that take a value as input to take a future value that is not computed yet at launch time.\n *\n * Note that it is user's responsibility to ensure that the result will be ready before use via external synchronization\n * or stream-ordering dependencies.\n *\n * \\code\n * int *d_intermediate_result;\n * allocator.DeviceAllocate((void **)&d_intermediate_result, sizeof(int));\n * compute_intermediate_result<<<blocks, threads>>>(\n *     d_intermediate_result,  // output\n *     arg1,                   // input\n *     arg2);                  // input\n * cub::FutureValue<int> init_value(d_intermediate_result);\n * cub::DeviceScan::ExclusiveScan(\n *     d_temp_storage,\n *     temp_storage_bytes,\n *     d_in,\n *     d_out,\n *     cub::Sum(),\n *     init_value,\n *     num_items);\n * allocator.DeviceFree(d_intermediate_result);\n * \\endcode\n */\ntemplate <typename T, typename IterT = T*>\nstruct FutureValue\n{\n    using value_type = T;\n    using iterator_type = IterT;\n    explicit __host__ __device__ __forceinline__ FutureValue(IterT iter):m_iter(iter) {}\n    __host__ __device__ __forceinline__ operator T() {\n        return *m_iter;\n    }\n\nprivate:\n    IterT m_iter;\n};\n\nnamespace detail {\n\n/**\n * \\brief Allows algorithms to instantiate a single kernel to support both immediate value and future value.\n */\ntemplate <typename T, typename IterT = T*>\nstruct InputValue\n{\n    using value_type = T;\n    using iterator_type = IterT;\n    __host__ __device__ __forceinline__ operator T() {\n        if (m_is_future) {\n            return m_future_value;\n        }\n        return m_immediate_value;\n    }\n    explicit __host__ __device__ __forceinline__ InputValue(T immediate_value): m_is_future(false), m_immediate_value(immediate_value) {}\n    explicit __host__ __device__ __forceinline__ InputValue(FutureValue<T, IterT> future_value): m_is_future(true), m_future_value(future_value) {}\n    __host__ __device__ __forceinline__ InputValue(const InputValue &other): m_is_future(other.m_is_future) {\n        if (m_is_future) {\n            m_future_value = other.m_future_value;\n        } else {\n          detail::uninitialized_copy(&m_immediate_value,\n                                     other.m_immediate_value);\n        }\n    }\n\nprivate:\n    bool m_is_future;\n    union\n    {\n        FutureValue<T, IterT> m_future_value;\n        T m_immediate_value;\n    };\n};\n\n} // namespace detail\n\n\n/******************************************************************************\n * Size and alignment\n ******************************************************************************/\n\n/// Structure alignment\ntemplate <typename T>\nstruct AlignBytes\n{\n    struct Pad\n    {\n        T       val;\n        char    byte;\n    };\n\n    enum\n    {\n        /// The \"true CUDA\" alignment of T in bytes\n        ALIGN_BYTES = sizeof(Pad) - sizeof(T)\n    };\n\n    /// The \"truly aligned\" type\n    typedef T Type;\n};\n\n// Specializations where host C++ compilers (e.g., 32-bit Windows) may disagree\n// with device C++ compilers (EDG) on types passed as template parameters through\n// kernel functions\n\n#define __CUB_ALIGN_BYTES(t, b)         \\\n    template <> struct AlignBytes<t>    \\\n    { enum { ALIGN_BYTES = b }; typedef __align__(b) t Type; };\n\n__CUB_ALIGN_BYTES(short4, 8)\n__CUB_ALIGN_BYTES(ushort4, 8)\n__CUB_ALIGN_BYTES(int2, 8)\n__CUB_ALIGN_BYTES(uint2, 8)\n__CUB_ALIGN_BYTES(long long, 8)\n__CUB_ALIGN_BYTES(unsigned long long, 8)\n__CUB_ALIGN_BYTES(float2, 8)\n__CUB_ALIGN_BYTES(double, 8)\n#ifdef _WIN32\n    __CUB_ALIGN_BYTES(long2, 8)\n    __CUB_ALIGN_BYTES(ulong2, 8)\n#else\n    __CUB_ALIGN_BYTES(long2, 16)\n    __CUB_ALIGN_BYTES(ulong2, 16)\n#endif\n__CUB_ALIGN_BYTES(int4, 16)\n__CUB_ALIGN_BYTES(uint4, 16)\n__CUB_ALIGN_BYTES(float4, 16)\n__CUB_ALIGN_BYTES(long4, 16)\n__CUB_ALIGN_BYTES(ulong4, 16)\n__CUB_ALIGN_BYTES(longlong2, 16)\n__CUB_ALIGN_BYTES(ulonglong2, 16)\n__CUB_ALIGN_BYTES(double2, 16)\n__CUB_ALIGN_BYTES(longlong4, 16)\n__CUB_ALIGN_BYTES(ulonglong4, 16)\n__CUB_ALIGN_BYTES(double4, 16)\n\n// clang-format off\ntemplate <typename T> struct AlignBytes<volatile T> : AlignBytes<T> {};\ntemplate <typename T> struct AlignBytes<const T> : AlignBytes<T> {};\ntemplate <typename T> struct AlignBytes<const volatile T> : AlignBytes<T> {};\n// clang-format on\n\n/// Unit-words of data movement\ntemplate <typename T>\nstruct UnitWord\n{\n    enum {\n        ALIGN_BYTES = AlignBytes<T>::ALIGN_BYTES\n    };\n\n    template <typename Unit>\n    struct IsMultiple\n    {\n        enum {\n            UNIT_ALIGN_BYTES    = AlignBytes<Unit>::ALIGN_BYTES,\n            IS_MULTIPLE         = (sizeof(T) % sizeof(Unit) == 0) && (int(ALIGN_BYTES) % int(UNIT_ALIGN_BYTES) == 0)\n        };\n    };\n\n    /// Biggest shuffle word that T is a whole multiple of and is not larger than\n    /// the alignment of T\n    using ShuffleWord = cub::detail::conditional_t<\n      IsMultiple<int>::IS_MULTIPLE,\n      unsigned int,\n      cub::detail::conditional_t<IsMultiple<short>::IS_MULTIPLE,\n                                 unsigned short,\n                                 unsigned char>>;\n\n    /// Biggest volatile word that T is a whole multiple of and is not larger than\n    /// the alignment of T\n    using VolatileWord =\n      cub::detail::conditional_t<IsMultiple<long long>::IS_MULTIPLE,\n                                 unsigned long long,\n                                 ShuffleWord>;\n\n    /// Biggest memory-access word that T is a whole multiple of and is not larger\n    /// than the alignment of T\n    using DeviceWord =\n      cub::detail::conditional_t<IsMultiple<longlong2>::IS_MULTIPLE,\n                                 ulonglong2,\n                                 VolatileWord>;\n\n    /// Biggest texture reference word that T is a whole multiple of and is not\n    /// larger than the alignment of T\n    using TextureWord = cub::detail::conditional_t<\n      IsMultiple<int4>::IS_MULTIPLE,\n      uint4,\n      cub::detail::conditional_t<IsMultiple<int2>::IS_MULTIPLE, uint2, ShuffleWord>>;\n};\n\n// float2 specialization workaround (for SM10-SM13)\ntemplate <>\nstruct UnitWord <float2>\n{\n    typedef int         ShuffleWord;\n    typedef unsigned long long   VolatileWord;\n    typedef unsigned long long   DeviceWord;\n    typedef float2      TextureWord;\n};\n\n// float4 specialization workaround (for SM10-SM13)\ntemplate <>\nstruct UnitWord <float4>\n{\n    typedef int         ShuffleWord;\n    typedef unsigned long long  VolatileWord;\n    typedef ulonglong2          DeviceWord;\n    typedef float4              TextureWord;\n};\n\n\n// char2 specialization workaround (for SM10-SM13)\ntemplate <>\nstruct UnitWord <char2>\n{\n    typedef unsigned short      ShuffleWord;\n    typedef unsigned short      VolatileWord;\n    typedef unsigned short      DeviceWord;\n    typedef unsigned short      TextureWord;\n};\n\n// clang-format off\ntemplate <typename T> struct UnitWord<volatile T> : UnitWord<T> {};\ntemplate <typename T> struct UnitWord<const T> : UnitWord<T> {};\ntemplate <typename T> struct UnitWord<const volatile T> : UnitWord<T> {};\n// clang-format on\n\n\n/******************************************************************************\n * Vector type inference utilities.\n ******************************************************************************/\n\n/**\n * \\brief Exposes a member typedef \\p Type that names the corresponding CUDA vector type if one exists.  Otherwise \\p Type refers to the CubVector structure itself, which will wrap the corresponding \\p x, \\p y, etc. vector fields.\n */\ntemplate <typename T, int vec_elements> struct CubVector;\n\n\nenum\n{\n    /// The maximum number of elements in CUDA vector types\n    MAX_VEC_ELEMENTS = 4,\n};\n\n\n/**\n * Generic vector-1 type\n */\ntemplate <typename T>\nstruct CubVector<T, 1>\n{\n    T x;\n\n    typedef T BaseType;\n    typedef CubVector<T, 1> Type;\n};\n\n/**\n * Generic vector-2 type\n */\ntemplate <typename T>\nstruct CubVector<T, 2>\n{\n    T x;\n    T y;\n\n    typedef T BaseType;\n    typedef CubVector<T, 2> Type;\n};\n\n/**\n * Generic vector-3 type\n */\ntemplate <typename T>\nstruct CubVector<T, 3>\n{\n    T x;\n    T y;\n    T z;\n\n    typedef T BaseType;\n    typedef CubVector<T, 3> Type;\n};\n\n/**\n * Generic vector-4 type\n */\ntemplate <typename T>\nstruct CubVector<T, 4>\n{\n    T x;\n    T y;\n    T z;\n    T w;\n\n    typedef T BaseType;\n    typedef CubVector<T, 4> Type;\n};\n\n\n/**\n * Macro for expanding partially-specialized built-in vector types\n */\n#define CUB_DEFINE_VECTOR_TYPE(base_type,short_type)                                                    \\\n                                                                                                        \\\n    template<> struct CubVector<base_type, 1> : short_type##1                                           \\\n    {                                                                                                   \\\n      typedef base_type       BaseType;                                                                 \\\n      typedef short_type##1   Type;                                                                     \\\n      __host__ __device__ __forceinline__ CubVector operator+(const CubVector &other) const {           \\\n          CubVector retval;                                                                             \\\n          retval.x = x + other.x;                                                                       \\\n          return retval;                                                                                \\\n      }                                                                                                 \\\n      __host__ __device__ __forceinline__ CubVector operator-(const CubVector &other) const {           \\\n          CubVector retval;                                                                             \\\n          retval.x = x - other.x;                                                                       \\\n          return retval;                                                                                \\\n      }                                                                                                 \\\n    };                                                                                                  \\\n                                                                                                        \\\n    template<> struct CubVector<base_type, 2> : short_type##2                                           \\\n    {                                                                                                   \\\n        typedef base_type       BaseType;                                                               \\\n        typedef short_type##2   Type;                                                                   \\\n        __host__ __device__ __forceinline__ CubVector operator+(const CubVector &other) const {         \\\n            CubVector retval;                                                                           \\\n            retval.x = x + other.x;                                                                     \\\n            retval.y = y + other.y;                                                                     \\\n            return retval;                                                                              \\\n        }                                                                                               \\\n        __host__ __device__ __forceinline__ CubVector operator-(const CubVector &other) const {         \\\n            CubVector retval;                                                                           \\\n            retval.x = x - other.x;                                                                     \\\n            retval.y = y - other.y;                                                                     \\\n            return retval;                                                                              \\\n        }                                                                                               \\\n    };                                                                                                  \\\n                                                                                                        \\\n    template<> struct CubVector<base_type, 3> : short_type##3                                           \\\n    {                                                                                                   \\\n        typedef base_type       BaseType;                                                               \\\n        typedef short_type##3   Type;                                                                   \\\n        __host__ __device__ __forceinline__ CubVector operator+(const CubVector &other) const {         \\\n            CubVector retval;                                                                           \\\n            retval.x = x + other.x;                                                                     \\\n            retval.y = y + other.y;                                                                     \\\n            retval.z = z + other.z;                                                                     \\\n            return retval;                                                                              \\\n        }                                                                                               \\\n        __host__ __device__ __forceinline__ CubVector operator-(const CubVector &other) const {         \\\n            CubVector retval;                                                                           \\\n            retval.x = x - other.x;                                                                     \\\n            retval.y = y - other.y;                                                                     \\\n            retval.z = z - other.z;                                                                     \\\n            return retval;                                                                              \\\n        }                                                                                               \\\n    };                                                                                                  \\\n                                                                                                        \\\n    template<> struct CubVector<base_type, 4> : short_type##4                                           \\\n    {                                                                                                   \\\n        typedef base_type       BaseType;                                                               \\\n        typedef short_type##4   Type;                                                                   \\\n        __host__ __device__ __forceinline__ CubVector operator+(const CubVector &other) const {         \\\n            CubVector retval;                                                                           \\\n            retval.x = x + other.x;                                                                     \\\n            retval.y = y + other.y;                                                                     \\\n            retval.z = z + other.z;                                                                     \\\n            retval.w = w + other.w;                                                                     \\\n            return retval;                                                                              \\\n        }                                                                                               \\\n        __host__ __device__ __forceinline__ CubVector operator-(const CubVector &other) const {         \\\n            CubVector retval;                                                                           \\\n            retval.x = x - other.x;                                                                     \\\n            retval.y = y - other.y;                                                                     \\\n            retval.z = z - other.z;                                                                     \\\n            retval.w = w - other.w;                                                                     \\\n            return retval;                                                                              \\\n        }                                                                                               \\\n    };\n\n\n\n// Expand CUDA vector types for built-in primitives\n// clang-format off\nCUB_DEFINE_VECTOR_TYPE(char,               char)\nCUB_DEFINE_VECTOR_TYPE(signed char,        char)\nCUB_DEFINE_VECTOR_TYPE(short,              short)\nCUB_DEFINE_VECTOR_TYPE(int,                int)\nCUB_DEFINE_VECTOR_TYPE(long,               long)\nCUB_DEFINE_VECTOR_TYPE(long long,          longlong)\nCUB_DEFINE_VECTOR_TYPE(unsigned char,      uchar)\nCUB_DEFINE_VECTOR_TYPE(unsigned short,     ushort)\nCUB_DEFINE_VECTOR_TYPE(unsigned int,       uint)\nCUB_DEFINE_VECTOR_TYPE(unsigned long,      ulong)\nCUB_DEFINE_VECTOR_TYPE(unsigned long long, ulonglong)\nCUB_DEFINE_VECTOR_TYPE(float,              float)\nCUB_DEFINE_VECTOR_TYPE(double,             double)\nCUB_DEFINE_VECTOR_TYPE(bool,               uchar)\n// clang-format on\n\n// Undefine macros\n#undef CUB_DEFINE_VECTOR_TYPE\n\n\n/******************************************************************************\n * Wrapper types\n ******************************************************************************/\n\n/**\n * \\brief A storage-backing wrapper that allows types with non-trivial constructors to be aliased in unions\n */\ntemplate <typename T>\nstruct Uninitialized\n{\n    /// Biggest memory-access word that T is a whole multiple of and is not larger than the alignment of T\n    typedef typename UnitWord<T>::DeviceWord DeviceWord;\n\n    static constexpr std::size_t DATA_SIZE = sizeof(T);\n    static constexpr std::size_t WORD_SIZE = sizeof(DeviceWord);\n    static constexpr std::size_t WORDS = DATA_SIZE / WORD_SIZE;\n\n    /// Backing storage\n    DeviceWord storage[WORDS];\n\n    /// Alias\n    __host__ __device__ __forceinline__ T& Alias()\n    {\n        return reinterpret_cast<T&>(*this);\n    }\n};\n\n\n/**\n * \\brief A key identifier paired with a corresponding value\n */\ntemplate <\n    typename    _Key,\n    typename    _Value\n#if defined(_WIN32) && !defined(_WIN64)\n    , bool KeyIsLT = (AlignBytes<_Key>::ALIGN_BYTES < AlignBytes<_Value>::ALIGN_BYTES)\n    , bool ValIsLT = (AlignBytes<_Value>::ALIGN_BYTES < AlignBytes<_Key>::ALIGN_BYTES)\n#endif // #if defined(_WIN32) && !defined(_WIN64)\n    >\nstruct KeyValuePair\n{\n    typedef _Key    Key;                ///< Key data type\n    typedef _Value  Value;              ///< Value data type\n\n    Key     key;                        ///< Item key\n    Value   value;                      ///< Item value\n\n    /// Constructor\n    __host__ __device__ __forceinline__\n    KeyValuePair() {}\n\n    /// Constructor\n    __host__ __device__ __forceinline__\n    KeyValuePair(Key const& key, Value const& value) : key(key), value(value) {}\n\n    /// Inequality operator\n    __host__ __device__ __forceinline__ bool operator !=(const KeyValuePair &b)\n    {\n        return (value != b.value) || (key != b.key);\n    }\n};\n\n#if defined(_WIN32) && !defined(_WIN64)\n\n/**\n * Win32 won't do 16B alignment.  This can present two problems for\n * should-be-16B-aligned (but actually 8B aligned) built-in and intrinsics members:\n * 1) If a smaller-aligned item were to be listed first, the host compiler places the\n *    should-be-16B item at too early an offset (and disagrees with device compiler)\n * 2) Or, if a smaller-aligned item lists second, the host compiler gets the size\n *    of the struct wrong (and disagrees with device compiler)\n *\n * So we put the larger-should-be-aligned item first, and explicitly pad the\n * end of the struct\n */\n\n/// Smaller key specialization\ntemplate <typename K, typename V>\nstruct KeyValuePair<K, V, true, false>\n{\n    typedef K Key;\n    typedef V Value;\n\n    typedef char Pad[AlignBytes<V>::ALIGN_BYTES - AlignBytes<K>::ALIGN_BYTES];\n\n    Value   value;  // Value has larger would-be alignment and goes first\n    Key     key;\n    Pad     pad;\n\n    /// Constructor\n    __host__ __device__ __forceinline__\n    KeyValuePair() {}\n\n    /// Constructor\n    __host__ __device__ __forceinline__\n    KeyValuePair(Key const& key, Value const& value) : key(key), value(value) {}\n\n    /// Inequality operator\n    __host__ __device__ __forceinline__ bool operator !=(const KeyValuePair &b)\n    {\n        return (value != b.value) || (key != b.key);\n    }\n};\n\n\n/// Smaller value specialization\ntemplate <typename K, typename V>\nstruct KeyValuePair<K, V, false, true>\n{\n    typedef K Key;\n    typedef V Value;\n\n    typedef char Pad[AlignBytes<K>::ALIGN_BYTES - AlignBytes<V>::ALIGN_BYTES];\n\n    Key     key;    // Key has larger would-be alignment and goes first\n    Value   value;\n    Pad     pad;\n\n    /// Constructor\n    __host__ __device__ __forceinline__\n    KeyValuePair() {}\n\n    /// Constructor\n    __host__ __device__ __forceinline__\n    KeyValuePair(Key const& key, Value const& value) : key(key), value(value) {}\n\n    /// Inequality operator\n    __host__ __device__ __forceinline__ bool operator !=(const KeyValuePair &b)\n    {\n        return (value != b.value) || (key != b.key);\n    }\n};\n\n#endif // #if defined(_WIN32) && !defined(_WIN64)\n\n\n/**\n * \\brief A wrapper for passing simple static arrays as kernel parameters\n */\ntemplate <typename T, int COUNT>\nstruct ArrayWrapper\n{\n\n    /// Statically-sized array of type \\p T\n    T array[COUNT];\n\n    /// Constructor\n    __host__ __device__ __forceinline__ ArrayWrapper() {}\n};\n\n\n/**\n * \\brief Double-buffer storage wrapper for multi-pass stream transformations that require more than one storage array for streaming intermediate results back and forth.\n *\n * Many multi-pass computations require a pair of \"ping-pong\" storage\n * buffers (e.g., one for reading from and the other for writing to, and then\n * vice-versa for the subsequent pass).  This structure wraps a set of device\n * buffers and a \"selector\" member to track which is \"current\".\n */\ntemplate <typename T>\nstruct DoubleBuffer\n{\n    /// Pair of device buffer pointers\n    T *d_buffers[2];\n\n    ///  Selector into \\p d_buffers (i.e., the active/valid buffer)\n    int selector;\n\n    /// \\brief Constructor\n    __host__ __device__ __forceinline__ DoubleBuffer()\n    {\n        selector = 0;\n        d_buffers[0] = NULL;\n        d_buffers[1] = NULL;\n    }\n\n    /// \\brief Constructor\n    __host__ __device__ __forceinline__ DoubleBuffer(\n        T *d_current,         ///< The currently valid buffer\n        T *d_alternate)       ///< Alternate storage buffer of the same size as \\p d_current\n    {\n        selector = 0;\n        d_buffers[0] = d_current;\n        d_buffers[1] = d_alternate;\n    }\n\n    /// \\brief Return pointer to the currently valid buffer\n    __host__ __device__ __forceinline__ T* Current() { return d_buffers[selector]; }\n\n    /// \\brief Return pointer to the currently invalid buffer\n    __host__ __device__ __forceinline__ T* Alternate() { return d_buffers[selector ^ 1]; }\n\n};\n\n\n\n/******************************************************************************\n * Typedef-detection\n ******************************************************************************/\n\n\n/**\n * \\brief Defines a structure \\p detector_name that is templated on type \\p T.  The \\p detector_name struct exposes a constant member \\p VALUE indicating whether or not parameter \\p T exposes a nested type \\p nested_type_name\n */\n#define CUB_DEFINE_DETECT_NESTED_TYPE(detector_name, nested_type_name)  \\\n    template <typename T>                                               \\\n    struct detector_name                                                \\\n    {                                                                   \\\n        template <typename C>                                           \\\n        static char& test(typename C::nested_type_name*);               \\\n        template <typename>                                             \\\n        static int& test(...);                                          \\\n        enum                                                            \\\n        {                                                               \\\n            VALUE = sizeof(test<T>(0)) < sizeof(int)                    \\\n        };                                                              \\\n    };\n\n\n\n/******************************************************************************\n * Simple enable-if (similar to Boost)\n ******************************************************************************/\n\n/**\n * \\brief Simple enable-if (similar to Boost)\n *\n * \\deprecated [Since 1.16.0] The cub::If APIs are deprecated.\n *             Use std::enable_if instead.\n */\ntemplate <bool Condition, class T = void>\nstruct CUB_DEPRECATED EnableIf\n{\n  using Type = typename std::enable_if<Condition, T>::type;\n};\n\n/******************************************************************************\n * Typedef-detection\n ******************************************************************************/\n\n/**\n * \\brief Determine whether or not BinaryOp's functor is of the form <tt>bool operator()(const T& a, const T&b)</tt> or <tt>bool operator()(const T& a, const T&b, unsigned int idx)</tt>\n */\ntemplate <typename T, typename BinaryOp>\nstruct BinaryOpHasIdxParam\n{\nprivate:\n/*\n    template <typename BinaryOpT, bool (BinaryOpT::*)(const T &a, const T &b, unsigned int idx) const>  struct SFINAE1 {};\n    template <typename BinaryOpT, bool (BinaryOpT::*)(const T &a, const T &b, unsigned int idx)>        struct SFINAE2 {};\n    template <typename BinaryOpT, bool (BinaryOpT::*)(T a, T b, unsigned int idx) const>                struct SFINAE3 {};\n    template <typename BinaryOpT, bool (BinaryOpT::*)(T a, T b, unsigned int idx)>                      struct SFINAE4 {};\n*/\n    template <typename BinaryOpT, bool (BinaryOpT::*)(const T &a, const T &b, int idx) const>           struct SFINAE5 {};\n    template <typename BinaryOpT, bool (BinaryOpT::*)(const T &a, const T &b, int idx)>                 struct SFINAE6 {};\n    template <typename BinaryOpT, bool (BinaryOpT::*)(T a, T b, int idx) const>                         struct SFINAE7 {};\n    template <typename BinaryOpT, bool (BinaryOpT::*)(T a, T b, int idx)>                               struct SFINAE8 {};\n/*\n    template <typename BinaryOpT> static char Test(SFINAE1<BinaryOpT, &BinaryOpT::operator()> *);\n    template <typename BinaryOpT> static char Test(SFINAE2<BinaryOpT, &BinaryOpT::operator()> *);\n    template <typename BinaryOpT> static char Test(SFINAE3<BinaryOpT, &BinaryOpT::operator()> *);\n    template <typename BinaryOpT> static char Test(SFINAE4<BinaryOpT, &BinaryOpT::operator()> *);\n*/\n    template <typename BinaryOpT> __host__ __device__ static char Test(SFINAE5<BinaryOpT, &BinaryOpT::operator()> *);\n    template <typename BinaryOpT> __host__ __device__ static char Test(SFINAE6<BinaryOpT, &BinaryOpT::operator()> *);\n    template <typename BinaryOpT> __host__ __device__ static char Test(SFINAE7<BinaryOpT, &BinaryOpT::operator()> *);\n    template <typename BinaryOpT> __host__ __device__ static char Test(SFINAE8<BinaryOpT, &BinaryOpT::operator()> *);\n\n    template <typename BinaryOpT> static int Test(...);\n\npublic:\n\n    /// Whether the functor BinaryOp has a third <tt>unsigned int</tt> index param\n    static const bool HAS_PARAM = sizeof(Test<BinaryOp>(NULL)) == sizeof(char);\n};\n\n\n\n\n/******************************************************************************\n * Simple type traits utilities.\n *\n * For example:\n *     Traits<int>::CATEGORY             // SIGNED_INTEGER\n *     Traits<NullType>::NULL_TYPE       // true\n *     Traits<uint4>::CATEGORY           // NOT_A_NUMBER\n *     Traits<uint4>::PRIMITIVE;         // false\n *\n ******************************************************************************/\n\n/**\n * \\brief Basic type traits categories\n */\nenum Category\n{\n    NOT_A_NUMBER,\n    SIGNED_INTEGER,\n    UNSIGNED_INTEGER,\n    FLOATING_POINT\n};\n\n\n/**\n * \\brief Basic type traits\n */\ntemplate <Category _CATEGORY, bool _PRIMITIVE, bool _NULL_TYPE, typename _UnsignedBits, typename T>\nstruct BaseTraits\n{\n    /// Category\n    static const Category CATEGORY      = _CATEGORY;\n    enum\n    {\n        PRIMITIVE       = _PRIMITIVE,\n        NULL_TYPE       = _NULL_TYPE,\n    };\n};\n\n\n/**\n * Basic type traits (unsigned primitive specialization)\n */\ntemplate <typename _UnsignedBits, typename T>\nstruct BaseTraits<UNSIGNED_INTEGER, true, false, _UnsignedBits, T>\n{\n    typedef _UnsignedBits       UnsignedBits;\n\n    static const Category       CATEGORY    = UNSIGNED_INTEGER;\n    static const UnsignedBits   LOWEST_KEY  = UnsignedBits(0);\n    static const UnsignedBits   MAX_KEY     = UnsignedBits(-1);\n\n    enum\n    {\n        PRIMITIVE       = true,\n        NULL_TYPE       = false,\n    };\n\n\n    static __host__ __device__ __forceinline__ UnsignedBits TwiddleIn(UnsignedBits key)\n    {\n        return key;\n    }\n\n    static __host__ __device__ __forceinline__ UnsignedBits TwiddleOut(UnsignedBits key)\n    {\n        return key;\n    }\n\n    static __host__ __device__ __forceinline__ T Max()\n    {\n        UnsignedBits retval_bits = MAX_KEY;\n        T retval;\n        memcpy(&retval, &retval_bits, sizeof(T));\n        return retval;\n    }\n\n    static __host__ __device__ __forceinline__ T Lowest()\n    {\n        UnsignedBits retval_bits = LOWEST_KEY;\n        T retval;\n        memcpy(&retval, &retval_bits, sizeof(T));\n        return retval;\n    }\n};\n\n\n/**\n * Basic type traits (signed primitive specialization)\n */\ntemplate <typename _UnsignedBits, typename T>\nstruct BaseTraits<SIGNED_INTEGER, true, false, _UnsignedBits, T>\n{\n    typedef _UnsignedBits       UnsignedBits;\n\n    static const Category       CATEGORY    = SIGNED_INTEGER;\n    static const UnsignedBits   HIGH_BIT    = UnsignedBits(1) << ((sizeof(UnsignedBits) * 8) - 1);\n    static const UnsignedBits   LOWEST_KEY  = HIGH_BIT;\n    static const UnsignedBits   MAX_KEY     = UnsignedBits(-1) ^ HIGH_BIT;\n\n    enum\n    {\n        PRIMITIVE       = true,\n        NULL_TYPE       = false,\n    };\n\n    static __host__ __device__ __forceinline__ UnsignedBits TwiddleIn(UnsignedBits key)\n    {\n        return key ^ HIGH_BIT;\n    };\n\n    static __host__ __device__ __forceinline__ UnsignedBits TwiddleOut(UnsignedBits key)\n    {\n        return key ^ HIGH_BIT;\n    };\n\n    static __host__ __device__ __forceinline__ T Max()\n    {\n        UnsignedBits retval = MAX_KEY;\n        return reinterpret_cast<T&>(retval);\n    }\n\n    static __host__ __device__ __forceinline__ T Lowest()\n    {\n        UnsignedBits retval = LOWEST_KEY;\n        return reinterpret_cast<T&>(retval);\n    }\n};\n\ntemplate <typename _T>\nstruct FpLimits;\n\ntemplate <>\nstruct FpLimits<float>\n{\n    static __host__ __device__ __forceinline__ float Max() {\n        return FLT_MAX;\n    }\n\n    static __host__ __device__ __forceinline__ float Lowest() {\n        return FLT_MAX * float(-1);\n    }\n};\n\ntemplate <>\nstruct FpLimits<double>\n{\n    static __host__ __device__ __forceinline__ double Max() {\n        return DBL_MAX;\n    }\n\n    static __host__ __device__ __forceinline__ double Lowest() {\n        return DBL_MAX  * double(-1);\n    }\n};\n\n#if !_NVHPC_CUDA\ntemplate <>\nstruct FpLimits<__half>\n{\n    static __host__ __device__ __forceinline__ __half Max() {\n        unsigned short max_word = 0x7BFF;\n        return reinterpret_cast<__half&>(max_word);\n    }\n\n    static __host__ __device__ __forceinline__ __half Lowest() {\n        unsigned short lowest_word = 0xFBFF;\n        return reinterpret_cast<__half&>(lowest_word);\n    }\n};\n#endif\n\n#if !_NVHPC_CUDA && !defined(CUB_DISABLE_BF16_SUPPORT)\ntemplate <>\nstruct FpLimits<__nv_bfloat16>\n{\n    static __host__ __device__ __forceinline__ __nv_bfloat16 Max() {\n        unsigned short max_word = 0x7F7F;\n        return reinterpret_cast<__nv_bfloat16&>(max_word);\n    }\n\n    static __host__ __device__ __forceinline__ __nv_bfloat16 Lowest() {\n        unsigned short lowest_word = 0xFF7F;\n        return reinterpret_cast<__nv_bfloat16&>(lowest_word);\n    }\n};\n#endif\n\n/**\n * Basic type traits (fp primitive specialization)\n */\ntemplate <typename _UnsignedBits, typename T>\nstruct BaseTraits<FLOATING_POINT, true, false, _UnsignedBits, T>\n{\n    typedef _UnsignedBits       UnsignedBits;\n\n    static const Category       CATEGORY    = FLOATING_POINT;\n    static const UnsignedBits   HIGH_BIT    = UnsignedBits(1) << ((sizeof(UnsignedBits) * 8) - 1);\n    static const UnsignedBits   LOWEST_KEY  = UnsignedBits(-1);\n    static const UnsignedBits   MAX_KEY     = UnsignedBits(-1) ^ HIGH_BIT;\n\n    enum\n    {\n        PRIMITIVE       = true,\n        NULL_TYPE       = false,\n    };\n\n    static __host__ __device__ __forceinline__ UnsignedBits TwiddleIn(UnsignedBits key)\n    {\n        UnsignedBits mask = (key & HIGH_BIT) ? UnsignedBits(-1) : HIGH_BIT;\n        return key ^ mask;\n    };\n\n    static __host__ __device__ __forceinline__ UnsignedBits TwiddleOut(UnsignedBits key)\n    {\n        UnsignedBits mask = (key & HIGH_BIT) ? HIGH_BIT : UnsignedBits(-1);\n        return key ^ mask;\n    };\n\n    static __host__ __device__ __forceinline__ T Max() {\n        return FpLimits<T>::Max();\n    }\n\n    static __host__ __device__ __forceinline__ T Lowest() {\n        return FpLimits<T>::Lowest();\n    }\n};\n\n\n/**\n * \\brief Numeric type traits\n */\n// clang-format off\ntemplate <typename T> struct NumericTraits :            BaseTraits<NOT_A_NUMBER, false, false, T, T> {};\n\ntemplate <> struct NumericTraits<NullType> :            BaseTraits<NOT_A_NUMBER, false, true, NullType, NullType> {};\n\ntemplate <> struct NumericTraits<char> :                BaseTraits<(std::numeric_limits<char>::is_signed) ? SIGNED_INTEGER : UNSIGNED_INTEGER, true, false, unsigned char, char> {};\ntemplate <> struct NumericTraits<signed char> :         BaseTraits<SIGNED_INTEGER, true, false, unsigned char, signed char> {};\ntemplate <> struct NumericTraits<short> :               BaseTraits<SIGNED_INTEGER, true, false, unsigned short, short> {};\ntemplate <> struct NumericTraits<int> :                 BaseTraits<SIGNED_INTEGER, true, false, unsigned int, int> {};\ntemplate <> struct NumericTraits<long> :                BaseTraits<SIGNED_INTEGER, true, false, unsigned long, long> {};\ntemplate <> struct NumericTraits<long long> :           BaseTraits<SIGNED_INTEGER, true, false, unsigned long long, long long> {};\n\ntemplate <> struct NumericTraits<unsigned char> :       BaseTraits<UNSIGNED_INTEGER, true, false, unsigned char, unsigned char> {};\ntemplate <> struct NumericTraits<unsigned short> :      BaseTraits<UNSIGNED_INTEGER, true, false, unsigned short, unsigned short> {};\ntemplate <> struct NumericTraits<unsigned int> :        BaseTraits<UNSIGNED_INTEGER, true, false, unsigned int, unsigned int> {};\ntemplate <> struct NumericTraits<unsigned long> :       BaseTraits<UNSIGNED_INTEGER, true, false, unsigned long, unsigned long> {};\ntemplate <> struct NumericTraits<unsigned long long> :  BaseTraits<UNSIGNED_INTEGER, true, false, unsigned long long, unsigned long long> {};\n\n\n#if CUB_IS_INT128_ENABLED \ntemplate <>\nstruct NumericTraits<__uint128_t>\n{\n  using T = __uint128_t;\n  using UnsignedBits = __uint128_t;\n\n  static constexpr Category       CATEGORY    = UNSIGNED_INTEGER;\n  static constexpr UnsignedBits   LOWEST_KEY  = UnsignedBits(0);\n  static constexpr UnsignedBits   MAX_KEY     = UnsignedBits(-1);\n\n  static constexpr bool PRIMITIVE = false;\n  static constexpr bool NULL_TYPE = false;\n\n  static __host__ __device__ __forceinline__ UnsignedBits TwiddleIn(UnsignedBits key)\n  {\n    return key;\n  }\n\n  static __host__ __device__ __forceinline__ UnsignedBits TwiddleOut(UnsignedBits key)\n  {\n    return key;\n  }\n\n  static __host__ __device__ __forceinline__ T Max()\n  {\n    return MAX_KEY;\n  }\n\n  static __host__ __device__ __forceinline__ T Lowest()\n  {\n    return LOWEST_KEY;\n  }\n};\n\ntemplate <>\nstruct NumericTraits<__int128_t>\n{\n  using T = __int128_t;\n  using UnsignedBits = __uint128_t;\n\n  static constexpr Category       CATEGORY    = SIGNED_INTEGER;\n  static constexpr UnsignedBits   HIGH_BIT    = UnsignedBits(1) << ((sizeof(UnsignedBits) * 8) - 1);\n  static constexpr UnsignedBits   LOWEST_KEY  = HIGH_BIT;\n  static constexpr UnsignedBits   MAX_KEY     = UnsignedBits(-1) ^ HIGH_BIT;\n\n  static constexpr bool PRIMITIVE = false;\n  static constexpr bool NULL_TYPE = false;\n\n  static __host__ __device__ __forceinline__ UnsignedBits TwiddleIn(UnsignedBits key)\n  {\n    return key ^ HIGH_BIT;\n  };\n\n  static __host__ __device__ __forceinline__ UnsignedBits TwiddleOut(UnsignedBits key)\n  {\n    return key ^ HIGH_BIT;\n  };\n\n  static __host__ __device__ __forceinline__ T Max()\n  {\n    UnsignedBits retval = MAX_KEY;\n    return reinterpret_cast<T&>(retval);\n  }\n\n  static __host__ __device__ __forceinline__ T Lowest()\n  {\n    UnsignedBits retval = LOWEST_KEY;\n    return reinterpret_cast<T&>(retval);\n  }\n};\n#endif\n\ntemplate <> struct NumericTraits<float> :               BaseTraits<FLOATING_POINT, true, false, unsigned int, float> {};\ntemplate <> struct NumericTraits<double> :              BaseTraits<FLOATING_POINT, true, false, unsigned long long, double> {};\n#if !_NVHPC_CUDA\n    template <> struct NumericTraits<__half> :          BaseTraits<FLOATING_POINT, true, false, unsigned short, __half> {};\n#endif\n#if !_NVHPC_CUDA && !defined(CUB_DISABLE_BF16_SUPPORT)\n    template <> struct NumericTraits<__nv_bfloat16> :   BaseTraits<FLOATING_POINT, true, false, unsigned short, __nv_bfloat16> {};\n#endif\n\ntemplate <> struct NumericTraits<bool> :                BaseTraits<UNSIGNED_INTEGER, true, false, typename UnitWord<bool>::VolatileWord, bool> {};\n// clang-format on\n\n/**\n * \\brief Type traits\n */\ntemplate <typename T>\nstruct Traits : NumericTraits<typename std::remove_cv<T>::type> {};\n\n\n#endif // DOXYGEN_SHOULD_SKIP_THIS\n\n\n/** @} */       // end group UtilModule\n\nCUB_NAMESPACE_END\n\n#endif // _JITIFY_INCLUDE_GUARD_1375AABB974C3530\n", "utility": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===-------------------------- utility -----------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX_UTILITY\n#define _LIBCUDACXX_UTILITY\n\n/*\n    utility synopsis\n\n#include <initializer_list>\n\nnamespace std\n{\n\ntemplate <class T>\n    void\n    swap(T& a, T& b);\n\nnamespace rel_ops\n{\n    template<class T> bool operator!=(const T&, const T&);\n    template<class T> bool operator> (const T&, const T&);\n    template<class T> bool operator<=(const T&, const T&);\n    template<class T> bool operator>=(const T&, const T&);\n}\n\ntemplate<class T>\nvoid\nswap(T& a, T& b) noexcept(is_nothrow_move_constructible<T>::value &&\n                          is_nothrow_move_assignable<T>::value);\n\ntemplate <class T, size_t N>\nvoid\nswap(T (&a)[N], T (&b)[N]) noexcept(noexcept(swap(*a, *b)));\n\ntemplate <class T> T&& forward(typename remove_reference<T>::type& t) noexcept;  // constexpr in C++14\ntemplate <class T> T&& forward(typename remove_reference<T>::type&& t) noexcept; // constexpr in C++14\n\ntemplate <typename T>\n[[nodiscard]] constexpr\nauto forward_like(auto&& x) noexcept -> see below;                               // since C++23\n\ntemplate <class T> typename remove_reference<T>::type&& move(T&&) noexcept;      // constexpr in C++14\n\ntemplate <class T>\n    typename conditional\n    <\n        !is_nothrow_move_constructible<T>::value && is_copy_constructible<T>::value,\n        const T&,\n        T&&\n    >::type\n    move_if_noexcept(T& x) noexcept; // constexpr in C++14\n\ntemplate <class T> constexpr add_const_t<T>& as_const(T& t) noexcept;      // C++17\ntemplate <class T>                      void as_const(const T&&) = delete; // C++17\n\ntemplate <class T> typename add_rvalue_reference<T>::type declval() noexcept;\n\ntemplate<class T, class U> constexpr bool cmp_equal(T t, U u) noexcept;         // C++20\ntemplate<class T, class U> constexpr bool cmp_not_equal(T t, U u) noexcept;     // C++20\ntemplate<class T, class U> constexpr bool cmp_less(T t, U u) noexcept;          // C++20\ntemplate<class T, class U> constexpr bool cmp_greater(T t, U u) noexcept;       // C++20\ntemplate<class T, class U> constexpr bool cmp_less_equal(T t, U u) noexcept;    // C++20\ntemplate<class T, class U> constexpr bool cmp_greater_equal(T t, U u) noexcept; // C++20\ntemplate<class R, class T> constexpr bool in_range(T t) noexcept;               // C++20\n\ntemplate <class T1, class T2>\nstruct pair\n{\n    typedef T1 first_type;\n    typedef T2 second_type;\n\n    T1 first;\n    T2 second;\n\n    pair(const pair&) = default;\n    pair(pair&&) = default;\n    explicit(see-below) constexpr pair();\n    explicit(see-below) pair(const T1& x, const T2& y);                          // constexpr in C++14\n    template <class U = T1, class V = T2> explicit(see-below) pair(U&&, V&&);    // constexpr in C++14\n    template <class U, class V> constexpr explicit(see below) pair(pair<U, V>&); // since C++23\n    template <class U, class V> explicit(see-below) pair(const pair<U, V>& p);   // constexpr in C++14\n    template <class U, class V> explicit(see-below) pair(pair<U, V>&& p);        // constexpr in C++14\n    template <class U, class V>\n    constexpr explicit(see below) pair(const pair<U, V>&&);                      // since C++23\n    template <class... Args1, class... Args2>\n        pair(piecewise_construct_t, tuple<Args1...> first_args,\n             tuple<Args2...> second_args);                                       // constexpr in C++20\n\n    constexpr const pair& operator=(const pair& p) const;                        // since C++23\n    template <class U, class V> pair& operator=(const pair<U, V>& p);            // constexpr in C++20\n    template <class U, class V>\n    constexpr const pair& operator=(const pair<U, V>& p) const;                  // since C++23\n    pair& operator=(pair&& p) noexcept(is_nothrow_move_assignable<T1>::value &&\n                                       is_nothrow_move_assignable<T2>::value);   // constexpr in C++20\n    constexpr const pair& operator=(pair&& p) const;                             // since C++23\n    template <class U, class V> pair& operator=(pair<U, V>&& p);                 // constexpr in C++20\n    template <class U, class V>\n    constexpr const pair& operator=(pair<U, V>&& p) const;                       // since C++23\n\n    void swap(pair& p) noexcept(is_nothrow_swappable_v<T1> &&\n                                is_nothrow_swappable_v<T2>);                     // constexpr in C++20\n    constexpr void swap(const pair& p) const noexcept(see below);                // since C++23\n};\n\ntemplate<class T1, class T2, class U1, class U2, template<class> class TQual, template<class> class UQual>\nstruct basic_common_reference<pair<T1, T2>, pair<U1, U2>, TQual, UQual>;         // since C++23\n\ntemplate<class T1, class T2, class U1, class U2>\nstruct common_type<pair<T1, T2>, pair<U1, U2>>;                                  // since C++23\n\ntemplate<class T1, class T2> pair(T1, T2) -> pair<T1, T2>;\n\ntemplate <class T1, class T2> bool operator==(const pair<T1,T2>&, const pair<T1,T2>&); // constexpr in C++14\ntemplate <class T1, class T2> bool operator!=(const pair<T1,T2>&, const pair<T1,T2>&); // constexpr in C++14, removed in C++20\ntemplate <class T1, class T2> bool operator< (const pair<T1,T2>&, const pair<T1,T2>&); // constexpr in C++14, removed in C++20\ntemplate <class T1, class T2> bool operator> (const pair<T1,T2>&, const pair<T1,T2>&); // constexpr in C++14, removed in C++20\ntemplate <class T1, class T2> bool operator>=(const pair<T1,T2>&, const pair<T1,T2>&); // constexpr in C++14, removed in C++20\ntemplate <class T1, class T2> bool operator<=(const pair<T1,T2>&, const pair<T1,T2>&); // constexpr in C++14, removed in C++20\ntemplate <class T1, class T2>\n  constexpr common_comparison_type_t<synth-three-way-result<T1>,\n                                     synth-three-way-result<T2>>\n    operator<=>(const pair<T1,T2>&, const pair<T1,T2>&);                               // C++20\n\ntemplate <class T1, class T2> pair<V1, V2> make_pair(T1&&, T2&&);                // constexpr in C++14\ntemplate <class T1, class T2>\nvoid\nswap(pair<T1, T2>& x, pair<T1, T2>& y) noexcept(noexcept(x.swap(y)));            // constexpr in C++20\n\ntemplate<class T1, class T2>\nconstexpr void swap(const pair<T1, T2>& x, const pair<T1, T2>& y) noexcept(noexcept(x.swap(y)));    // since C++23\n\nstruct piecewise_construct_t { explicit piecewise_construct_t() = default; };\ninline constexpr piecewise_construct_t piecewise_construct = piecewise_construct_t();\n\ntemplate <class T> struct tuple_size;\ntemplate <size_t I, class T> struct tuple_element;\n\ntemplate <class T1, class T2> struct tuple_size<pair<T1, T2> >;\ntemplate <class T1, class T2> struct tuple_element<0, pair<T1, T2> >;\ntemplate <class T1, class T2> struct tuple_element<1, pair<T1, T2> >;\n\ntemplate<size_t I, class T1, class T2>\n    typename tuple_element<I, pair<T1, T2> >::type&\n    get(pair<T1, T2>&) noexcept; // constexpr in C++14\n\ntemplate<size_t I, class T1, class T2>\n    const typename tuple_element<I, pair<T1, T2> >::type&\n    get(const pair<T1, T2>&) noexcept; // constexpr in C++14\n\ntemplate<size_t I, class T1, class T2>\n    typename tuple_element<I, pair<T1, T2> >::type&&\n    get(pair<T1, T2>&&) noexcept; // constexpr in C++14\n\ntemplate<size_t I, class T1, class T2>\n    const typename tuple_element<I, pair<T1, T2> >::type&&\n    get(const pair<T1, T2>&&) noexcept; // constexpr in C++14\n\ntemplate<class T1, class T2>\n    constexpr T1& get(pair<T1, T2>&) noexcept; // C++14\n\ntemplate<class T1, class T2>\n    constexpr const T1& get(const pair<T1, T2>&) noexcept; // C++14\n\ntemplate<class T1, class T2>\n    constexpr T1&& get(pair<T1, T2>&&) noexcept; // C++14\n\ntemplate<class T1, class T2>\n    constexpr const T1&& get(const pair<T1, T2>&&) noexcept; // C++14\n\ntemplate<class T1, class T2>\n    constexpr T1& get(pair<T2, T1>&) noexcept; // C++14\n\ntemplate<class T1, class T2>\n    constexpr const T1& get(const pair<T2, T1>&) noexcept; // C++14\n\ntemplate<class T1, class T2>\n    constexpr T1&& get(pair<T2, T1>&&) noexcept; // C++14\n\ntemplate<class T1, class T2>\n    constexpr const T1&& get(const pair<T2, T1>&&) noexcept; // C++14\n\n// C++14\n\ntemplate<class T, T... I>\nstruct integer_sequence\n{\n    typedef T value_type;\n\n    static constexpr size_t size() noexcept;\n};\n\ntemplate<size_t... I>\n  using index_sequence = integer_sequence<size_t, I...>;\n\ntemplate<class T, T N>\n  using make_integer_sequence = integer_sequence<T, 0, 1, ..., N-1>;\ntemplate<size_t N>\n  using make_index_sequence = make_integer_sequence<size_t, N>;\n\ntemplate<class... T>\n  using index_sequence_for = make_index_sequence<sizeof...(T)>;\n\ntemplate<class T, class U=T>\n    constexpr T exchange(T& obj, U&& new_value)\n      noexcept(is_nothrow_move_constructible<T>::value && is_nothrow_assignable<T&, U>::value); // constexpr in C++17, noexcept in C++23\n\n// 20.2.7, in-place construction // C++17\nstruct in_place_t {\n  explicit in_place_t() = default;\n};\ninline constexpr in_place_t in_place{};\ntemplate <class T>\n  struct in_place_type_t {\n    explicit in_place_type_t() = default;\n  };\ntemplate <class T>\n  inline constexpr in_place_type_t<T> in_place_type{};\ntemplate <size_t I>\n  struct in_place_index_t {\n    explicit in_place_index_t() = default;\n  };\ntemplate <size_t I>\n  inline constexpr in_place_index_t<I> in_place_index{};\n\n// [utility.underlying], to_underlying\ntemplate <class T>\n    constexpr underlying_type_t<T> to_underlying( T value ) noexcept; // C++2b\n\n}  // std\n\n*/\n\n#ifndef __cuda_std__\n#include <__config>\n#endif //__cuda_std__\n\n#include \"__assert\" // all public C++ headers provide the assertion handler\n#include \"__debug\"\n#include \"__functional/binary_function.h\"\n#include \"__functional/hash.h\"\n#include \"__functional/reference_wrapper.h\"\n#include \"__functional/unary_function.h\"\n#include \"__functional/unwrap_ref.h\"\n#include \"__functional/weak_result_type.h\"\n#include \"__fwd/get.h\"\n#include \"__tuple_dir/sfinae_helpers.h\"\n#include \"__tuple_dir/structured_bindings.h\"\n#include \"__utility/as_const.h\"\n#include \"__utility/auto_cast.h\"\n#include \"__utility/cmp.h\"\n#include \"__utility/convert_to_integral.h\"\n#include \"__utility/declval.h\"\n#include \"__utility/exchange.h\"\n#include \"__utility/forward_like.h\"\n#include \"__utility/forward.h\"\n#include \"__utility/in_place.h\"\n#include \"__utility/integer_sequence.h\"\n#include \"__utility/move.h\"\n#include \"__utility/pair.h\"\n#include \"__utility/piecewise_construct.h\"\n#include \"__utility/priority_tag.h\"\n#include \"__utility/rel_ops.h\"\n#include \"__utility/swap.h\"\n#include \"__utility/to_underlying.h\"\n#include \"__utility/unreachable.h\"\n\n#include \"__memory/construct_at.h\"\n#include \"__memory/voidify.h\"\n\n#include \"limits\"\n#include \"type_traits\"\n#include \"version\"\n\n// standard-mandated includes\n#include \"concepts\"\n#include \"version\"\n\n// [utility.syn]\n#ifndef _LIBCUDACXX_HAS_NO_SPACESHIP_OPERATOR\n#include \"compare\"\n#endif\n#include \"initializer_list\"\n\n// [tuple.helper]\n#include \"__tuple_dir/tuple_element.h\"\n#include \"__tuple_dir/tuple_size.h\"\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n#endif  // _LIBCUDACXX_UTILITY\n", "version": "#define cudaDeviceSynchronize() cudaSuccess\n// -*- C++ -*-\n//===--------------------------- version ----------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _LIBCUDACXX_VERSIONH\n#define _LIBCUDACXX_VERSIONH\n\n\n/*\n  version synopsis\n\nMacro name                                              Value   Headers\n__cpp_lib_adaptor_iterator_pair_constructor             202106L <queue> <stack>\n__cpp_lib_addressof_constexpr                           201603L <memory>\n__cpp_lib_allocate_at_least                             202106L <memory>\n__cpp_lib_allocator_traits_is_always_equal              201411L <deque> <forward_list> <list>\n                                                                <map> <memory> <scoped_allocator>\n                                                                <set> <string> <unordered_map>\n                                                                <unordered_set> <vector>\n__cpp_lib_any                                           201606L <any>\n__cpp_lib_apply                                         201603L <tuple>\n__cpp_lib_array_constexpr                               201811L <array> <iterator>\n                                                        201603L // C++17\n__cpp_lib_as_const                                      201510L <utility>\n__cpp_lib_associative_heterogeneous_erasure             202110L <map> <set> <unordered_map>\n                                                                <unordered_set>\n__cpp_lib_assume_aligned                                201811L <memory>\n__cpp_lib_atomic_flag_test                              201907L <atomic>\n__cpp_lib_atomic_float                                  201711L <atomic>\n__cpp_lib_atomic_is_always_lock_free                    201603L <atomic>\n__cpp_lib_atomic_lock_free_type_aliases                 201907L <atomic>\n__cpp_lib_atomic_ref                                    201806L <atomic>\n__cpp_lib_atomic_shared_ptr                             201711L <atomic>\n__cpp_lib_atomic_value_initialization                   201911L <atomic> <memory>\n__cpp_lib_atomic_wait                                   201907L <atomic>\n__cpp_lib_barrier                                       201907L <barrier>\n__cpp_lib_bind_back                                     202202L <functional>\n__cpp_lib_bind_front                                    201907L <functional>\n__cpp_lib_bit_cast                                      201806L <bit>\n__cpp_lib_bitops                                        201907L <bit>\n__cpp_lib_bool_constant                                 201505L <type_traits>\n__cpp_lib_bounded_array_traits                          201902L <type_traits>\n__cpp_lib_boyer_moore_searcher                          201603L <functional>\n__cpp_lib_byte                                          201603L <cstddef>\n__cpp_lib_byteswap                                      202110L <bit>\n__cpp_lib_char8_t                                       201907L <atomic> <filesystem> <istream>\n                                                                <limits> <locale> <ostream>\n                                                                <string> <string_view>\n__cpp_lib_chrono                                        201611L <chrono>\n__cpp_lib_chrono_udls                                   201304L <chrono>\n__cpp_lib_clamp                                         201603L <algorithm>\n__cpp_lib_complex_udls                                  201309L <complex>\n__cpp_lib_concepts                                      202002L <concepts>\n__cpp_lib_constexpr_algorithms                          201806L <algorithm>\n__cpp_lib_constexpr_bitset                              202207L <bitset>\n__cpp_lib_constexpr_charconv                            202207L <charconv>\n__cpp_lib_constexpr_cmath                               202202L <cmath> <cstdlib>\n__cpp_lib_constexpr_complex                             201711L <complex>\n__cpp_lib_constexpr_dynamic_alloc                       201907L <memory>\n__cpp_lib_constexpr_functional                          201907L <functional>\n__cpp_lib_constexpr_iterator                            201811L <iterator>\n__cpp_lib_constexpr_memory                              202202L <memory>\n                                                        201811L // C++20\n__cpp_lib_constexpr_numeric                             201911L <numeric>\n__cpp_lib_constexpr_string                              201907L <string>\n__cpp_lib_constexpr_string_view                         201811L <string_view>\n__cpp_lib_constexpr_tuple                               201811L <tuple>\n__cpp_lib_constexpr_typeinfo                            202106L <typeinfo>\n__cpp_lib_constexpr_utility                             201811L <utility>\n__cpp_lib_constexpr_vector                              201907L <vector>\n__cpp_lib_coroutine                                     201902L <coroutine>\n__cpp_lib_destroying_delete                             201806L <new>\n__cpp_lib_enable_shared_from_this                       201603L <memory>\n__cpp_lib_endian                                        201907L <bit>\n__cpp_lib_erase_if                                      202002L <deque> <forward_list> <list>\n                                                                <map> <set> <string>\n                                                                <unordered_map> <unordered_set> <vector>\n__cpp_lib_exchange_function                             201304L <utility>\n__cpp_lib_execution                                     201902L <execution>\n                                                        201603L // C++17\n__cpp_lib_filesystem                                    201703L <filesystem>\n__cpp_lib_format                                        202106L <format>\n__cpp_lib_forward_like                                  202207L <utility>\n__cpp_lib_gcd_lcm                                       201606L <numeric>\n__cpp_lib_generic_associative_lookup                    201304L <map> <set>\n__cpp_lib_generic_unordered_lookup                      201811L <unordered_map> <unordered_set>\n__cpp_lib_hardware_interference_size                    201703L <new>\n__cpp_lib_has_unique_object_representations             201606L <type_traits>\n__cpp_lib_hypot                                         201603L <cmath>\n__cpp_lib_incomplete_container_elements                 201505L <forward_list> <list> <vector>\n__cpp_lib_int_pow2                                      202002L <bit>\n__cpp_lib_integer_comparison_functions                  202002L <utility>\n__cpp_lib_integer_sequence                              201304L <utility>\n__cpp_lib_integral_constant_callable                    201304L <type_traits>\n__cpp_lib_interpolate                                   201902L <cmath> <numeric>\n__cpp_lib_invoke                                        201411L <functional>\n__cpp_lib_invoke_r                                      202106L <functional>\n__cpp_lib_is_aggregate                                  201703L <type_traits>\n__cpp_lib_is_constant_evaluated                         201811L <type_traits>\n__cpp_lib_is_final                                      201402L <type_traits>\n__cpp_lib_is_invocable                                  201703L <type_traits>\n__cpp_lib_is_layout_compatible                          201907L <type_traits>\n__cpp_lib_is_nothrow_convertible                        201806L <type_traits>\n__cpp_lib_is_null_pointer                               201309L <type_traits>\n__cpp_lib_is_pointer_interconvertible                   201907L <type_traits>\n__cpp_lib_is_scoped_enum                                202011L <type_traits>\n__cpp_lib_is_swappable                                  201603L <type_traits>\n__cpp_lib_jthread                                       201911L <stop_token> <thread>\n__cpp_lib_latch                                         201907L <latch>\n__cpp_lib_launder                                       201606L <new>\n__cpp_lib_list_remove_return_type                       201806L <forward_list> <list>\n__cpp_lib_logical_traits                                201510L <type_traits>\n__cpp_lib_make_from_tuple                               201606L <tuple>\n__cpp_lib_make_reverse_iterator                         201402L <iterator>\n__cpp_lib_make_unique                                   201304L <memory>\n__cpp_lib_map_try_emplace                               201411L <map>\n__cpp_lib_math_constants                                201907L <numbers>\n__cpp_lib_math_special_functions                        201603L <cmath>\n__cpp_lib_memory_resource                               201603L <memory_resource>\n__cpp_lib_move_only_function                            202110L <functional>\n__cpp_lib_node_extract                                  201606L <map> <set> <unordered_map>\n                                                                <unordered_set>\n__cpp_lib_nonmember_container_access                    201411L <array> <deque> <forward_list>\n                                                                <iterator> <list> <map>\n                                                                <regex> <set> <string>\n                                                                <unordered_map> <unordered_set> <vector>\n__cpp_lib_not_fn                                        201603L <functional>\n__cpp_lib_null_iterators                                201304L <iterator>\n__cpp_lib_optional                                      202110L <optional>\n                                                        201606L // C++17\n__cpp_lib_out_ptr                                       202106L <memory>\n__cpp_lib_parallel_algorithm                            201603L <algorithm> <numeric>\n__cpp_lib_polymorphic_allocator                         201902L <memory_resource>\n__cpp_lib_quoted_string_io                              201304L <iomanip>\n__cpp_lib_ranges                                        201811L <algorithm> <functional> <iterator>\n                                                                <memory> <ranges>\n__cpp_lib_ranges_chunk                                  202202L <ranges>\n__cpp_lib_ranges_chunk_by                               202202L <ranges>\n__cpp_lib_ranges_iota                                   202202L <numeric>\n__cpp_lib_ranges_join_with                              202202L <ranges>\n__cpp_lib_ranges_slide                                  202202L <ranges>\n__cpp_lib_ranges_starts_ends_with                       202106L <algorithm>\n__cpp_lib_ranges_to_container                           202202L <deque> <forward_list> <list>\n                                                                <map> <priority_queue> <queue>\n                                                                <set> <stack> <string>\n                                                                <unordered_map> <unordered_set> <vector>\n__cpp_lib_ranges_zip                                    202110L <ranges> <tuple> <utility>\n__cpp_lib_raw_memory_algorithms                         201606L <memory>\n__cpp_lib_reference_from_temporary                      202202L <type_traits>\n__cpp_lib_remove_cvref                                  201711L <type_traits>\n__cpp_lib_result_of_sfinae                              201210L <functional> <type_traits>\n__cpp_lib_robust_nonmodifying_seq_ops                   201304L <algorithm>\n__cpp_lib_sample                                        201603L <algorithm>\n__cpp_lib_scoped_lock                                   201703L <mutex>\n__cpp_lib_semaphore                                     201907L <semaphore>\n__cpp_lib_shared_mutex                                  201505L <shared_mutex>\n__cpp_lib_shared_ptr_arrays                             201707L <memory>\n                                                        201611L // C++17\n__cpp_lib_shared_ptr_weak_type                          201606L <memory>\n__cpp_lib_shared_timed_mutex                            201402L <shared_mutex>\n__cpp_lib_shift                                         201806L <algorithm>\n__cpp_lib_smart_ptr_for_overwrite                       202002L <memory>\n__cpp_lib_source_location                               201907L <source_location>\n__cpp_lib_span                                          202002L <span>\n__cpp_lib_spanstream                                    202106L <spanstream>\n__cpp_lib_ssize                                         201902L <iterator>\n__cpp_lib_stacktrace                                    202011L <stacktrace>\n__cpp_lib_starts_ends_with                              201711L <string> <string_view>\n__cpp_lib_stdatomic_h                                   202011L <stdatomic.h>\n__cpp_lib_string_contains                               202011L <string> <string_view>\n__cpp_lib_string_resize_and_overwrite                   202110L <string>\n__cpp_lib_string_udls                                   201304L <string>\n__cpp_lib_string_view                                   201803L <string> <string_view>\n                                                        201606L // C++17\n__cpp_lib_syncbuf                                       201803L <syncstream>\n__cpp_lib_three_way_comparison                          201907L <compare>\n__cpp_lib_to_address                                    201711L <memory>\n__cpp_lib_to_array                                      201907L <array>\n__cpp_lib_to_chars                                      201611L <charconv>\n__cpp_lib_to_underlying                                 202102L <utility>\n__cpp_lib_transformation_trait_aliases                  201304L <type_traits>\n__cpp_lib_transparent_operators                         201510L <functional> <memory>\n                                                        201210L // C++14\n__cpp_lib_tuple_element_t                               201402L <tuple>\n__cpp_lib_tuples_by_type                                201304L <tuple> <utility>\n__cpp_lib_type_identity                                 201806L <type_traits>\n__cpp_lib_type_trait_variable_templates                 201510L <type_traits>\n__cpp_lib_uncaught_exceptions                           201411L <exception>\n__cpp_lib_unordered_map_try_emplace                     201411L <unordered_map>\n__cpp_lib_unreachable                                   202202L <utility>\n__cpp_lib_unwrap_ref                                    201811L <functional>\n__cpp_lib_variant                                       202102L <variant>\n__cpp_lib_void_t                                        201411L <type_traits>\n\n*/\n\n#ifndef __cuda_std__\n#include <__config>\n#endif // __cuda_std__\n\n// If we are replacing the stl we need to define our own feature test macros.\n// However, if we are in __cuda_std__ we need to define our own symbols to not\n// conflict with the host stl.\n// At the same time we want bring in all feature test macros from host\n#ifdef __cuda_std__\n#if __has_include(<version>) // <version> should be the smallest include possible\n#include <version>\n#elif !defined(_LIBCUDACXX_COMPILER_NVRTC)\n#include <ciso646> // otherwise go for the smallest possible header\n#endif\n#endif // __cuda_std__\n\n#if defined(_LIBCUDACXX_USE_PRAGMA_GCC_SYSTEM_HEADER)\n_Pragma(\"GCC system_header\")\n#endif\n\n// We unconditionally define `__cccl_lib_meow` so that there is only one place to set the value\n#if _LIBCUDACXX_STD_VER > 11\n# define __cccl_lib_chrono_udls                          201304L\n# define __cccl_lib_complex_udls                         201309L\n#ifdef _LIBCUDACXX_IS_CONSTANT_EVALUATED\n# define __cccl_lib_constexpr_complex                    201711L\n#endif\n# define __cccl_lib_concepts                             202002L\n# define __cccl_lib_exchange_function                    201304L\n# define __cccl_lib_expected                             202211L\n// # define __cccl_lib_generic_associative_lookup           201304L\n# define __cccl_lib_integer_sequence                     201304L\n# define __cccl_lib_integral_constant_callable           201304L\n# define __cccl_lib_is_final                             201402L\n# define __cccl_lib_is_null_pointer                      201309L\n# define __cccl_lib_make_reverse_iterator                201402L\n// # define __cccl_lib_make_unique                          201304L\n# define __cccl_lib_null_iterators                       201304L\n# define __cccl_lib_optional                             202110L\n// # define __cccl_lib_quoted_string_io                     201304L\n# define __cccl_lib_result_of_sfinae                     201210L\n# define __cccl_lib_robust_nonmodifying_seq_ops          201304L\n# if !defined(_LIBCUDACXX_HAS_NO_THREADS)\n// #   define __cccl_lib_shared_timed_mutex                 201402L\n# endif\n# define __cccl_lib_span                                 201902L\n// # define __cccl_lib_string_udls                          201304L\n# define __cccl_lib_transformation_trait_aliases         201304L\n# define __cccl_lib_transparent_operators                201210L\n# define __cccl_lib_tuple_element_t                      201402L\n# define __cccl_lib_tuples_by_type                       201304L\n#endif // _LIBCUDACXX_STD_VER > 11\n\n#if _LIBCUDACXX_STD_VER > 14\n# if defined(_LIBCUDACXX_ADDRESSOF)\n#   define __cccl_lib_addressof_constexpr                201603L\n# endif\n// # define __cccl_lib_allocator_traits_is_always_equal     201411L\n// # define __cccl_lib_any                                  201606L\n# define __cccl_lib_apply                                201603L\n# define __cccl_lib_array_constexpr                      201603L\n# define __cccl_lib_as_const                             201510L\n# if !defined(_LIBCUDACXX_HAS_NO_THREADS)\n#   define __cccl_lib_atomic_is_always_lock_free         201603L\n# endif\n# define __cccl_lib_bind_front                           201907L\n# define __cccl_lib_bool_constant                        201505L\n// # define __cccl_lib_boyer_moore_searcher                 201603L\n# define __cccl_lib_byte                                 201603L\n# define __cccl_lib_chrono                               201611L\n// # define __cccl_lib_clamp                                201603L\n// # define __cccl_lib_enable_shared_from_this              201603L\n// # define __cccl_lib_execution                            201603L\n// # define __cccl_lib_filesystem                           201703L\n# define __cccl_lib_gcd_lcm                              201606L\n# define __cccl_lib_hardware_interference_size           201703L\n# if defined(_LIBCUDACXX_HAS_UNIQUE_OBJECT_REPRESENTATIONS)\n#   define __cccl_lib_has_unique_object_representations  201606L\n# endif\n# define __cccl_lib_hypot                                201603L\n// # define __cccl_lib_incomplete_container_elements        201505L\n# define __cccl_lib_invoke                               201411L\n# if !defined(_LIBCUDACXX_HAS_NO_IS_AGGREGATE)\n#   define __cccl_lib_is_aggregate                       201703L\n# endif\n# define __cccl_lib_is_invocable                         201703L\n# define __cccl_lib_is_swappable                         201603L\n# define __cccl_lib_launder                              201606L\n# define __cccl_lib_logical_traits                       201510L\n# define __cccl_lib_make_from_tuple                      201606L\n// # define __cccl_lib_map_try_emplace                      201411L\n// # define __cccl_lib_math_special_functions               201603L\n// # define __cccl_lib_memory_resource                      201603L\n// # define __cccl_lib_node_extract                         201606L\n// # define __cccl_lib_nonmember_container_access           201411L\n# define __cccl_lib_not_fn                               201603L\n// # define __cccl_lib_parallel_algorithm                   201603L\n// # define __cccl_lib_raw_memory_algorithms                201606L\n// # define __cccl_lib_sample                               201603L\n// # define __cccl_lib_scoped_lock                          201703L\n# if !defined(_LIBCUDACXX_HAS_NO_THREADS)\n// #   define __cccl_lib_shared_mutex                       201505L\n# endif\n// # define __cccl_lib_shared_ptr_arrays                    201611L\n// # define __cccl_lib_shared_ptr_weak_type                 201606L\n// # define __cccl_lib_string_view                          201606L\n// # define __cccl_lib_to_chars                             201611L\n# define __cccl_lib_type_trait_variable_templates        201510L\n# define __cccl_lib_uncaught_exceptions                  201411L\n# define __cccl_lib_unordered_map_try_emplace            201411L\n# define __cccl_lib_variant                              201606L\n# define __cccl_lib_void_t                               201411L\n#endif // _LIBCUDACXX_STD_VER > 14\n\n#if _LIBCUDACXX_STD_VER > 17\n# undef  __cccl_lib_array_constexpr\n# define __cccl_lib_array_constexpr                      201811L\n// # define __cccl_lib_assume_aligned                       201811L\n# define __cccl_lib_atomic_flag_test                     201907L\n# define __cccl_lib_atomic_float                         201711L\n# define __cccl_lib_atomic_lock_free_type_aliases        201907L\n# if !defined(_LIBCUDACXX_HAS_NO_THREADS)\n#   define __cccl_lib_atomic_ref                         201806L\n#endif\n// # define __cccl_lib_atomic_shared_ptr                    201711L\n# define __cccl_lib_atomic_value_initialization          201911L\n# if !defined(_LIBCUDACXX_AVAILABILITY_DISABLE_FTM___cpp_lib_atomic_wait)\n#   define __cccl_lib_atomic_wait                        201907L\n# endif\n# if !defined(_LIBCUDACXX_HAS_NO_THREADS) && !defined(_LIBCUDACXX_AVAILABILITY_DISABLE_FTM___cpp_lib_barrier)\n#   define __cccl_lib_barrier                            201907L\n# endif\n# define __cccl_lib_bit_cast                             201806L\n# define __cccl_lib_bitops                               201907L\n# define __cccl_lib_bounded_array_traits                 201902L\n# if !defined(_LIBCUDACXX_NO_HAS_CHAR8_T)\n#   define __cccl_lib_char8_t                            201811L\n# endif\n// # define __cccl_lib_constexpr_algorithms                 201806L\n// # define __cccl_lib_constexpr_dynamic_alloc              201907L\n# define __cccl_lib_constexpr_functional                 201907L\n// # define __cccl_lib_constexpr_iterator                   201811L\n// # define __cccl_lib_constexpr_memory                     201811L\n// # define __cccl_lib_constexpr_misc                       201811L\n// # define __cccl_lib_constexpr_numeric                    201911L\n// # define __cccl_lib_constexpr_string                     201907L\n// # define __cccl_lib_constexpr_string_view                201811L\n// # define __cccl_lib_constexpr_swap_algorithms            201806L\n// # define __cccl_lib_constexpr_tuple                      201811L\n// # define __cccl_lib_constexpr_utility                    201811L\n// # define __cccl_lib_constexpr_vector                     201907L\n// # define __cccl_lib_coroutine                            201902L\n# if _LIBCUDACXX_STD_VER > 17 && defined(__cpp_impl_destroying_delete) && __cpp_impl_destroying_delete >= 201806L\n#   define __cccl_lib_destroying_delete                  201806L\n# endif\n// # define __cccl_lib_endian                               201907L\n// # define __cccl_lib_erase_if                             201811L\n// # undef  __cccl_lib_execution\n// # define __cccl_lib_execution                            201902L\n# if !defined(_LIBCUDACXX_AVAILABILITY_DISABLE_FTM___cpp_lib_format) && !defined(_LIBCUDACXX_HAS_NO_INCOMPLETE_FORMAT)\n// #   define __cccl_lib_format                             202106L\n# endif\n// # define __cccl_lib_generic_unordered_lookup             201811L\n// # define __cccl_lib_int_pow2                             202002L\n// # define __cccl_lib_integer_comparison_functions         202002L\n// # define __cccl_lib_interpolate                          201902L\n# if defined(_LIBCUDACXX_IS_CONSTANT_EVALUATED)\n#   define __cccl_lib_is_constant_evaluated              201811L\n# endif\n// # define __cccl_lib_is_layout_compatible                 201907L\n# define __cccl_lib_is_nothrow_convertible               201806L\n// # define __cccl_lib_is_pointer_interconvertible          201907L\n# if !defined(_LIBCUDACXX_HAS_NO_THREADS)\n// #   define __cccl_lib_jthread                            201911L\n# endif\n# if !defined(_LIBCUDACXX_HAS_NO_THREADS) && !defined(_LIBCUDACXX_AVAILABILITY_DISABLE_FTM___cpp_lib_latch)\n// #   define __cccl_lib_latch                              201907L\n# endif\n// # define __cccl_lib_list_remove_return_type              201806L\n// # define __cccl_lib_math_constants                       201907L\n// # define __cccl_lib_polymorphic_allocator                201902L\n// # define __cccl_lib_ranges                               201811L\n// # define __cccl_lib_remove_cvref                         201711L\n# if !defined(_LIBCUDACXX_HAS_NO_THREADS) && !defined(_LIBCUDACXX_AVAILABILITY_DISABLE_FTM___cpp_lib_semaphore)\n// #   define __cccl_lib_semaphore                          201907L\n# endif\n// # undef  __cccl_lib_shared_ptr_arrays\n// # define __cccl_lib_shared_ptr_arrays                    201707L\n// # define __cccl_lib_shift                                201806L\n// # define __cccl_lib_smart_ptr_for_overwrite              202002L\n// # define __cccl_lib_source_location                      201907L\n// # define __cccl_lib_ssize                                201902L\n// # define __cccl_lib_starts_ends_with                     201711L\n// # undef  __cccl_lib_string_view\n// # define __cccl_lib_string_view                          201803L\n// # define __cccl_lib_syncbuf                              201803L\n// # define __cccl_lib_three_way_comparison                 201907L\n// # define __cccl_lib_to_address                           201711L\n// # define __cccl_lib_to_array                             201907L\n// # define __cccl_lib_type_identity                        201806L\n# define __cccl_lib_unwrap_ref                           201811L\n#endif // _LIBCUDACXX_STD_VER > 17\n\n#if _LIBCUDACXX_STD_VER > 20\n// # define __cccl_lib_adaptor_iterator_pair_constructor    202106L\n// # define __cccl_lib_allocate_at_least                    202106L\n// # define __cccl_lib_associative_heterogeneous_erasure    202110L\n// # define __cccl_lib_bind_back                            202202L\n// # define __cccl_lib_byteswap                             202110L\n// # define __cccl_lib_constexpr_bitset                     202207L\n// # define __cccl_lib_constexpr_charconv                   202207L\n// # define __cccl_lib_constexpr_cmath                      202202L\n// # undef  __cccl_lib_constexpr_memory\n// # define __cccl_lib_constexpr_memory                     202202L\n// # define __cccl_lib_constexpr_typeinfo                   202106L\n# define __cccl_lib_forward_like                         202207L\n// # define __cccl_lib_invoke_r                             202106L\n# define __cccl_lib_is_scoped_enum                       202011L\n// # define __cccl_lib_move_only_function                   202110L\n// # define __cccl_lib_out_ptr                              202106L\n// # define __cccl_lib_ranges_chunk                         202202L\n// # define __cccl_lib_ranges_chunk_by                      202202L\n// # define __cccl_lib_ranges_iota                          202202L\n// # define __cccl_lib_ranges_join_with                     202202L\n// # define __cccl_lib_ranges_slide                         202202L\n// # define __cccl_lib_ranges_starts_ends_with              202106L\n// # define __cccl_lib_ranges_to_container                  202202L\n// # define __cccl_lib_ranges_zip                           202110L\n// # define __cccl_lib_reference_from_temporary             202202L\n// # define __cccl_lib_spanstream                           202106L\n// # define __cccl_lib_stacktrace                           202011L\n// # define __cccl_lib_stdatomic_h                          202011L\n// # define __cccl_lib_string_contains                      202011L\n// # define __cccl_lib_string_resize_and_overwrite          202110L\n# define __cccl_lib_to_underlying                        202102L\n# define __cccl_lib_unreachable                          202202L\n\n#endif // _LIBCUDACXX_STD_VER > 20\n\n#ifndef __cuda_std__\n#if _LIBCUDACXX_STD_VER > 11\n# define __cpp_lib_chrono_udls                          __cccl_lib_chrono_udls\n# define __cpp_lib_complex_udls                         __cccl_lib_complex_udls\n#ifdef __cccl_lib_constexpr_complex\n# define __cpp_lib_constexpr_complex                    __cccl_lib_constexpr_complex\n#endif\n# define __cpp_lib_concepts                             __cccl_lib_concepts\n# define __cpp_lib_exchange_function                    __cccl_lib_exchange_function\n# define __cpp_lib_expected                             __cccl_lib_expected\n# define __cpp_lib_generic_associative_lookup           201304L\n# define __cpp_lib_integer_sequence                     __cccl_lib_integer_sequence\n# define __cpp_lib_integral_constant_callable           __cccl_lib_integral_constant_callable\n# define __cpp_lib_is_final                             __cccl_lib_is_final\n# define __cpp_lib_is_null_pointer                      __cccl_lib_is_null_pointer\n# define __cpp_lib_make_reverse_iterator                __cccl_lib_make_reverse_iterator\n# define __cpp_lib_make_unique                          201304L\n# define __cpp_lib_null_iterators                       __cccl_lib_null_iterators\n# define __cpp_lib_quoted_string_io                     201304L\n# define __cpp_lib_result_of_sfinae                     __cccl_lib_result_of_sfinae\n# define __cpp_lib_robust_nonmodifying_seq_ops          __cccl_lib_robust_nonmodifying_seq_ops\n# ifdef __cccl_lib_shared_timed_mutex\n#   define __cpp_lib_shared_timed_mutex                 201402L\n# endif\n# define __cpp_lib_span                                 __cccl_lib_span\n# define __cpp_lib_string_udls                          201304L\n# define __cpp_lib_transformation_trait_aliases         __cccl_lib_transformation_trait_aliases\n# define __cpp_lib_transparent_operators                __cccl_lib_transparent_operators\n# define __cpp_lib_tuple_element_t                      __cccl_lib_tuple_element_t\n# define __cpp_lib_tuples_by_type                       __cccl_lib_tuples_by_type\n#endif // _LIBCUDACXX_STD_VER > 11\n\n#if _LIBCUDACXX_STD_VER > 14\n# ifdef __cccl_lib_addressof_constexpr\n#   define __cpp_lib_addressof_constexpr                __cccl_lib_addressof_constexpr\n# endif\n# define __cpp_lib_allocator_traits_is_always_equal     201411L\n# define __cpp_lib_any                                  201606L\n# define __cpp_lib_apply                                __cccl_lib_apply\n# define __cpp_lib_array_constexpr                      __cccl_lib_array_constexpr\n# define __cpp_lib_as_const                             __cccl_lib_as_const\n# ifdef __cccl_lib_atomic_is_always_lock_free\n#   define __cpp_lib_atomic_is_always_lock_free         __cccl_lib_atomic_is_always_lock_free\n# endif\n# define __cpp_lib_bind_front                           __cccl_lib_bind_front\n# define __cpp_lib_bool_constant                        __cccl_lib_bool_constant\n// # define __cpp_lib_boyer_moore_searcher                 __cccl_lib_boyer_moore_searcher\n# define __cpp_lib_byte                                 __cccl_lib_byte\n# define __cpp_lib_chrono                               __cccl_lib_chrono\n# define __cpp_lib_clamp                                201603L\n# define __cpp_lib_enable_shared_from_this              201603L\n// # define __cpp_lib_execution                            __cccl_lib_execution\n# define __cpp_lib_filesystem                           201703L\n# define __cpp_lib_gcd_lcm                              __cccl_lib_gcd_lcm\n# define __cpp_lib_hardware_interference_size           __cccl_lib_hardware_interference_size\n# ifdef __cccl_lib_has_unique_object_representations\n#   define __cpp_lib_has_unique_object_representations  __cccl_lib_has_unique_object_representations\n# endif\n# define __cpp_lib_hypot                                __cccl_lib_hypot\n# define __cpp_lib_incomplete_container_elements        201505L\n# define __cpp_lib_invoke                               __cccl_lib_invoke\n# ifdef __cccl_lib_is_aggregate\n#   define __cpp_lib_is_aggregate                       __cccl_lib_is_aggregate\n# endif\n# define __cpp_lib_is_invocable                         __cccl_lib_is_invocable\n# define __cpp_lib_is_swappable                         __cccl_lib_is_swappable\n# define __cpp_lib_launder                              __cccl_lib_launder\n# define __cpp_lib_logical_traits                       __cccl_lib_logical_traits\n# define __cpp_lib_make_from_tuple                      __cccl_lib_make_from_tuple\n# define __cpp_lib_map_try_emplace                      201411L\n// # define __cpp_lib_math_special_functions               __cccl_lib_math_special_functions\n// # define __cpp_lib_memory_resource                      __cccl_lib_memory_resource\n# define __cpp_lib_node_extract                         201606L\n# define __cpp_lib_nonmember_container_access           201411L\n# define __cpp_lib_not_fn                               __cccl_lib_not_fn\n# define __cpp_lib_optional                             __cccl_lib_optional\n// # define __cpp_lib_parallel_algorithm                   __cccl_lib_parallel_algorithm\n# define __cpp_lib_raw_memory_algorithms                201606L\n# define __cpp_lib_sample                               201603L\n# define __cpp_lib_scoped_lock                          201703L\n# ifdef __cccl_lib_shared_mutex\n#   define __cpp_lib_shared_mutex                       201505L\n# endif\n// # define __cpp_lib_shared_ptr_arrays                    __cccl_lib_shared_ptr_arrays\n# define __cpp_lib_shared_ptr_weak_type                 201606L\n# define __cpp_lib_string_view                          201606L\n// # define __cpp_lib_to_chars                             __cccl_lib_to_chars\n# define __cpp_lib_type_trait_variable_templates        __cccl_lib_type_trait_variable_templates\n# define __cpp_lib_uncaught_exceptions                  __cccl_lib_uncaught_exceptions\n# define __cpp_lib_unordered_map_try_emplace            __cccl_lib_unordered_map_try_emplace\n# define __cpp_lib_variant                              __cccl_lib_variant\n# define __cpp_lib_void_t                               __cccl_lib_void_t\n#endif // _LIBCUDACXX_STD_VER > 14\n\n#if _LIBCUDACXX_STD_VER > 17\n// # undef  __cpp_lib_array_constexpr\n// # define __cpp_lib_array_constexpr                      __cccl_lib_array_constexpr\n// # define __cpp_lib_assume_aligned                       __cccl_lib_assume_aligned\n// # define __cpp_lib_atomic_flag_test                     __cccl_lib_atomic_flag_test\n// # define __cpp_lib_atomic_float                         __cccl_lib_atomic_float\n// # define __cpp_lib_atomic_lock_free_type_aliases        __cccl_lib_atomic_lock_free_type_aliases\n# ifdef __cccl_lib_atomic_ref\n#   define __cpp_lib_atomic_ref                         __cccl_lib_atomic_ref\n#endif\n// # define __cpp_lib_atomic_shared_ptr                    __cccl_lib_atomic_shared_ptr\n// # define __cpp_lib_atomic_value_initialization          __cccl_lib_atomic_value_initialization\n# ifdef __cccl_lib_atomic_wait\n// #   define __cpp_lib_atomic_wait                        __cccl_lib_atomic_wait\n# endif\n# if !defined(_LIBCUDACXX_HAS_NO_THREADS) && !defined(_LIBCUDACXX_AVAILABILITY_DISABLE_FTM___cpp_lib_barrier)\n#   define __cpp_lib_barrier                            __cccl_lib_barrier\n# endif\n// # define __cpp_lib_bit_cast                             __cccl_lib_bit_cast\n// # define __cpp_lib_bitops                               __cccl_lib_bitops\n// # define __cpp_lib_bounded_array_traits                 __cccl_lib_bounded_array_traits\n# ifdef __cccl_lib_char8_t\n#   define __cpp_lib_char8_t                            __cccl_lib_char8_t\n# endif\n// # define __cpp_lib_constexpr_algorithms                 __cccl_lib_constexpr_algorithms\n// # define __cpp_lib_constexpr_dynamic_alloc              __cccl_lib_constexpr_dynamic_alloc\n// # define __cpp_lib_constexpr_functional                 __cccl_lib_constexpr_functional\n// # define __cpp_lib_constexpr_iterator                   __cccl_lib_constexpr_iterator\n// # define __cpp_lib_constexpr_memory                     __cccl_lib_constexpr_memory\n// # define __cpp_lib_constexpr_misc                       __cccl_lib_constexpr_misc\n// # define __cpp_lib_constexpr_numeric                    __cccl_lib_constexpr_numeric\n// # define __cpp_lib_constexpr_string                     __cccl_lib_constexpr_string\n// # define __cpp_lib_constexpr_string_view                __cccl_lib_constexpr_string_view\n// # define __cpp_lib_constexpr_swap_algorithms            __cccl_lib_constexpr_swap_algorithms\n// # define __cpp_lib_constexpr_tuple                      __cccl_lib_constexpr_tuple\n// # define __cpp_lib_constexpr_utility                    __cccl_lib_constexpr_utility\n// # define __cpp_lib_constexpr_vector                     __cccl_lib_constexpr_vector\n// # define __cpp_lib_coroutine                            __cccl_lib_coroutine\n# ifdef __cccl_lib_destroying_delete\n#   define __cpp_lib_destroying_delete                  __cccl_lib_destroying_delete\n# endif\n// # define __cpp_lib_endian                               __cccl_lib_endian\n# define __cpp_lib_erase_if                             201811L\n// # undef  __cpp_lib_execution\n// # define __cpp_lib_execution                            __cccl_lib_execution\n# ifdef __cccl_lib_format\n// #   define __cpp_lib_format                             __cccl_lib_format\n# endif\n// # define __cpp_lib_generic_unordered_lookup             __cccl_lib_generic_unordered_lookup\n// # define __cpp_lib_int_pow2                             202002L\n# define __cpp_lib_integer_comparison_functions         202002L\n# define __cpp_lib_interpolate                          201902L\n# ifdef __cccl_lib_is_constant_evaluated\n#   define __cpp_lib_is_constant_evaluated              __cccl_lib_is_constant_evaluated\n# endif\n// # define __cpp_lib_is_layout_compatible                 __cccl_lib_is_layout_compatible\n# define __cpp_lib_is_nothrow_convertible               __cccl_lib_is_nothrow_convertible\n// # define __cpp_lib_is_pointer_interconvertible          __cccl_lib_is_pointer_interconvertible\n# ifdef __cccl_lib_jthread\n#   define __cpp_lib_jthread                            __cccl_lib_jthread\n# endif\n# ifdef __cccl_lib_latch\n#   define __cpp_lib_latch                              __cccl_lib_latch\n# endif\n// # define __cpp_lib_list_remove_return_type              __cccl_lib_list_remove_return_type\n// # define __cpp_lib_math_constants                       __cccl_lib_math_constants\n// # define __cpp_lib_polymorphic_allocator                __cccl_lib_polymorphic_allocator\n// # define __cpp_lib_ranges                               __cccl_lib_ranges\n// # define __cpp_lib_remove_cvref                         __cccl_lib_remove_cvref\n# ifdef __cccl_lib_semaphore\n#   define __cpp_lib_semaphore                          __cccl_lib_semaphore\n# endif\n// # undef  __cpp_lib_shared_ptr_arrays\n// # define __cpp_lib_shared_ptr_arrays                    __cccl_lib_shared_ptr_arrays\n// # define __cpp_lib_shift                                __cccl_lib_shift\n// # define __cpp_lib_smart_ptr_for_overwrite              __cccl_lib_smart_ptr_for_overwrite\n// # define __cpp_lib_source_location                      __cccl_lib_source_location\n// # define __cpp_lib_ssize                                __cccl_lib_ssize\n// # define __cpp_lib_starts_ends_with                     __cccl_lib_starts_ends_with\n// # undef  __cpp_lib_string_view\n// # define __cpp_lib_string_view                          __cccl_lib_string_view\n// # define __cpp_lib_syncbuf                              __cccl_lib_syncbuf\n// # define __cpp_lib_three_way_comparison                 __cccl_lib_three_way_comparison\n// # define __cpp_lib_to_address                           __cccl_lib_to_address\n// # define __cpp_lib_to_array                             __cccl_lib_to_array\n// # define __cpp_lib_type_identity                        __cccl_lib_type_identity\n# define __cpp_lib_unwrap_ref                           __cccl_lib_unwrap_ref\n#endif // _LIBCUDACXX_STD_VER > 17\n\n#if _LIBCUDACXX_STD_VER > 20\n// # define __cpp_lib_adaptor_iterator_pair_constructor    __cccl_lib_adaptor_iterator_pair_constructor\n// # define __cpp_lib_allocate_at_least                    __cccl_lib_allocate_at_least\n// # define __cpp_lib_associative_heterogeneous_erasure    __cccl_lib_associative_heterogeneous_erasure\n// # define __cpp_lib_bind_back                            __cccl_lib_bind_back\n// # define __cpp_lib_byteswap                             __cccl_lib_byteswap\n// # define __cpp_lib_constexpr_bitset                     __cccl_lib_constexpr_bitset\n// # define __cpp_lib_constexpr_charconv                   __cccl_lib_constexpr_charconv\n// # define __cpp_lib_constexpr_cmath                      __cccl_lib_constexpr_cmath\n// # undef  __cpp_lib_constexpr_memory\n// # define __cpp_lib_constexpr_memory                     __cccl_lib_constexpr_memory\n// # define __cpp_lib_constexpr_typeinfo                   __cccl_lib_constexpr_typeinfo\n# define __cpp_lib_forward_like                         __cccl_lib_forward_like\n// # define __cpp_lib_invoke_r                             __cccl_lib_invoke_r\n# define __cpp_lib_is_scoped_enum                       __cccl_lib_is_scoped_enum\n// # define __cpp_lib_move_only_function                   __cccl_lib_move_only_function\n// # define __cpp_lib_out_ptr                              __cccl_lib_out_ptr\n// # define __cpp_lib_ranges_chunk                         __cccl_lib_ranges_chunk\n// # define __cpp_lib_ranges_chunk_by                      __cccl_lib_ranges_chunk_by\n// # define __cpp_lib_ranges_iota                          __cccl_lib_ranges_iota\n// # define __cpp_lib_ranges_join_with                     __cccl_lib_ranges_join_with\n// # define __cpp_lib_ranges_slide                         __cccl_lib_ranges_slide\n// # define __cpp_lib_ranges_starts_ends_with              __cccl_lib_ranges_starts_ends_with\n// # define __cpp_lib_ranges_to_container                  __cccl_lib_ranges_to_container\n// # define __cpp_lib_ranges_zip                           __cccl_lib_ranges_zip\n// # define __cpp_lib_reference_from_temporary             __cccl_lib_reference_from_temporary\n// # define __cpp_lib_spanstream                           __cccl_lib_spanstream\n// # define __cpp_lib_stacktrace                           __cccl_lib_stacktrace\n// # define __cpp_lib_stdatomic_h                          __cccl_lib_stdatomic_h\n// # define __cpp_lib_string_contains                      __cccl_lib_string_contains\n// # define __cpp_lib_string_resize_and_overwrite          __cccl_lib_string_resize_and_overwrite\n# define __cpp_lib_to_underlying                        __cccl_lib_to_underlying\n# define __cpp_lib_unreachable                          __cccl_lib_unreachable\n\n#endif // _LIBCUDACXX_STD_VER > 20\n\n#endif // !__cuda_std__\n\n#endif // _LIBCUDACXX_VERSIONH\n", "version.cuh": "#ifndef _JITIFY_INCLUDE_GUARD_2872E5C2BB5B739D\n#define _JITIFY_INCLUDE_GUARD_2872E5C2BB5B739D\n#define cudaDeviceSynchronize() cudaSuccess\n/******************************************************************************\n * Copyright (c) 2011-2022, NVIDIA CORPORATION.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the NVIDIA CORPORATION nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n ******************************************************************************/\n\n/*! \\file version.cuh\n *  \\brief Compile-time macros encoding CUB release version\n *\n *         <cub/version.h> is the only CUB header that is guaranteed to\n *         change with every CUB release.\n *\n */\n\n/*! \\def CUB_VERSION\n *  \\brief The preprocessor macro \\p CUB_VERSION encodes the version\n *         number of the CUB library.\n *\n *         <tt>CUB_VERSION % 100</tt> is the sub-minor version.\n *         <tt>CUB_VERSION / 100 % 1000</tt> is the minor version.\n *         <tt>CUB_VERSION / 100000</tt> is the major version.\n */\n#define CUB_VERSION 200200\n\n/*! \\def CUB_MAJOR_VERSION\n *  \\brief The preprocessor macro \\p CUB_MAJOR_VERSION encodes the\n *         major version number of the CUB library.\n */\n#define CUB_MAJOR_VERSION     (CUB_VERSION / 100000)\n\n/*! \\def CUB_MINOR_VERSION\n *  \\brief The preprocessor macro \\p CUB_MINOR_VERSION encodes the\n *         minor version number of the CUB library.\n */\n#define CUB_MINOR_VERSION     (CUB_VERSION / 100 % 1000)\n\n/*! \\def CUB_SUBMINOR_VERSION\n *  \\brief The preprocessor macro \\p CUB_SUBMINOR_VERSION encodes the\n *         sub-minor version number of the CUB library.\n */\n#define CUB_SUBMINOR_VERSION  (CUB_VERSION % 100)\n\n/*! \\def CUB_PATCH_NUMBER\n *  \\brief The preprocessor macro \\p CUB_PATCH_NUMBER encodes the\n *         patch number of the CUB library.\n */\n#define CUB_PATCH_NUMBER 0\n\n#endif // _JITIFY_INCLUDE_GUARD_2872E5C2BB5B739D\n"}